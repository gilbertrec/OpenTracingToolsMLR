Addtionally, the microservices APM maintains a low overhead.
With theAppDynamics Alerting Extension for JIRA extension, you can leverage your existing ticketing infrastructure to notify the operations team and resolve performance degradation issues.Resolve issues in a more efficient, automated process.Figure 5: AppDynamics health rule violation as shown in Atlassian’s JIRA2.
Chat room members can see a brief description of the health rule violation or event and get more detail on AppDynamics by following the URL provided in the alert message.Team members can now collaborate in real-time, wherever they are to resolve issues quickly.Figure 6: AppDynamics health rule violation as shown in Atlassian’s HipChat.3.
AppDynamics Monitoring Extension for BambooThe AppDynamics Monitoring Extension for Bamboo extracts various build statistics like the number of tests failed from Bamboo and shows them in the AppDynamics Metric Browser.
Together you get an always on solution that detects performance and availability issues, alerts you to them and shows you, at the code level, where the problem is.
This means you can fix performance problems before end users even see them.Below is a screenshot of response times from the AlertSite UXM interface, followed by a screenshot of the trace into the AppDynamics interface.The trace is seamless, just click through.
Because it automatically maps these new pieces of your architecture, they can also monitor isolated microservices and track their KPIs.
If a user can't access the system, it can lead to a whole mess of problems for everyone involved.
Microservices iQ can analyze what threads are blocking each other and causing application slowdowns, helping teams synchronize their data between microservices.
Such modern development is iterative, but it’s still slow and hard to maintain for rapidly changing apps.AppDynamics has solved these problems.
"Cloud-based applications present a unique challenge - nodes appear and disappear on the fly," said Bansal.
• Automatic remediation of common performance problems, dynamic scaling and cloud    bursting.
• Automatic remediation for common performance problems like JVM/machine restarts.
The software includes a dashboard with a lot of information that breaks down your demographics into geographic regions and shows which regions are experiencing the highest or lowest load and response times or errors, and what time of day they tend to occur.
The developer called me, and I think it was a Friday afternoon, everything breaks on a Friday afternoon.” Unfortunately, the wrong data was being displayed in the user interface (UI).
“Let’s check out this AppDynamics, maybe it can reveal some information we’re missing.” Once they’d pulled up the flowmap, it was soon apparent that the application was unfortunately talking to the wrong database.
Full end-to-end visibility is critical for developing and running modern, highly distributed applications in production.
This release provides stand alone, enterprise-class tooling for MongoDB and highlights just how creaky and tired the monitoring supplied by the legacy database vendors has become.
Check out the video below to see how customers are using AppDynamics and MongoDB as critical components in the new generation IT shop.
Users have no patience for applications with a sub-optimal experience.
Once you find out that your end-users are not experiencing optimal performance, you should able to rapidly identify the exact offending line of code in Python application impacting the end-user and address them.
After you learn that end-users experience is degrading, you need to understand that the application component causing the problem and then you may need to drill down into the underlying infrastructure in cloud (AWS, Azure, etc.)
New AppDynamics Software Hunts Memory Leaks, Finds Root Cause, and it's All in Production Java memory issues are common and often difficult to diagnose.
They rely on heap dumps instead of runtime data, and the heap dump approach is not suitable for large heap sizes that are commonly found today.
Some profilers have non-heap dump approaches, but they only capture shallow object sizes.
With today's release of AppDynamics 3.0, they're showing companies the value of a new approach - memory leak detection and root cause diagnostics in the production environment.“Memory leaks create havoc for countless organizations with mission-critical java applications,” said AppDynamics CEO Jyoti Bansal.
“Best case scenario, a memory leak causes your system to slow down, dragging application performance well below established SLAs.
Worst case scenario, your servers crash completely and you don’t know why.
Bansal lists some of the other distinguishing features of AppDynamics:Low-cost algorithm for object deep size calculationAutomatic Java collection instrumentationDynamic access/allocation code path analysisLive object instance trackingAutomatic memory leak detection with best-fit linear regression analysisAppDynamics 3.0 also builds on its feature set for highly distributed cloud environments.
So application Performance Management has been around for a while, though it seems like many developers are not comfortable with it yet.
What was once considered a luxury is becoming commonplace: Rapid new deployments in production mean more chances to introduce errors to your systems architecture, slow it down, and maybe even crash it.
Features Both New Relic and AppDynamics can be broken down into 6 different products, all reporting to a main dashboard interface.
Must have metrics include transaction response time, error rate, throughput (Requests per Minute) on NewRelic and load (calls/min) on AppDynamics.
Anyhow, enough with this dev tool psychology, but it’s worth noting that a similar map is also available on New Relic: New Relic’s application map One of the thorny issues here is alerting and reporting, with so many metrics and moving parts, it’s hard to identify which matters most.
Is it a low error rate?
Here’s an example for the way this score is calculated: Calculating Apdex, now sum this over all requests for a given time and you’ll get the score AppDynamics on the other hand, doesn’t believe in Apdex (as they explained in an article called ”Apdex is Fatally Flawed”).
For example, the definition of a slow transaction might vary under low and high loads on the system.
AppDynamics on the left, New Relic on the right with some sample data In this category, AppDynamics offers a few more features than New Relic, mostly around memory: heap size & utilization, garbage collection stats divided by gens and memory leak detection.
AppDynamics Server Monitoring - Memory featuresBottom line: AppDynamics provides deeper insights into garbage collection and memory leak detection beyond the standard metrics.
How to Solve the errors you find To go beyond the reporting and alerting of errors by AppDynamics and New Relic, many of our users add Takipi to their toolbox.
Whenever a new exception is thrown or a log error occurs - Takipi captures it and shows you the variable state which caused it, across methods and machines.
Takipi will overlay this over the actual code which executed at the moment of error – so you can analyze the exception as if you were there when it happened.
The Threat Behind Digital TransformationThe age of digital disruption is upon us, as the last decade alone has proven in terms of technology and disruption with the progression of organizations like Uber, Airbnb, and Netflix.
WWe are also on the brink of new disruptions in industry, including banking or payments, insurance, healthcare, construction, packaging, and many more.
The cost of experimentation continues to decrease, with lower cost computing models that allow for the rental of resources and software.
MyWipro typically processes 23.6 million transactions a day, so rock-solid performance is critical.
With existing application monitoring tools, we were having difficulty handling higher numbers of business transactions, huge volumes of data, and complex applications.
We broke it down: in one year, the number of end-user facing applications increased 15 percent.
Check out the full schedule and list of speakers, and don’t forget to register!
It doesn't need to create a new object each time as all instances would be the same.
Times are in micro-seconds:TestTypical latency99.99% latencyJava Serialization, non-capturing 33.9 µs215 µsJava Serialization, capturing 36.3 µs704 µsJava Serialization, with an enum7.9 µs134 µsChronicle Wire (Text), non-capturing20.4 µs147 µsChronicle Wire (Text), capturing22.5 µs148 µsChronicle Wire (Text), with an enum1.2 µs5.9 µsChronicle Wire (Binary),   non-capturing11.7 µs103 µsChronicle Wire (Binary), capturing12.7 µs135 µsChronicle Wire (Binary), with an enum1.0 µs1.2 µsWhat Does it Mean to Use an enum?While using a lambda is simple, it is not as efficient, so you need an alternative should it appear that using a lambda is causing a performance problem.enum Functions implements SerializableFunction<String, String> {    APPEND_STAR {        @Override        public String apply(String s) {            return s + '*';        }    }}To see what using an enum makes a difference you can compare how much data needs to be sent to the server.
Developers want to rush ahead and compile some groundbreaking code under extremely tight schedules, while operations teams try to slow everyone down to identify systemic risks from accidents or malicious actors.
It became more closely connected with electronic systems after World War II, when the IEEE created the Reliability Society.
That standard led to the creation of a class of operations experts who knew enough code to recover the site and put the last stable release back into production as fast as possible.Treynor explained the impetus for creating this new category at Google with his typical deadpan humor: One of the things you normally see in operations roles as opposed to engineering roles is that there’s a chasm not only with respect to duty, but also of background and of vocabulary, and eventually, of respect.
However, Treynor also pointed out how standard dev vs. ops friction can be costly to businesses in other ways.
He said:100% is the wrong reliability target for basically everything.
But, in general, for any software service or system you can think of, 100% is not the right reliability target because no user can tell the difference between a system being 100% available and, let’s say, 99.999% available.
Because typically there are so many other things that sit in between the user and the software service that you’re running that the marginal difference is lost in the noise of everything else that can go wrong.This response shifts the focus from specific uptime metrics, which may not act as accurate proxies for user expectations, to a reliability index based on market realities.
Treynor explained:If 100% is the wrong reliability target for a system, what, then, is the right reliability target for the system?
The term DevOps is likely to disappear as the two distinct divisions merge in the new world, where UX is everything and updates may be pushed out weekly.
Thread Contention Analysis: Given the independent nature of components in microservice architectures, it is more likely that a particular microservice is invoked as part of multiple business transactions and can become a performance bottleneck for those transactions if it blocks their execution.
The new thread contention analyzer helps identify methods, within the scope of service endpoints, where threads are blocked by identifying block time, blocking object and the blocking line of code.
Manually instrumenting these large number of microservices and setting static threshold for altering can be a very difficult task if not impossible.
Why IT People Hate Their JobsEarlier this year, Cisco company AppDynamics fostered a global study polling CIOs and senior to mid Information Technology (IT) professionals over the topic of digital transformation.
Their results noted the following conclusions:78% believe their corporation is not ready for digital transformation56% of technologists report a surplus of outdated technology in their IT department86% of IT professionals think their organization lags behind other IT teams in skills, qualities, and knowledge45% see themselves lagging 5+ years behind current technologies48% are currently in the mindset that their skills are overlooked, with plans to seek a new employer within the next two yearsThe full report can be found here and provides insight into why some IT professionals hate their jobs.
ConclusionAs global IT continues to embrace foundational digital transformation concepts, corporations without a plan or strategy will find themselves chasing the industry and will be at a disadvantage to any competitors.
We got you covered!This week in the world of technology, a rocket explosion destroys a critical satellite, the FBI investigates the hacks on election databases,VMware launches a cloud software bundle, and Amazon’s IoT button arrives debuts in the UK.Rocket Explosion Leaves Facebook’s Internet Initiative Grounded – The Wall Street Journal, September 1Falcon 9, the SpaceX rocket that exploded during a prelaunch test, took with it a critical satellite that would have brought internet connectivity to a few hundred thousand people in the rural area of Sub-Saharan Africa.
Mark Zuckerberg has since expressed his disappointment in the launch failure by the aerospace company founded by Elon Musk.
However, Facebook has shown so much commitment to this initiative of global connectivity that it’s hard to imagine that a destroyed satellite will halt the project.
In both incidents, the voter registration systems had to be taken offline for a number of days after the FBI first discovered the attacks.
In Arizona, officials reported that no data has been compromised.
It isn’t until some months or even years later until the data sets are dumped on the web to be exposed to a wide range of malicious use.
Tweet at us or leave a comment!
The incident was automatically detected and alerted upon by AppDynamics when the Busy Threads JMX metric shot up to 182.
AppDynamics sends notifications detailing busy thread counts5:34 p.m. – Details from AppDynamics show that call volume is down, response time is up, errors are up and network I/O is down.
Initial suspicion is that the load balancer may be throttling traffic due to poor performance.
5:38 p.m. – Company procedure is followed by disabling the server from the load balancer so that it will not receive any more traffic.
There is no need to recycle the application server.
Screenshot showing problematic JDBC call as the culprit.
Without having AppDynamics in place to provide fault domain isolation this type of problem usually ends up in a long conference call where all support personnel for this application must participate until service has been restored.
There is no need to waste significant company resources any more.
Stop the “all hands on deck” madness and see how AppDynamics can help your company today.
The new battleground in Analytics Part 1: Transforming APM into Application IntelligenceThis post was originally written by Kyle Duffy appeared first on Application Performance Monitoring Blog from AppDynamics.
I’d be in a much more technical space, solving technology problems more than business issues.
As an APM tool, AppDynamics can help people quickly find bad code, inefficient code, database problems and a host of other things that negatively impact the experience for end users of applications.
Edmunds.com:  With AppDynamics they went from having 10 people working on a single problem for several days to fixing things in a few hours.
30 minutes later the problem was resolved.
Those 2,000 servers were no longer needed.
By being plugged directly into an application’s code you can get real-time analytics without the challenges and costs associated with traditional analytics.
We have covered a number of different ways to attack OutOfMemoryErrors so far - profilers, JDK bundled tools and heap dump analyzers.
Our today’s mercenaries will be Application Performance Management tools, or APMs.APM are positioned as the Holy Grails on a quest for a solution of your production environment’s performance problems.
I was always suspicious about Jacks of all trades.
But let us cast all the doubts aside and just try them out.
New Relic has no memory leak tracking capabilities yet, so there we lost a second contestant.
This left us with AppDynamics alone.
Which is all nice, but not exactly what we were looking for.As our goal was to let AppDynamics help us finding a root cause of memory leak we have suffered for so long we switch to “Automatic Leak Detection” tab and activate it (it is switched off by default).
Then we start “On Demand Capture Session” and let the application run under the load of our stress tests.
Results are... a bit disappointing.
No luck here today.ConclusionIt was very difficult for me to write this post.
First of all I had only one tool to test, and to get even this one, was not an easy task.
Second, I wasted several hours before I saw a first bit of information in AppDynamics dashboard (do not ask, I will not write about it).
And third, I have no results to show my readers.
If you have fire in the house, it is too late to go fetching them.
I cannot recommend it for problem solving.P.S.
We have covered a number of different ways to attack OutOfMemoryErrors so far - profilers, JDK bundled tools and heap dump analyzers.
I was always suspicious about Jacks of all trades.
But let us cast all the doubts aside and just try them out.
New Relic has no memory leak tracking capabilities yet, so there we lost a second contestant.
This left us with AppDynamics alone.
Which is all nice, but not exactly what we were looking for.As our goal was to let AppDynamics help us find the root cause of the memory leak we have suffered from so long, we switch to “Automatic Leak Detection” tab and activate it (it is switched off by default).
Then we start an “On Demand Capture Session” and let the application run under the load of our stress tests.
The results are... a bit disappointing.
No luck here today.ConclusionIt was very difficult for me to write this post.
First of all I had only one tool to test, and getting even this one was not an easy task.
Secondly, I wasted several hours before I saw the first bit of information in AppDynamics dashboard (do not ask, I will not write about it).
And thirdly, I have no results to show to my readers.
If you have fire in the house, it is too late to go fetching them.
I cannot recommend it for problem solving though.P.S.
Why Gartner Identified Application Owners As a Key Use Case for APM SuitesEven from a business perspective, application performance management (APM) has become a critical tool in turning business, user, and performance data into relevant insights.
In the past, application owners were provided infrastructure metrics and health data from IT operations teams, but realizing that this information was not enough to make business decisions has enabled APM tools to be bought and driven by the LOB buyer, who has limited technical capabilities.
Storage teams have to rack, stack, allocate, and configure new storage quickly to meet demand and don’t have the time to do a detailed analysis on the anticipated workload of every application that will connect to and use the storage.
And therein lies the problem.
Databases that once played nicely together on the same spindles can become the worst of enemies and sink the performance of multiple applications at the same time.
no agent required) to your NetApp controllers and collects the performance and configuration information that you need to identify the root cause of performance issues.
The ResultAfter an incredibly difficult 2 minutes of configuration work we are ready for the payoff.
Imagine being able to detect an end user problem, drill down through the code execution, identify the slow SQL query, and isolate the storage volume that is causing the poor performance.
End-to-end transaction tracing becomes not just ideal, but essential.
Setting static response time thresholds too low and your team will be inundated with alerts.
If there’s a performance issue, the corresponding node will intuitively flag the problem and illustrate the affected nodes.Along with your application flowmap, we know it’s important to monitor and communicate your applications’ health to a wider internal audience.
The challenges with this approach are several-fold:Tools have minimal integration or common context, which makes it near impossible to manage the application or its business transactions.Tools are designed for subject-matter experts, so it’s hard to provide value to the ops team as a whole.Tools have high total cost of ownership, since every tool has to be independently procured, installed and managed, and staff have to be trained in their use.How rampant is this problem?
Potential respondents were filtered to include only those actively involved in enterprise application development/management/delivery at the executive, middle manager, or “hands on” line staff levels.The respondent mix was approximately:1/3 Europe and 2/3 North America0% small, 55% mid-sized, and 25% large companies45% executives (director or above), 25% middle managers, 30% line staffThe results were nothing short of astounding!Over 65% of enterprises have more than 10 monitoring tools43% have over 25 monitoring tools.Read the full report here.Just about every user of monitoring tools complains about the challenges of having too many tools without any situational awareness.
As shown in the chart below, the key reasons (besides price) for poor APM adoption were, indeed, the complexity of the tools and poor integration between tools.Reasons Why APM Tools Are Not More Widely UsedSource: Gartner Survey Analysis: End-User Experience Monitoring Is the Critical Dimension for Enterprise APM Consumers (June 2015)These challenges are effectively addressed by our recently released Unified Monitoring solution — it solves the problems of siloed monitoring and glue-ware integration.
As Jason Briggs, monitoring engineering manager at Paychex, puts it, “AppDynamics replaced five tools we once used to try to troubleshoot application problems.
DevOps, although becoming a commonly used job title, is not a role or person and there is no playbook or rule set to follow.
These metrics should span several areas which may have been considered disjointed in the past.
If the metrics do not allow linkage between a release and business performance, attribution gaps remain.
And unfortunately, many enterprises today analyze metrics that have a lack of linkage or relationship between them.
Additionally, AppDynamics provides metrics to drive visibility inside the application without creating an additional burden for developers.
Or if there is an “execute” action that accepts a “command” URI argument, then break the transaction based on the “command” segment.
Do you have any business transactions that you really do not care about?
Or is there a Node.js CLI cron job that runs every night, takes a long time, but because it is offline and does not impact the end user, you do not care?
If so then exclude these transactions so that they do not add noise to your analysis.2.
If you were to increase this interval to 50 milliseconds, you will lose granularity.
If you are finely tuning your application then you may want 10-millisecond granularity, but if you have no intention of tuning methods that execute in under 50 milliseconds, then why do you need that level of granularity?
If your application response times have low volatility then you might want to decrease your thresholds to alert you to problems sooner.
Obviously AppDynamics would know nothing about that socket connection and how it was being used, so the answer to our problem was to identify the method call that makes the connection to the AS/400 and identify it as a custom back-end resource.
Capturing Contextual InformationWhen performance problems occur, they are sometimes limited to a specific browser or mobile device, or they may only occur based on input associated with a request.
If the problem is not systemic (across all of your servers), then how do you identify the subset of requests that are causing the problem?The answer is that you need to capture context-specific information in your snapshots so that you can look for commonalities.
The process can be summarized as follow:     AppDynamics observes that a business transaction is running slow       It triggers the capture of a session of snapshots       On each snapshot, it captures the contextual information that you requested and associates it with the snapshot  The result is that when you find a snapshot illustrating the problem, you can review this contextual information to see if it provides you with more diagnostic information.The only warning is that this comes at a small price: AppDynamics uses code instrumentation to capture the values of methods parameters.
This article reviewed a few core tips and tricks that anyone implementing an APM strategy should consider.
Gartner Identified Application Development as a Key Use Case for APM SuitesWith the the rise in SaaS-based products and mobile applications, application development teams are becoming a critical stakeholder in application performance.
That visualization prioritizes the end-to-end business transactions performance, not just the health of the application and infrastructure nodes.While you’re able to track performance issues through benchmarking in production environments, it’s also critical to be able to capture potential issues in pre-production usage as well.
The same problems show up in high-speed trading applications, e-commerce sites, mobile apps and online games – so we thought we’d put together some of the most common performance problems in a blog series to show you how to find, fix and prevent them.
In this blog post we’ll take a look at a pretty common problem that can be tricky to detect in a large application: the Select N + 1 problem.
The N + 1 problem is a performance anti-pattern in which an application makes N + 1 database calls (where N is the number of objects fetched).
Like most anti-patterns, this isn’t necessarily a problem in itself, but under certain circumstances (where N is large, for example) it will cause performance to degrade by making hundreds or even thousands of database calls for a single business transaction.
In plain English: You’re spamming your database with really small, fast queries instead of using one or two more complex ones.
So you do this:SELECT id FROM Parentand then execute a query for each record:SELECT * FROM Child WHERE parent_id = ?This isn’t necessarily a bad way of doing it, especially if there aren’t many parents or children.
The problem only becomes worse with an increase in traffic.
How to Find It with AppDynamicsLike I said, this isn’t always a problem, but when it is it can be hard to find.
“It lets us bridge the gap between anecdotes from users and actual, actionable information.” – Cornell UniversityHow to Fix ItThis problem happens most often when you’re using a persistence engine or an object/relational mapper (ORM) like Hibernate and you’re using lazy loading.
This command pumps the number 0 into that file as fast as it can basically crushing the disk.
(Use Ctrl-C to kill this command before you fill up your disk.
I used a test instance of AppDynamics for Databases to remotely monitor each database instance (yep, no agent install required).
To initiate monitoring I opened up my AppDynamics for Databases console, navigated to the agent manager, clicked the “add agent” button, and filled in the fields as shown below (I selected MySQL as the database type):My remote agent didn’t connect the first time I tired this because I forgot to configure iptables to let my connection through even though I had set up my AWS firewall rules properly (facepalm).
With this increasing software complexity, there is also a correlated demand from users for highly-responsive, real-time digital services.
IT needs to detect issues proactively and keep MTTR low.
Oftentimes you get to a customer site and it ends up being a missing index of the database which has been slowing everything down.
In other cases, a simple code change can introduce a severe performance problem and finding the source can be a daunting challenge when the application is distributed and don't know the first place to start.
Troubleshooting poor performance means knowing exactly which node or tier is failing.
Distributed applications do not have a single point of failure.
Everything here and every single node on every tier suspect - from the operating system, to middleware, to third-party products, hardware, network configuration, and your own software logic.
As the complexities of applications grows, trying to find the slow pieces becomes increasingly more difficult.
Application Performance Monitoring (APM) step up to solve the problem by looking at the inner workings of your cloud based applications.
Oftentimes there are customers that are being affected, so speedy resolution is crucial.
Anticipating problemsA better approach is to actually anticipate problems before they occur.
This means finding and fixing performance problems immediately before they become critical.
If performance objectives are not met, it is important for network administrators, developers, and business stakeholders to get some numbers and facts early on and head off any future problems.
This is also a core building block for publish subscribe patterns.
However, if this application starts to perform badly, troubleshooting can be a challenge.
Identifying the performance bottleneck can be tricky.
[HttpPost]        // Attribute to help prevent cross-site scripting attacks and        // cross-site request forgery.
ConclusionMulti-tiered applications are notoriously difficult to troubleshoot, especially when hosted in cloud infrastructures.
When there is an outage or when performance slows, it is difficult to identify the root cause or location of the issue.
Even more difficult is to anticipate problems before they dramatically affect end users.
In short, AppDynamics provides holistic, multi-dimensional view at the business transaction level as well as the lower level infrastructure component performance.
Or if there is an “execute” action that accepts a “command” URI argument, then break the transaction based on the “command” segment.
Do you have any business transactions that you really do not care about?
Or is there a PHP CLI cron job that runs every night, takes a long time, but because it is offline and does not impact the end user, you do not care?
You can increase this interval to 50 milliseconds, but you will lose granularity.
If you are finely tuning your application then you may want 10-millisecond granularity, but if you have no intention of tuning methods that execute in under 50 milliseconds, then why do you need that level of granularity?
If your application response times have low volatility then you might want to decrease your thresholds to alert you to problems sooner.
Obviously AppDynamics would know nothing about that socket connection and how it was being used, so the answer to our problem was to identify the method call that makes the connection to the AS/400 and identify it as a custom back-end resource.
When you do this, AppDynamics treats that method call as a tier and counts the number of calls and captures the average response time of that method execution.You might be able to use the out of the box functionality, but if you have special requirements then AppDynamics provides a mechanism that allows you to manually define your application tiers by using the PHP API functions to further tailor your application.Capturing Contextual InformationWhen performance problems occur, they are sometimes limited to a specific browser or mobile device, or they may only occur based on input associated with a request.
If the problem is not systemic (across all of your servers), then how do you identify the subset of requests that are causing the problem?The answer is that you need to capture context-specific information in your snapshots so that you can look for commonalities.
The process can be summarized as follow:     AppDynamics observes that a business transaction is running slow       It triggers the capture of a session of snapshots       On each snapshot, it captures the contextual information that you requested and associates it with the snapshot  The result is that when you find a snapshot illustrating the problem, you can review this contextual information to see if it provides you with more diagnostic information.The only warning is that this comes at a small price: AppDynamics uses code instrumentation to capture the values of methods parameters.
This article reviewed a few core tips and tricks that anyone implementing an APM strategy should consider.
We can no longer rely on dashboards that update once or twice per day or even once or twice per hour.
For critical software applications, reducing MTBA means knowing a problem exists within one or two minutes.
If application performance causes a decrease in business performance, we can fire notifications via email, SMS, Slack, or any other third-party service.
An infrastructure failure costs $100,000 per hour, according to the survey respondents.
A critical application failure costs a staggering $500,000 to $1 million per hour.
A little more than a third of those outages last from one to 12 hours; 17 percent of infrastructure failures and 13 percent of application failures last more than a full day.
And the same old tools aren’t likely to work in the faster-paced collaborative DevOps environment; those who try to fit old tools to DevOps have a failure rate of 80 percent.
Rather than looking for a needle in the haystack or struggling to find answers when the application code changed and you no longer have the right logs to store the data to get you granular insights, why not use a solution that simply differentiates and leverages the power of application performance management solution and logs together to provide comprehensive IT operational intelligence.Without further adieu, here are the top 3 reasons to power your operational insights using AppDynamics Log Analytics solution:1.
This approach identifies customer-impacting issues quickly and provides deep visibility into application health making AppDynamics Application Performance Management (APM) solution of choice for enterprises running an application in production with minimum overhead.However at times trouble goes beyond application code.
Industry’s First Auto-correlation Between APM and LogsBut why stop there?
Businesses are under unprecedented pressure to deliver new software and technology to users faster than ever, and with zero margin for error.
A recent Forrester survey found 64 percent of businesses are dissatisfied with the time it takes to release new features to customer-facing business services or applications.
Harness’ mission is to make the practice of continuous delivery accessible to every business, empowering engineering teams to move fast and ship code without the fear of failed deployments.
By applying unsupervised machine learning to the process — a new technology called Continuous Verification — the platform understands an application’s baseline environment and can initiate automatic rollbacks when irregular activity is detected, avoiding application downtime or widespread failures.
To date, Harness has reduced deployment time from many weeks to a few hours and has reduced errors by nearly 99 percent.
“Software engineering teams need a platform that’s intuitive and powered by modern AI to meet the demand for incredibly fast, high-quality releases.
Harness uses machine learning to detect the quality of deployments and automatically roll back failed ones, saving time and reducing the need for custom scripting and manual oversight.
They may even delete the app entirely, the company will realize that the poor app experience is hurting the company brand and costing it customers and good will, and there will be frantic calls that somebody has to DO SOMETHING about it to make it right.
At this point it’s likely that IT will have to get involved because now the app is become a critical part of the company’s business strategy and is tied into the enterprise IT infrastructure.
In the traditional APM world, IT can add monitoring of the application performance via the application infrastructure without having to modify the app itself because they have that direct access to the systems where the application is being hosted and run.The process for adding monitoring of the performance of mobile application as it is being used by your customers isdifferent due to the level of indirection involved and the lack of direct access to the devices where the application is running.Since the mobile application is being used directly by your customers on their personal or corporate mobile phones, the mechanism of monitoring the performance of the app “in production” is called Mobile Real-User Monitoring or Mobile RUM.
In step 3 of the data flow, the AppDynamics mobile application agent will send information to the Mobile RUM Cloud (3A in the diagram) or the Mobile RUM Server (3B in the diagram) including the object ID, the NSURL (in the case of iOS or the Android equivalent), any crash API data from a previous crash, and any custom data that you may have chosen to collect for your application.The Mobile RUM Cloud or Mobile RUM Server collect this data from all of the mobile application clients that your customers are using, does some processing/aggregation of the data and then passes it on to the next step.
There is no permanent data storage in this step.The second option is to choose to use either the AppDynamics SaaS Controller or the AppDynamics on-premise Controller.
In step 4 of the data flow, the processed/aggregated data is sent from either the Mobile RUM Cloud or Mobile RUM Server to either the SaaS Controller or the on-premise Controller.The Controller is where all of your application performance data is correlated, baselined, stored, and accessed for monitoring, alerting, analysis, and action by all of the people in your organization that are involved in the running, maintenance, operation, and business of your application.Your employees access the Controller via the AppDynamics web-based portal where they can have role-specific views of your application performance data and can work to collaborate to resolve issues faster (troubleshooting, problem identification and isolation) via the War Room or monitor the business via custom performance operations and business/executive dashboards.
Visualizing and Tracking Your MicroservicesThere is no question that microservices architectures are the current rage for software design.
Inconsistent storage also causes issues should a disaster arise and recovery be necessary.
Technical debt is the buildup of old and possibly short-term decisions, which cause systems rigidity.
I reached out to Adrian Cockroft on this specific topic and got the following back from him:“Replication across data centers is handled using Cassandra or Riak for each data store; it’s an orthogonal problem.
Today’s monitoring approaches consist of the following broken strategies:http://zohararad.github.io/presentations/micro-services-monitoring/In this plan, status codes are logged and examined for an individual microservice.
There is no way to determine the health of the application being delivered to a user, which is a major issue.
This approach helps determine the health of each component, but once again, this flawed approach is similar to the way server monitoring works today, which is completely broken.
If there is a service failure that cascades to other service failures, determining root cause is virtually impossible due to the asynchronous nature of microservices.
So what about those who don’t want to pay for software?
You’ll likely pay with people and time, or actual money.
It seems like everyone has an opinion about what is wrong yet there are still very few facts available to the public.
You can see really bad performance on both the client and server side on average.
Now let’s take a look at some individual page requests to see where their problems were.
This is the type of frustrating web application behavior that drives end users crazy.
When we look at the list of our end user requests we see that there are AJAX errors associated with the myProfile web page (right below the line highlighted in blue).
Those AJAX requests are to blame since they are actually responsible for calling other service components that provide the profile details (SOA issues rearing their ugly head).
Let’s take a look at one of the failed AJAX requests to see what happened.
It means that the HTTP server was able to accept the request but can’t actually do anything with it because there are server side problems.
Ugh… more server side problems.
There are 2 key problems from the server side that need resolving immediately.
1) Functional Integration ErrorsWhen 100s of services developed by different teams come together, they exchange data through APIs using XML (or similar) formats (note the AJAX failures shown above).
Ensuring that all data exchange is accurate and errors are not occurring requires proper monitoring and extensive integration testing.
Healthcare.gov was obviously launched without proper monitoring and testing and that’s a major reason why many user interactions are failing, throwing errors and insurance companies are getting incomplete data forms.
To be clear, this isn’t an unusual problem.
To accelerate this process of finding and fixing these problems at least 5X faster, a product like AppDynamics is required.
We identify the source of errors within and across services much faster in test or production environments so that Developers can fix them right away.
2) Scalability bottlenecksThe second big problem is that there are performance bottlenecks in the software that need to be identified and tuned quickly.
Bottlenecks will occur in some services as they get overwhelmed by requests or start to have resource conflicts.
To find and remediate these code issues quickly you must have a monitoring tool that automatically finds the problematic code and shows it to you in an intuitive manner.
Finding and fixing bottlenecks and errors in custom applications is why AppDynamics exists.
Our customers typically resolve their application problems in minutes instead of taking days or weeks trying to use log files and infrastructure monitoring tools.
From a commercial Java standpoint, software vendors in 2008 may have released only a few variations of their application — for instance, specific WAR (web archives) — to account for differences in application servers.
Fast forward to today: commercial vendors must expose entirely how the sausage is made and provide a litany of formats to deploy and integrate with CI/CD and hybrid cloud infrastructures.
ChallengesIn a cloud-based infrastructure, the key concerns facing enterprises include complex cloud management, overall cost of infrastructure, and the lack of a single pane of glass for metrics.
These leads to pain points in Day-1 and Day-2 operations, a lack of choice for developers, slow provisioning times, and overly complicated management stacks.
If a dashboard shows all problems, all it does is tell you there’s a problem with the application.
We then should go find the problem.
He explains why graphs are a pet peeve, as they often have no point of reference and are crying out to show the baselining capabilities of AppDynamics.
Andy also dismisses dashboards that just look pretty, but are functionally useless.
Top 5 Mobile APM Myths: Myths 3-5 if you’re just tuning in, please check out my   previous post where i dispel myths 1 & 2  , bad app ratings are uncontrollable and it’s impossible to understand your backend services.
however, these can all be avoided and under your control with the right mapm solution to give you the proper insights to give your users a seamless experience.
are they abandoning the shopping cart at any specific points in the checkout process?
myth #5: there’s no way to know the business impact of the performance issues of my mobile app most mapm tools in the market today are too developer-centric.
they deliver crash analytics and performance delays caused by delayed response from backend services but little else.
ignoring the business context is like missing out on half the picture.
There’s often a problem with performance monitoring of too much data, leading to franken-monitoring.
No matter what industry, speed index is essential to know in order to continue delivering exceptional results.
There is no doubt that you need tools to do DevOps, you can read this article that describes different DevOps Tools.
If you’ve been anywhere near the news lately, you have probably seen how severe the effects of a performance outage can be to airlines on a global scale.
Hundreds of flights were delayed or canceled across the nation.
Airline officials said the failure of a router began a domino effect that crashed critical systems.
Unfortunately, the airline’s backup systems failed too.
The website crashed as well, prompting company officials to estimate a loss of $5 to $10 million in ticket sales.
However, scrapping systems and rebuilding is not always a sure fix.Monitoring software ensures that even if a bug slips through, you’ll catch it before your customers do.
Delays create an immediate negative impact—all the more reason to implement software performance monitoring programs.Later that year, the same airline had another technology glitch when the airline’s ticketing system did not allow passengers to check in.
Initially, the airline said they did not know the cause of the technology breakdown that impacted their website, reservation centers and mobile application, describing the problem as “system-wide.” The company said that of the 3,600 flights scheduled that day, 450 experienced delays, which resulted in long lines and backups at major airports from the nation’s capital to the West Coast.
When one piece goes down, they all go down, yet another reason performance monitoring in a large distributed environment is vital.Several travel experts said they believed the crash was due to the company trying to do too much on legacy computer programs.
The system breaks down, passengers get frustrated, and the company loses millions of dollars.
But once a system crashes, everything is so interconnected that a host of other airline functions are affected.
Then it takes time to go through the long list of things that must be restarted and restored after a crash.
Earlier that year, two other major domestic airlines suffered multiple performance outages as well.
The computer systems designed for smaller airlines cannot handle the passenger load when the airlines merge.One obvious way this could have been avoided is to run stress tests against the newly merged hybrid systems and monitor them using an APM solution like AppDynamics to filter errors before going live.
It’s critical to monitor everything from end-user transaction monitoring, infrastructure, network performance management, and capacity in the cloud, as well as rapidly identify problems like application delays.
The DevOps Vicious Cycle the above image depicts the devops vicious cycle (define / plan –> construct / build –> execute / run), with the landscape of capabilities and jungle of tools around it.
The New Battleground in Analytics Part 2: Transforming APM into Application IntelligenceThis post was originally written by Kyle Duffy appeared first on Application Performance Monitoring Blog from AppDynamics.In Part 1 of this series, I talked about my transition from the world of Analytics into the APM space and my assertion that APM is simply another form of Analytics.
Unfortunately these databases are primarily designed for inserting small amounts of data.
When complex queries run against it there can be a significant performance hit to the application, which is a very bad thing.
Often the delay in usable information means you’re looking at information that’s hours, days, or even weeks old.
In the last few years there have been some disruptive technologies introduced that can simplify this.
That’s how it sees everything and identifies performance problems and bottlenecks, often before end users are impacted.
In real time you can answer questions like:What was the revenue impact by product category associated with the two marketing campaigns we ran on Black Friday?Which Android users experienced failed checkout transactions in the last 24 hours, and what was in their cart at the time?How many customers experienced a slow ‘submit reservation’ transaction in the last hour from a Chrome browser in New York and what was the total dollar amount of those reservations?How many transactions originated from Tier 1 partners over the last 90 days and what was the resource utilization and revenue associated with those requests?
Health rules are created using the health rule wizard.Health rules establish the health status of an entity by defining levels of performance based on metrics; for example, CPU utilization (for a server) is too high.When the performance of an entity affected by the rule violates rule’s conditions, a health rule violation exists.Policies provide a mechanism for automating monitoring and problem remediation.
Second, unlike RUM, it notifies you when your site or application is unavailable; RUM reports nothing when your site/app is unavailable (since there’s no real-user interaction to report), or when a custom fail message is displayed.
Health rules are created using the health rule wizard.Health rules establish the health status of an entity by defining levels of performance based on metrics; for example, CPU utilization (for a server) is too high.When the performance of an entity affected by the rule violates rule’s conditions, a health rule violation exists.Policies provide a mechanism for automating monitoring and problem remediation.
However, poor performing e-commerce and mobile applications taking over 3 seconds to respond are fatal to retailer’s reputation, brand and revenue.There are no second chances in this digital world: when you’re not available, someone else is – your competitors are just a click away.
Black Friday and Cyber Monday) impact revenue immediately as many of the consumers would be deterred from using a retailer again after a negative experience.Promoting agility in software management processes.
How IT Can Redeem Itself from Critical OutagesIn my previous post, I discussed how AppDynamics Application Analytics can rapidly troubleshoot gradually degrading apps.
In this blog post, I’ll discuss how IT can redeem themselves from unfortunate outages leveraging ITOA or application analytics.With so many factors impacting application performance today, it is not surprising that applications frequently are either slow or simply stall entirely.
Sometimes it’s the server, sometimes it’s the database, and some days it’s the mobile network carrier to blame.
With the complexity of today’s environments, business transactions (such as logging in, adding to a cart, or checking out) have many opportunities to fail.
These transactions often fail long before they reach the backend operational databases where all the “committed” transactions are stored.
Applications are bound to be slow or stall; It’s how IT redeems itself from such critical outages with their business stakeholders that matter most.AppDynamics Application Analytics provides real-time visibility to deliver insights into both aggregated or rolled-up metrics, as well as details into individual customer interactions.
Now, this transaction failed on the network carrier and never hit the application inside the data center servicing such requests.
So, the application infrastructure has no knowledge of John’s attempt at purchasing the shirt.
In another case, Susan was trying to buy a purse from her browser but her order got stuck in a queue due to errors in the messaging queue.
How do we prevent from drowning in run-time data?A lot of companies are facing the same problem.
When problems occur in their IT stack, they don’t know where it originates.
Was it a change, an overload, an attack or something else?
If something happens in your IT stack, you will see a lot of red dots and you will probably get a lot of e-mails which say there is something broken.
So at this level you are able to get more in-depth insight on the systems your own team is using.But what if something fails somewhere deep down in your IT stack, which affects your team?
Any change or minor failure in your IT landscape can create a domino effect and eventually stop the delivery of core business functions.
For this problem, we introduce level three of the Monitoring Maturity Model.Level 3 - Create a Total OverviewAt level three we don’t only look at all the states, events, and metrics, but also look at the dependencies and changes.
So teams have a much easier job finding the cause of a failure.
After staring the service, it will send a notification to your Slack channel (you can strip this piece if no slack desired).
Notify SlackCreate another script in /opt/scripts/slackpost.sh with the following content:#!/bin/bash# Usage: slackpost "" "" ""webhook_url=$1if [[ $webhook_url == "" ]]then        echo "No webhook_url specified"        exit 1fishiftchannel=$1if [[ $channel == "" ]]then        echo "No channel specified"        exit 1fishifttext=$*if [[ $text == "" ]]then        echo "No text specified"        exit 1fiescapedText=$(echo $text | sed 's/"/\"/g' | sed "s/'/\'/g" )json="{\"channel\": \"$channel\", \"text\": \"$escapedText\"}"curl -s -d "payload=$json" "$webhook_url"Finally, use chmod +x /opt/scripts/*sh to make both scripts runnable.
Performance Linksheet [Updated 7/28/15]Here are some neat introductions, tutorials, and news stories from the performance space[This linksheet was reviewed by DZone MVB Rob Kenworthy.
See why Swiss Insurance Die Mobiliar deploying Oracle Database in-memory proves otherwise.
All of those monitoring tools might actually be hurting your stragety.
No, Peter Cushing is not to blame.
Then ask them, what more can they do?If you throw the book at them, it’s just checking the box, said Rosen.That’s a faux sense of maybe compliance, but it’s not security.What Rosen advises is to look more at intent with cloud companies and understand that there are going to be gaps, there are going to be risks.
Inventories that were already difficult to keep current because of VM sprawl might now have to accommodate containers, too.
Issues arising from unexpected migration of VM images might be made significantly worse when the containers running on them can be relocated with a few keystrokes.”Earlier this year, AppDynamics unveiled Microservices iQ to address these visibility issues daunting DevOps teams today.Infographic – Container Monitoring 101 from AppDynamicsWith Microservices iQ, DevOps teams can:Automatic discovery of entry and exit points of your microservice as service endpoints for focused microservices monitoringTrack the key performance indicators of your microservice without worrying about the entire distributed business transaction that uses itDrill down and isolate the root cause of any performance issues affecting the microservice
How to Deal with Slow Unit Tests with Visual Studio Test RunnerOne of the most dreadful problem of Unit Testing is slow testing.
Detecting Deadlocks without DrudgeryEliminating deadlocks in Java is hard.
You can try to find them by hand, but that's tedious and error prone.
SSDs don’t replace disk; they replace the RAM you would be using to cache enough disk pages to make up for the terrible random IO performance of spinning disk arrays.
We all write code where we don't create threads ourselves but our code is still being executed in multi-threaded environment.
Full stop.
We’ve probably all seen those testing reports that are full of graphs of response time versus req/sec, CPU utilisation curves, disk IO throughput, error rates ad nauseam.
All that effort & expense is wasted.
Drill Down to the Code LevelOne of my favourite things when load testing with APM tools is being able to drill down to the stack trace level and identify the calls that are the most problematic.
We generally have all the relevant parties on a conference call or HipChat chat session while we test and we are constantly exchanging information, screenshots, links to APM snapshots and the developers are often able to code fixes there and then because we can rapidly pinpoint the pain points.
Stop the “Blame Game”“make the enemy poor performance, not each other…”Traditionally in the old school (pre-APM tools) days, load tests were often conducted by external load testing consultancies who would come in, do the testing, and then deliver some big report on how things went.
The customer would assemble their team together in a conference room to go through the report, which often triggered the “blame game” – Ops blaming Dev, Dev blaming QA, QA blaming Ops, Ops blaming the hosting provider, the hosting provider blaming the customer’s code and around and around it would go.
But with the right APM tools in place we’ve found this negative team dynamic can be avoided.
It’s more about “what are we going to do about this problem we can see here in the APM tool” and less about trying to allocate blame when no-one really knows where the problem actually resides and they don’t want to be left holding the can.
The system-thinking, holistic view of the application’s performance promulgated by the APM tool makes performance the enemy, not each other.
Why Every PHP Application Should Use an OpCacheThis article was originally written by Rob Bolton for the AppDynamics blog.PHP 5.5 introduced opcode caching into the core via OPCache.
The transaction goes through a standard Symfony controller and renders a basic html form to login — there are no databases or external services involved.
If your PHP scripts are very basic, including only the minimal amount of code to process the request, as compared to using a framework, then the reduction in response time will also be limited.
The issues outlined in this research include:  I&O is often not entirely sure what it is doing (in terms of metrics) and why.
I&O often makes the mistake of focusing on technology rather than business metrics These show a systemic issue, this is partially why dashboarding, and having as many as possible is normally the goal of many I&O professionals.
I use this graphic to explain the problem with this approach:The purpose of these screens and all of the email alerts are for visibility, but instead we just overload ourselves with non-relevant information.
Forrester recommends this in the latest research “Tie compensation of staff members to metrics, and set initiatives for improvement”.Finally, the most relevant recommendation made in this research is to “Aggregate metrics into a dashboard that uses language the business will understand.” This means measurements such as lost business transactions, lost customers, or other issues related to response time.
Changes are always scary.
They mean taking a risk, having a chance to lose and fall behind your competitors.
This cloud application migration service replicates all the data without performance disruption or any other losses.
This cloud application migration service involves AIOPs for precise problem root identification.
AWS Migration Acceleration Program (MAP)As it was mentioned in the beginning, changes can be scary.
Your team may be reluctant to the cloud migration.
However, it’s never late to gain competencies.
The Poor Misguided CMDBIt’s not an exciting or glamorous subject but it’s an absolutely critical concept for properly managing your applications and infrastructure.
I say it was only a starting point because invariably the information within the CMDB was wrong.
It was either originally input wrong, not updated regularly enough, or updated with incorrect information.
Not very efficient!
Performance troubleshooting – When I was asked to get involved with a performance problem, one of the first things I wanted to understand was all of the components that made up the application and any external dependencies.
As I said before, invariably the CMDB was wrong.
There were usually components missing from the CMDB, and components in the CMDB that were no longer part of the application, and incorrect dependencies, and, and , and…Salvation by Auto Discovery and Dependency Mapping, but Not ReallySo what’s a good IT department supposed to do about this problem?
The problem, as I would later realize, is that agentless discovery tools only see what’s going on when they login and scan the host.
What that means is that all of those transient (short lived) services calls into or out of each application are misses by the discovery tool unless they happen to be running at nearly the exact time of the scan.
To add further insult to injury, most organizations don’t want a bunch of scanning activity going on during heavy business hours so the scans are typically relegated to the middle of the night when there is little or no load on the applications that are being scanned.
This amplifies the transient communication dependency mapping problem.
I won’t comment much on this but I will say that this creates another slew of deployment problems from a political and technical perspective and the thought of every trying it again makes me wince in pain.
Forrester calls this a SIS (service information system) in their research paper titled “Reinvent The Obsolete But Necessary CMDB”.
From my perspective it is incredibly difficult and inefficient to manage a datacenter or group of applications without implementing this type of concept.
Here are some examples…Hung/Unresponsive JVM/CLR – Initiate and store thread dump, restart application on offending node.
Transactions throwing excessive errors – search log files for errors and send list to appropriate personnel, based upon error type possibly probe individial components deeper (see #2 below)Application Operations2.
Probe application components – This one is really useful for figuring out difficult application problems but requires more effort to set up than #1.
The basic concept is that you need to find out from the application support team what steps they would take manually to trouble shoot their application if it were slow or broken.
Imagine that the application response times get slow so these troubleshooting measures are automatically invoked and you get an email with the exact root cause within minutes of performance degradation.
With this actionable information Bob is able to immediately match the pricing of the competition and sales rates return to normal before too much damage is done.
Your level of automation is limited only by your imagination.
what if you get to know that one of them has been hacked and there is a probability that your personal data has been exposed?
it will definitely stir up panic and anxiety, as every important application builds dependability over a period of time and usage, especially financial applications.
without automation, it will end up crushing all but the most trivial app efforts.''
most importantly, perfecto's reporting and analysis platform enables teams to assess the quality and focus on the problem areas to make necessary changes.
Recent analytics and researches reveal that IT complexity and performance challenges are killing digital transformation initiatives and thus causing significant digital performance problems in organizations as quite often as once every 5 days.
Applications are no longer designed as monolithic business processes.
Single User Performance TestingMulti-User Performance TestingParameters: Latency, Throughput, Errors, Resource Utilization.
People can have good and bad ones, even when using exactly the same website or mobile application.
When was the last time you submitted a report for a software error?
Fortunately, these wasted interactions are preventable using Application Performance Monitoring tools.
Why You Need Application Performance Monitoring ToolsPart of the problem is that different operating systems, browsers, connection speeds, devices and locations mean that any number of combinations could be interacting with your application.
Local testing is useful but fails to identify the vast amount of variables that users can encounter when trying to use your app.
How do you fake millions of users interacting with your application and get an accurate view of what may happen when you deploy into production?
Mainly, Raygun focuses on errors, crashes, and performance issues that users encounter.
One of the features that saves the most time is smart error grouping, which collects errors underneath a single root cause so you don't get flooded with notifications.
Another time-saving feature of Raygun is the ability to identify authenticated users, so you get a view of which specific users encountered problems, and to what extent.
The unique profile will have a list of every error or crash that user has encountered, which browser they use, and all the devices they use to access your app.
Key FeaturesFull-text search and filteringTrack multiple applications using any language and frameworkError, crash, and performance tracking in one platformUnlimited end usersDiscover critical bugsAdd comments and mention team members on issuesVersion/deployment trackingAttach tags, custom data objects, and user dataJavaScript source mapsAutomatic error reportingSeamless integration that takes minutesIntelligent error groupingError notifications via email, Slack, HipChat etcFull stack traces2.
With a ton of insights into how the elements of our application interact, New Relic helps tune the experience for users and identify potentially disastrous issues before they become problems.
Though New Relic is not as strong on the error and crash diagnosis as other dedicated crash reporting and error tracking solutions, you can pair it up with a tool like Raygun to give you the best of both worlds.
Key FeaturesApp availability monitoring, alerting, and notificationProduction Thread Profiler featureAutomatic application topology mappingPlatform pluginsHistograms and percentilesPerformance data API accessReal-User response time, throughput, and breakdown by layerJVM performance analyzerError detection, alerting, and analysisDatabase call response time and throughputCode-level diagnostics, transaction tracing, and stack traceApp response time, throughput, and breakdown by componentSlow SQL and SQL performance detailsApdex scoreReal-User breakdown by web page, browser, geographyCustom dashboardsTrack individual business transactionsX-Ray sessions for business transactionsCross-application tracing for distributed appsAvailability, scalability, deployment reports3.
So, if your app is painfully slow for customers located there, you'll need to know about it.
For this reason, Real User Monitoring (RUM) tools are essential for identifying performance problems and their environments.
Pingdom also monitors your website for downtime, alerting you right away when a disaster happens, and it goes offline.
Key FeaturesEnd User MonitoringApplication health dashboardReal-time business transaction monitoringReports and visibility into your applicationVisualize and manage your entire appOperational dashboardsDetect business impact and performance spikesAnalyze impact of agile releasesIsolate bottlenecks in your applicationDeep code-level diagnostics in productionIdentify root cause with complete code diagnosticsDynamic scaling in the cloudAutomatic business transaction discoveryDiagnose the root cause of the problems 90% fasterDiscover and visualize your application topology and businessTroubleshoot performance and availability issuesSet up proactive alerting to find problemsMonitor 24/7 what matters most-your key business transactionTroubleshoot bottlenecks 90% fasterMonitor hybrid environments with Java, .NET, PHP, and Node.jsOverviewIn conclusion, each of the performance monitoring tools above offers something slightly different, but are all built around the same goal - seeing what your users are doing when they encounter problems.
Application Performance Monitoring tools present the symptoms of the problem in a clear and visual way to aid in diagnosis and ultimate resolution.
Ultimately, performance problems are a huge contributor to dissatisfactory software experiences.
In times when our users are very demanding regarding speed and uptime (particularly in web-based applications), APM tools are very useful for detecting and diagnosing performance-related problems in production environments, without significantly adding overhead to resource consumption.
Typically, APM tools expose information about response times, load, transactions, resources consumption, network data and other performance metrics of a system application and infrastructure.
Each reader will decide if the advantages and disadvantages here apply for her/his specific situation, based on what they are looking for in an APM tool or in her/his personal experience using software with these characteristics.
On the other hand, it has a free Lite version, but it presents really poor data and has only 2 hours of data retention.
Flexible low prices.
Disadvantages:It can be only used for AWS components and therefore for applications running only on Amazon servers.
I found some scripts made by third parties to get metrics for non-AWS servers but they aren't an "official" solution and I haven't tested them.
The user interface is not really friendly for data analysis making it difficult to perform data correlation.
No transaction tracing.
No memory usage metrics by default.
Their number of visits is limited to 100k.
Disadvantages:The user interface is a little complex, especially for users without experience on this tools.
CA APM GUI:Conclusion and ComparisonAs you are probably thinking, the conclusion of the post is that there is no definitive APM tool to choose over the others.
There have been two types of misconfiguration errors we’ve seen often in the field:logging configuration is copied from staging settingswhile deploying the application to production environment, logging wasn’t fully configured and the logging failed to log any dataTo take a closer look, I have a couple of sample applications to show how the problems could manifest themselves.
These sample applications were implemented using MVC5 and are running in Windows Azure and using Microsoft Enterprise Library Exception Handling and Logging blocks to log exceptions to the SQL database.
There is no specific preference regarding logging framework or storage, just wanted to demonstrate problems similar to what we’ve seen with different customers.
To restate what we see above — this is a failure while trying to log the original exception which could be anything from a user not being able to log into the website to failing to checkout.
Situation #2 During deployment the service account wasn’t granted permissions to write to the logging databaseThis looks similar to the example above but when we drill inside the error we can see the request has an internal exception happened during the processing:The exception says the service account didn’t have permissions to run the stored procedure “WriteLog” which logs entries to the logging database.
From the performance perspective, the overhead of security failure is less from timeouts in the example above but the result is the same — we won’t be able to see the originating exception.
Not fully documenting or automating the application deployment/configuration process usually causes such problems.
Let’s check the EntLigLogging database — it has no rowsHere’s some analysis to explain why this happened:We found exceptions when the application was logging the errorThis means there was an original error and the application was trying to report it using loggingLogging failed which means the original error was never reported!
And… logging doesn’t log anywhere about its failures, which means from a logging perspective the application has no problems!!
Typically, loggers are implemented similar to the following example:Logging is the last option in this case and when it fails nothing else happens as you see in the code above.
Just to clarify, AppDynamics was able to report these exceptions because the agent instruments common methods like ADO.NET calls, HTTP calls, and other exit calls as well as error handlers, which helped in identifying the problem.
Going back to our examples, what if the deployment and configuration process is now fixed and fully automated so there can’t be a manual mistake?
Do you still need to worry?
Unfortunately, these issues happen more often than you’d expect, here is another real example.
As a result, the response time and error rates have spiked.
Here are the exception details:The worst part is at 10:15pm the application was not able to report about its own problems due to the database being completely full, which may incorrectly be translated that the application is healthy since it is “not failing” because there are no new log entries.
We’ve seen enough times that the logging database isn’t seen as a critical piece of the application therefore it gets pushed down the priority list and often overlooked.
This problem could be avoided entirely unless your application receives an unexpected surge of traffic due to a sales event, new release, marketing campaign, etc.
Other than the rare Slashdotting effect, your database should never get to full capacity and result in a lack of logging.
Without sufficient room in your database, your application’s performance is in jeopardy and you won’t know since your monitoring framework isn’t notifying you.
Key points:Logging adds a new dependency to the applicationLogging can fail to log the data – there could be several reasons whyWhen this happens you won’t be notified about the original problem or a logging failure and the performance issues will compoundThis would never happen to your application, would it?
Broken out in 3 stages, the MU programs provide financial incentives for the “meaningful use” of certified Electronic Medical Records (EHR) technology.
A Problem ArisesTwo days after the final upgrade outage, and just as everyone was ready to head home after a number of sleepless nights, a frantic call came into the upgrade command center.
The call came from the nurse shift supervisor of the emergency department (ED).
If you have ever met an ED nurse, you will understand it when I say that an ED nurse is not someone you want to upset.
In order to provide visibility and bring order into a very intense operation, the ED relies on a number of critical tools.
But because this particular hospital is the only Level I trauma center in the region, this wasn’t an option.
The command center became a war room; Clinical analysts from the hospital, project managers from both sides of the implementation, a large ensemble of high-level clinical and executives from the hospital, the entire infrastructure team, DBA’s, interface engine administrators, and developers from 3 different continents, were all locked in and given clear instruction: “Don’t leave until this issue is resolved”.
The situation in the emergency room was coincidentally turning into an emergency itself.
The Chief Medical Information Officer (CMIO) of the hospital brought the vendor project manager to tears and the ED nurses were gathering their torches and pitchforks and marching against the IT department.
All appeared lost and after days of outage and close to 1,000 man-hours spent trying to find the root cause of the problem, everyone was ready to walk out.
Many of them had life threatening conditions, and the queue outside the ED was only growing longer.
As the team monitored the application, looking for clues as to why the application was failing, we noticed that the UpdateCycle transaction volume was 100x what was expected.
Considering there were ~10 viewers at any given time, the system was designed to support tens of transaction per minute, and was failing because we were receiving thousands of transactions per minute.
A faulty client side configuration was overloading the server and causing it to generate slow responses back to the clients.
Systems are becoming more and more complex and are not easy to manage.
The advantages of building and running software with this architecture outweigh the disadvantages.
Isolated development approach results in highly independently deployable service, and testable service.
With serverless:The underlying infrastructure is fully managed by the cloud platforms, resulting in very low maintenance costs The deployed serverless services are highly availableEasier to scale globally with high concurrencyEvent-driven architecture which means that it is used only when a relevant event is triggeredMost serverless are pay-for-usage which results in low cost for running the services Digital Modernization Utilizing both microservices-based architecture and serverless together is what is called as Digital Modernization.
With the advent of microservices and serverless different challenges arise like:Monitoring the high number of microservicesIdentifying the root cause for failureAddressing the failure quickly Testing across the various features Monitoring end-user conversions Adapting to continuous upgrades and changes to the systems Simple DevOps strategy will not be enough to manage such a complex system.
However, recent studies have shown that despite advancements in APM technology and the plethora of APM tools available, this overabundance can actually be a problem.
Many companies are suffering from a case of too much monitoring, which can lead to less useful information.While it might seem counterintuitive, application performance monitoring is a space where less is more.
Moreover, EMA’s research concluded that when several APMs are in place, the overload is actually making monitoring more difficult.The majority of these solutions lack proper integrations.
The point of monitoring is to locate, identify, and prevent problems.
According to EMA’s analysis, 33% of problems are reported by users.
Rather, this amalgamation of monitoring tools can even degrade the state of monitoring.
Technical Solutions Used for Performance and MonitoringTo gather insights for DZone's Performance and and Monitoring Research Guide, scheduled for release in June, 2016, we spoke to 10 executives, from nine companies, who have created performance and monitoring solutions for their clients.Here's who we talked to:Dustin Whittle, Developer Evangelist, AppDynamics | Michael Sage, Chief DevOps Evangelist,Blazemeter | Rob Malnati, V.P.
Concerns Around Performance and MonitoringTo gather insights for DZone's Performance and Monitoring Research Guide, scheduled for release in June, 2016, we spoke to 10 executives, from nine companies, who have created performance and monitoring solutions for their clients.Here's who we talked to:Dustin Whittle, Developer Evangelist, AppDynamics | Michael Sage, Chief DevOps Evangelist, Blazemeter | Rob Malnati, V.P.
Clients don’t want to just know there’s a problem, they want to know how to mitigate it and see what happened.
Additional Considerations Regarding Performance and MonitoringTo gather insights for DZone's Performance and and Monitoring Research Guide, scheduled for release in June 2016, we spoke to 10 executives, from nine companies, who have created performance and monitoring solutions for their clients.Here's who we talked to:Dustin Whittle, Developer Evangelist, AppDynamics | Michael Sage, Chief DevOps Evangelist,Blazemeter | Rob Malnati, V.P.
Why are so many people so upset about it?
Here’s your instant history of an app that tapped into far more demand than the developers could handle.
It’s a game where you capture little monsters and make them fight each other – part insect collection and part sci-fi gladiator arena.
If it could be mapped, it became a battleground.
Niantic had to issue an apology and said it never intended to collect all that private data, but industry watchdogs just have to take their word for it.At one point, due to this privacy problem, their sign-up process was broken and the company had to turn away new signups.
That can be particularly bad from a developer perspective because most apps lose 60 percent of their customers over the first month, and 96 percent after a year.
What could go wrong?
A few days after Pokémon Go’s official release, Forbes posted an analysis that declared, “The launch has been an unmitigated disaster.
The list of things that have gone wrong with the launch goes on and on.
Normally, the last days leading up to pre-launch can be frantic.
Others have made the same mistakes, though.As for the production team at Pokémon Go, could they or should they have anticipated so much traffic?
Yes, and Niantic’s launch problems might have been catastrophic if it were another business critical application.
The average cost of a critical application failure per hour is $500,000 to $1 million, according to a research by analyst firm IDC.
Nearly a quarter of apps are only opened once.Many enterprises are using AppDynamics to avoid such catastrophic situations and to minimize the negative impact on their brand reputation.
The most important takeaway from the story of Pokémon Go is that companies of all sizes need to prioritize performance monitoring before errors start impacting customers.
Service layerAvoid long transactionsCache application data, that are remain same for all users at server level.
Cache frequently used parameters and query resultsNetworkAvoid large data transfer by using GZIP compressionMinimize HTTP requests, use CSS Sprite and image mapsSetup Content Delivery Network (CDN)  if the application content are geographically distributedJVMJVM Heap sizeHeap size determines how often and how long VM spends on GC.
Configure parallel GC & concurrent GC - Parallel GC uses multiple treads for garbage collection and concurrent gc will   stop the application only during the initial mark and sweep phase.
taking this a step further, it’s much easier to get into trouble today than ever before when new code shipping cycles are cut down to weeks and sometimes days or even multiple times a day.
to avoid being run down by the zombies, here’s the survival kit setup you need to fully understand the impact of new code on your system.
tools to check:   1. on-premise:   splunk  2. saas:   sumo logic  3. saas:   loggly  4. open source:   graylog2  5. open source:   fluentd  6.   the elk stack  (open source):   elasticsearch  +   logstash  +   kibana   performance monitoring  so the release cycles are cutting down and log files are becoming larger, but that’s not all: the number of user requests grows exponentially and they all expect peak performance.
tools to check:   7.   appdynamics  8.   new relic   new players:  9.   jclarity  10.   plumbr  11.   ruxit  12.   dripstat   debugging in production  release cycles are down, log files grow large, user requests explode, and… the margin for error simply doesn’t exist.
when an error does come - you need to be able to solve it right away.
large-scale production environments can produce millions of errors a day from hundreds of different locations in the code.
while some errors may be trivial, others break critical application features and affect end-users without you knowing it.
traditionally, to identify and solve these errors you’d have to rely on your log files or a log management tool to even know an error occurred, let alone how to fix it.
with takipi, you’re able to know which errors pose the highest risk and should be prioritized, and receive actionable information on how to fix each error.
looking at errors arising after new deployments, takipi addresses 3 major concerns:  1. know which errors affect you the most  - detect 100% of code errors in production, including jvm exceptions and log errors.
over 90% of takipi users report finding at least one critical bug in production during their first day of use.
2. spend less time and energy debugging  - takipi automatically reproduces each error and displays the code and variables that led to it - even across servers.
this eliminates the need to manually reproduce errors, saves engineering time, and dramatically reduces time to resolution.
3. deploying without risk  - takipi notifies you when errors are introduced by a new version, and when fixed errors come back to haunt you.
tools to check:   13.   takipi   alerting and tracking  release cycles, log files, user requests, no margin for error and… how you’re going to follow up on it all?
you might think this category overlaps with the other’s and the truth is that you’re probably right, but when all of these tools have their own pipelines for letting you know what went wrong - it gets quite cluttered.
takeaway #6:  consider using an incident management system to handle information overload.
answering a crucial question that seems trivial: is the website available?
another angle to tackle information overload is error tracking that goes beyond the features of log analyzers: smart dashboards to manage your exceptions and log errors.
In this blog post we will explore a real world example of how to avoid finger pointing and get right down to figuring out how to fix the problem.
One Rotten Apple Can Ruin The Whole BunchThis story dates back to June of 2012 but I just came across it so it is new to me.
In this case though, both teams were able to collaborate and quickly realize what the problem was as a result of having a monitoring solution that was available to everyone.
The problem becomes that companies make these types of decisions without any idea of how it affects their site performance.
To no one’s surprise, Oracle was the most frequently mentioned.
Powered by standard connectivity and protocols such as Bluetooth, Wifi, and NFC driven by protocols such as Zigbee and ubiquitous APIs.
Battery technology has largely not evolved for 15 years or more, this is a major limitation to today’s devices and connected things.
For operations, there are no solutions.
There’s a lack of expertise of what to look for and how to look for it.
The framework is integrated with Jenkins and there is a variety of trend graphs (percentage of failed requests, mean response time, etc.)
Apps that perform well will engage the customer — poor app performance is a sure fire way to lose the customer and their business.
With so many failure points (millions if you do the math!
What components should we focus on when the transaction performance has degraded?
Isolate effects of change and releases fasterIn the mobile world, change happens all the time; in fact, 2-week app release havebecome the norm now.
With different teams managing the app and the application tiers, the change and release management process may seem a bit chaotic and may cause performance issues (after all, changes are responsible for 80% of all application health issues).
How has the latest upgrade of the app/application affected the app performance.
In summary, app performance is extremely critical.
86% of users will delete an app after one poor experience.
While AWS remains the largest cloud computing player in the space, the hyper-growth market is expected to grow $27.8 billion.Don’t Expect The FBI To Tell Apple How It Broke Into That iPhone – Fast Company, March 30After seven weeks of intense legal battling, FBI told Apple – “I don’t need you anymore.” Last week, the case took a surprising turn when a judge canceled a particularly anticipated court hearing just hours before its scheduling.
The intensely brewing battle between the FBI and the tech industry began when the government requested that Apple write new code that would unlock an iPhone belonging to one of the terrorists from the San Bernardino shooting.
Why give up the secret tool they were so desperately seeking?Key takeaway: This highly publicized and heavily debated legal battle was much more than one single encrypted iPhone and more about setting a dangerous precedent in a much larger discussion around law enforcement, national security, privacy, and individual privacy.
In addition to the new game, Uber is also offering $10,000 to any hackers who find bugs in its systems that could potentially expose customers’ personal information.
Tweet at us or leave a comment below!
There are numerous features and updates that keep on coming, and sometimes, it’s hard to tell the wheat from the chaff.
Sticking with comparisons, during the past year we looked into Java SE and Java EE, the latter being a constant source of confusion for its users.
Some Things Never ChangeOur top blog post proves what we already know: log files still suck.
We hear you, logs are a pain since you have to sift through them in order to find what went wrong.
With OverOps you can reproduce each error or exception, as if you were then when they happened.
OverOps can also work with your current log solution, adding a link to every error in the logs that gives you the root cause.
There is usually at least one relational database lurking somewhere within the overall application flow and understanding the behavior of these databases is major factor in rapidly troubleshooting application problems.
In 2009, Amazon launched their RDS service which basically allows anyone to spin up a MySQL, Oracle, or MS-SQL instance whenever the urge strikes.
The provided CloudWatch monitoring metrics are high level statistics and not helpful in troubleshooting SQL issues.
so i joined the 20+ other people on the conference call, listened to hear what the problem was and what had already been done, and began digging through my mountains of graphs.
within the first 30 minutes of pouring over my never ending streams of data i realized that i had no clue where any of the data points should be for each metric at any given time.
i had no reference point to decipher good data points from bad data points.
i have months of this data just waiting for me to look at and determine what’s gone wrong.
this was the hell i had resigned myself to when i made that fateful statement in my head “no problem!”.
click-flip, click-flip, click-flip, click-flip… my eyes are strained and my head is throbbing.
i want the pain to end but i’m a performance geek that doesn’t give up.
i’ve looked at so many charts by now that i can no longer remember why i was zeroing in on a particular metric in the first place.
i slow start banging my head on my desk in frustration.
“what changed?” it’s also one of the toughest questions to answer in a short amount of time.
ideally i want this all in context of the problem that has been identified either by an alert or an end user calling in a trouble ticket (i’d rather know about the problem before a customer calls though).
we veterans of the performance wars now have a bigger gun in the battle to restore performance faster.
We asked to describe the channels through which the respondents discovered the presence of the problem.
284 respondents listed 365 tools being used as some respondents were using up to five tools to monitor their deployments:The places on the podium are somewhat surprising:Most common answer to the question was “None”, meaning that 21% of the respondents used no tools whatsoever to monitor the production site.The most common tool used is still the 15-year old Nagios.
It is rather weird to see the the large APM vendors (CA, Compuware and BMC) being beaten by the simplest tool possible – namely Pingdom.As the survey was listed on our site, we do admit that Plumbr position in this list is most likely biased, so take our place in this list with a healthy grain of salt.
there is a lot of confusion in the market today.
myth #1: unforgiving app ratings on the app store are an unfortunate reality    there is no such thing as bug-free code.
your users understand this reality but are looking for well thought-out designs that have fewer performance and crash issues.
in order to avoid brutal app ratings, you need to a ship a good product, but more importantly show you are willing to respond to your user problems and fix them promptly.
use state of the art crash analytics and network request analytics to understand how your app is performing.
resolution of information should not be lost when troubleshooting performance and crash issues.
for the lack of better information, the mobile developer is quick to blame the back end service.
they often struggle to isolate the mobile transaction delays caused by their services and get defensive.
and if the entire infrastructure can be managed with a single pane of glass, there will be no information lost in translation and the mobile dev and it ops team can collaborate to focus on nailing down the end-user experience rather than play the blame game.
this is the most critical requirement we hear from our customers time and again.
By empowering companies to measure and baseline everything, conversational UX becomes a central nervous system for the entire organization.
Customer segment monitoring: When a user requests the customer journey for a particular service or application, the assistant could provide a graphical representation revealing bottlenecks or critical metrics (such as a drop in the number of conversions) that must be addressed immediately.
Metrics and ExamplesDevelopment MetricsThe better the development process, the less the burden on the test and operation teams and the less test and production errors of the application.
Technical Debt: It refers to the time spent resolving the errors in the code.
Faulty units are usually found with static analysis tools, and estimations are given automatically by the tool.
Here, the total value of the time given to correct these issues refers to technical debt.
The higher the complexity value, the more difficult it is to read or maintain a code.
Each code block such as if, case, for increments the Cyclomatic Complexity value by one.
Applications with high duplication value are difficult to change.
Tools: Sonarqube, Cast...Code Review: We can check the written codes through static analysis, but in order to be limited in the rules and to minimize the possibility of overlook, another developer or team is required to review and comment.
Fast typing allows us to see steps that are overlooked or difficult to read instantly.
In addition, the more frequent feedback is given with the continuous and sufficient number of tests, the sooner the team becomes aware of possible errors and takes precautions.
The risk calculation can be given as "error probability x severity" or directly as "error cost".
This process is difficult to be manual and operated continuously.
Number of Test Runs: The purpose of the tests is to give feedback to the team about possible functionality and / or non-functional errors without your customers noticing.
The sooner the feedback is given, the faster the action is taken and the growth of the error is prevented.Tests run every two weeks are slow to give feedback.
So if there is more error than you expected, you may not have enough time to solve it.
Test Data Management: Running tests with fixed data is not enough to catch errors.
Leaving these processes to the initiative of the people may cause any step in the process to be skipped or disruption when the person with the initiative leaves the team.
This gives both continuous feedback to the business units and the choice to take early action on possible errors.
Deployment Time: Long deployment times result in low feedback.
Tools:New Relic, Dynatrace, AppDynamics, Riverbed, Datadog...Error Rate: It specifies the error rate on the server side of the application.
If this error is high, it indicates that the application is constantly getting errors on the server side.
Errors are expected to be minimized by addressing them with APM.
For example, you can examine ordering scenarios end-to-end and identify if there are any problems preventing the process from completing.
Rendering Time: Even though the application response time is of sufficient performance, the processing time of the responded page on the browser may be long or receive errors.
In this case, we may be missing that the user viewed the page late.
The most common problems are:  Large in size and non-optimized images Too many DOM iterationsCSS selectors and file sizes are longer than usualOver-library dependencyThe project is not designed to be mobile-friendlyLoops whose algorithm is not developed correctlyLegacy technologies that are not updatedTools: New Relic, Dynatrace, AppDynamics, Riverbed, Datadog.
Crash Rate: It indicates the rate of errors that will cause the application to close on mobile applications.
These errors are desired to be minimized.Tools: Firebase Crashlytics, Countly, New Relic.
It is calculated taking into account response times and errors.
(Good response time x 1.0) + (Acceptable response time x 0.5) + (Unacceptable Response time x 0.0) + Bad Transactions x 0.0 and their sum divided by the total number of transactions.Tools: New Relic, Dynatrace, AppDynamics, Riverbed, Datadog.
To the left of the dashboard is Technical Debt and Unit Test Coverage information from Sonarqube.
At the bottom, there is Average Response Time and Error Rate information from New Relic.
The purpose of the metrics monitoring is not to create work pressure on the team; on the contrary, it is to talk about the place where the team is and is expected to reach with numerical values.
Don't Give Up on Agile ProcessesSince transition to agile processes requires a cultural transformation, it will not be easy.
A developer who is having trouble reading his own code may have trouble reading another developer's code.
With these tools, you have the chance to see the experience of end users in your application, the errors they encounter and the response times of the system in real time.
Set Objectives and KPIsIt is very important to have objectives and KPIs that determine how we are and where we will go, with the motto "You can't improve if you can't measure".
There is no direct meaning of the maturity level here.
), just in-time products and services, and every other category of commerce, the amount and velocity of activity is directly affected by the performance of the sites and apps.
If your website is slow to load, especially on mobile devices, then customers will do less business with you or they may even abandon you completely.This summer, AppDynamics teamed up with Internet Retailer to measure the homepage mobile performance at 3G and 4G network speeds of the top 500 retailers globally based on their revenue.
But companies need to be very disciplined about understanding the performance impact of their graphics and UXD choices or they can quickly bloat the size of their pages, slowing them down, turning customers off due to the poor performance and thereby wasting all of the time, money, and effort they put into trying to attract customers to their site in the first place.The second approach is to try to minimize the number the number of resources loaded per page view.
You might also influence third-party scripts on your app or site, or you might crash 3rd-party devices and servers if you don’t prepare them properly.
Geo metrics – Don’t ignore the geography aspect of app usage.
How the Java Ecosystem Has EvolvedIn order to more thoroughly understand the state of the Java ecosystem today, we interviewed 11 executives with diverse backgrounds and experience with Java technologies, projects, and clients.Specifically we spoke to:Anthony Kilman, Tech Lead, AppDynamics | Gil Tene, CTO, Azul Systems | Bhartendu Sharma, Vice President of Operations, Chetu | Charles Kendrick, CTO and Chief Architect, Isomorphic Software | Fred Simon, Co-Founder and Chief Architect, JFrog | Ray Auge, Senior Software Architect, Liferay | Michael Hunger, Lead Developer Advocate, Neo Technology | Brandon Allgood, PhD, CTO, Numerate | Dr. Andy Piper, CTO, Push Technology | Jonas Bonér, Founder and CTO, Typesafe | Toomas Rὅmer, CTO and Founder, ZeroTurnaround The Java ecosystem is massive.
Sun’s lack of leadership and major missteps (e.g.
JavaFX, JSF) have led to a plethora of conflicting approaches in basic areas of the Java platform like UI and data binding.How has the Java ecosystem evolved from your perspective?
Keys to Performance and Monitoring - Executives' PerspectivesTo gather insights for DZone's Performance and and Monitoring Research Guide, scheduled for release in June, 2016, we spoke to 10 executives, from nine companies, who have created performance and monitoring solutions for their clients.Here's who we talked to:Dustin Whittle, Developer Evangelist, AppDynamics | Michael Sage, Chief DevOps Evangelist, Blazemeter | Rob Malnati, V.P.
Data from our product can show you why people are not using an app – it may be slow, broken, or just not useful.
There used to be monolithic apps but existing agent-based solutions will not support microservices.It’s all about tools.
Our team has gone from four to 14 while we’ve gone from having $1 million clients to $100 million.We provide visibility and control in three ways: 1) Reduce risk by understanding connection with who and how.
Which one should you choose and which one should you leave?
Because during GC pauses, entire JVM freezes – no customer transactions will be processed.
If you want low latency and low footprint then throughput will degrade.
Load and Performance Testing With JenkinsAdding performance tests to your CI scope will help you reduce the risk of performance degradations whenever you add a new feature or fix a bug on your product.
Let Your Applications Talk to YouIn our quest to solve every memory leak in the Java world we get in touch with many teams, who struggle with performance issues of their applications.
MethodC, in relation to methods A and B, is not worth touching at all.You should take into account 2 things:When method1 calls method2, which calls method3, which in turn calls method4, then the cumulative total time for method1 in the table above will contain cumulative times for method2, method3 and method4.
We really do NOT recommend using CPU profilers for that purpose.
What we discovered was that none of the existing tools does precisely what one might expect them to do – you get vast amount of data but only seldom an actual solution to your problem.
We have tried seven different tools over the course of the series: three memory profilers, two command line tools bundled with the JDK, a heap dump analyzer and an APM solution.
Even getting our hands on some of them was not an easy quest.
Why do I have to go through all this mess to get things up and running?
With the others we had a variance of results:crashes, sometimes together with the application;tons of flickering dashboards, graphics, buttons, bells and whistles, but no answer;console full of cryptic text and java class names;… or application adapted for snails and turtles, with response times increased 5-350 times.
All that proves that when a year ago we decided to bring our own memory leak detector to the market, with the aim to shedding some light into this shadowy valley of misery and despair, our assessment of the current situation was correct.
You want to be able to act in no time and post-mortem an outage with handy, detailed reports.
The vast majority of organizations use some sort of logging system — it could log errors, traces, information messages or debugging information.
Logging brings good insights about the application behavior, especially about failures.
However, by being part of the application, logging also participates in the execution chain, which can have its disadvantages.
While working with customers we often see the negative consequences when logging alone introduced the adverse impact to the application.
Online sales are particularly crucial for these retail companies during this period and this is the time when their applications are under most stress.
To better explain the logging overhead I created a lightweight .NET application that:implemented using ASP.NETperforms lightweight processinghas an exception built inexceptions are always handled within try…catch statementexceptions are either logged using log4net or ignored based on the testIn my example I used log4net as I recently diagnosed a similar problem with a customer who was using log4net, however this could be replaced for any other framework that you use.
As you can see not only is the average response time significantly higher now, but also the throughput of the application is lower.
The snapshot is collected automatically when there is a performance problem or failure and includes full call graph with timings for each executed method.
By investigating the call graph produced, we see that log4net method, FileAppender, renders error information to a file using FileStream.
On the right you can see duration of each call and the most time was spent in WriteFileNative as it was competing with similar requests trying to append the log file with error details.
This is a clever concept and works adequately for low throughput applications, but as you can see the average response time is still in a similar range as the non-asynchronous version, but a slightly lower throughput is achieved.
And the article cites similar data showing that the time spent on mobile properties of certain publishers jumped 40% in the last 12 months to 52% of the time spent viewing their content.The problem, though, is that revenue from mobile advertising is not rising nearly quickly enough to offset the decreases from the desktop.
There are several reasons for that, including the limited screen size of mobile, which limits the number of ads that can be displayed.
Another lies within the format of mobile ads, which are more likely to deter the viewer because they are more intrusive or distracting than on desktop, where it’s easier for the viewer to ignore ads unless it is extremely relevant for them.But what the article completely misses is the direct relationship between content performance and ad revenue.
Hold 3rd party services and content providers accountable    Slow 3rd party services and content can KILL your page load performance    You need to have SLAs in place    You need to monitor the 3rd parties so you can hold them accountable    If an ad network is slow, it will hurt your page load speeds and your ability to generate revenue3.
IPM tools tend to work from the server layer and provide a more agnostic and less user-focused visibility of infrastructure performance, allowing for isolation of performance issues down to the server.
These key trends have caused APM to be increasingly critical and strategic for most organizations while IPM has become less relevant.
We’ve also seen the recent fire sale of assets from Boundary to BMC.
Specifically we spoke to:Anthony Kilman, Tech Lead, AppDynamics |Gil Tene, CTO, Azul Systems |Bhartendu Sharma, Vice President of Operations, Chetu |Charles Kendrick, CTO and Chief Architect, Isomorphic Software | Fred Simon, Co-Founder and Chief Architect, JFrog | Ray Auge, Senior Software Architect, Liferay |Michael Hunger, Lead Developer Advocate, Neo Technology |Brandon Allgood, PhD, CTO, Numerate | Dr. Andy Piper, CTO, Push Technology | Jonas Bonér, Founder and CTO, Typesafe | Toomas Rὅmer, CTO and Founder, ZeroTurnaround | Several expressed concern over the ongoing tensions between Oracle and Google (Android) and its potential to hurt Java.Here’s what they said:I’m concerned with Android.
Oracle and Google need to stop fighting over Android and work together to come up with good solutions.
I had an underlying fear Oracle would be less dedicated to keeping Java open than Sun was (acquired in 2010) but this has not come to fruition.
The patent fight with Google has been disconcerting.
I’m not a big fan and I don’t think we need it.
No.
This is very dangerous.
IT doesn’t have a historical base - this is a problem for all of these new languages.
There’s a lot of wasted effort.
I’m concerned with Oracle and Google fighting over Android.
I’m afraid of the big players - they could do something that’s not in the best interest of Java from my perspective.
The JCP helps but big companies can do what they want - they’re a necessary evil.
Industry Insights: Regulating Failure (Reg SCI)When examining the complexity in today’s applications and environments, and why APM technologies are becoming more critical by the day, those responsible for an application’s lifecycle must understand what an application does.
The failure to see and troubleshoot is a constant struggle for those supporting applications.
The entities covered by the new reg SCI mandates must also perform annual reviews including the testing of disaster recovery procedures of secondary sites, and their ability to handle the same volume of transactions with the same responsiveness as the primary sites.
The thing is, with a slightly elderly infrastructure, reactive and mostly backend-focused instrumentation, it's not before the next day that they know a store was offline.
The overall annual revenue loss exceeds 7 digits, and most importantly, too often, users go to pick their order and see a puzzled and weary staff.
Not good.
You proactively execute these user flows and alert on any outages and responsiveness degradation.
So you learn about user behavior, analytics etc., but you're dependent on users going through poor experience to know something's wrong.
Again, not a comprehensive comparison, but enough for now.
Option 2 means that you know some server or service is misbehaving, but you don't know the impact.
iOS and Android Applications Least Likely to Have Current or Planned MigrationsEnterprises no longer fear migrating existing applications with almost half (45.9%) of respondents have already migrated some custom-developed browser-based applications to the cloud and another 38.7% planning to do so within the next two years.
According to Puppet's 2017 State of DevOps Survey, top performers in the industry enjoy deployments that are 200 times faster, failure recovery that is 24 times faster and change failure rates that are three times lower than those of their competitors.
Identify and Monitor the Problem AreasIf you can't identify all problem areas, identify as many as possible.
Remember that the best monitoring starts before there's a problem and extends beyond the crisis.
Monitoring problematic areas is another of the essential parts of system monitoring.
In addition to the fact that identifying problem areas reduces unneeded notifications that can bog down and disable teams, keeping an eye on both potential bad things and actual bad things makes it easier to circumvent issues and respond quicker and more efficiently when they crop up.
This allows you to establish standard metrics and recognize what is abnormal enough to pay close attention to.
The first is the conditioned fear of some monster lurking behind the scenes that could pounce at any time.
The second, of course, is the actual monster of downtime on a critical system.
Modern enterprises have spent millions to mitigate the risk and prevent their businesses from having a really bad day because of an outage.
In my discussions, most companies I have talked to say their number one cause of outages and customer interruptions is ultimately related to the deployment of new or upgraded code.
Often I hear the operations team has little or no involvement with an application until it's put into production.
It is a bit ironic that this is also the area where companies tend to drastically under-invest.
Failing over broken or slow code from one server to another does not fix it.
Adding more servers to distribute the load can mitigate a problem, but can also escalate the cost dramatically.
In most cases, the solutions they apply don't address the primary cause of the problems.
Working for a company that specializes in open-source databases, we get a lot of calls on issues that have prevented companies' end users from using critical applications.
Many of these problems are fixable before they cost a loss of revenue and reputation.
Make a Replica of Your Production Environment If you want to find out the real problems of the system you are testing, you need to test it in an environment as similar as possible to the one it will be working on.
If you’re not sure about the measurements you are setting, go talk to the Product Manager.
If you’re not sure, speak to the Product Manager.
This will also reduce your own stress levels.
The question is, are consumers demanding digital or are innovators leading the pace of change?The panel was in agreement—consumers are in the driver's seat.
“All banks have the same problems but the capabilities to solve these problems have changed.
Established banks are huge organizations who may have had employees work for them for upwards of 20 years, so changing the mindset of these employees is no small task.Shane asked the panel what they considered to be their biggest barrier to change.
Using HTTP often leads to unreliable data transfer, lost messages, and unresponsive devices waiting to resynchronize with the cloud.
How does the MQTT solution support disaster recovery?
Test difficult use cases, such as no persistence after a restart or no replication (full replication with 2 nodes).
Are their debug tools to create trace recordings to isolate misbehaving clients?
We asked, "What have I failed to ask you that you think we need to consider with regards to containers?"
Ultimately a business problem to be able to show the value containers are providing.
We see a lot of companies and developers diving into containers without thinking about cost and technical debt over the long term.
Help communities to decrease risks and threats.
Security is not already taken care of.
What technologies will be made obsolete by containers?
The lack of visibility is a critical problem that is worsening as the instance lifespan reduces, components span private, and public clouds and external service dependency increase.
Figure 1a is a grouping of multiple instances and its “resolution” is low.
Golden signals such as latency, error rates, throughput and saturation metrics should be captured and displayed for nodes and edges.
Operations teams can then define the Service-level Objectives (SLOs) for the critical services and set alerts/paging for them.
This can greatly reduce the well-known problem of alert fatigue.
Application maps also highlight potential single points of failures and congestion hotspots.
Practically, sampling is enabled in production and heavy recording rules are often avoided unless root cause analysis is being performed.
This can be tricky when calls are made to legacy services or OSS components.
Also tricky when different languages are used, e.g.
End to end trace is often misleading as it does not capture the load on services (i.e.
Though with modern microservice patterns this is less of a problem as fewer API endpoints exist on services compared to monolithic applications.
Hard to track specific business transactions end to end without automatic trace ID correlation.
And wouldn't it be great if those vulnerabilities showed up automatically in GitHub Issues?
You can configure the types or severities of vulnerabilities that you want to be opened as tickets, but I suggest starting with everything.
Java Profilers: 3 Different Types and Why You Need All of ThemDebugging performance issues in production can be a pain and in some cases impossible without the right tools.
(Note: some profilers can work off thread and memory dumps in a limited fashion.)
All the profilers so far have been great for development, but tracking how your system performs in production is critical.
The .NET framework provides different types of synchronization strategies, including locks/monitors, inter-process mutexes, and specialized locks like the Reader/Writer lock.Regardless of why you have to synchronize your code or of the mechanism you choose to synchronize your code, you are left with a problem: there is a portion of your code that can only be executed by one thread at a time.In addition to synchronization and locking, make sure to measure excessive or unnecessary logging, code dependencies, and underlying database and infrastructure issues.Read the full eBook, Top 5 .NET Performance Metrics, Tips & Tricks here.Top PHP Performance MetricsYour PHP application may be utilizing a backend database, a caching layer, or possibly even a queue server as it offloads I/O intensive blocking tasks onto worker servers to process in the background.
For each iteration within the infinite loop, the event loop executes a block of synchronous code.
Node.js – being single-threaded and non-blocking – will then pick up the next block of code, or tick, waiting in the queue as it continue to execute more code.
Although it is a non-blocking model, various events that potentially could be considered blocking include:Accessing a file on diskQuerying a databaseRequesting data from a remote webserviceWith Javascript (the language of Node.js), you can perform all your I/O operations with the advantage of callbacks.
Common caching problems include:Loading too much data into the cacheNot properly sizing the cacheWhen measuring the performance of a cache, you need to identify the number of objects loaded into the cache and then track the percentage of those objects that are being used.
Apdex is Fatally Flawed i’ve been looking into a lot of different statistical methods and algorithms lately, and one particularly interesting model is apdex.
the formulathe core fatal flaw in the apdex specification is the formula used to derive your apdex index.
another thing that application performance veterans should pick up on here is that static thresholds stink.
static thresholds are notorious for causing    alert storms   (set too low) or for missing problems (set too high).
this manual adjustment philosophy leads to a lack of consistency and ultimately makes historical apdex charts useless if there have been any manual modifications of “t”.
if t=1 i’ll tolerate four seconds, but if t=3 no way am i waiting 12 seconds on a fast connection … which brings me to another point.
the notion of a universal threshold for satisfied/tolerating is bunk even for the same page (let alone different pages).
users are abandoning your website and shopping elsewhere since their check-outs are lost in oblivion.
apdex is not a good way to tell if your application is servicing your business properly.
apdex, like most other forms of analytics, must be understood for what it is and applied to the appropriate problem.
It’s a big problem and finding the balance between making sure everything is captured and not overloading everybody that’s responding to these issues is a key DevOps transformational issue.
When launching new products or services monitoring is often the last thing to be considered and often overlooked.
If they’re not putting them at the right thresholds, or not tweaking them the right way it creates a lot of noise, and that’s a very common problem that can be avoided with the right planning architecture.
Giving them data on the health of parts of their application is meaningless to them.
We are often asked to look at conversion rates, page response time, cart abandonment, and things like that.
Using a Single ConsoleA common problem is that engineers have to look at multiple screens to measure infrastructure and other data.
If it doesn’t impact the users, they’re probably not going to care.
If we are not adding new alerts, we may miss something.
But if we are not making sure the alerts are cleaner and more relevant, then we are going to wind up in the alert fatigue situation.
Pay as you go: Power up your Agileload by renting from 50 to 10000 virtual users.
End-to-end diagnostic features: The tool not only monitors the front-end but also the back-end to detect the reason for performance degradations.
JMeter allows you to perform various testing activities like performance, load, stress, regression and functional testing, in order to get accurate performance metrics against your web server.
This also means you won’t hit any trouble when using different local machines and cloud servers to run and create your tests.
In case the results exceed a threshold, it is possible to automatically mark the test(s) as failed.
Application PerformanceCan a better UX simultaneously deliver a worse user experience?
Ignoring the underlying performance bottlenecks and tricking the user with a UI band-aid is akin to placing tape over a crack in your drywall; you’re only covering up the symptom of an underlying problem.
As iOS developer Rusty Mitchell reported:  “A Facebook test indicating that when their users were presented with a custom loading animation in the Facebook iOS app, they blamed the app for the delay.
But when users were shown the iOS system spinner, they were more likely to blame the system itself.”Perceived performance techniques are part a larger trend in design beyond the world of software known as benevolent deception.
Adar said that these deceptions arise from the stress of opposing market forces.
The complexities must be simplified on the front end and deception is the most direct route.
A bigger gulf means bigger tensions, and more and more situations where deception is used to resolve these gaps.”Beyond Benevolent DeceptionOne step beyond benevolent deception is volitional theater, which refers to functions and displays that don’t correspond to the underlying processes.
They just give impatient riders something to do until the doors close on their own.
They shorten the perceived wait time by distracting people with the imitation of control.
System Images Deception: Reframe What the System Is DoingThis category of deceptive UX includes sandboxing.
In reality, the tutorial is a “safe” version of the environment that was built to be less sensitive to mistakes to simplify the learning process.
Mental Model Deceptions: Give Users a Specific Mental Model of How the System OperatesExplainer videos may be the worst offenders in this category since they apply dramatic and distracting metaphors in an attempt to engage distracted prospects.
These mental models help sales, but support teams then have to explain how things really work to frustrated customers.
Some of their insights and arguments are presented on the Dark Patterns website.
In the past, developers argued over whether application monitoring or network monitoring were more important.
Here’s how an AppDynamics engineer reframed the problem: “For me, users experience ‘business transactions’ — they don’t experience applications, infrastructure, or networks.
So, the question, ‘Monitor the application or the network?’ is really the wrong question.
This is how you can start solving the problems that matter most to customers.
That’s how you isolate the root cause of performance problems that matter most to specific users.
We’ve discussed this with application teams who have isolated problems related to runtime exceptions for Java-based applications in production, but they tended to gloss over those that didn’t break the application.
That’s a mistake we addressed in a series about top application performance challenges.
The slowness becomes contagious to all transactions being served by the application.
Don’t ignore them.
Users don’t lower their expectations to compensate for processing power.
Can a better UX simultaneously deliver a worse user experience?
Ignoring the underlying performance bottlenecks and tricking the user with a UI band-aid is akin to placing tape over a crack in your drywall — you’re only covering up the symptom of an underlying problem.
As iOS developer, Rusty Mitchell reported, “A Facebook test indicating that when their users were presented with a custom loading animation in the Facebook iOS app, they blamed the app for the delay.
But when users were shown the iOS system spinner, they were more likely to blame the system itself.”Perceived performance techniques are part a larger trend in design beyond the world of software known as “benevolent deception.”A Study of Benevolent DeceptionIn a report by Microsoft and university researchers, author Eytan Adar said his team found increasing use of “benevolent deception” in a range of software applications.
Adar said that these deceptions arise from the stress of opposing market forces.
The complexities must be simplified on the front end and deception is the most direct route.
A bigger gulf means bigger tensions, and more and more situations where deception is used to resolve these gaps.”Beyond Benevolent DeceptionOne step beyond benevolent deception is “volitional theater,” which refers to functions and displays that don’t correspond to the underlying processes.
They just give impatient riders something to do until the doors close on their own.
Designers refer to non-operation controls as “placebo buttons.” They shorten the perceived wait time by distracting people with the imitation of control.
Three Types of Volitional TheaterAdar’s research identified three major trends in the way that volitional theater is being used in modern UX application design:System Images Deception: Designed to Reframe What the System Is DoingThis category of deceptive UX includes “Sandboxing.” That’s where developers create a secondary system that operates in a way that’s different from a full application.
In reality, the tutorial is a “safe” version of the environment that was built to be less sensitive to mistakes to simplify the learning process.
Mental Model Deceptions: Designed to Give Users a Specific Mental Model About How the System OperatesExplainer videos may be the worst offenders in this category, since they apply dramatic and distracting metaphors in an attempt to engage distracted prospects.
These mental models help sales, but support teams then have to explain how things really work to frustrated customers.
Some of their insights and arguments are presented on the Dark Patterns website.
In the past, developers argued over whether application monitoring or network monitoring were more important.
Here’s how an AppDynamics engineer reframed the problem:“For me, users experience ‘Business Transactions’ — they don’t experience applications, infrastructure, or networks.
So the question ‘Monitor the application or the network?’ is really the wrong question.
This is how you can start solving the problems that matter most to customers.
That’s how you isolate the root cause of performance problems that matter most to specific users.
We’ve discussed this with application teams who have isolated problems related to runtime exceptions for Java-based applications in production, but they tended to gloss over those that didn’t break the application.
That’s a mistake we addressed in a series about Top Application Performance Challenges.
The slowness becomes contagious to all transactions being served by the application.
Don’t ignore them.
Users don’t lower their expectations to compensate for processing power.
This can cause problems for developers looking to enhance their virtual tool chest.
It’s enhanced log and error tracking feature make it stand out from the rest.
Identify which part of your application stack is the bottleneck and which web requests are being affected.
Capture critical details about what your code is doing with code profiling.
No access to pointers.
Pointers can cause problems for developers, and Zephir doesn’t allow them.
It becomes increasingly difficult to maintain solid infrastructure without a version control system in place.
This should not be a preferred solution or one that replaces the need for standard version control.
Easily destroy environments when they are no longer needed.
The initiative produced a PHP scanner that could check for vulnerabilities.
The previous version is still available but not supported.
Complete risk detection mechanism.
The following table indicates the key challenges:CategoryDescriptionTechnologyFull-scale environment is not available for performance testingProcessLimited time available for environment setup, load test script and data setup to execute full-scale load testProcessUnstable builds: Since development and testing activities go simultaneously break-off time is needed within a Sprint.
It is possible to detect anomalies, draw patterns from sprint level test results, and find defects using ML techniques.
Continuous Performance Testing StrategyIt has been observed that 70-80% of performance defects actually do not require a full-scale performance testing in a performance lab.
These defects can be found using a signal user or small scale load test in the lower environment if a good APM tool is used to profile and record execution statistics at each stage of SDLC.
The target of this shift left approach is to find and fix all code related performance defects as development is in process.
These should be lower of insolation or component level test.
Performance analysis, data trend, abnormally detection, and cutting down performance analysis by more than 50% if we use machine learning algorithm.
In Agile methodology, since full-scale performance test is not done all the time in a performance lab, production performance testing is highly critical to make sure the system can handle user traffic without any issues.
The percentage of the user set is increased gradually if there are no issuesIf there are issues found, either we do not increase the load further or new features will be pulled back.
Stop the test if real user experience degrades below the predefined threshold.
Need for detailed triage meeting in WAR rooms is minimized by tracing the sessions, pinpointing exact issuesConsistent metrics to analyze performance issues across the environment and it is accessible to development, testing teams, business users all the time.
The following diagram depicts how APM tools help in every Agile lifecycle stage:Performance Risk AssessmentUser story performance risk assessment not only helps identify key user stories which should be performance tested but also helps decide which user stories can be given lower priority.
The following table indicates the parameters that decide the probability and impact of failure.
The higher the risk factor, the higher the priority:Probability/ImpactParameterUser Story # 1 User Story # 2 Factor#1Degree of change from the previous sprint11Factor#2Stringent Performance SLA54Factor#3No.
of Users impacted43Factor#4Public Visibility and impact on the brand43Weighted ImpactImpact of Failure42Risk FactorRisk Factor208Right SkillsetIn Agile manifesto, performance engineers need to be a good developers with data science tools expertise to apply ML and automate tasks, analyze, and derive useful insights from data.
Software containers are a standard unit of software that isn’t affected by what code and dependencies are included within it.
Each container has its own isolated user space, and it’s possible to run many different containers on one host.
Containers are isolated in a host using two Linux kernel features: Namespaces and Control Groups.There are six namespaces in Linux and they allow a container to have its own network interfaces, IP address, etc.
Once the container is running, you can manage it, interact with the app and then stop and remove the container when you’re done.
Consider what’s happened with augmented reality (AR) in this year alone.
Defining the scope of problems in execution, runtime architecture, and communications.
Tools for going deeper into the components identified as sources of the problems.
At the intersection of technology and finance, the role of the CIO has become the locus for all the most critical data analytics.
Attaching and detaching microservices from the main functionality of the application is never as easy in practice as it is in theory.
Struggles with team cohesion.
Many developers have developed their skills in isolation and may have difficulty aligning their work habits with a tighter team structure.
Everyone suffers when there is internal friction between functionality and security.
PHP Performance Crash Course, Part 1: The Basics we all know performance is important, but   performance tuning  is too often an afterthought.
one of the biggest annoyances in writing php is having to write a long list of needed includes at the beginning of each script (one for each class).
do blocking work in the background   often times web applications have to run tasks that can take a while to complete.
the solution is to queue blocking work to run in background jobs.
leverage     http caching    http caching is one of the most misunderstood technologies on the internet.
don’t worry, i’ll wait.
seriously, go do it!
they solved all of these caching design problems a few decades ago.
WordPress Gets Trounced by GhostI wanted to see what all the Node.js hype was about so I decided to run some head to head load tests using Ghost (Node.js) and WordPress (PHP).
The results were incredible with Ghost soundly trouncing WordPress.
There is a new blogging platform that was recently made available to the general public called Ghost.
The SetupTo provide a little background, Ghost is just a blogging platform and nothing more while WordPress is a full up CMS.
I wanted to make this comparison as fair as possible so I limited my load testing scripts to executing against only the blog pages.
I wanted to test the most common configurations so I used NginX to front end Ghost and used Apache to front end WordPress.
I had both Ghost (listening on port 80) and WordPress (listening on port 8080) running at the same time but only applied load to one blogging platform at any given time.
Reference slides 20-23 of the following presentation for the details on using Sproxy https://speakerdeck.com/dustinwhittle/agile-performance-testing-checklistI ran Sproxy against both Ghost and WordPress and ended up with my list of URLs.
I ran siege for 10 minutes with 100 concurrent connections and a 1 second delay between batches of web requests.
The results are shown below…Siege load test results for Ghost with Nginx under heavy load.
As you can see from the output shown above, Ghost with Nginx outperformed WordPress with Apache by about 678% when looking at total transactional throughput over a 10 minute test.
During the load test, Ghost ran with only 1 process and Nginx had a total of 2 processes.
To find out I dropped the number of concurrent connections to 10 and set the delay between batches of connections to 5 seconds.
From a CPU perspective Ghost only consumed ~4% on average during this test while WordPress consumed ~30% on average.
Siege load test results for Ghost with Nginx under light load.
The ConclusionGhost is way faster and can handle way more load than WordPress while also consuming much less CPU resource (Ghost also has considerably less functionality than WordPress but that’s not relevant for the purpose of this test).
In a future blog post I am going to monitor Ghost with Nodetime and will monitor WordPress with AppDynamics.
By unifying developer and operations groups and emphasizing monitoring and automation, companies are increasing the speed and frequency of deployments and recovering faster from failures.
Historically, however, the failure of DevOps initiatives is high.
This adaptable methodology is one that I have developed over years of trial and error working as a performance engineering lead and architect.
Lean-Agile is rooted in Lean Manufacturing and focuses on eliminating waste.
By reducing waste, you will increase the speed at which you get things done.
Think about how much time systems in QA, UAT, and other lower environments sit idle.
If you are like many organizations there is an immensity of wasted compute resources that can be converted into productive time by implementing and automating continuous testing.
On a NASA mission, the failure of even a small component can be catastrophic.
They must have a positive or negative correlation coefficient to other metrics.
In the modern software-defined world, when companies implement application performance monitoring absent a well-defined strategy this is an all-too-common problem faced by DevOps.
Organizations will eliminate waste and achieve "Leanness" by continuously running a multitude of small, repeatable, concurrent tests.
Hobbled by Old HabitsDuring the days of monolithic applications, the approach of discreetly testing individual components would have been hard, if not impossible.
Instead of testing individual services of an application with fast, repeatable performance tests and leveraging mocks for dependencies, many organizations run performance tests against the entire system as if it were still monolithic.
When these large-scale tests do break, it's often hard to pinpoint the failure because they cannot be replicated consistently.
Third, you want to replace downstream services with mocks wherever possible.
The 6 Most Common Performance Testing Mistakes, and How to Fix ThemAs a performance testing consultant for the last 10 years, you could say that it's second nature for me to look for performance patterns.
One of the patterns I have observed over my career is that, regardless of the project size, company, or the technology being used, the same types of performance testing mistakes get made over and over and over.
Unfortunately, however, those shortcuts can lead to costly performance testing mistakes and oversights.
Inadequate User Think Time in ScriptsHitting your application with hundreds or thousands of requests per second without any think time should only be used in rare cases where you need to simulate this type of behavior – like a Denial of Service attack.
What are the transactions that have a high business cost if they were to fail under load?
Setting Up Inadequate Infrastructure MonitoringYour execution results like throughput, transaction response times, and error information aren’t overly helpful unless you can see how your target infrastructure is coping with the scenario.
It's a common problem — I have heard many testers ask why their response times are taking minutes instead of seconds.
The problem can lie either in the load generation or the target application infrastructure.
How do you solve this problem?
This enables you to view system resource utilization while running your tests, ensuring that no bottlenecks are present on the load generation side.
Using Hard Coded Data in Every RequestUsing the same data in your HTTP request for every user is not a realistic usage scenario.
Here’s common scenario:A load test runs with a target number of users, and the tester see that response times and error rates are within acceptable ranges.
Expected throughput however, is lower than anticipated.
How can this be when your load testing platform is reporting very few transaction related errors?
Once you have a test with transaction response times, error rates, and throughput rates all in the expected zone, are you in the clear?
This might not be much of an issue with a single transaction, but multiply this over 1,000 concurrent users and it can severely impact your system's responsiveness.
To pinpoint if you are overloading a load injector, look for error logging, out of memory exceptions, and CPU or network utilization stats.
With remote work, COVID-19, and the extra stress of decision fatigue leading to reduced cognitive capacity, our own ability to deliver has changed.
Customers want new features and reliability, and existing processes may not support this.
According to AppDynamics’ “The Agents of Transformation Report 2020: COVID-19 Special Edition,” there are three main reasons for the accelerated urgency of digitalization:Rise in-app complexity: 80% of teams struggle with spikes in traffic and lack of observability.
Organizations that don’t pivot in this direction risk losing the market.
55% of respondents said slowness was the most frustrating problem when using digital services.
They’ve only lost a few seconds.
Additionally, today's environments are no longer simple, monolithic architectures.
This increases the risk of having islands of dissociated data.
Unfortunately, tools can add to this tsunami of data, creating new islands of data.
Customer experience lives across these islands and hides within that data tsunami.
And insight into this data is hard to glean.
Humans achieve this with a central nervous system (CNS).
Observability as the Central Nervous SystemObservability is the central nervous system for the digital enterprise.
Anomalies are recorded via your error budget.
Error budgets suggest the amount of unplanned system failure that you can have for your SLO.
SLOs capture failure within a timespan and use those values to show where your error budget stands.
If slowdowns reach a rate your customers will find unacceptable, it will trigger an alert.
Developers don’t want to get blocked on releases by operations.
Operations: Operations wants to limit business risk.
Yet, SLOs don’t come without their challenges.
If your SLO starts to drop below your threshold, you get an early indicator of a real problem in your system.
At what point do I stop working on features and start working on reliability?
One way to know this is by consulting your error budget.
An error budget should contain a policy dictating when to stop feature work.
This could happen after two consecutive windows of error budget depletion.
Embarking on the SLO JourneyAccording to Blameless’ own engagements with various organizations, over half of SLO implementation initiatives fail the first time.
This is often because they lack the right process and culture.
Document the user journey: This is where you’ll map out the critical services for your user.
What points are most visible and painful to them?
Define error budgets and policies to make sure reliability remains a priority.
If a human doesn’t need to intervene, no one should get paged.
Blameless CultureSLOs are difficult to get right, and failure is inevitable.
Additionally, you’ll need to set guardrails that work against blame and attribution.
There is no such thing as human error; each error can be traced back to a systemic failure.
According to APPDYNAMICS, a part of Cisco, 81% technologists said in a report that COVID-19 has created the biggest technology pressure for their organizations.
Due to a single code base and single-tiered applications, they were more prone to various risks.
Security:Old systems lack security controls due to unsupported versions, old tools, traditional architecture, and obsolete methods.
Lack of collaboration and high cohesiveness increases the time-to-market of application and cost.
Single Point of Failure:If any module, function, or functionality fails, the entire system fails.
These systems are difficult to scale up due to large codebase and performance problems.
It creates lots of code rework and may result in a delayed timeline, response, and disturbed workflow.
Faster decisions are important to avoid delayed time-to-market.
There was no automation and a decentralized environment increased dependency on each other to get the job finished.
I decided to publish a few parts here to see if anything triggers a discussion.It would be published as separate posts:–Introduction (a short teaser)–Cloud–Agile–Continuous Integration-New Architectures (this post)-New TechnologiesCloud seriously impacts system architectures that has a lot of performance-related consequences.First, we have a shift to centrally managed systems.
Mitigating performance risks moves to SaaS vendors.
It will allocate resources automatically – but you need to pay for them.
It makes it difficult to analyze results as the underlying system is changing all the time.
When the full set of APM tools are used properly they can provide alerting and trending information so you find out whether the change to the application or system did no harm, some harm or really caused a problem.
Unfortunately, these tools are brought in to a company as a reaction to a problem and not brought in before there is a problem.
It starts with the developer, engineer, or operations person or team, who have to solve the problem.
So, the people on the ground are stuck, no APM tool.
No one really knows why the system is slow, in fact, they usually can’t even tell you what they mean by slow, is it 5 seconds, 10, or 42 seconds.
Don’t forget call centers.
Payback period: There will be an initial investment required ($100K), recurring charges ($15K), and one time training expenses.
They are no longer selling hardware, they need to adopt to the software model.
Industrial needs ruggedized appliances with out of the box connectivity, using low power that can be installed in a remote site.
They need to see how IoT impacts business.Don’t ignore big data.
How to take dumb devices, add sensor technology, make smart and collect data to a central location wirelessly via IoT gateways (e.g.
Know how to scale at the lowest cost.Intelligent compression of IoT data.APIs will be everywhere.
Risk assessment platforms are not the best model.
You can’t stop serving the software after you ship.
Reliability becomes more important with mission critical IoT networks.LE Bluetooth connected.
This used to be designed sequentially, sold to channels, and forgotten.
Operating systems and the development platform is on top.What technical solutions are you using to solve real-world problems with IoT?
It’s at a nascent stage.We're going through the storming, forming and hype to the real apps that solve business problems, specific apps in specific verticals.
All will see greater efficiency at lower cost.We don’t do individual devices, we take a bunch of devices ban them together in a profile.
Reduce costly downtime.
Qualcomm's core network capabilities provide a service where developers can test and validate against the network.Standardization is the biggest problem with chip sets and operating systems.
Changes in Performance and MonitoringTo gather insights for DZone's Performance and and Monitoring Research Guide, scheduled for release in June, 2016, we spoke to 10 executives, from nine companies, who have created performance and monitoring solutions for their clients.Here's who we talked to:Dustin Whittle, Developer Evangelist, AppDynamics | Michael Sage, Chief DevOps Evangelist, Blazemeter | Rob Malnati, V.P.
We want to understand the user need and the user journey by capturing every user every where they are accessing your app or site.When we have a cloud there is no access to the full stack, only access to the API is made available.
We identify the problem but do not tell the client how to fix it, we use APM for that.
You lose visibility with AWS or Heroku.
Host metrics alone are not the answer since you can no longer see the host.
Not all were great.
I'm not sure  how this relates to PowerShell DSC and how it compares to its  open-source equivalents as Vagrant and Puppet.
You can do yourself too, for instance  by using AutoRest to  generate code from a swagger-enabled rest API that hides the HTTP  ugliness.
The speaker mentioned that you can also use it to  monitor iOS and Android apps, but I missed the explanation on how that  would work.
It  tries to ignore debugger overhead as much as possible and is available  in every edition of Visual Studio 2015.
Crash rate is the average crashes per app loads (an app load is the launch of an app).
The typical crash rate is 1-2%, but this varies widely depending on the type of app, its usage, maturity, etc.
Users may have some tolerance for slower response times, but the data generally shows that anything over 3-4 seconds total response time and the majority of user (60% or greater) will abandon the transaction and may even delete your app altogether.
It is critical because you want to make sure that as the load increases, your application performance doesn’t degrade.
Network errors – network errors are typically the service provider or HTTP errors seen by the app when the app is interfacing to a networked service.
Network errors can lead to crashes or slow response time (with multiple retries).
However, the metrics are very limited – with crash data only – and not real-time enough for enterprises to quickly troubleshoot and fix the issues.
Skills shortages were the critical gap, and the talent shortage is being felt by CIOs.
Real World Problems Being Solved With Performance and MonitoringTo gather insights for DZone's Performance and and Monitoring Research Guide, scheduled for release in June, 2016, we spoke to 10 executives, from nine companies, who have created performance and monitoring solutions for their clients.Here's who we talked to:Dustin Whittle, Developer Evangelist, AppDynamics | Michael Sage, Chief DevOps Evangelist,Blazemeter | Rob Malnati, V.P.
Product Management, Dyn | Andreas Grabner, Technology Strategist, Dynatrace | Dave Josephson, Developer Evangelist, and Michelle Urban, Director of Marketing, Librato | Bob Brodie, CTO, SUMOHeavy | Christian Beedgen, CTO and Co-Founder, Sumo Logic | Nick Kephart, Senior Director Product Marketing, ThousandEyesWe asked these executives, "What real-world problems are you solving with performance and monitoring?
"Here's what they told us:We have a client in  Sweden that was building a monolithic enterprise but the app was no longer able  to scale the way it needed to.
They broke the monolith into microservices but it failed due to integration issues.
Shifting quality to the left of the build cycle to find and correct  problems as early as possible.Video is a  disproportionate amount of internet traffic in the evening due to Netflix and  streaming live events.
We help clients  know where to deploy in the cloud and automatically balance traffic loads while  providing mean time to innocence.Movie Tix had a  problem with their site when Star Wars was announced for advanced purchase.
They used our product to monitor and  manage demand in real-time.
Users of AWS  monitor performance with AWS Cloudwatch which provides a very limited view.
Any IaaS will be  frustrated with traditional monitoring because it does not scale.
More customers are moving  to IaaS or app synching multiple data sources.Thee ways: 1) Reactive – reduce  mean time to resolution while running into an SLA violation.
3) Proactive – find automatically  in the data, look for an emerging pattern that seems unclean and may cause a  problem down the road – risk identification.
Provide warning ahead of time that  something might be wrong.What problems you are solving with performance and monitoring?
Most Common Issues Affecting Performance and MonitoringTo gather insights for DZone's Performance and and Monitoring Research Guide, scheduled for release in June, 2016, we spoke to 10 executives, from nine companies, who have created performance and monitoring solutions for their clients.Here's who we talked to:Dustin Whittle, Developer Evangelist, AppDynamics | Michael Sage, Chief DevOps Evangelist,Blazemeter | Rob Malnati, V.P.
There are ten common problems I see causing apps to crash: 1) database access: loading too much data inefficiently; 2) microservice access: inefficient access and badly designed Service APIs; 3) bad frameworks: bottlenecks under load or misconfiguration; 4) bad coding: CPU, Sync and Wait Hotspots; 5) inefficient logging: even too much for Splunk & ELK; 6) invisible exceptions: frameworks gone wild!
; 7) exceptions: overhead through Stack Trace generation; 8) pools & queues: bottlenecks through wrong sizing; 9) multi-threading: locks, syncs & wait issues; and, 10) memory: leaks and garbage collection impact.
Take and shift everything left in development to correct problems  earlier before you check the code in to production.
We can detect the patterns of problems and prevent them.People do not understand the implications of performance on their business.
Most people have no clue how to measure performance.
We discover what the problems are and use tools to dig in and analyze.
We’ll use our tools on our customers’ systems for a month to identify the technical problems and the business problems are typically cultural.
Businesses try to solve problems from the top down.
People that are affected are afraid to speak up.
You need to talk to CSRs, helpdesk and production to learn how people are solving problems.
We make it as easy to manage as much as possible avoiding noise at the micro level while identifying problem areas that need to be addressed.
Maybe databases but the stack is so deep and there are so many elements it’s difficult to identify a consistent area of concern.
People are consistently surprised by what becomes a problem.
We’re able to find problems more regularly with a combination of performance monitoring tools.The database does not account for the amount of traffic for capacity planning.
This makes it possible to identify bad design.Latency.
Be able to quantify the latency of the pieces you are working on and predicting where problems may be given the architectural design.Customers want to solve the domain of operational monitoring in one fail swoop but you need two different types of products to do that because they’re two different jobs.
There’s no longer a single box.
The smoking gun is almost always something you didn’t expect – much more subtle like a software cache missing, running out of memory, very application specific, a server can’t talk to another server due because the LDAP is down, service degradation.What are the most common issues you see affecting performance and monitoring?
Safari has been slow to provide performance data and need to respond more quickly or people will stop using the browser due to its poor performance.
JIRA for problem resolution tracking.
Stop wasting time, look at the right stuff and automate monitoring.
We want to understand what the user was doing when they experienced the problem.
One of the hard core differences between companies is the attitude of the people you show a graph to.
Don’t bury yourself in technical debt.
Don’t get behind, it’s hard to catch up.The skill set has not changed as much over time as the lens through which the developer needs to see things.
Know whether or not you’re introducing performance degradation.
Work with ops more closely and understand operational issues.Three suggestions: 1) When logging adopt a structured format, avoid XML if you can.
3) Logging is a low overhead way of tracking.
Our research indicates that up to 84% of consumer would delete an app after a crash (insert link to study here).
For even more data about the consequences of poor mobile and web application performance, check out this DZone article and infographic including this stunner that 63% of consumers “wouldn’t do business with a company via any channel after having transaction problems on [a] mobile device.”Given the brutal nature of the competition in the mobile application market, it’s imperative for all enterprises (not just retailers) to provide a flawless, engaging, and delightful digital experience.
Performance is often taken for granted or just assumed or something that developers are responsible for, rather than being considered a key strategic requirement equal in importance to the other elements of the mobile application strategy.But if the consumer has a poor experience as a result of a crash, and error, or even just a slow response time due to a back-end software problem with a payment, inventory, search, commerce, fulfillment or whatever other corporate business transaction system, then no amount of fancy UI/UX is going to rescue that customer interaction.Even more importantly, it’s not just about the mobile and web apps, because those apps are just the front end to your core enterprise applications.
What if slow performance on your web or mobile app is actually due to a back-end system problem with commerce, reservation, order, fulfillment, scheduling or some other system?All your customer sees is that there was a problem or they waited until they switched to a competitor app instead.
If enterprises don’t have a fully thought out digital experience monitoring strategy, especially including mobile application performance monitoring, then they are going to have lower conversion rates, higher bounce rates, and poorer KPIs.While it’s already too late to implement mobile application performance monitoring (APM) strategy for the back to school rush, retailers, and other enterprises still have time to deploy mobile APM in place for the upcoming holiday seasons if they start now.Apropos, Gartner has just recently published the 2016 market guide for Mobile Application Performance Monitoring (subscription required).
Before APM, developers would primarily leverage code profilers, which were essentially limited to pre-production phase of software or application lifecycle.
Since the overhead introduced by code profilers was unacceptable for production environments, the first generation of APM tools such as those from Wily Technologies and Dynatrace were not widely used for always-on production monitoring use cases.
However, APM is extremely limited when it comes to monitoring a new breed of applications called “data-first” applications.
This infrastructure could be either in a public cloud, such as Amazon Web Services or an on-premises datacenter.APM for Data-First ApplicationsAPM tools come up woefully short when applied to the data-first world given their inability to look beyond the code layer.
The first problem is that they can’t see into the inner workings of distributed data frameworks.
For example, detecting an Apache Kafka broker as a remote call is not good enough.
To measure app health, it is vital to track the consumer lag on a per topic basis where topic represents the app level data partition.Similarly, alerting whenever there are under-replicated or offline partitions is critical to monitoring overall Kafka cluster health.
However, this provides no additional value given critical performance metrics are already available on these frameworks via common endpoints such as JSON and JMX.
Finally, there are overhead and transaction explosion concerns to worry about.The second problem is the lack of correlated troubleshooting in data-first applications.
Since distributed transaction tracing is not possible, there is no correlated, end-to-end performance view that can serve as the foundation for troubleshooting.
OpsClarity applies data science constructs such as anomaly detection and event correlation to rapidly troubleshoot issues using common concerns such as throughput, latency, error rate, back pressure etc.While OpsClarity can certainly co-exist with an APM tool in a data-first application, we see customers choosing to cover the application code layer by writing custom metrics to the OpsClarity Custom Metrics API.
Cloud Migration Tips Part 4: Failure Breeds SuccessWelcome back to my series on migration to the cloud.
How do you know if your application flow changes over time and causes problems?
Simple, when there’s a problem you waste a bunch of time trying to figure out what components were involved in the problematic transaction so that you can isolate the problem to the right component.
Response Times: Slow sucks!
You need to stress your new cloud application to the point of failure and understand how to respond BEFORE you set users free on your application.
Fail when it doesn’t matter so that you can success when the pressure is on.
Bugs: The Secret Killer to 5-Star Mobile App ReviewsBugs.
Once upon a time – and not so very long ago – that word had only one meaning: annoying little critters that crawl, sting, bite, and can generally make your life a misery.
However, both terms represent a common theme: annoyances.
Bugs can wreak havocThe little bugs that infect computer code may not have wiped out millions of human lives – at least not yet (a computer bug reportedly nearly triggered World War III in the 1980s).
But software errors certainly have caused deaths on a smaller scale through incidents such as aviation, traffic, and medical accidents.
For the most part, though, death isn’t a likely consequence of bugs infesting the code of modern apps.
But other forms of devastation?
There’s also NASA, whose $327 million Mars Climate Orbiter disintegrated entering orbit around the Red Planet, the result of a coding error.
Or check with Intel, they released an entire line CPUs in the 1990s containing a bug that caused an arithmetic error.
Lots of red faces and an epic PR fail in addition to the cost in cash.
The point of these examples is to show no matter how small the issue, the results can be catastrophic.
When bad news can be good newsSure, it’s bad to have bugs and issues in your application, however, they’re nearly unavoidable.
You can turn the bad news into good news by finding those bugs during the development process.
Solving the most critical issues in the development and QA process is imperative.
In the same IDC report mentioned above, we found 13 percent of application failures last more than a full day.
Doesn’t sound that terrifying until you realize the hourly cost of these downtimes ranges from $500,000 to $1 million PER HOUR.
These aforementioned bugs now become nasty when they’re affecting your App Store rating.
In a different study we conducted with the University of London, we found 86% of users deleted an app after a poor performance (read the report here).
Major brands can’t simply iterate their product based on user feedback; they can’t afford that first wave of bad reviews.
This missing group made sense, because they most likely exhibit in their own industries, such as in gaming conventions, instead of coming to a generalized cloud computing event.
PayPal was undergoing an Agile transformation and our small team of historically matrix aligned, specialty engineers, was challenged to adapt.
Plan for longer soak and stress tests as part of the release process, but have one or more per-sprint, and even nightly, performance tests that can be continually executed to proactively measure performance, and identify defects as they are introduced.
Embrace tooling, but consider these factors impacted by Agile development: Can your toolset automatically, and continuously discover, map and diagnose failures in a distributed system without asking you to configure what methods should be monitored?
If the overhead of your tooling degrades performance under high loads, it’s ineffective in a performance environment.
Don’t let your performance monitoring become your performance problem.
if you’re not as well versed in the term devops as you want to be i suggest you read my “    devops scares me   ” series.
worst developer on the planet the real question in my mind is where do you draw the line between dev and ops.
before you get upset and start yelling that dev and ops should collaborate and not have lines between them let me explain.
i am a battle hardened operations guy.
simply put, i’m probably the worst developer on the planet.
i would never trust myself to write code in any language to run a business.
experts needed operations is a mix of implementing standard processes, planning for the future, and sometimes working feverishly to try and solve problems that are impacting the business.
bad operations decisions can cost companies a lot of money and significant reputation damage in a short period of time.
bad code written by inexperienced developers can have the same effect but can be detected before causing impact.
find the application problems and bad code before they find you by    trying appdynamics for free today   .
Because there aren’t 10 newbie mistakes, there are about one million.
I’m more interested in subtle mistakes and problems.
There’s only one mistake here.
Yes, because things can go terribly wrong.
Here’s how wrong they can go:http://www.intertech.com/Blog/15-worst-computer-software-blunders/9: Top 10 Java People You Should KnowYou think this list is incomplete?
Java thread concurrency issues and inadequate application or middleware resource "budgeting" such as timeouts are next.
Finally, combine JVM thread dump analysis and APM products together for faster root cause analysis and deep insight of your application, middleware and JVM performance hot spots.
For production system troubleshooting, APM products, JVM thread dumps and GC/heap runtime data analysis through Java VisualVM and Memory Eclipse Analyzer are part of my main Arsenal and Survival Kit.
Key to providing this personalized service is software and if, at any point, the associated applications have a performance issue then customer  experience is ruined.
The result; the customer visits a different coffee shop next time and probably vents their frustration on social networks.
This impacts the coffee shop’s revenue and damages their brand.
With the rise of IoT, more and more devices will be connected and rely on critical, or even life-dependent, devices.
We asked them for their suggestions for developers working with Java.Specifically we spoke to:Anthony Kilman, Tech Lead, AppDynamics | Gil Tene, CTO, Azul Systems | Bhartendu Sharma, Vice President of Operations, Chetu | Charles Kendrick, CTO and Chief Architect, Isomorphic Software | Fred Simon, Co-Founder and Chief Architect, JFrog | Ray Auge, Senior Software Architect, Liferay | Michael Hunger, Lead Developer Advocate, Neo Technology | Brandon Allgood, PhD, CTO, Numerate | Dr. Andy Piper, CTO, Push Technology | Jonas Bonér, Founder and CTO, Typesafe | Toomas Rὅmer, CTO and Founder, ZeroTurnaround | A consistent theme is the size of the ecosystem and the amount of pre existing code and programs that are available in the libraries and the user groups.
The most critical infrastructures are built in Java or a JVM language.
And, it’s always possible to shoot yourself in the foot by overcomplicating what you’re building.
We’re currently building technology pre designed to fail due to evolutionary changes in the industry.
There’s no ego in code.
Put yourself at risk by contributing.
Specifically, we find that if a developer has spent too much time with Java to the exclusion of other languages, they tend to expend heroic efforts structuring code so that it’s possible for the Java compiler to check more error conditions.
Most developers are not good at building a new thread.
Java 8 has made some strides in making JavaScript more part of the JVM, but it is still a struggle to use other languages, with a lot of issues in the areas of error reporting and debugging.Agree or disagree?
Real-World Problems Being Solved by JavaWe talked with 11 business professionals who have been involved with the Java ecosystem for most, if not all, of their career and asked them about some of the real-world problems being solved by Java.Specifically we spoke to:Anthony Kilman, Tech Lead, AppDynamics |Gil Tene, CTO, Azul Systems |Bhartendu Sharma, Vice President of Operations, Chetu |Charles Kendrick, CTO and Chief Architect, Isomorphic Software | Fred Simon, Co-Founder and Chief Architect, JFrog | Ray Auge, Senior Software Architect, Liferay |Michael Hunger, Lead Developer Advocate, Neo Technology |Brandon Allgood, PhD, CTO, Numerate | Dr. Andy Piper, CTO, Push Technology | Jonas Bonér, Founder and CTO, Typesafe | Toomas Rὅmer, CTO and Founder, ZeroTurnaround | Here’s what they told us:Big server side, Big Data, large websites, and it serves as the platform for Twitter.
Not a substantial shift since 1995.
Only since IoT and mobile are we seeing more/different problems being solved by Java.
It has been evolving and as the product becomes more complex, it became difficult to navigate the maintenance field and stay nimble.
Reactive applications are elastic and power mission critical industries.
What are some of the real-world problems you see being solved by Java?
There is cultural inertia that’s made worse by the acceleration of technology development.
Speed to market bogged down by how hard it is to release software.
Companies need to adopt a DevOps culture where they ship less and ship less risk.
Environment management is a constant struggle in web application development.
Lost time due to added complexity in managing environments creates inefficiencies that lead to higher cost, lower job satisfaction, and degraded quality.
Keep up with what’s going on so you don’t isolate the employees.
Providing a seamless experience across form factors is extremely difficult to execute well.
We see many problems getting apps configured and built.
No longer self-contained.
OtherBad technology choices early on can easily lead to a complete re-work of the application.
What are the most common problems you see web application development?
To gather insights on the state of web application development today, we spoke with 12 executives who are familiar with the current state of the industry and asked them, "What are some real-world problems solved (use cases) by web applications developed by you or your clients?"
3) A major chip manufacturer uses several of our applications to collaboratively analyze performance and defects in batches of chips, to refine their manufacturing processes.
They have blue tooth monitors that link to the smartphone that verify the patient is taking the recommended action (weighing, checking blood pressure, taking medications).
We need to check for defects earlier.
Headless example with our plug-in through HTTP watches the coder scanning, and flags errors and suggests corrections, with microservices in the cloud that can scale to maintain a performance equivalent.
When newmedia began working with us, they had consistent problems engaging new developers and delivering reliable hosted products.
Feedback cycles were slow because there was no concept of continuous delivery.
Continuous pipelines made it trivial for employees to move code into staging or production environments.
At first, developers jumped on mobile-specific frameworks that didn't work on desktops, then onto so-called "mobile first" frameworks that had a desktop UI, just a very poor one.
Enterprise developers are slowly waking up to the fact that, for enterprise apps, desktop use is still very common, and underpowered components are not acceptable on the desktop or even on tablets and larger phones.
Why mess with VMs, deploy a bunch of software packages, write tons of glue logic and abstraction layers, figure out end-to-end security, debug, and tune all that?
NO, I don’t want more VMs or vDisks and don’t sell me HCI.
What I do want is an Amazon-like service, where developers can use the platform directly and leverage pre-integrated Lego building blocks and where I can jump from development to production or scale in a few clicks.
For example, when it comes to latency or performance-sensitive workloads, or when your company is big enough and it’s too expensive, or when you want to avoid getting locked-in.
Many IoT workloads are not that cloud friendly and will, therefore, get handled close to the edge.
Yes, they also go after the “edge,” but with more hardware and no real software story to go along with it.
No, not the 10th converged offering from Chad’sVCE group, or a “cloud-native” washed HCI with VMware + Pivotal.
Another challenge is that sales teams are compensated on and know how to sell boxes, not services or solutions, let alone AI.
Lost in the push to build such an integration platform, though, is how difficult it is to do.
Keep no secrets: In a DevOps environment, processes happen fast and some mistakes will be made.
Systems and processes are too complex for teams to be isolated with their own platforms.
Covering security holes with firewalls is not good enough.
Criminals are like rodents, and they can squeeze through the tiniest vulnerabilities in your code to steal data or take advantage of your environment.
Just be mindful that the road to get there is not easy.
Quick Word on OWASPThe OWASP Foundation typically publishes a list of the top 10 security threats on an annual basis (2017 being an exception where RC1 was rejected and revised based on inputs from market experts).
The goal is to make IT professionals aware of the most prominent threats out there - based on the technology choices prevalent in the industry and the kind of security attacks being encountered - and rank them based on various factors such as Exploitability, Prevalence, and Detectability.
#Security ThreatDescriptionMitigationA1InjectionInjection flaws, such as SQL, OS, and LDAP injection, occur when untrusted data is sent to an interpreter as part of a command or query.
The attacker’s hostile data can trick the interpreter into executing unintended commands or accessing unauthorized data.
In MuleSoft, it can be enforced as a security policy on the MuleSoft API Manager:a) JSON Threat Protection b) XML Threat ProtectionA2Broken Authentication and Session ManagementApplication functions related to authentication and session management are often not implemented correctly, allowing attackers to compromise passwords, keys, session tokens, or to exploit other implementation flaws to assume other users’ identities.
Attackers may steal or modify such weakly protected data to conduct identity theft, credit card fraud, or other crimes.
External entities can be used to disclose internal files using the file URI handler, internal SMB file shares on unpatched Windows servers, internal port scanning, remote code execution, and denial of service attacks, such as the Billion Laughs attack.
In MuleSoft, it can be enforced as a security policy on the MuleSoft API Manager for XML Threat Protection.
A5Broken Access ControlVirtually all web applications verify function level access rights before making that functionality visible in the UI.
All these settings should be defined, implemented, and maintained as many are not shipped with secure defaults.
XSS allows attackers to execute scripts in the victim’s browser which can hijack user sessions, deface web sites, or redirect the user to malicious sites.
Enforced as a Security policy on MuleSoft API Manager - including JSON Threat protection, XML threat protection and CORS policy (if need be).
A8Insecure DeserializationInsecure deserialization flaws occur when an application receives hostile serialized objects.
Insecure deserialization leads to remote code execution.
Even if deserialization flaws do not result in remote code execution, serialized objects can be replayed, tampered or deleted to spoof users, conduct injection attacks, and elevate privileges.
So, if exploited, they can cause serious data loss or server takeover.
Applications using these vulnerable components may undermine their defenses and enable a range of possible attacks and impacts.
A10Insufficient Logging and MonitoringInsufficient logging and monitoring, coupled with missing or ineffective integration with incident response allows attackers to further attack systems, maintain persistence, pivot to more systems, and tamper, extract or destroy data.
The problem is that most developers do very poor requirements analysis.
This leads to a costly retrofit or full project reboot.
Development costs go way up and deadlines are badly missed.
Firms need to look for a balance: avoided the outdated, overblown frameworks from the largest vendors, but also avoid the technology-du-jour that isn't mature, doesn't quite work everywhere, and may be abandoned in the near future.
UI/UX/CXSEO is critical to the development process.
Every member of the team needs a clear picture of the business objectives and problems addressed by the application in addition to a firm understanding of how the team is expected to work together.
Analyze where the app is failing and how much it is hurting your business.
No breaking changes as we move forward.
The barrier to entry is development and installation.
There is a lot of confusion about the term APM.
This type of monitoring provides information about the response times and errors end users are seeing on their device (mobile, browser, etc...).
This information is used to troubleshoot complex code issues that are responsible for poor performance or errors.
1) Establishing a BaselineThis test is an extension of App Decomposition, and with this test, we want to look at the performance of the application when the application is under no load whatsoever to derive a baseline.
With every service running in a loop, I have baselines for the fastest and slowest calls and can easily pull up the transactions with the lowest and highest percent time spent on the CPU.
As agile and DevOps adoption increases, it will become increasingly important to remove obstacles that will slow teams down.
Relying on outside teams to execute load tests will mean that teams need to either slow down to get their app tested or forgo critical load testing altogether.
One of the main hesitations to using open-source load testing tools is the need to test non-web protocols such as Citrix and SAP GUI.
As these protocols continue to be sunset by their creators, the emphasis on tools that can test non-web protocols will reach an all-time low.
Some stats from Wipro show that non-web protocols barely account for 20 percent of the market today, and we can expect that number to be even lower in 2019.
Here's what they told us:ReactForrester did a wave report on low code in October 2017.
Different buying patterns in low code by developers for developers and then branching to citizen business development.
Airbnb changing from React Native back to Native hard to keep up to date with changes in iOS and Android.
No one owning mobile right now.
It is interesting to note that open source adoption lies at the heart of DevOps given that the majority of DevOps tools and technologies in the market are open source, which reduces the total cost of ownership and testing infrastructure.
This enables faster provisioning and deployment of an IoT test environment, which means faster recovery from test failures, directly improving test cycle time.
Rapid developments could mean that breaking changes could come at any time or that new attacks could be identified as code is rapidly iterated on.
There’s a lot of hard work to do.
Lack of knowledge about container technology.
Don’t silo VMs and containers or it will make the integration even more difficult.
Pay attention to base images and white outs.
Portability in isolation with build and management issues need to catch up on tooling to ship in a robust manner.
Early in the adoption curve of mainstream use transitioning from dev/test to production without knowing how great an opportunity it is for service teams.
Focus on the right tools for the right problems.
No, not really.
No.
3 Common Mobile App Performance Problems and How to Avoid ThemWhy is the issue of mobile app performance pushed aside by their creators?
It’s probably difficult to master it and pinpoint the most important aspects while planning mobile application programming processes.
For now, let’s ignore the question of its category (games, business, education, lifestyle), as it’s not really important at this point.
DevicesThe first and probably the most frequently forgotten factor concerns the devices themselves.
These can include devices with worse units (weaker processor, less RAM).
This should be a warning signal for testers to start their tests with the oldest devices.
In any case, this doesn’t justify the programmers, who often copy the wrong project templates out of laziness, and start the applications only on the newest devices – ones that deal with processing complicated operations without any problems.
The most frequent errors directly affecting performance result from the app asking the server for data too often, or a bad structure for storing the data in the cache.
From my experience, problems sometimes occur due to a lack of explicit information that the application is to operate oﬄine.
Sometimes, re-developing an already complex application can be very risky, as this can generate additional errors (which are difficult to solve).
I think that this problem concerns developing the layer of communication with the server in business more so than in games, which, as can be assumed, should operate oﬄine.
By ‘offline’ I also mean a poor Internet connection, such as 3G or EDGE, which isn’t always 100% sufficient.
The problem can be further complicated due to e.g.
Unfortunately, in this case, we don’t always have a direct impact through ongoing development.
Therefore, we should not let them feel the need to immediately uninstall our newly developed software or, even worse, feel that they have an old phone and they should replace it.
Walk through the steps your users are taking to understand their pain points from their workflow.
No amount of brilliant coding can make up for a bad choice early on.
How to design for problems.
There are no quarterly patch cycles.
Pay more attention to how the app is used, its impact on infrastructure, how it is deployed, and how it is monitored.
It should be embarrassing to have a bug that causes a product stoppage.
We have one client where one hour of downtime costs them $8 million in lost revenue.
It automatically performs management tasks like software patching, setup, configuration, hardware provisioning, failure recovery, backups etc.
There's no risk of losing workloads, as it continuously monitors clusters.
This ensures that there are no deadlocks occurring due to multiple threads of writing/reading multiple information into a disk.
So, you do not have the freedom to use multiple CPUs.
Monitoring the metrics is not easy.
Openness can lead to potential problems that you are unable to anticipate.
Identifying bad actors more quickly and getting them off the system.
The risk of insider threats and human mistakes grows.
Open source is no more or less secure than first-party code; however, it does not age well.
If a vulnerability is announced it needs to be fixed in a timely manner.
If you wait five years, it will cost a lot more to fix and you will have exposed a lot more vulnerabilities.
We’re still not there with security.
The OWASP Top 10 is still a problem.
Technology companies that solve these problems distance themselves from competitors in a way that is impossible to catch.
As a result, there are many organizations today that do not realize their future is questionable because of the performance, agility, and precision of their competitors.
The fact is, while CSS-as-such is now relatively consistent across browsers, the nasty browser bugs and inconsistencies never had to do with CSS, but with events, auto-sizing, data binding and network behavior, and other areas.
Developers are slowly, slowly realizing this - mostly through costly project overruns and failures.
Less of a need for a full-stack developer today with such a diverse set of skills that no one can possibly know with any depth.
A full-stack developer is more likely to miss things than a specialty developer.
AppSec Instrumentation Addresses AppSec Skills ShortageAccording to ISACA’s State of Cybersecurity 2020 Report, which is based on data gathered from more than 2,000 respondents in more than 100 countries, cybersecurity threats continue unabated while a cybersecurity skills gap is presenting serious challenges to organizations.
The biggest skills gaps reflect these candidate priorities as well: soft skills (32%), IT knowledge and skills (30%), insufficient business insight (16%), cybersecurity technical experience (13%), and lack of hands-on training (10%).
And the current situation is going to get even worse.
For example, (ISC)², the world’s largest nonprofit membership association of certified cybersecurity professionals, estimates that the cybersecurity workforce must grow at a rate of 145% globally to keep pace with the current demand.
A Rethink of AppSec Is RequiredMuch has been written about what can be done to address the cybersecurity skills shortage.
Yet, little progress is occurring—things seem to be getting worse if you read some of the latest reports.
An AppSec platform powered by instrumentation such as the Contrast DevOps-Native AppSec Platform automates vulnerability identification as well as the verification of vulnerability remediation.
And when it comes to the cybersecurity skills shortage, this no longer is a factor for AppSec.
Developers can manage vulnerabilities directly within the application with no security expertise required.
As a consequence, developers can manage AppSec themselves without the security team needing to hire hard-to-find, specialized DevSecOps staff.
This enables organizations to provide more releases, reduce manual deployments, and minimize failure risk in production.
And it’s important to keep everyone on the same page to avoid conflicts in teams.
Research from the Ponemon Institute found that, in 2020, 71% of enterprises reported their portfolio of applications were more vulnerable to attacks and, additionally, more than 50% of respondents said security is not adequately emphasized during the development of new applications.
Despite all the testing in CI, code still breaks because tests just can’t detect 100% of errors.
If CI could detect 100% of errors, the term hot fix would not exist.
DevOps teams must deal with logged exceptions, swallowed exceptions, and uncaught exceptions (defects) so they turn on the logs, error trackers, and APM tools.
Swallowed exceptions are caught and handled, but no data is emitted so the error is essentially invisible.
Let’s call a spade a spade here, not all developers are equal and sometimes a bad developer can make things worse and the logs, error trackers, and APM tools can’t magically make them better at coding.
Sentry, Rollbar, Raygun, Bugsnag, Airbrake, and others for error tracking.
Despite all of it, developers can struggle to find and fix errors.
We can get lost in the logs, APM tools, and error trackers due to a lack of context.
OverOps provides complete context to resolve every error and can even Slack the developer who wrote the code.
The cloud migration trend was accelerated by a significant drop in cloud platform pricing in 2013, led by Amazon’s AWS.
We don’t have any storage available and the EMC storage we ordered is still stuck in customs…” The frustration drove us to look for more agile alternatives and eventually led us to migrate to the cloud.
How much are you going to pay for Lambda or for BigQuery?
Basically, you have NO idea upfront: it depends on your usage.
The FinOps team is charged with evaluating the business need, usually based on extensive resource tagging.
The problem is even greater with Infrastructure-as-Code since developers actually write and maintain the infrastructure in their git repositories.
A simple “git push” can lead to a major cost degradation, but since developers don’t have the tools to take ownership of the process, they usually don’t take it into account.
To summarize, forget about old-school cloud cost management.
The cloud has moved from CapEX to OpEX, demanding new solutions.
No trials, no license purchases – just the tools you need to delve behind the scenes of your code execution.
It can be used to analyze productive heap dumps to calculate the retained sizes of objects, see who is preventing the Garbage Collector from collecting objects, and run a report to automatically extract leak suspects.
Alternative Java Process Monitoring ToolsAs a programmer, you know that the default is not always the best choice.
Java Application Performance Monitors (APMs)Application performance monitors are very rarely free.
In fact, the majority of AMPs used range from fairly affordable to enterprise-exclusive — which is really a shame when you consider how important it is to monitor application performance, especially with a complex application topology executing (hopefully) in cohesion on the same server.
Not only because this is a sure way to spot bugs and issues but also because users expect and demand performance from the apps you build.
Keep in mind, ini_set() is only used during the lifetime execution of the script and will not persist its value permanently.
error_reportingPHP has multiple levels of errors: E_ALL, E_NOTICE, E_WARNING, etc.
This directive allows you to decide the error pain threshold.
A common trick that some developers are unaware of is to convert your errors into exceptions.
Using this method, you can “catch” your errors and “handle” them as you normally would.
Furthermore, use this setting in conjunction with display_errors when configuring how to handle displaying errors in production.
Note: setting error_reporting to E_ALL or -1 will show all possible errors, but this depends on your version of PHP.
Securityregister_globalsThis is reasonably one of the most dangerous directives in PHP.
This directive went from ON to OFF by default in PHP 4.2 and removed it in 5.4, so the industry has heavily matured away from this dangerous practice.
Unfortunately, this created a nightmare when you have addslashes() and stripslashes() throughout your code, and you’re ripping your hair out trying to keep track of when and how PHP is magically manipulating your data (pun intended).
In short, turn this off, and forget it ever existed.
expose_phpWhen set to ‘On’, this will expose to the public that you’re running PHP and the version number via X-Powered-By: in the HTTP header.
If this is turned on, you’re exposing yourself to security threats that may be exposed by your version of PHP or exposing that you’re running PHP itself.
The less that hackers know about your environment, the less of a strategy they have on how to attack you.
Thus, if you experience an error during execution, you can choose to send the user a friendly error message instead of a half interpreted broken HTML page.
This can happen for various reasons such as bad code, a hung database, or slow responses from third-party web service APIs.
If your threshold is high, your customers will naturally suffer from poor performance, regardless.
Note: You can set this value to 0 for infinite time, but that is not a good idea in production for obvious reasons.
This setting will set a maximum allowed quantity of memory to be allocated to the script so that if your script is running havoc, you can cap it and protect your server resources.
This is also, of course, a bad idea.
Historically, a form would upload an entire file to the server before it was accepted or rejected.
This means a user could upload a 500mb file, tie up server resources, be at the mercy of their Internet upload speed, and then finally have their inputs rejected.
Imagine if their form failed validation because of an improper email address: they would have to repeat the process.
This is a dangerous practice and a potential loophole for hackers too.
However, when you configure your PHP, always remember that your goal is to strike the right balance without sacrificing the end-user experience.
DevOps Scares Me - Part 4: Dev and Ops Collaborate Across the LifecycleToday we are blogging a little differently than usual.
Are there mocks of the user experience?
How do you alert and remediate when there are problems?
In theory, developers should do this before they commit any code, but oftentimes problems don’t show up until you have production traffic running under production infrastructure.
The goal of this step is really to simulate as much as possible everything that can go wrong and find out what happens and how to remediate.
Bees with Machine Guns: A utility for arming (creating) many bees (micro EC2 instances) to attack (load test) targets (web applications).
The last step of testing is discovering all of the possible failure scenarios and coming up with a disaster recovery plan.
For example what happens if we lose a database or a data center or have a 100x surge in traffic.
No way!
Participate in functional, load, stress, leak, etc.
For as long as I can remember, the development teams have thrown applications over the production wall for the operations staff to deal with when there are problems.
Sure, some problems like hardware issues, network issues, and cooling issues are purely on the shoulders of operations–but what about all of those application-specific problems?
For example, there are problems where the application is consuming way too many resources, or when the application has connection issues with the database due to a misconfiguration, or when the application just locks up and has to be restarted.
It was really difficult back in those days to say with any certainty that the problem was application related and that a developer needed to be involved.
Today’s monitoring tools have changed that and allow for problem isolation in just minutes.
Production DevOps is all about:deploying code in a fast, repeatable, scalable mannerrapidly identifying performance and stability problemsalerting the proper team when a problem is detectedrapidly isolating the root cause of problemsautomatic remediation of known problems and rapid manual remediation of new problems (runbooks and runbook automation)Your application must always be available and operating correctly during business hours (this may be 24x7 for your specific application).
Alerting Tools:PagerDutyIn case of failures alerting tools are crucial to notify the ops team of serious issues.
The operations team will usually have a runbook to turn to when things go wrong.
As a developer the biggest issue during the maintenance phase is working with the operations team to deploy new versions and make critical bug fixes.
The other primary concern is troubleshooting production problems.
Even when no new code has been deployed, sometimes failures happen.
One of the patterns I have observed over my career is that, regardless of the project size, company, or the technology being used, the same types of performance testing mistakes get made over, and over, and over.
Unfortunately, however, those short-cuts can lead to costly performance testing mistakes and oversights.
Inadequate User Think Time in ScriptsHitting your application with hundreds or thousands of requests per second without any think time should only be used in rare cases where you need to simulate this type of behavior (perhaps a Denial of Service attack?).
What are the transactions that have a high business cost if they were to fail under load?
Setting Up Inadequate Infrastructure MonitoringLoad generation isn't the only important part of a performance testing scenario.
The execution results gained from a scenario such as throughput, transaction response times, and error information isn't overly helpful unless you can see how your target infrastructure is actually coping with the scenario.
It's a common problem - I have heard many testers ask why their response times are taking minutes instead of seconds.
The problem can lie either in the load generation or the target application infrastructure.
So how do you solve this problem?
This enables you to view system resource utilization while running your tests, ensuring that no bottlenecks are present on the load generation side.
Ignoring System or Script Errors Even Though Response Times and Throughput May Look FineWhen running a load test - there are several things to keep an eye on to ensure you are running a valid test.
Take the following example that can be seen quite often:A load test runs with a target number of users and the user observes response times and error rates to be in acceptable ranges.
However, expected throughput is lower than expected.
How can this be when Flood is reporting very little transaction related errors?
From the logs, we can see a number of script related issues that are causing the script replay to exit out of a substantial amount of iterations due to script errors.
This will significantly impact your target throughput and is the primary source for observed throughput to be lower than expected.
This might not be much of an issue with a single transaction but multiply this over 1,000 concurrent users and it can severely impact your system's responsiveness.
Overloading Load GeneratorsThe last of our 6 common performance testing mistakes is the overloading of load generators due to one or more of the following:Too many concurrent users on a single load injection node.
error messages in the JMeter execution logs.
Error LoggingErrors are often represented as I/O Exceptions reported in the Test Execution logs similar to the following:o.a.j.p.h.s.HTTPHC4Impl$6: I/O exception (org.apache.http.NoHttpResponseException) caught when processing request to {s}-> https://flooded.io:443: The target server failed to respondo.a.j.p.h.s.HTTPHC4Impl$6: I/O exception (java.net.SocketException) caught when connecting to {s}-> https://flooded.io:443: Broken pipe (Write failed)
So, no pressure then.
According to the Aberdeen Group, large companies lose an average of$686,250 per hour of downtime.
Under 100ms is perceived as reacting instantly, while a 100ms to 300ms delay is perceptible.
40% of mobile visitors will abandon a site after a three-second delay, so speed of response or perceived speed are critical in determining whether an online purchase takes place or is abandoned out of frustration.
A Continuous Delivery ModelA high velocity of small, incremental releases were deployed with little if any negative impact, supported by automated configuration, deployment, and release management technologies, and processes.
These delays should not have kicked off debate as to whose team was or was not responsible and how to address the pain.
If the full end-to-end business transaction view was obtained, enterprises could identify where online visitors were at any moment in time and if they were at risk of abandoning a site due to poor responsiveness.
Is there a delay in the final stage of the purchase cycle?
Failed to Prepare, Prepared to FailYes, it’s a well-worn phrase, but it’s especially apt when applied to Cyber Monday.
Failure to prepare could have a highly detrimental impact through customer attrition, lost revenue opportunities, brand reputation, and social media naming and shaming.
Strategic Field Engagement, Mimecast | Faisal Memon, Product Manager, NGINX | Arvind Mehrotra, President and Global Business Head – Infrastructure Management Services, NIIT Technologies | Jens Eckels, Director, PaaS Business Group, Oracle | Pat Harper, SVP Operations, PGi | Joan Wrabetz, CTO, Quali | Partha Seetala, CTO, Robin Systems | Nick Kephart, Senior Director Product Marketing, ThousandEyes | Kiran Bondalapati, CTO and Co-Founder, ZeroStack.When we asked these executives "What have I failed to ask you that you think we need to consider with regards to developing and deploying applications on the cloud?"
Each one is unique and opaque so it’s tough to source and evaluate.
The biggest hurdle for the developers is the inability to share ideas.
Developers are more in control but they have not been responsible for the budget in the past.
There was a GitHub incident where someone checked in and someone else had gotten access to their apps which were no longer in a sandbox.
No one could predict where we are today five years ago and no one can predict where we will be five years from now.
Doing a blueprint without a provider is also helpful–there will be no preconceived notions or restrictions.There’s a new way of doing things.
App monitoring fault tolerance of everything in the stack across all components and services.
Migration is a highly critical factor.Developers will still need to program and collaborate.
A good developer will understand all three.Test for failure.
If anything fails in my system, then we need to bring down the service so we don’t put our, or the customers’, data at risk.
Plan for failure.
Don’t give up on a testing your system for failure.
It’s better for you to find the problems in the app yourself than an intruder.A lot of companies fail to see the soft cost when considering cloud for their apps.
But if you stop there, the true power of cloud could go unrealized.Anything we've missed in our series on developing and deploying cloud-based applications?
Companies can pay for the infrastructure that they need for any given hour of the day, scaling up to meet high user demand during peak hours and scaling down during off peak hours.
Stolen CPU is not as diabolical as it sounds, it is a relative measure of the cycles of a CPU that should have been available to run your processes, but were not available because the hypervisor diverted cycles away from your instance.
The key to identifying noisy neighbors is to observe the metrics over time pay particular attention to the stolen CPU time when your virtual machine is idle.
If you see discrepancies in the stolen time when your CPU is idle then that indicates that you are sharing the same hardware with other customers.
For example, when your CPU usage is idle and the stolen CPU percentage is 10% once and then another time the stolen CPU usage is 40% then chances are that you are not simply observing hypervisor activity, but rather are observing the behavior of another virtual machine.In addition to reviewing the operating system CPU utilization and stolen percentage, you need to cross reference this with Amazon’s CloudWatch metrics.
If the CloudWatch CPUUtilization metric is at 100% for your EC2 instance and your operating system is not reporting that you’ve reached 100% of your ECU capacity (one core of your physical CPU type / 1.0-1.2 GHz) and the CPU stolen percentage is high, then you have a noisy neighbor that is draining compute capacity from your virtual machine.
If the operating system reports that we’re at 30% CPU utilization with a high stolen CPU usage and CloudWatch reports that we’re at 100% usage, then we have identified a noisy neighbor.Figure 1 presents an example of the CloudWatch CPU Utilization as compared to the operating system CPU utilization and stolen percentages, captured graphically over time.
CloudWatch vs OS CPU MetricsOnce you’ve identified that you have a noisy neighbor, so how should you react?
You have a few options, but here’s what we recommend:Observe the behavior to determine if it is a systemic problem (is the neighbor constantly noisy?
)If it is a systemic problem then move.
While this might not be the best choice if you live in an apartment building, in Amazon it is simple: start a new AMI instance, register it with your ELB, and decommission the old one.
Consumers with their attention locked on other priorities often express that they have no time to learn new interfaces.
To get there, most companies will need to use analytics to further refine their IoT services/products until it reaches that point.In outlining the phenomena, Alvin Toffler said: “We may define future shock as the distress, both physical and psychological, that arises from an overload of the human organism’s physical adaptive systems and its decision-making processes.” Individual IoT gadgets (such as smart thermostats, drone-mounted cameras, real-time inventory sensors, and health wearables) improve the end user’s life, but the cumulative effect can trigger IoT overload.The same applies to all of your customers, whether that means consumers, businesses or end-users.
Our production monitoring requires a very low (<2%) resource overhead.
Low cardinality does not reliably lead to low disk usage.Long-term Trend of Storage CostsWhen measuring ElasticSearch (ES) storage usage, it is important to realize that the short-term trend does not represent a long-term average.
Unfortunately, that always left us with blind spots.
We could not be sure if the trend is dependent or independent of other parameters.
For example, is the trend describing average record cost the same for low cardinality and high cardinality columns?
We suspect this result has to do with the fact that we did not disable the _source field for the documents.Secondly, the per-character cost goes up significantly (even as high as 7 bytes per character) if the strings are short.
We need more and different use cases, practical examples of how IoT and big data are solving real business problems and providing ROI.
We’re going from stand alone to end-to-end systems.
Understand the business problem and the use case.
Here's who we talked to:Craig McNeil, Managing Director of IoT, Accenture | Prathap Dendi, General Manager, AppDynamics | Aaron Lint, Vice President of Research, Arxan | Rod McLane and Justin Ruiz, Marketing, Ayla Networks | Suraj Kumar – General Manager, Digital as a Service, Axway | Paul Hanson, CEO, bbotx, Inc. | Mikko Jarva, CTO, Comptel | Brad Bush, COO, and Jeanette Cajide, VP of Corporate Development, Dialexa | Scott Hilton, Executive Vice President Products, Dyn | Anders Wallgren, CTO, Electric Cloud | Mathieu Baissac, Vice President Product Manager, Flexera | Darren Guccione, CEO, Keeper Security | Tony Paine, CEO, Kepware | Johan den Haan, CTO, Mendix | Joan Wrabetz, CTO, Quali | Tom Hunt, CEO, WindspringHere’s what they told us when we asked, “What are the real world problems being solved by IoT at your organization or by one of your clients?”We have a client using sensor technology to padlock gates at a petroleum storage facility and then using sensors to measure the change in the pressure of the tank and to know how much is being taken from the tank so they can prevent “piggybacking” (another truck following on the heels of the legitimate tanker to "steal" petroleum).
It's is a pain point that doesn’t discriminate.
Three examples: 1) Temperature sensors in medicine packaging show that the average freight company, and hospital are not doing an adequate job keeping medicines at the temperature that ensures efficacy.
If the light goes out, all three are affected, as is the crop.
Given the breadth of IoT solutions, a variety of problems are addressed.
Updating the firmware manually was inconvenient and expensive.
Smart home manufacturers are being driven by fear.
Furthermore, there is a lot of uncertainties associated with IoT because of this data.
Being able to analyze and sort data as it is being generated is no longer a vision.
In fact, Fabrizio Biscotti, research director at Gartner argues, “IoT deployments will generate large quantities of data that need to be processed and analyzed in real time.
And doing it in a timely manner, dare I say it – in real-time – will be absolute hell.
Even worse, many people are stuck with tools that are cumbersome to use.
That preparation will include an investment in data management tools to make the most of an IoT strategy.Harbor Research predicts that if you have connected products with no long-term data services strategy, then “you’re in the Pervasive Internet of Things booster rocket… when the booster runs out of fuel (product-centric profits), you’ll fall back to Earth.
performance issues, errors and exceptions happen all the time and we have to know what’s going on.
takipi      replaces logging in production jvms and lets you see the variable state that caused each log error and exception.
in most cases we’re not even aware of the data we actually need until it’s too late and the user is affected.
do we think that a user might not complete a certain transaction?
cory introduced a negative feedback loop, in which an output is compared to the desired value and changed accordingly:  the negative feedback loop.
source: cory watson  in cory’s words: “if the internal state goes bad, the work goes bad.
while cory mentioned that it’s not about the technology, we can’t ignore the variety of tools available for all of our measuring needs.
if you’re already familiar with these tools and you want to find the right one for you, you can check out our comparisons of the big names dominating this field.
debugging in production when something bad happens in development, the debugger of the ide is right there to help you understand what went wrong.
takipi      replaces logging in production jvms and lets you see the variable state that caused each log error and exception.
Be it the effort estimation strategy, test planning, the defect management cycle, or the tools knowledge requirement, performance testing is quite different.
Realistic simulation of end user access patterns both quantitatively and qualitatively is a key factor towards successful performance testing but, unfortunately, this is not measured or expressed using a metric.
Deep dive analysis of why a layer specific performance metrics doesn’t meet the SLA can be considered as an engineering activity.Usually, the confusion starts when it comes to performance bottleneck analysis.
7 Deadly DevOps Sins and How to Avoid ThemWhile there are many ways to do DevOps correctly, there are specific cardinal sins that will put you afoul of the Church of DevOps.
From lacking an incident management tool to handle critical alerts to treating DevOps as a job title, there are many ways for you to hurt your status as a class-A DevOps shop.
As Matt Juszczak of Bitilancer writes:  Calling yourself or somebody else a DevOps Engineer, a DevOps System Administrator or a DevOps Tester reflects a fundamental misunderstanding about DevOps.
This confusion is contributing to a lot of project and program frustrations and failures that are hurting teams and companies, sidetracking careers, and creating backlash for recruiters.
Metrics such as Mean Time To Resolution (MTTR) or Mean Time to Failure (MTBF) are important but you should also focus on process and people metrics.
At the very least, a new DevOps tool should follow the Hippocratic oath and should do no harm to any team.
As one engineer writes,  If daily routines will be impacted by the new “DevOps” tool, getting early buy-in from affected teams is key.
You Think That Failure Is UnacceptableCompanies might be automating correctly and have management buy-in, but the DevOps team gets it wrong when they don’t embrace failure.
Netflix, for example, actually tries to anticipate failure so they are ready for when scenarios such as downed servers or non-functioning code do show up.
On the philosophical end, management needs to realize that failure is part of the practice of creating and releasing code.
Rather than having painful post-mortems that focus on finger pointing, teams need to focus on constructive, blameless post-mortems that look at understanding the issues and how they can be avoided in the future.
Ideally, a failed release is met with:  “New tests...built around your mistake so that it’s caught next time and everyone acts like it’s simply another day.
Devs cannot create code and throw it over the wall when they are finished and expect Ops to deploy it.
If Devs see that their code is causing numerous problems and is waking them up in the middle of the night, they will be more conscientious about writing and testing their code.
Not Using Critical Alerting ToolsRelying on ineffective critical alerting tools to notify engineers of critical incidents will serve to magnify many other sins that DevOps commits.
It is indeed quite sinful to ignore these points and continue with tools that don’t effectively alert your on-call team.
ConclusionCommitting any of these DevOps sins is not a mark that a company is irredeemable.
Instead, recognizing the fault is probably the first way towards correcting it.
The proliferation of new services, requirements, and devices in diverse geographic locations has made visibility into the entire network critical.
You need to be able to see where all of your data is residing to understand how performance is, or is not, being optimized.
However, it’s important to identify and focus on key business metrics, or else you run the risk of being overwhelmed with data.
While more tools are coming online, some providers are enabling disparate tools to provide an integrated view to the client, which results in greater visibility into the entire pipeline and faster time to problem resolution.
There continues to be a lack of knowledgeable professionals that know distributed computing and parallel processing.
Understanding the product, load, load tests, and performance graphs is critical.
In the future, performance and monitoring tools will automatically react to issues and know the difference between mitigating and fixing problems.
They’ll be able to do this by collecting more data and identifying a dynamic system to determine what the problem may be before it affects the customer.
Just as data is used to solve problems, it can also be used to change the way performance testing is done and measured.
The biggest concerns about performance and monitoring today are the lack of collaboration, identification of KPIs and how to measure them, and expertise.
Don’t assume the model you have in your mind is correct and know you’re going to get it wrong.
Once a problem is identified and remediation proposed, there is a need to test and validate that the change has completely fixed the problem.
Users would complain about a problem, you’d go to operations and ask for a thread dump, and then you’d spend some time poring over log files looking for errors, exceptions, or anything that might indicate a problem.
If you’re depending on log files to find and troubleshoot performance problems, then chances are your users are suffering – and you’re losing money for your business.
In this blog post we’ll look at how and why logging is no longer enough for managing application performance.
This is the ideal scenario, but more often than not the logs were of very little use and the operations team would have to wait for another user to complain about a similar problem and kick off the process again.
Apart from some rudimentary server monitoring tools that could alert the operations team if a server was unavailable, it was the end users who were counted on to report problems.
Logging is Inherently ReactiveThe most important reason that logging was never truly an application performance management strategy is that logging is an inherently reactive approach to performance.
Typically this means an end user is the one alerting you to a problem, which means that they were affected by the issue, and (therefore) so was your business.
A reactive approach to application performance loses you money and damages your reputation.
So, logging isn’t going to cut it in production.
You’re Looking for a Needle in a HaystackAnother reason why logging was never a perfect strategy is that system logs have a particularly low signal to noise ratio.
Sifting through log files can be a very time-consuming process, especially as your application scales, and every minute you spend looking for a problem is time that your customers are being affected by a performance issue.
Even then, their hunches could be wrong, especially if it’s a performance issue that you’ve never encountered before.
In a crisis, every second counts, and these costly processes (while important) can cost your organization money if your application is down.
The developer who wrote the code is ultimately the one who decides what gets logged, and the verbosity of those logs is often limited by performance constraints in production.
As a result, it can be difficult just to know where to start if you’re depending on log files for troubleshooting performance issues.
Even if you have centralized log collection, finding which tier a problem is on is often a large challenge on its own.
Logging is Simply Not EnoughLogging is not enough: modern applications require application performance management to enable application owners to stay informed to minimize the business impact of performance degradation and downtime.
Logging is simply not enough information to get to the root cause of problems in modern distributed applications.
The problems of production monitoring have changed and so has the solution.
Your end users are demanding and fickle, and you can’t afford to let them down.
Seemingly, the most extensive mechanism for data violations in IT now is via either unintentional or malicious abuse of user credentials, particularly the login info.
— Ask yourself in case the data is supplied in this manner which could be incorporated readily into internal tracking tools to stop information silos.
And that is when you’re able to perform an inventory of the conformity problems that are related.
SaaS Security ProblemsIt’s the SaaS provider’s job to keep multiple users from viewing the data of each other.
Besides providing accessibility to data about previous operation, OpTier use predictive algorithms to assess present trends and alert administrators to possible operation problems that are forthcoming.
As with other cloud-established program tracking programs, consolidated views of outline data about program performance, from both the end user standpoint and from lower level network and program viewpoints are offered by New Relic.
Cultural MetricsAlthough cultural metrics are difficult to apply hard dollar value to, DevOps is about resolving conflict in the workplace, eliminating stress and avoiding burnout – and they are measurable.
While you might be stumped on ideas for influencing cultural, especially in well-established enterprises, it is by no means impossible.
Part of the DevOp’s agenda is about improving working conditions – depressurising and destressing environments and having everyone working together in harmony and eliminating disasters, catastrophes, blame and brinkmanship.
Why ask for an external DevOps Maturity AssessmentWhile no one’s going to understand your business as well as yourselves, we often meet organizations who are struggling to find the time – they know there are improvements to be made but they are so busy with firefighting they can’t conceive of stopping and taking stock of their current position.
This term is more commonly used when talking about Application Performance Management and the speed at which an outage or performance issue can be fixed, but equally can be used when talking about testing and eliminating defectsThis is guest post by our partners at Ranger 4.
Weary after years of costly and time-consuming warroom battles, IT organisations turned to APM solutions to give an objective application-level view of production incidents.
The bad news?
At a higher level, this helped lead to a more constructive relationship between the development shop and their customer – moving things away from the edge of litigation, constant finger-pointing, and blame shifting.
The system is developed in-house, and typical with most financial services institutions, the actual development team itself is located ‘near-shore’ in Eastern Europe to cut costs.
Another thing that is very common in the financial services industry is regulation, and this poses a problem in this scenario.
Bringing external developments in-houseOf course, as we all know the only constant in life is change, so no outsource is a one-way journey.
There is only so much you can distill onto a whiteboard in a brain dump session, however long and well-intentioned.
Often times people get this wrong and naively think they should focus on the edge cases.
Solid architectural decisions like doing blocking work in the background via tasks, proactively caching expensive calls, and using a reverse proxy cache will get you much further than arguing about single quotes or double quotes.
Guzzle takes the pain out of sending HTTP requests and the redundancy out of creating web service clients.
Moving work to the background with Resque and RedisAny process that is slow and not important for the immediate http response should be queued and processed via non-blocking background tasks.
There are a lot of systems available for managing messaging layers or task queues, but I find Resque for PHP dead simple.
A very simple introduction to adding Resque to your application:1) Define a Redis backendResque::setBackend('localhost:6379');2) Define a background taskclass MyTask{public function perform(){// Work work workecho $this->args['name'];}}3) Add a task to the queueResque::enqueue('default', 'MyTask', array('name' => 'AppD'));4) Run a command line task to process the tasks with five workers from the queue in the background$ QUEUE=* COUNT=5 bin/resqueFor more information read the official documentation or see the very complete tutorial from Wan Qi Chen:Part 1 : IntroductionPart 2 : Queue systemPart 3 : InstallationPart 4 : WorkerPart 5 : Job class and implementationPart 6 : Integrate Resque into CakePHP with CakeResquePart 7 : Start and stop workers with FresquePart 8 : A look into php-resque-ex, a fork with more featuresPart 9 : Resque analytics with ResqueBoardMonitor production performanceI use AppDynamics, an application performance management software designed to help dev and ops troubleshoot performance problems in complex production applications.
In fact, in a recent report, Goldman Sachs pointed out that “key obstacles are gone.” This report lists several examples ranging from bandwidth to hardware.
This potential partially obscures IoT contradictions, as it is both emergent and has been around awhile.
Eventually, the market will grow so large and technology will advance so that nearly all markets will have no choice but cut the cords, invest in analytics, and go IoT.
“Like all technology revolutions,” he explains, “the path was not a straight line.
We had sock puppets and business models based on the elusive quest for eye-balls, and outrageous promises of new businesses and social upheavals.
No one today would argue that the Internet has not added immeasurable value to the world and changed our lives forever.
We had no historical data, and no insight into database calls, threads, etc.,” said Strick.
Bush website with an EUR of 17.9 and a fully loaded time of 18.2 seconds.Of course, these results are not surprising when we consider the other factors of a website’s design, such as the fact that Trump’s website is comparatively lightweight, coming in at only 0.89 MB of data downloaded for 36 elements, compared to 6.02 MB and 118 elements for Fiorina, and Bush’s website weighing in at a whooping 8.3 MB and 126 elements.This data is further nuanced when you also look at the first-render times, in which case Bush actually was fastest at 1.9 seconds.
So at least some content is starting to render quickly (faster than Trump at 2.8 and Carly at 3.4), even though it takes a very long time for the complete page to load.These parameters reflect very conscientious design decisions and tradeoffs that are being made by the respective campaigns and their web design and content teams, given the many conflicting and contradictory goals and objectives for the candidate’s sites.
The websites (and experience) are not just about engagement; they’re also about raising a significant amount of cash.
Christie cut five seconds, while reducing his homepage by nearly almost 3X from 5.7 MB to 2.4 MB.
In particular, Marco Rubio’s performance degraded by three seconds, as his EUR time went from 6.7 to 9.7 seconds, when he nearly doubled the data on the homepage from 2.45 MB to 4.4 MB.
If you haven’t had your mid-life crisis, you’re thinking about having it soon.
And you may still call your test tool Mercury LoadRunner, refusing to acknowledge HP’s purchase of Mercury since it happened eight years ago.You want to get with the times, but you can’t.
(To paraphrase a former University of Florida coach, Urban Meyer, who refused to ever mention Florida State University (FSU) during his entire tenure at the University of Florida — instead calling them “that school out west” — I’ll do the same in this blog post.
Making Thread Dumps IntelligentLong back I had learnt about something called Log MDC, and I was a big fan of it.
I was suddenly able to make sense of anything that happens in log files and pin-point to a specific log entry and find what’s right or wrong with it especially when it was about debugging a bug in production.In 2013 I was commissioned to work on a project that was running through some troubled waters (combination of several things) and almost every week I had to go through several Java Thread Dumps trying to make sense what’s happening in the application to make it stop.
jStack was one of the most helpful tools that I had worked with but the thread dumps being bumps had no contextual information that I could work with.
I was stuck with seeing 10(s) of dumps with stack traces of what classes are causing the block but there was no information of what’s call and what inputs were causing the issues and it got frustrating very fast.
I Explored ways in which I can use something similar to Log4j’s NDC but have that in threads so that my dumps mean something.
I am not going to rewrite everything they said, so here is a link to their blog post.So last week I am starting a new project and as I get into coding the framework (using Spring 4.1 and Spring Boot), this is the first class I am writing for the application and ensuring that the filter gets into the code ASAP which not only helps us in post-production but also makes my development logs meaningful.A copy of the code for both Log4j NDC, and setting up a ThreadName is below.import java.io.IOException;import java.text.SimpleDateFormat;import java.util.Date;import javax.servlet.FilterChain;import javax.servlet.ServletException;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.web.filter.OncePerRequestFilter;/** * This is a very Spring opinionated HTTPFilter used for intercepting all requests and decorate the thread name with additional contextual * information.
", ex);// this is an internal filter and an error here should not impact// the request processing, hence eat the exception}try {filterChain.doFilter(request, response);} finally {try {thread.setName(threadOriginalName);} catch (Exception ex) {LOGGER.error("Failed to reset the thread name.
", ex);// this is an internal filter and an error here should not// impact the request processing, hence eat the exception}}}}/** * Generic filter for intercepting all requests and perform the following generic tasks: *  * <ul> * <li>Intercepts the request and then pushed the user domain into the session if one exists.</li> * <li> Pushes a uniquely generated request identifier to the LOG4J NDC context.
</li> * </ul> */public class RequestInterceptorFilter implements Filter{    /**     * <p>     * <ul>     * <li>Initializes the LOG4J NDC context before executing an HTTP requests.</li>     * <li>Pushes the domain into the session</li>     * </ul>     * </p>     */    public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException    {        HttpServletRequest httpRequest = (HttpServletRequest) request;        if (httpRequest.isRequestedSessionIdFromCookie() && !httpRequest.isRequestedSessionIdValid())        {            // TODO: Need to define an session expiration page and redirect the application to that page            // As of now this is a non-issue as we are handling session expirations on Flex (Front-end) and hence            // no request will come to server in case the session timeout occurs            // HttpServletResponse httpServletResponse = (HttpServletResponse) response;            // httpServletResponse.sendRedirect(httpRequest.getContextPath() + "?expired");        }        else        {            // Create an NDC context string that will be prepended to all log messages written to files.
For any of these "mission-critical" applications, performant behavior should include:Minimal or no downtime due to preventable outages.
Most applications see a daily trend of peak data usage during the day followed by lower points during the night or off-hours.
For instance, if your application is used mostly in a business context, you may see a drop in activity over the weekend.
Vertically, we're seeing a stacking of function calls, where each block in this graph is a function, and horizontally we're seeing how much time elapsed in the execution of each function.
From this, we can identify two obvious patterns: the large spike on the right seems significant, but the width of the block shows that even though there is a very deep call depth, it is consuming a minor percentage of the overall CPU time.
There are two much wider blocks, however, one on the left and one in the center, that don't have the same call depth but are taking up considerably more of our CPU time.
To discover the most costly parts of our application, we identify the blocks that are taking up the most horizontal space without anything stacked on top of them.
So we would begin by inspecting the two blocks that sit on top of the two widest stacks, then we would consider the blocks they are stacked directly on top of as there is also some additional CPU time being used by those.
A memory leak can eventually cause a Node.js process to crash.
Some processes may need to broken out to perform CPU-intensive, synchronous tasks.
Heap usage is a bit more tricky.
Exceeded CPU Usage ThresholdCommon Triggers: * Blocking Synchronous Tasks * CPU intensive computationCommon Threshold Settings: * Limit: 65% CPU Threshold * Action: Trigger CPU Snapshot * Action: Ops team alerts (Slack, Webhooks, Email)What Is the Impact of Monitoring Node.js Application Performance?
If you're using AppDynamics, it will automatically utilize the N|Solid API to avoid some of the more costly monitoring of its own.
Otherwise, performant and reliable production tooling around perf monitoring can be a bit of a pain.
I often hear developers discussing new languages or libraries but rarely care enough to jump in and figure out why it’s important to me.
Even more interesting.Realization #2: Node.js is used to build server side applications.This information brings up an important question: Isn’t it a bad thing to have a single threaded application server if you want high scalability?
There are a ton of arguments on both sides and nothing that swayed me in either direction.
The pros are faster turnaround times, the product can be tweaked (noticed I say tweaked and not changed) during the build phase, QA happens almost in parallel to dev, and defects are found much earlier.
No one has the time to take 2 days for data preparation and smoke testing, 1 full day for executing and load test for 6 hours and then 1-2 days for result collation and analysis.
For the more end to end testing, we tried to backup prod data, mask it, and then dump it in the performance test env, but that failed utterly due to various reasons.
We tried to pull logs after the test and parse them after each round, but that was ineffective.
As "Mad Eye" Moody from Harry Potter put it, “constant vigilance.”
Customers go to development shops with a half-built app because they picked the wrong technology for the job.
Chip manufacturers can use web applications to collaboratively analyze performance and defects in batches of chips to refine their manufacturing process, which results in higher quality chips at lower cost and less waste.
Testing is still being done manually, and environment management is a constant struggle.
Speed to market is bogged down by how hard it is to release software.
Bad choices early on and a lack of flexibility can lead to an expensive and time-consuming re-work of the application.
Apps are no longer self-contained.
We need to identify bad actors more quickly and get them off the system.
Open source is no more or less secure than first-party code; however, it does not age well.
If a vulnerability is announced it needs to be fixed in a timely manner.
Walk through the steps your users are taking to understand their pain points and their workflow.
K8ss provides mobility and agility when to run your workload, avoid vendor lock-in.
1) Vendors need to be on top of technology, so their products will meet your needs and solve your problems.
2) Do not use public cloud, containers, and DevOps technologies in isolation.
The complexity of the Docker CLI (command line interfaces) lends itself to the introduction of human error through forgetting CLI strings, and there is no pre-validation of commands prior to execution.
You can collapse the memory cache and database into the application container.
There are multiple verticals and two dozen applications of real-world problems being solved using containers.
The most common issue affecting the orchestration and deployment of containers is lack of knowledge and experience.
People will find examples of K8 implementations but fail when they move to production because they have not used all of the features and lack the proper configuration for deployment.
People need to be concerned that the frequency of attacks and exploits will continue to increase.
There is a lack of knowledge about container technology and a lack of education for end users, developers, and security professionals.
Do not silo the two or it will make integration even more difficult.
We will not reach the peak for another three years.
The container orchestration war was settled and that has helped us move beyond evaluating technology to accelerating the process and getting to advanced concepts of which there are a lot to grasp.
Still a lot of unresolved problems.
Common Node.js Development Mistakes (Part 1)Just face it: You are making or have already made (at least) some of these common Node.js development mistakes that I'm about to put into the spotlight!
Node.js is, nevertheless, susceptible and vulnerable to a number of developer mistakes.
These mistakes, ironically, stem precisely from:Improperly implementing Node.js into one's projects.
That you, too, risk making and thus to... “sabotage” yourself!
Mistakes which:Have an impact on your app projects' performance.
Blocking the Event Loop: Too Common and Almost Impossible to AvoidHere's the context:Node.js, as we all know it, is a single-threaded environment.
This can only mean that if God forbid, something blocks the event loop, everything gets blocked!
And all it takes is a piece of a CPU-bound request, connected to various clients, to block the event loop.
Now how can you avoid making this mistake when building your Node.js app?
You try to avoid CPU intensive work in the front-facing Node.js instances (those that clients connect to simultaneously).
Using the Nesting Callbacks Approach Leads to  “Callback Hell”Nesting callbacks deep into your code, to the point where it gets difficult to read, error-prone and “untamed,” is a well known “practice” with Node.js developers.
Probably one of the most common Node.js development mistakes.
Are you, too, one of those who stubbornly consider callback nesting inevitable for handling asynchronous operations?
Speaking of which, here are some examples of monitoring programs:StrongLoopAppDynamicsNew RelicConcurixOverlooking the need to keep your app closely monitored, all the time, is one of those common Node.js development mistakes that many underestimate.
Using Console.log to Debug Your CodeTry to resist the temptation of just plugging in console.log whenever something goes wrong.
Now, here's the “ugly truth” behind over exploiting console.log for debugging your code:By putting your Node.js app “on hold.” Each time you insert console.log, then restart your server/app, you're actually slowing down the development process.
This “handy, yet unorthodox” approach to debugging will only leave you with unruly code.
This is what the Debug module's built for, after all!Just use the debug function and... leave it there, instead of plugging in and deleting console.log multiple times.
Node.js Development Mistakes: Taking Number for Integer Data TypeFirst of all, do keep this in mind: in JavaScript, numbers are floating point data!
This is when trouble happens.
To avoid these types of “exceptional” situations, with a negative impact on your system, remember that:Operators handle integers and float differently.
In conclusion:miscalculating your float, and implicitly its limits, as well, still is one of those common Node.js development mistakes that programmers often make.
Here’s what we learned:   You may also like:  Microservices Migration Use CasesBusiness ProblemsOne word of caution, don’t do it for the sake of doing it.
Know the business problem you are trying to solve — manage scale, develop features quickly, manage the geographic distribution of data.
The main concern around migration to microservices is the risk of limited planning.
Scaling, orchestration, dependency management, operational visibility — all of these become exponentially more difficult.
Without the appropriate skills, culture, and tooling, these initiatives can be doomed to failure.
Some legacy applications work just fine and there is no need to “fix them if they are not broken”.
Logging, monitoring, exception handling, fault tolerance... All of these surrounding infrastructure issues need to be thought through.
There are a lot more components to monitor and track, and more chances of local failures.
The system needs to be designed to handle failure at every interaction point.
We’re investing in a backward compatibility layer to minimize disruption.
Other The difficulty of hiring people with the skillset.
It's hard to find the right developers.
Lack of documentation.
A critical component to make microservices in place.
Highly-regulated industries and those that use waterfall development methodologies and have less frequent software releases, such as healthcare, government, oil and gas, and manufacturing, may not benefit from a move to microservices architectures.
They will continue to revolve around the business problem they’re trying to solve they will reorganize around smaller organizations/projects.
“Micro” can be misleading.
All of those problems are made worse because microservices-based apps tend to have a lot more moving parts.
If you look at the ecosystem you see more companies specializing in one of those problems, there’s a whole industry popping up around the operations of microservices.
We need to educate on how to interact with environments and find problems more quickly.
1) Microservices are critical for parallel programming, DevOps, and the cloud to grow.
4) The architecture is the critical part to get right.
We asked, "What have I failed to ask you that you think we need to consider with regards to migrating to microservices?"
Even though not directly related, security is becoming part of the microservices design.
Also, think about the potential for failure.
The claim around integration difficulty is reminiscent of the challenges of SOA and suggests that developers are using SOA as a model for microservices, which is not a good plan of attack.
Developers need to understand that each microservice only serves data to a limited range of other microservices, so the communications protocols should necessarily be compact and simple.
An enterprise should already be on that organizational and cultural journey before refactoring existing applications and should have a compelling technical requirement – restrictive dependencies, inability to scale, high rate of change, etc.
I think one of the areas missing is around a multi-cloud and hybrid environment.
With this consistency model, the application must be able to handle consistency conflicts.
It cannot take advantage of managed solutions.
It needs to operate reliably with almost no real-time visibility for the development team.
(Note: some profilers can work off thread and memory dumps in a limited fashion.)
Screenshot: Retrace Action OverviewPros: The ability to monitor your most critical environment: Production.
Cons: Can be difficult to connect to in a staging and production environment.
Tracking Failed Requests is also very useful, which can be done by aggregating on HTTP Response Codes.
Tracking All Java ExceptionsOne of the biggest causes of performance problems can be application exceptions.
It is important to aggregate and monitor all of your exceptions to find critical problems, new errors, and monitor error rates over time.
Cons: NonePopular Tools: APM providers, Raygun, StackifyRead more: How GWB Found Hidden Exceptions and Application Performance Problems8.
By monitoring JavaEE Servers in both production and QA environments, you become able to make decisions based on trends, before problems become too severe.
Memory AnalysisApplication memory analysis after a crash can help with identifying the cause of a memory leak.
You can instruct the JVM to dump the heap on an OutOfMemoryError exception by adding the following argument to the JVM:-XX:+HeapDumpOnOutOfMemoryErrorThe heap dump file can be loaded into an analyzer: Eclipse MAT.
You must pay attention to the data pipeline(s).
3) Don’t abuse the environment variables.
Think about how to store, log, and monitor – this is critical with containers.
Understand what happens in the layers of Docker file, base image, where the software package comes from, understanding whiteouts, building Docker images and running them through the scanner to see vulnerabilities.
Get your hands dirty and know where the potential problems are.
It will help with automatic deployments, updates, rollbacks, troubleshooting (since we can say that container 1 is bad, but container 2 is working), and most of all, scaling production environments.
Do not use containers as a scapegoat for all errors.
Knowing how the content of your container relates to overall application performance is critically important for avoiding issues that are noticeable to your end users.
Monitoring is no longer just for ops teams.
It can let you know when things are heading in a dangerous direction, sometimes enabling intervention prior to a failure point.
When problems do occur, monitoring your infrastructure tells you quickly if it’s an app problem or infrastructure problem.
If your service is a dependency to others and your instance is down, it’s critical that the development team have this information.
Application MonitoringStabilityStability monitoring measures overall error levels and helps you understand when application errors have reached a critical level that could impact your users’ experience.
This is especially important since buggy applications are frustrating and considered unreliable by end users.
In the past, logs were used to track application errors, and engineers would investigate them when they received customer complaints.
This was a painful experience for both engineering teams trying to piece together what caused a bug, and also for the user who experienced the bug and was responsible for reporting it.
Since then, engineering teams have standardized on error monitoring software that automatically captures and alerts on errors, but this still requires lots of engineering overhead to triage the list of errors continually.
Stability monitoring adds logic to the process of monitoring for errors.
It provides a definitive metric that lets engineering teams know when errors are impacting stability to the point that they must spend time debugging.
Slowdowns in request processing from both the server-side and client-side can result in a poor user experience.
Performance bottlenecks in your code base can produce downstream effects on multiple services, sometimes leading to a cascading failure.
Setting up KPIs for any service where higher CPU or memory usage could result means you are notified closer to the source of any problem occurring.
The downside of RUM is that the data can be noisy as each user’s internet speed and system influence the metrics collected.
Synthetic user monitoring is frequently overlooked in the monitoring toolset available to engineers.
One downside to SUM is that it requires setting up the behavioral scripts to capture the information.
It started with Jonah Kowall, a former Gartner APM analyst, and today VP of market development at AppDynamics, writing about Misunderstanding "Open Tracing" for the Enterprise.
His criticism is that OpenTracing is not a broadly adopted open standard and explains that Enterprises have highly heterogeneous environments that require automated, agent-based tracing.
Alois Reitbauer, Chief Technology Strategist at dynaTrace, reacts in his blog A CTO's strategy towards OpenTracing that OpenTracing is not a standard and agrees with Jonah about the need for standardization of TraceContext.
In microservices architectures, understanding dynamic dependencies using topology and graph analysis is more difficult than in traditional architecture.
Instana's AI leverages our unique Dynamic Graph which provides the context to pinpoint to the root cause of problems and ultimately arrive at accurate causation.
We predict that there will be no ultimate reason to drive to a single tracing standard.
Observability vs. MonitoringObservability is a hot topic at the moment, stirring a lot of debate principally around the argument that observability is different than monitoring.
It does not make any sense to continually push out changes without knowing if they make things better or worse.
Without meaningful analysis, you've fallen short of the whole purpose of creating observability and performing monitoring in the first place.
The most difficult form of observability is distributed tracing within and between application services.
Modern application delivery has shifted to CI/CD, containerization, microservices, and polyglot environments creating a new problem for APM vendors and for observability in general.
New software is deployed so quickly, in so many small components, that the production profilers of the SOA generation have trouble keeping pace.
They have trouble identifying and connecting dependencies between microservices, especially at the individual request level.
This strategy MIGHT be acceptable for SOA applications, but is completely unacceptable in the microservices world.
The problem is so pervasive that the Cloud Native Computing Foundation (CNCF) has multiple open source observability projects in either the Incubation or Graduated phase.
Take pent-up demand and use cases to pull out portions of the larger monolith and piece together in more manageable chunks.
If you have trouble deploying monolithic apps on a regular basis it will multiply with microservices.
It's the next step for being more aggressive, separating functionality into smaller and smaller chunks.
Fortunately, there are a variety of tools for each aspect of SRE: monitoring, SLOs and error budgeting, incident management, incident retrospectives, alerting, chaos engineering, and more.
Without logging latency, availability, and other reliability metrics throughout your system, you’ll have no way of knowing where to invest your development efforts.
Monitoring can be broken down into four main categories:Resource monitoring: reports on how servers are running with metrics such as RAM usage, CPU load, and remaining disk space.
Network monitoring: reports on incoming and outgoing traffic which can be broken down into the frequency and size of specific requests.
SLOs and Error BudgetingOnce monitoring is in place, there’s no better way to put that data to work than building SLOs and error budgets around them.
The inverse of the SLO is the error budget: the amount of room left on the SLO before exceeding the threshold.
As your SLO and error budget will be key decision-making tools in development decisions, find tools that can display changes over time.
Incident ManagementFailure is inevitable.
In an SRE mindset, incidents aren’t failures or setbacks, but unplanned investments in reliability.
This also informs SLOs, as follow-up actions can include increasing monitoring in certain trouble areas to get early warnings of future issues before they become customer-facing.
Chaos EngineeringChaos engineering is a disciple practiced for testing resilience.
Chaos engineering tools such as Gremlin and Chaos Monkey simulate outages, intense server loads, or other crises that could jeopardize reliability.
These experiments take place in small replica environments with no consequence to the live build of the service.
To be effective, a chaotic engineering tool will need to affect systems as if it was a real external threat.
Your chaos engineering tool should give you meaningful results across experiments.
Chaos engineering provides an opportunity for incident responders to build experience using and refining procedures.
IT departments have sold the transition to the cloud as a self-service haven where development teams no longer suffer from the multi-day-ticket hell of days past and are freed to move at their own pace.
When done properly, automation reduces a team’s workload, reduces errors (or rapidly creates them), and provides living documentation of a team’s workflow.
Since much of the drive towards automation comes from the migration to the cloud, a platform designed by and for operators, this is not surprising.
Fortunately, we have a solution to the current crisis brought on by the application of 2008 operations approaches to 2018 DevOps problems: development, the other half of our favorite portmanteau.
And lest the monorepo crusaders—you know who you are—look down upon the microservices practitioners and scoff at the troubles beset them by their dozens of repositories, let’s not forget that monorepos, whose builds and deployments are typically more complicated than microservice repositories, are no better served by untestable YAML and shell scripts.
The only difference is that when you make a mistake in an untestable monorepo build script, you break everyone’s build.
He will be missed.
Without carefully monitoring key metrics like uptime, network load, and resource usage, you’ll be blind to where to spend development efforts or refine operation practices.
In architecture with physical servers, information on hardware health—like CPU temperatures and component uptime—can also be helpful to avoid server failure.
Continuous delivery enables applications to released quickly, reliably & frequently, with less risk.
Containers provide lightweight virtualization by dynamically dividing a single server into one or more isolated containers.
Here, we have grouped tools and solutions as per the problem they solve.
Culture: Adopting cloud-native practices needs a cultural change where teams no longer work in independent silos.
As you may have noticed in the above infographic there are several projects, tools, and companies trying to solve similar problems.
Developers should now be able to immediately see the potential impact of code changes, remediate and fix poor app behavior earlier in the lifecycle.
This visibility during development allows developers to see the performance impact of their changes in almost real-time, plus it helps eliminate finger-pointing when something goes wrong.
When operations share real-user monitoring insights with development, apps can be future-proofed to conquer the stresses of production deployment.
Proactive alerts on a wide array of error conditions and faults helped developers diagnose and repair Java application performance related issues faster.
Applications Manager was able to break down every web transaction to the PostgreSQL database level and track each and every single query like a “SELECT” or “INSERT” statement fired at the database level and their corresponding response times, which in turn helped developers to fine-tune the discrepancies found at the DB level, if any.
At present, Applications Manager's capability of code-level diagnostics is limited to Java, .NET, and Ruby on Rails applications.
Applications Manager has a set record of capacity planning for various Oracle and IBM applications and Unix and Solaris based systems, but it could not be proved useful in the case of Java applications.
It helped achieve the developer’s goals of hitting the actual pain point and being pointed in the right direction in the application and associated areas before the end user is affected.
The tool does not currently support a SAAS based deployment.
We were able to receive application performance degradation alerts like memory leak, longer running components timely and trigger the actions associated.
They can automate and expedite fixing the common performance problems introducing runbooks.
Log AnalysisAble to provide file content level monitoring wherein it can identify some error patterns & strings for Java applications running on Centos 7 and Redhat 7 systems.
Root Cause AnalysisDetection of the source of performance degradation lacks machine learning.
For you, one critical pillar for achieving this is having reliable and highly performant web pages.
With an exclusive test environment, the results will be more or less predictable and won’t be affected if another person, for example, runs something else at the same time, causing the response times to soar, generating false positives, and wasting a whole lot of time.Another key aspect is that the tests should have acceptance criteria (assertions) as close as possible so that at the slightest system regression, before any negative impact, some validation fails, indicating the problem.
Say we run the first test with 100 concurrent users and as a result, there are no crashes, the response times are below 100ms (at least the 95th percentile), and the throughput is 50 TPS.
We will also be getting errors (exceeding 1%, for example) that would indicate that we are saturating the server.
It’s likely that response times from then on will begin to increase significantly because some process or connection or something begins to stick amid all the architecture of the system.Following this scenario, imagine we get to 350 concurrent users and we have a throughput of 150 TPS, with 130 millisecond response times and 0% errors.
This is the basic methodology for finding the knee when doing stress testing when we want to know how much our servers can scale under the current configuration.
Then, the test that we will schedule to continue running frequently is the one that executes 350 users, is expected to have less than 1% error with expected response times below 130 times 1.2 ms (this way we give ourselves a margin of 20%).
The important components are going to be the performance testing tool (for load simulation), the Continuous Integration engine, the visualization and storage of monitoring data, and deeper information on the system’s behavior for analyzing problems and providing more information.ResultsThe day someone enters a line of code that worsens the response time by 20% or harms the scalability of the login, it will be detected almost immediately.
You cannot assume a certain VM.
Understand the business needs and the needs of the customer and provide a simple solution to the problem.
Fail fast before deployed rather than using a testing organization.Think about the parts that can fail–use AWS and GCQ to bring up instances to see what works and what fails.
Logging - Important to be able to debug issues and capture everything about a failure so it can be fixed with little assistance from the customer.
Multi-tenancy - the toughest hurdle for all on-premise developers since the application is not being built for a single customer behind a firewall.
Don’t repeat the mistakes of the past.
Identify what are and are not acceptable performance levels.1) Tech perspective of V5 of no app.
Does Monitoring Still Suck?
which problems at which company sizes.
First, in some ways, monitoring still sucks, in ways we’ll explain in more detail below.
Second, the pain is actually going to get worse as more companies move to microservices.
There were a lot of different tools in the “Other” category, but no particular tool stood out.
Spammy AlertsIf there was one consistent complaint from all the companies we spoke to, it was about overenthusiastic alerting.
The problem is only getting worse as companies scale out on more servers or run microservices on continuously changingcloud environments.
However, most ofthe monitoring tools they used suffered from poor usability and dated UIs, so the collected data was siloed away for the eyes of the operations team alone.
Unfortunately, it’s clear that current monitoring tools have not been designed around this microservice- centric model, and most suffer from poor usabilityand adoption in teams outside operations.
Conclusion There are many more monitoring tools available in the four years after “#monitoringsucks” became a DevOps meme, but our research shows that many organizations are still struggling with monitoring.
We mock the external APIs for our unit tests, and use sandboxed versions where possible.For Kevin: how does CircleCI perform its functional integration tests to fit in with its CI/CD process?
We use New Relic but our mobile developers hate it.Kevin: I’m not personally aware of mobile monitoring tools other than New Relic and AppDynamics.
defection detection times)Kevin: This is another question that is hard to answer in broad terms, but here are just a handful of thoughts:Some measure of velocity (either scrum-style points/sprint, number of features/week, etc)Defect rate (e.g.
That is “50% coverage” means there is definitely a lot of code that that isn’t tested, but “95% coverage” does not mean “everything’s fine, all cases covered!” It is entirely possible to have 100% coverage and many possible program states untested.How do you measure feature quality / bug incidence in your team?Fred: First a caveat: don’t read into this too much.
Lots of bugs found by testing can mean you’re testing more rigorously or your app is more buggy or both.
So penalizing for bugs is likely to incentivize bad behavior.
Again, this isn’t anyones ‘fault’.What can be useful though is measuring test failures over time, especially broken down by browser, OS etc.
However, your other teams should continue working on new stuff, and assuming that you’re doing CD this should not be a problem.How do you recommend catching up on missing tests as a result of tests not being an important part of the first few months of the company, when these features already work anyway, don’t often change, and there are business deadlines to meet unrelated to these features?Fred: I’m assuming that you’re asking about the first few months of existence of the company.
:)Kevin: This process is very similar to any performance optimization problem.
Other time, there may be large numbers of tests all suffering from the same performance issue.
For example, it shouldn’t take a black-box browser test to make sure that a form controller throws an error for a date in the future (that can be done with a unit test).
Unfortunately there’s no one-size-fits-all solution to make tests faster.
Ultimately you will have to dig in and see what’s taking time in your own test suite.Did you miss the live webinar with Kevin and Fred?
Don’t worry – you can download the on-demand recording and catch what you missed about getting to continuous deployment!
The huge volumes of traffic surging to sites on Cyber Monday can cause them to crash, leading site owners to miss out on thousands or even hundreds of thousands of dollars in sales.
In 2011, many of the top 55 retail websites were down for at least part of the day, disappointing customers who were looking for good deals from their favorite retailers.The Downside of Cyber Monday: Failures and FlopsEven as recently as 2015, some retailers struggled to cope with the challenges posed by Cyber Monday.
Although the major retailer Target managed to prevent a complete crash on the big day, many customers experienced the frustration of being placed in the online equivalent of a long checkout line when the message “Please hold tight” appeared on the site.
Target claims that forcing some customers to wait to access the site helped to manage the demands on the server, preventing a complete crash.
Consumers feel disappointed, frustrated and sometimes even angry when they hear about a great deal but aren’t able to access the site that is offering it.
Many businesses spend time planning and advertising attractive promotions for Cyber Monday, but if you fail to equip your server with the resources it needs to handle the customers that come pouring in, you will likely fail to capitalize on your investment.
Load Test Your SiteBefore Cyber Monday arrives, it is a good idea to load test your site with extremely high amounts of traffic in a pre-production environment and filter all that triggers a fail point.
This will let you see where your site is failing, so you can address it before the big day.3.
Using a single data center can result in a bottleneck, which can lead to site failure.
Remember Mobile UsersIf you have a separate mobile site for your smartphone and tablet users, do not forget to ensure that it is just as well-supported as your main site when Cyber Monday rolls around.
Mobile users often have slower connections than desktop users, so it is even more important to minimize lag for these users and keep page loading times as short as possible.5.
Having your production environment monitored 24/7 can help you avoid costly technical mishaps that can result in loss of both revenue and brand equity.
Not only will your company lose sales opportunities but the technical liabilities may damage your reputation among consumers.
microservices iterate over the basic separation of concerns principle and put a name on insights from teams of engineers who crafted solutions to real problems they needed to solve.
mix the two, and you get a petri dish of all sorts of awkward problems.
put it in production, and the problems quadruple.
as   bryan cantrill  points out on   his talk in qcon  , "debugging devolved into an oral tradition, folk tales of problems that were made to go away."
i can remember the exact instant when i realized that a large part of my life from then on was going to be spent in finding mistakes in my own programs."
we were trained into thinking that it's just about making problems go away.
problem #1: as if monitoring a monolith wasn't hard enough whether you're gradually breaking down a monolithic app to microservices or building a new system from scratch, you now have more services to monitor.
for example, one of the scenarios described in a recent   continuous discussions podcast  was a bad version that needs to be rolled back.
so we need to identify which of the services needs to be rolled back, what would be the impact of the rollback on the other services, or maybe we just need to add some capacity, but then it could just push the problem to the next service in line.
takeaway #1:  if you thought monitoring a monolith architecture was hard, it's 10 times harder with microservices and requires a bigger investment in planning ahead.
problem #2: logging is distributed between services logs, logs, logs.
the it equivalent of carbon emission in the shape of overflowed hard drives and crazy splunk bills/elk storage costs.
now, when investigating a scenario related to some user transaction, you'll have to pull out all the different logs from all the services that it could have gone through to understand what's wrong.
for all log errors and warnings coming from production jvms, we inject a smart link into the log, which leads to the event's analysis.
as a side effect, ops procedures and monitoring are also breaking down per service and lose their power for the system as a whole.
problem #3: one service's issue can cause problems elsewhere if you follow up on some broken transaction in a specific service, you don't have the guarantee that the same service you're looking at is to blame.
there could be several possible scenarios going on, even if the service behaves as expected and there's no problem to be found:   the input it received is bad, and you need to understand what made the previous service misbehave.
or there's more than one service that benefits to the problem?
whatever the problem is, the first step with microservices is to understand where to start looking for answers.
problem #4: finding the root cause of problems alright, let's carry on with the investigation.
the starting point now is that we've nailed down the problematic services, pulled out all the data there is to pull — stack traces and some variable values from the logs.
the code that's actually broken.
takeaway #4:  when the root cause of an error in a microservice spans across multiple services, it's critical to have a centralized root cause detection tool in place.
two problems that can happen here relate to keeping your dependencies in check.
1. if you have a cycle of dependencies between your services, you're vulnerable to distributed stack overflow errors when a certain transaction might be stuck in a loop.
reproducing a problem will prove to be very difficult when it's gone in one version and comes back in a newer one.
takeaway #5:  in a microservice architecture, you're even more vulnerable to errors coming in from dependency issues.
final thoughts debugging immediately gets you in the mindset of making problems go away — this is literally the direct meaning of the word debugging.
when you think of it with the system's context in mind, there's much more to it than ad hoc problem solving.
Most are made to be permanent attachments to walls or machines, but some are small enough to be hidden away where no one will notice them.
None is fine grained enough to reliably pinpoint individual mobile devices inside buildings.
What makes Eddystone unique is that it offers a way for beacons to deliver a packet of data and a URI so there is no need for an app at all.
Once a proximity measure is calibrated and hard coded into the beacon, it uses those configurations to estimate proximity.
Battle of the PaaS: Python Apps in the CloudIn the early days of the web, web pages were static and did not change.
Your app will scale, and the load will balance automatically in response to fluctuations in demand.
It takes less than a minute to get started, and you can upgrade as you grow with paid plans that start as low as five dollars a month.
Experiment with various simple programs at no charge to develop your concept.
It is a no muss, no fuss option that gets you up and running with shell access control.Power users with some resources will more than likely turn to stalwarts such as Google App Engine and Amazon Web Services.
The Definition of BlockchainAs you might expect from a new technology, blockchain has conflicting definitions.
One of the most compact definitions comes from Deloitte: “Despite its apparent complexity, a blockchain is just another type of database for recording transactions — one that is copied to all the computers in a participating network.”The blocks are arranged in fixed structures that include a header and the content.
The header block contains all the metadata like a time-stamp, reference number, and a link to the previous block.
The content block contains a validated list of digital assets and instructional statements.
They know that bitcoin is a software-defined currency preferred by anonymous users possibly doing illegal things.
That’s when the FBI shut down operations at the Silk Road site, which used bitcoins to make payments anonymous for drugs and other illegal activities.
Anonymous virtual currency is just one very limited application of blockchain and isn’t what IBM had in mind when it launched a test of blockchain this year for more accurate record keeping in its supply chain.
In emerging markets where it is difficult to track and verify land ownership, it is used as the backbone of a comprehensive land title system.
The Promise and the PerilDespite all the positive possibilities blockchain opens up for more efficient exchanges, monetary and otherwise, Gartner’s Furlonger also pointed out that, “In its current form, blockchain suffers from significant limitations in scalability, governance, and flexibility.” Scalability is limited by the vast amount of computer and power resources required.
it identifies 100% of errors, prioritizes the critical ones, and gives you the actionable information needed to fix them.
this includes the exact variable state that caused the error across its stack trace.
the integration:    a new way to create tickets and get in-depth error information.
for errors within takipi, you can click the “create jira issue” button to submit a ticket with a full report including the stack trace and variable values which led to the error.
when looking at a ticket in jira, the integration adds links to takipi for you to get in-depth information on the specific error.
zephyr test management capabilities in jira   conclusion   jira is an excellent ticketing and issue tracking tool, but using it without integrations is like using your smartphone without apps.
Without the ability to correlate downstream service requests it can be very difficult to understand how requests are being handled within your platform.
This might be fine for some use cases, but no so much for others, especially when you are building microservices.
The problem with using ThreadLocal variables within the asynchronous approach is that the Thread that initially handles the request (and creates the DeferredResult/Future) will not be the Thread doing the actual processing.
This can be achieved by extending Callable with the required functionality: (don’t worry if example Calling Class code doesn’t look intuitive – this adaption between DeferredResults and Futures is a necessary evil within Spring, and the full code including the boilerplate ListenableFutureAdapter is in my GitHub repo):public class CorrelationCallable<V> implements Callable<V> {    private String correlationId;    private Callable<V> callable;    public CorrelationCallable(Callable<V> targetCallable) {        correlationId = RequestCorrelation.getId();        callable = targetCallable;    }    @Override    public V call() throws Exception {        RequestCorrelation.setId(correlationId);        return callable.call();    }}//...
This might be fine for some use cases, but no so much for others, especially when you are building microservices.
This can be achieved by extending Callable with the required functionality: (don’t worry if example Calling Class code doesn’t look intuitive – this adaption between DeferredResults and Futures is a necessary evil within Spring, and the full code including the boilerplate ListenableFutureAdapter is in my GitHub repo):public class CorrelationCallable<V> implements Callable<V> {    private String correlationId;    private Callable<V> callable;    public CorrelationCallable(Callable<V> targetCallable) {        correlationId = RequestCorrelation.getId();        callable = targetCallable;    }    @Override    public V call() throws Exception {        RequestCorrelation.setId(correlationId);        return callable.call();    }}//...
One respondent expressed concern that Sun’s lack of leadership and major missteps (citing JavaFX and JSF) have led to a number of conflicting approaches in basic areas of the platform like UI and data binding.
Members of the IT community have an inherent bias against anything that’s been around for more than a few years.
A more specific concern is the poor expressiveness of the Java language, which can result in code that takes longer to write, is harder to read, and tends to be rigid in the face of evolving requirements.
Java continues to struggle from JAR hell, a problem similar to DLL hell, which .NET solved years ago.
Project Jigsaw, which is planned for Java 9, should alleviate the problem; however, a definitive solution has not yet been found.
Concerns with the Java ecosystem revolve around the omnipresent tension between Oracle and Google.
If Oracle does not resolve their differences with Google, this could cause many partners to look for other solutions, causing Java to stagnate.
The future of the Java ecosystem lies in IoT, mobile, and enterprise app development.
Some parting thoughts for developers: The Java ecosystem is massive, and the Java community has already solved a lot of problems.
Do not begin creating solutions from scratch until you’ve thoroughly researched pre-existing solutions.
In the last few years, I have come across several interviews, discussions with client-facing groups and customers, and what they have in common is a high demand for performance engineers, not just performance testers.
Before one worries about performance engineering skills, learn the basics of web development technologies.
Harbor container registry scans images for vulnerabilities while also providing image signing and validation.
Most people just expose the daemon and assume TLS is secure enough; what they forget is that TLS certs are portable, and anyone with the key has 100% access to Docker.
No keys are stored in configuration.
We have our own orchestration framework, but it needs more isolation and hardening on the host which is VMware on Amazon.
firewalls, restricted access lists, intrusion detection systems).
We employ the weakest link philosophy.
1) Let the customer know if there are issues with running containers rogue or running with vulnerabilities.
OtherApplication and dependency management problem.
This became alarmingly clear when the massive series of DDoS attacks hit Dyn in 2016, and much of the websites and applications on the Internet became inaccessible to huge swaths of users across North America and Europe.
The DNS lookup and resolution process should take milliseconds, but if something goes wrong along the way, your browser will lag, be unable to access the site, or worse, get hijacked and redirected to another (potentially malicious) site.
“All these changes can lead to “tremendous risk for the installed base whose businesses depend on those services,” says Brian Zeman, COO at NS1 in a recent blog on the Oracle announcement.
There are a host of problems that can go wrong within the multi-step DNS process.
This means you can’t detect issues fast enough (or at all), and also cannot properly troubleshoot the issue.
It allows you to look at the performance of all the nameservers involved and detect any errors along the entire DNS resolution chain.
It also gives you a window into the database records used by DNS servers (from MX to CName Records), which enables the diagnosis of the specific cause(s) of an error such as misconfiguration, DNS Cache Poisoning, or insecure zone transfers.
As we’ve recently been discussing, it is also critical to monitor your Service Level Agreements (SLAs), and not simply rely on the vendor’s assessment of their service, which might not bring the accountability you require as a buyer.
Furthermore, you can’t monitor DNS solely using inferred DNS metrics by monitoring HTTP URLs, as you are not truly monitoring the DNS service and the vendor infrastructure providing the service.
It allows you to trace queries through the complicated web of DNS hierarchy, pinpoint the issues, and solve any problem with your vendor, whoever they may be amidst these changing times.
DNS is the first interaction a customer has with your brand; it’s the most critical, and at the same time, the most fragile because of UDP.
Scalability, fault tolerance, and geographic distributions.
The learning curve was steep, but it has helped us to apply common principled solutions to very complex problems.
With microservices, every outage is like a murder mystery.
It comes down to accepting CI/CD you’re going to have trouble.
People either try to rewrite and fail miserably or just do greenfield with no plan for integration.
The way applications are stitched together is poorly documented and people have trouble understanding the anatomy of their legacy apps.
Lack of pragmatism about the problems you are trying to solve.
Lack of documentation.
Poor documentation results in no trust.
Documentation is not seen as valuable.
It should be as small as needed but no smaller.
Understanding how much to break off has to match the business problem to solve and the structure of the organization.
Dependencies1) Dealing with complex middleware (ESB/SOA) that no longer is required but is part of the current implementation.
That’s not an easy thing to do.
Propagating and consuming events is often the go-to solution for these issues but it’s not trivial to adopt.
MonitoringTenacity, you have to see it through to completion when things get scary.
Monitoring becomes a huge issue because it reduces worry.
One common issue is that migrating to microservices is a bit of a cultural shift, as the legacy applications may have been around for many years and some IT teams can be reluctant to change.
Monitoring the application is even more critical as one microservice can cause an issue somewhere else.
Management overhead is the biggest issue that comes from moving from one to multiple services and how to communicate between services that do not share memory.
Scalability, fault tolerance, and geographic distributions.
It may not be worthwhile to refactor very old applications that are built with old languages and databases.
2) Microservice orchestration (deployment, upgrades, rollbacks) require more effort compared to monolithic architecture.3) Operations, diagnostics, debugging require new tooling since a single computation or request can touch multiple isolated microservices.
Rewriting from scratch has many issues — the “why” of some of the features may be difficult to reconstruct, the schedule for the new app may be uncertain.
Go through the pain of doing it the right way or leave it where it is.
Don’t forget it should do one thing and one thing only.
Attackers have the potential to extend their threat beyond the container and into an underlying OS and other containers if container security is not implemented.
Since the workflow in a microservices architecture can be represented by a directed acyclic graph (DAG), developers need to think about how a system can be broken down into smaller components, and lightweight messaging protocols, that allow many different subtasks to be reused in different parts of the workflow.
Look carefully at how to refactor your application and see how it can be broken apart.
Understand the critical components and what is available for you to leverage.
We are in a time when you need to be fast and agile, so you cannot afford to create things from scratch.
If it’s not the right thing to do, stop there.
In implementing microservices it’s critical to design and code for failure.
Patterns like retries, circuit breakers, timeouts, resource pooling, consistent failure logging, and tracing need to be commonplace.
Earlier migrations of legacy apps were constrained by the perceived limitations of microservices architectures.
KubernetesCompanies moving to the cloud, first try to lift and shift and then realized it increased costs and worsened SLAs.
One client started on AWS but realized they needed to go to GCP since retailers did not want to work with AWS.
Before containers, you ended up rebuilding most of the applications, or you had to use multiple virtual machines to host different components, which could have been implemented using different programming languages and components (with possibly conflicting versions).
OtherMoving to microservices the problem is not just writing the code.
The other problem is all the infrastructure will change.
You will need to learn an all-new set of operations tools and that’s another problem to solve.
It has not gotten any easier.
There is no silver bullet or automated way to do this.
The devil is in the details.
Know who can be impacted if something goes wrong.
Microservices' expertise is still hard to find.
Another option teams take when it’s hard to maintain the monolith and still add new functionality quickly is to build new features as microservices.
The concepts of hybrid integration and phased migration have changed the dynamics, so it no longer is about rip and replace.
People find examples of K8 implementations but fail when they move to production because they have not used all of the features and have a lack of proper configuration for deployment.
FUD – perception that containers are not secure.
Also, utilities struggle with management logging.
There’s currently a shortage of experienced Kubernetes, Docker, and other tool engineers and DevOps people.
Lack of experience, people don't know what containers are or don't know the advantages of using them.
You need to reframe the problem.
ToolsRequiring every workload to run in a container but running legacy with no way to update and port to a new environment.
There are so many networking options available in Docker alone, let alone third-party network add-ins like those from WeaveWorks.
Overlay driver is great at “just making networking work” but it does so at considerable performance degradation (10% of native).
Organizations’ inability to keep stateful data when it’s being moved across multiple sites.
Docker and Rocket can be difficult to keep up with tools.
People don’t know how much they have out there and whether they have a missed configuration.
Containers often lead to compartmentalized thinking, causing people to lose sight of the bigger picture: customer experience on the application.
The public cloud is also cost-effective; you pay for what you need, without having redundant servers sitting idly by.
Cons: One con to be aware of is vendor lock-in: some providers make it very difficult (and expensive!)
It also makes it difficult to scale quickly to meet user demand — you either have to plan on having redundant servers sitting around (to accommodate sudden spikes in traffic to your site), which isn't cost effective, or maintain fewer servers with the hope that your site won't go down during a major event.
Pros: When demand fluctuates, a hybrid cloud architecture enables you to scale up to the public cloud, without incurring the cost of having idle servers.
Cons: Ultimately, the hybrid cloud serves more as a transition strategy than anything else; it's really a way to hold on to data and systems that you're not quite ready to move to the public cloud.
Even security concerns with the public cloud aren't necessarily mitigated by a hybrid — or private — option: when managing your own data center (whether in a private or hybrid cloud capacity), you're on your own when defending from denial-of-service (DoS) attacks, and security is yours alone to manage and bolster.
it offers an in-browser widget with no backend required that is automatically injected to the monitored web page.
meaning that it aims to explain how every transaction gets executed, trace the flows between the components and (bad joke ahead) pinpoints problematic areas and potential bottlenecks.
pinpoint also lets you see the request count and response patterns so you’ll be able to identify potential problems.
you can view critical details that include cpu usage, memory/garbage collection, and jvm arguments.
to get started, all you need to do is drop the .jar file into the web-inf/lib folder or by including a small new section in the web.xml file.
to get started with glowroot, you need to download and unzip the main installation file and add -javaagent:path/to/glowroot.jar to your application’s jvm arguments.
glowroot’s dashboard   bottom line:  if clean and simple is what you’re looking for, no doubt you’d want to check out glowroot over the other tools here.
kamon is distributed as a core module with all the metric recording and trace manipulation apis and optional modules that provide bytecode instrumentation and/or reporting capabilities to your application.
the only problem is that once you get that haystack in which the problem was found, you have to start digging around looking for the actual needle that caused it.
Understand and define different business domains in an isolated manner.
Solve the real problem around engineering and scaling.
Isolation and API guarantees are key elements.
Isolation enables developers to iterate quickly and independently.
Ensure microservices are adequately decoupled but avoid premature decomposition.
Microservices are a management issue moving from hundreds of developers working on an application to no more than 10 to12 people per team.
The culture of the organization is critical as this is where we see the manifestation of communication problems.
Things happen faster once you’ve broken things down.
A team bigger than ten people is not efficient and hinders autonomy.
The biggest and most relevant use case is fraud detection built out machine learning models and putting in line with microservices.
You cannot stop supporting the monolith.
Microservices are more difficult to debug and troubleshoot.
How to know something is going wrong when it’s going wrong is key.
Without monitoring, you’re not able to see where the problem is.
We are confronting a lack of maturity in tools to monitor and troubleshoot.
Infrastructure will be less of a barrier to entry.
Look for trends that hide complexity from developers and make them as productive as they can be withoutworrying about infrastructure.
For instance, APM tools can understand transactions across processes and can provide insights into which service in a graph of services leads to business functionality (service) disruption.
Step 2: Isolate Top Services That Break Your CriterionOur initial tests showed we took about three seconds for API calls when we loaded the PowerBI dashboard with the response times seen:The next step would be to isolate the services, which consumes more than 80 percent (or 90 percent, in your case, basically the majority) of the latency slow down.
Based on tests, we were able to isolate to the following APIs (which seem to have the largest datasets):Step 3: Triage and Find the Performance IssueResource Bottleneck This is the first area to investigate: is the JVM process struggling with garbage collection or not having enough CPU cycles?
Area of the Problem We also noticed the memory allocated was 1.3 Gb, which was lesser than what the infrastructure folks had allocated.
Application Code Profile deeper if the issue is in application code — once you have isolated which services are causing a majority of the performance bottleneck, you dig deeper where the slow down is really happening.
New Relic allows custom instrumentation (add methods inside your application that are instrumented by default, which you doubt without needing to restarting the application), and in our case, we found a count operation that calls into MongoDB count API for that incoming query range.
We noticed a significant amount of transaction time was getting spent in this block of the method code.
Still, our callSummary API was not doing that great.
NOTICE: The countForQuery method is not in the top methods consuming time.
In our case, we doubted MongoDB's performance and resource allocation.
Indexes Based on the above inferences, we doubted if indexes are an issue.
But we concluded, since our APIs calls are very open-ended in terms of query patterns from PowerBI, there was not much improvement we could squeeze out with more indexes.
Fix whatever problems you are having today before you start the migration.
Microservices will exacerbate a preexisting problem.
Resilience, reliability, scalability, and fault isolation are all benefits of microservices.
The most common issues around the migration to microservices involve a lack of knowledge and skills, right-sizing services, identifying dependencies, monitoring, and data management.
Migrating to microservices requires a cultural shift as legacy applications have likely been around for a number of years and IT teams can be reluctant to change.
It should be as small as needed but no smaller.
Monitoring becomes a huge issue as it reduces worry and one microservice can cause an issue elsewhere.
Concerns regarding migration to microservices include identifying the business problem, complexity, knowing when it’s appropriate, and infrastructure and management requirements.
Scaling, orchestration, dependency management, and operational visibility all become exponentially more difficult.
Without the appropriate skills, culture, and tooling, these initiatives can be doomed to failure.
Organizations and microservices will continue to revolve around the resolution of business problems.
Microservices will be able to incorporate business logic so the business can focus on where its expertise lies.
There are opportunities and problems as environments get bigger with regard to how to manage the complexity and the cattle.
Don’t forget your microservice should do one thing and one thing only.
Think about how the system can be broken down into smaller components and lightweight messaging protocols.
Do it the right way or leave it where it is.
so folks can just drop in source code and the rest just happens.
Moving the application is not as challenging as moving the underlying data and maintaining the underlying services.
Here's what they told us:DataIf you don’t slice and dice responsibilities correctly you run into problems.
Think about the potential problems you will encounter.
Managing data and managing states is where a lot of people have problems managing stateful and stateless data.
If crash and burn, you can just replace if it's stateless.
You cannot stop supporting the monolith.
This was a hassle developers didn’t worry about in past.
Microservices are more difficult to debug and troubleshoot.
No tools currently exist to inspect the entire application.
How do we instrument our app and get data back to interrogate the platform and the app, and when going well to compare when they start to degrade?
Visibility and exposing to the people who have responsibility for the applications – developers.
Lack of visibility into end-to-end processes that span multiple microservices: 59% of respondents.
Ambiguous error handling, leading to unaddressed errors at the boundaries between microservices: 50% of respondents.
Difficulty hiring developers who have the right skill set: 35%.
If you don’t have a platform and your culture is not aligned properly it’s difficult to get microservices into the product.
1) Could become the wild west without proper governance leveraging a service that exposes data that should not be exposed.
It's not easy to find the answer people are looking for.
Not sure what question to ask to solve the problem.
Without automation, if you are having a hard time deploying large things deploying hundreds or thousands of smaller things can’t be done without automation.
Frameworks, functions, serverless, moving so fast it's difficult to pick the right tool.
We're getting close to the point where a developer can create a service and deploy without caring where it resides.
We currently see a lot of confusion around data and databases.
We’ll continue to go down the path see how hard it is and not get the value they expected and then abandon and go to serverless.
See the hype of serverless and it will be distracting.
There’s a lot of uncertainty when building a new product.
If you look at your capacity to build an application and try a bunch of different things a lot of which will fail.
If you are constantly incurring infrastructure pain for no good reason up-leveling has to occur.
Security perspective – microservices are more insecure than monolithic apps because of their context and disposable nature.
Organizations will misapply, misunderstand, and fail to understand the complexity of microservices.
Be realistic and realize microservices are not a magic solution to every problem.
Without monitoring, you are not able to see where the problem is.
It’s really important to have visibility into these business processes—to know the current state of the business, to see when something has gone wrong, so problems can be quickly addressed—but most microservice monitoring tools were built with single services in mind or concentrate on relatively short call stacks, rather than long-running, cross-microservice flows.
We’ve observed at a couple of customers that choreographed microservices get a lot of hype, meaning that the microservices use peer-to-peer or event-driven communication and often neglect orchestration completely.
These customers quickly lose sight of their end-to-end flows, making it at least hard to change anything in the flow.
We have glitches and errors we didn’t see in the past with traditional virtual machines.
Problems are not easy to pinpoint and identifying the root cause can be hard.
Small shops adopting microservices because of their desire to be on the cutting edge is not beneficial.
Make technology choices in a vendor-neutral way so vendors can fight over you.
Lack of standards body has resulted in confusion.
Focus on solving business or technical problems rather than semantics.
There’s no one right answer for all things.
There are not enough best practices developed.
It's early but we see people going in and struggling.
The hype has gone up, people are confronted with difficult decisions about tradeoffs and they need to evaluate if microservices is the right approach.
Building applications for cloud platforms with the assumption things will fail is an area where as many architectures as possible should go toward technology.
Once you push past 500 to 1000 topics you start stressing a Kafka cluster out.
To be pervasive those type of problems need to be solved.
Kafka helps fulfill a need, but you’ll run into problems.
Microservices almost gives you carte blanch because they are disposable, people think about redoing tomorrow and forget about it.
Pushing services out is fun and easy but if they are not run properly with the correct tooling, they fall over, and the impact is exponentially worse with each microservice you add.
Picking the right tool, the right programming language, and the right framework to develop your services, based on your requirements, is critical.
Model threats early in the SDLC.
Microservices are not a magic panacea and more is not necessarily better.
The first stab is straightforward and then you discover tendons that still connect pieces and you slowly and painstakingly fix them.
If you take a team that doesn’t know the application well and tease it apart, you’ll have a mess.
Think hard about how to discover and remediate issues.
You can’t hide by not knowing something.
When business transactions span multiple microservices, the data for those microservices cannot be backed up independently since they comprise a single business transaction.
Learn but be skeptical, why are we doing this?
Failure is inevitable when working with distributed systems (e.g.
This is relevant both when isolating failures to prevent one service from bringing down an entire application (for example, implementing a “fail fast” pattern using a circuit breaker or similar tool) as well as taking responsibility for failure rather than passing the responsibility back to upstream services or in the worst case even the end user.
How do you ensure every “long-running flow” that spans multiple microservices always finishes once it starts, even if something goes wrong (or if it can’t finish, that you notify the right person to look into the problem)?
To be fair, there are times when pushing the responsibility of a failure to the client might make business sense—the important distinction is that this decision should always be made intentionally, and not just because state management and retry logic are difficult to implement.
The scope of a problem is no longer contained within a binary – it may involve hundreds of components.
Understanding the complexity of multiple services running and knowing how to identify where troubles occur in a particular instance of a service is also troublesome.
Log files will no longer cut it.
If you encounter a problem, you can get up to speed and contribute code.
Agile DevOps would not be successful without microservices.
Autonomy – a team bigger than ten people is not efficient.
Instead of focusing on the rigidity and stability of monolithic applications, modern architectures need to be built to change.
This enables adopters to address disruptive trends in their industries before they can have a negative effect on their business.
You can finish things and leave them alone.
Once you finish something you leave it and move on, no maintenance overhead.
Things happen faster once you’ve broken things down – that’s the loose coupling.
Reduces risk with canary deployment patterns.
Make CI/CD more pervasive but people still struggle with contract definition interface management.
Remember microservices principles like data isolation.
From a business point of view, we’re in the risk mitigation space when someone is looking for a loan, they have a problem, they need money.
When writing my application, I’m leveraging a relational database which abstracts away a lot of nasty problems.
Infrastructures can and do fail and the effect of breaking into smaller services we build for reliability expecting things to fail.
It can become more difficult to detect or troubleshoot issues inside a system that’s made up of many microservices.
Getting Started With Modern Application Security AutomationEvery company faces a terrible dilemma — either turn the business into a software business and risk the potential catastrophic downside of getting hacked or refuse to engage in digital transformation and get put out of business by companies that are good at software.
The Dangers of Digital Transformation Given the choice, most companies choose software.
Unfortunately, it's often difficult to see that the cybersecurity risk is exponentially greater than the risk associated with the old real-world process.
We are simply not very good at writing secure software.
The average web application is millions of lines of custom code, open source libraries, and configuration files, and it suffers from a staggering 26.7 serious vulnerabilities.
These vulnerabilities are primarily well-understood weaknesses, such as SQL injection, path traversal, cross-site scripting, weak access control, and using libraries with known vulnerabilities.
The risk isn't limited to your public-facing "external" applications.
In an era of cloud, containers, services, and deperimeterization, the distinction between "internal" and "external" is mostly meaningless.
And there are thousands of these rules, and many of them are tricky, if not downright counterintuitive.
To make matters worse, almost all of these rules are generic and need considerable interpretation to apply to your particular code in your particular environment.
Imagine if you have a portfolio of 100 applications, you have to verify 1,000 rules, you release code several times a day, and these apps (like all apps) are being attacked in production.
Many organizations handle this problem by only assessing and protecting their "critical" applications, limiting the rules they check, and only checking periodically.
Shockingly, most organizations leave the majority of their applications totally unchecked and unprotected.
The more apps you have and the faster you release code, the worse the situation becomes.
Like the parable of the blind men investigating an elephant, these tools can only see one "view" of an application, and therefore make a lot of mistakes.
And automatically starting a scan as part of the CI/CD process does nothing to ameliorate these problems.
Prevent exploits in production using Runtime Application Self-Protection (RASP).
If the behavior of any of your applications or APIs violates a rule representing a vulnerability, weak library, or attack, you'll get an instant notification with all the relevant details, right down to the exact line of code responsible.
But beware of "shifting wrong" by pushing inaccurate scanners onto development teams that are ill-equipped to deal with large numbers of false alarms.
Every company should know exactly who is attacking them, what specific attack vectors they are using, and which applications and APIs they are targeting.
Unfortunately, most organizations are almost completely blind — their only attack visibility is based on very noisy WAF data.
You've worked off your vulnerability backlog and new vulnerabilities are fixed as soon as they are introduced.
Your pentesters are used to test new risks, and deliver their results as rules, not reports, so their test can be automated and run continuously from that point forward.
It's the fastest path to taking control of your software inventory, eliminating your vulnerabilities, and preventing your company from being exploited.
Unlike crash reporting, which manages the detailed information on errors, APM tools surface problems on the server-side of your app.
Confusingly, APM is often used by vendors as an umbrella term for almost anything to do with user experiences, code performance, and application architecture.
All this can make it difficult to know the differences.
Metric-Based APM ToolsSome APM tools are metric-based, and most are designed as an early warning system for major outages.
APM tools collect billions of data points, so a well-rounded tool will not only tell you when your application is experiencing problems, it will tell you why.
Understand and compare the impact of your deployments on critical metrics such as changes to the Apdex scores, new errors introduced, regressed errors, and impact on performance.
Server MonitoringArguably most critical to APM tools is understanding the server story.
One of the biggest advantages of an APM tool is visibility into the causes of problems.
You should be able to see detailed information on the user, function calls, database calls, and external calls so you can troubleshoot performance problems faster.
Tracing must have a low overhead, and should never slow down your application.
Almost 1/3 of software businesses count on their end-users to report these errors, so deployment tracking features alone can save many hours of development time.
Raygun correlates software errors to deploymentsA critical feature of Application Performance Management is to be able to understand and compare the impact of your deployments on page load times.
An APM tool should be able to help you understand and compare the impact of your deployments on key metrics such as changes to the Apdex scores, new errors introduced, regressed errors, and the impact on performance.
Real User MonitoringWhile server monitoring and crash reporting are essential for functioning applications, APM tools traditionally don’t capture anything about the user.
What caused the problem to happen to this user?
What was their journey path before encountering the problem?
Was their whole session poor or just a single page/view?
With crash reporting, real user monitoring and Application Performance Management tool working together, you can ensure that the data you are looking for is correct and that you are actually fixing the problem for the user quickly.
Crash ReportingCrash reporting and error tracking tools are a critical part of the APM strategy.
Otherwise, you have no context about an error.
Screenshot of the error summary page Raygun Crash ReportingDedicated crash reporting tools, on the other hand, offer the full context of an error with a detailed before and after user journey, full stack trace, and local variables.
APM, Crash Reporting, and RUM Work TogetherWhen you add Application Performance Management to crash reporting and real user monitoring data, you start to build a very clear picture of how your software is acting for your customers.
If you already have an APM tool, you can get even more context on errors and resolve issues faster with Raygun Crash Reporting and Real User Monitoring.
change is feared throughout most organizations of any type, so the adoption of new methodologies can quite challenging.
nowadays, businesses are expected to quickly deliver flawless applications that focus on user experience, but without the right tools, applications, and behavior, this seemingly simple task can turn into a complicated mess.
ultimately, faulty delivery translates into missed business opportunities.
paas offerings may require little to no client-side hosting expertise and include preconfigured features in simple frameworks.
examples of vendors and tools:   heroku  ,   google app engine  ,   aws elastic beanstalk    saas   : if you’re familiar with google and facebook, you’ve already been exposed to software as a service (saas).
if problems are brought to light in testing or qa, the software has to be recoded or go even further back in the development process.
testing is implemented early on and often so that developers can fix problems and make adjustments while they build, providing better control over their projects and reducing a lot of the risks associated with the waterfall methodology.
integration and delivery   continuous integration (ci)   – developers integrate code into a shared repository multiple times a day and each isolated change to the code is tested immediately in order to detect and prevent integration problems.
docker  containers linux containers are lightweight virtualization components that run isolated application workloads.
examples of vendors and tools:   github  ,   bitbucket  ,   jfrog  ,   artifactory    github   bug tracking a bug tracker is a system that aggregates and reports software bugs and defects.
examples of vendors and tools:   new relic  ,   appdynamics  ,   datadog   infrastructure monitoring –  tools in this category automatically detect and alert about degradations in underlying physical or virtual resource performance and availability.
The findings of a mobile app user survey conducted by Dimensional Research showed that 80% of app users will only attempt to use a problematic app three times or less and 36% said that an app with slow performance issues made them have a lower opinion of the company.
Unfortunately, a user may have a poor experience with your app’s performance, even if the cause of the issue has nothing to do with the app itself.
Users Are FickleWhen a mobile app’s performance doesn’t meet user expectations, there is a very high chance they will abandon it, resulting in a loss of potential revenue.
Ratings can also suffer, which leads to a decline in downloads.
One app maker reported that even a 0.1 drop in an online app rating caused a 5% decline in downloads, while a 0.3 decrease resulted in a 60% drop.
Although not terribly common, 7% of users will write a bad review if an app has errors, crashes regularly, or stops responding, and 8% will give it a poor star rating for the same reasons.
Also make sure that the image size is right because it’s a waste of bandwidth to rely on the browser to scale a high-resolution image into a smaller width and height.
They should also be rigorously tested for performance and monitored frequently.
It then shows you a report with everything you should know about your app, its weaknesses, and how you can fix them.
APM helps detect and diagnose deep-level application performance problems to maintain an expected level of service across the board.
There are several companies that provide APM Management tools such as New Relic and AppDynamics, Make Your Company’s Culture Performance-CentricAs TechBeacon editor, Todd DeCapua, has stressed many times, performance is now everyone’s concern from the engineering department to the marketing department.
Typical Business Use Cases for Kafka ConnectIT departments constantly struggle with requirements from business units that must connect and integrate all kinds of custom and off-the-shelf applications.
cURL call no.
cURL call no.
ConclusionIf your team faces any of the problems described in this article, you should give Kafka Connect a try.
Once Kafka Connect is up and running, your team can focus on solving actual business problems.
Static Monolithic ArchitecturesWith static monolithic architectures, monitoring is a reasonably well-understood problem.
Logs can be grepped from a single log file, and if something is wrong with the application, operators might simply SSH into the box to poke at it.
It can be difficult to pinpoint how the behavior of a given service affects the user’s experience as the system operates in varying states of partial failure and services interact in unique ways.
With microservices, it’s no longer practical or even feasible to grep log files or SSH into the box to debug a problem.
It’s no use for proactive monitoring because it’s too much noise, and it’s no use for reactive debugging because it’s pre-aggregated.
There’s not much you can do when all you have are rolled-up time-series metrics, and it’s just as difficult to correlate this data back to customer impact.
We know that we know how much memory the system can use before it moves outside of its operating boundaries and bad things happen.
We might; however, be unaware of the unintended side effects of this decision, and it might be based more on theory and conjecture than data.
For example, “instances churn because the orchestrator restarts the process when it approaches its memory limit, causing sporadic failures and slowdowns.” This was an unforeseen consequence of our orchestrator implementation.
Aggregated metrics alone aren’t enough — they don’t have the granularity nor the context needed.
Worst yet, the reliability of an integration may be less visible than your own APIs and backend.
If the login functionality is broken, you’ll have many customers complaining they cannot log into your website.
However, if your Slack integration is broken, only the customers who added Slack to their account will be impacted.
On top of that, since the integration is asynchronous, your customers may not realize the integration is broken until after a few days when they haven’t received any alerts for some time.
Dropping alerts because your Slack or PagerDuty integration is unacceptable from a customer experience perspective.
What to monitorLatencySpecific API integrations that have an exceedingly high latency could be a signal that your integration is about to fail.
What percent of API calls are failing?
In order to track reliability, you should have a rigid definition on what constitutes a failure.
For example, a data API integration that returns no matches or no content consistently could be considered failing even though the status code is always 200 OK. Another API could be returning bogus or incomplete data.
Data validation is critical for measuring where the data returned is correct and up to date.
Not every API provider and integration partner follows suggested status code mappingAvailabilityWhile reliability is specific to errors and functional correctness, availability and uptime is a pure infrastructure metric that measures how often a service has an outage, even if temporary.
This means tracking your API usage with each integration partner is critical to understand when your current usage is close to the plan limits or their rate limits.
Many APM agents capture metrics at the process level which are great for capturing infrastructure metrics like JVM threads profiling or memory percentage but are poor at capturing application-specific context.
When you're consistently running into latency issues or failures with a single vendor, let them know.
If you already have a full audit log showing what happened, the partner may be able to tweak their infrastructure to better accommodate your access patterns or even refund you if they are failing contractual obligations.
Having obsolete platforms and applications does not align to the agility requirements of today, much less tomorrow.
Gary Duan, CTO at NeuVector:Better integration of purpose-built toolings for development, monitoring, threat visibility, and protection throughout the entire pipeline and at runtime.
The monitoring tools most organizations deploy to catch vulnerabilities create blind spots and bottlenecks that are only growing.
This problem is made worse by siloed teams, manual processes, and outdated approaches that leave vulnerabilities missed in preproduction and production environments.
Today, most of these parts are managed as isolated requirements or items.
Most organizations don’t have visibility into who is attacking them, what attacks they’re using, and which systems they are targeting.
We are pretty bad at this as 20 years of Appsec haven’t moved the needle.
Note that ordinary Appsec typically only looks for inadvertent mistakes, not malicious code.
We are also very weak here because with current SCA tools we can’t even stop using libraries with *known* vulns much less deliberately malicious code.
RASP can help prevent zero-day library vulns from being exploited.
An attack here can do anything a malicious developer could do.
Attackers, who have historically focused on (1) have started probing (2) and (3) in recent years.
Peter Oggel, Chief Technology Officer at Irdeto:Automation enables DevSecOps to monitor an attack surface that is increasingly widespread, and almost impossible to monitor without automation technology.
Just looking for CVEs is a commodity and does not protect against the biggest issue: all the big attacks of 2021 went after unknown CVEs.
It is critical to look for malicious code in addition to CVEs.
Deep Learning can provide fast verdicts in milliseconds (similar to how self-driving cars make decisions in milliseconds to drive) at scale so that thousands of containers can be deep scanned per day for supply chain and other attack vectors.
Saumitra Das, CTO and Co-founder of Blue Hexagon:Wider variety of tools available to be integrated into CI/CD ranging from vulnerability scanners to checking for exposed credentials and malware.
Threat modeling is increasingly a piece of DevSecOps where developers and security teams can collaborate.
The change is significant, and we see its effects in the form of business-led rapid delivery cycles to balance both revenue and risk concerns.
Keatron Evans, Infosec Skills Author, Infosec:As more organizations began to migrate to various cloud services, the security problems became more complex and more integrated into the software, i.e., we went from physical networks to software-defined networks (SDNs).
Note: When an application is experiencing slowness or is unresponsive, these commands would be very handy to perform preliminary investigations before jumping into memory and thread dumps.
You can also generate a thread dump and memory dump on the remote machine from JVisualVM when connected through the JMX Remote port.
Note: Enabling debugger mode on a production environment is not a good option.
Because we are setting the security policy as “java.security.AllPermission,” it is not safe to run this tool in a production environment.
Using Jmap and JstackThe jmap and jstack commands are used to export the Memory dump and Thread dumps respectively from the JVM.
Memory Dumpjmap -F -dump:file=[file path]/heapdump.hprof [pid]Thread Dump jstack -l [pid] >  thread-sampling.tdumpThese dumps can be imported into JVisualVM for further analysis.
Generating memory and thread dumps on a remote machine can be done by providing the hostname/IP address of the remote machine in the jmap/jstack commands.
Note: Extracting the memory dump when the application is running should be considered as a last resort on a production server.
The application completely hangs for minutes (based on how much memory has been occupied) until the dump is created.
The right parameters can be set by performing load tests and a lot of trial and error.
Note: The VM parameters to print GC details are not recommended for your production environment, as the file I/O is a costly operation (even for printing GC-related information), which might impact the performance of the application.
-XX:ParallelGCThreads --> No of Parallel GC threads.
Setting the VM Parameter to Generate a Memory dump File Upon an “Out of Memory ” ErrorIt is very important to obtain a memory dump file when JVM crashes due to the “Out of Memory” error so that it can be analyzed later.
-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=[file path]This will enable the .hprof file to be created in the path specified in –XX:HeapDumpPath when the “Out of Memory” error occurs.
This is like a flight’s black box, which is required to analyze the cause of the crash.
Here’s how to whip yours into shape so mobile app performance problems don’t cost you downloads, revenue, and brand perception.Poor Mobile App Performance: A Horror StoryI can vividly remember one of the horrors I faced due to poor mobile app performance, which occurred while traveling across Europe with nothing but two friends, a backpack, and my iPhone in tow.All summer long, I relied on mobile apps for getting from one place to the next.
The next page never loaded!After clicking the same submit button over and over again to no avail, I entered a panicked state as I wondered what this meant for me (and my wallet!
If I did do it all over again, would I end up being charged twice if it had actually worked the first time?Whether my fears were founded or not, it was not a pleasant experience.
I stopped using the app and lost trust in the company (that I won’t name but rhymes with mice-pine).It goes to show that just one negative experience a customer has with your app can seriously trouble or end your relationship with them.Indeed, it’s important for apps to work well in order to maintain or elevate a consumer’s opinion of a brand because a poorly functioning app could sour it.The findings of a 2015 mobile app user survey conducted by Dimensional Research showed that 80% of app users will only attempt to use a problematic app three times or less and 36% said that an app with slow performance issues made them have a lower opinion of the company.In this post, I’ll explain how mobile app performance affects ratings and revenue and share tips for improving performance in order to maintain or elevate your brand’s reputation.What Is Mobile App Performance?Keep in mind that, when we discuss mobile, it includes not only smartphones, but also their cousins: the legions of tablets, smartwatches, and other smart devices on the market today.Performance is one of the factors of app quality that has to do with how it behaves when experiencing certain load sizes and in various other situations.
One app maker reported that even a 0.1 drop in an online app rating caused a 5% decline in downloads, while a 0.3 decrease resulted in a 60% drop.
There are several sites that do this, but the one I typically use is compressor.io.Also make sure that the size of the image is right because it’s a waste of bandwidth to rely on the browser to scale a high-resolution image into a smaller width and height.
They should also be rigorously tested for performance and monitored frequently.One misconception that many developers have is thinking you have to wait until the end of development to start the testing process.
Common problems experienced when building software for the cloud can be broadly grouped into the following categories:Difficulty in communicating the new design and deployment topology that results from embracing the cloudProblems with creating an architecture that works in harmony with (and leverages) cloud infrastructureLack of testing in a cloud-based environmentLack of understanding of the underlying cloud fabric and its propertiesLimited strategy for application and platform monitoringDifficulty in designing for bizarre (and partial) cloud infrastructure failure modesThe remainder of this article will discuss these issues in greater depth, and also provide strategies for overcoming or mitigating the accompanying challenges.
All of this information can typically be summarized in a code repository README file, and suggested section headings for this documentation can be found below:Component description responsibilitiesComponent initialization instructionsProfiles available (i.e., modes of operation)Component external interactions (i.e., collaborations)State characteristics (i.e., stateless, mandatory sticky- sessions)Data store and cache interactions (in/out of process, eviction policy, etc.
It is also beneficial at the infrastructural testing phase of the build pipeline to follow the example of simulate failure modes that can be experienced in a heavily virtualized and networked environment.
When software is running on compute resource that is not shared, there are no “noisy neighbours” that are attempting to compete for contended physical resources, such as the CPU, memory, and disk access.
Data stores and middleware should also be monitored, and modern applications typically expose these metrics out of the box via a series of mechanisms.
For example, MySQL exposes a proprietary API, Solr exposes statistics via JMX, and RabbitMQ exposes data via an HTTP interface.
Key metrics for an application can be exposed using frameworks such as Codahale’s Metrics or StatsD.
The same can be said from an e-commerce site where failure in the recommendation component automatically shuts down the entire site—a user will never receive an incorrect recommendation; but is this really the correct response to the problem that the business would expect (or want)?
The vast majority of cloud vendor APIs and SDKs make this operation trivial, and many vendors provide an automated approach to handling this common use case.
Furthermore, we need to be able to differentiate between problems in our application and problems in dependencies.From a business transaction perspective, we can identify and measure external dependencies as being in their own tiers.
Common caching problems include:Loading too much data into the cacheNot properly sizing the cacheI work with a group of people that do not appreciate Object-Relational Mapping (ORM) tools in general and Level-2 caches in particular.
Their concern with these tools is mostly unfounded when the tools are configured properly, but the problem they have identified is real.
In short, they dislike loading large amounts of interrelated data into memory when the application only needs a small subset of that data.When measuring the performance of a cache, you need to identify the number of objects loaded into the cache and then track the percentage of those objects that are being used.
Or is the cache sized appropriately?A common problem when sizing a cache is not properly anticipating user behavior and how the cache will be used.
The first 100 calls will load the initial set of objects into the cache, but subsequent calls will fail to find the objects they are looking for.
Garbage CollectionOne of the core features that Java provided, dating back to its initial release, was garbage collection, which has been both both a blessing and a curse.
With the exception of the Azul JVM, all JVMs suffer from major garbage collections.
Because of the advent of the cloud, applications can now be elastic in nature: your application environment can grow and shrink to meet your user demand.
A careful examination of your PHP ecosystem will help you avoid suffering performance loss in areas you can otherwise solve for easily.
If you are uncertain about how your environment is setup, it may be time to audit your environment and reconsider old architectural decisions.
Arguably, this method is more secure: using suEXEC mod_cgi can keep the execution of PHP code outside of the scope of Apache meaning that faulty or malicious code cannot go outside of the scope of interpretation.
Unfortunately, the bad outweighs the good with this method; because of its inefficient, poor performance, and heavy taxation on server resources, it quickly fell out of preference for serious PHP developers.
This also guarantees no PHP script execution by any user other than the owner.
You may probably already have mod_php installed; your phpinfo() dump will show Apache 2.0 Handler under the Server API block.
Thus, you do not spawn a new PHP process per request, and your overhead is low as PHP preloads as an Apache child process.
A disadvantage to mod_php is that all your PHP application files will be owned and executed by the Apache user.
This will cause some loss in visibility into which specific users may be impacting server resources.
I do not think for the modern PHP application this is a serious issue; you are probably not running multiple websites on a single server but rather powering one giant PHP application across a farm of servers.
A major disadvantage to FastCGI is that it is taxing on your memory consumption (although less so than mod_php).
However, you’ll need to find the right tradeoff because keeping data in memory also means lowered CPU usage and faster access to that data, thus reducing the overall response time of your requests.
Furthermore, we need to be able to differentiate between problems in our application and problems in dependencies.From a business transaction perspective, we can identify and measure external dependencies as being in their own tiers.
For each iteration within the infinite loop, the event loop executes a block of synchronous code.
Node.js – being single-threaded and non-blocking – will then pick up the next block of code, or tick, waiting in the queue as it continue to execute more code.
Although it is a non-blocking model, various events that potentially could be considered blocking include: Accessing a file on disk Querying a database Requesting data from a remote webserviceWith Javascript (the language of Node.js), you can perform all your I/O operations with the advantage of callbacks.
Thus, if the event loop is blocked by code from transaction X, the slowdown of the execution may impact the performance of transaction Y.This non-blocking, single-threaded nature of Node.js is the fundamental difference in how code execution may potentially impact all the requests within the queue and how with other languages it does not.
Memory leaks are caused when application code reserves memory in the heap and fails to free that memory when it is no longer needed.
If you choose to ignore memory leaks, Node.js will eventually throw an error because the process is out of memory and it will shut down.In order to understand how GC works, you must first understand the difference between live and dead regions of memory.
Everything else is considered dead and is targeted for cleanup by the GC cycle.
The V8 GC engine identifies dead regions of memory and attempts to release them so that they’re available again to the operating system.Upon each V8 Garbage Collection cycle, hypothetically the heap memory usage should be completely flushed out.
Unfortunately, some objects persist in memory after the GC cycle and eventually are never cleared.
Eventually, the memory leak will increase your memory usage and cause your application and server performance to suffer.
As you monitor your heap memory usage, you should pay attention to the GC cycle before and after.
Specifically, you should pay attention to a full GC cycle and track the heap usage.
As a general rule, you should be concerned if heap usage grows past a few GC cycles and does not clear up eventually.Once you’ve identified that a memory leak is occurring, you options are to then collect heap snapshots and understand the difference over time.
Performing a heap dump may be taxing on your application so once a memory leak has been identified, diagnosing the problem is best performed on a pre-production environment so that your production applications are not impacted on performance.
Diagnosing memory leaks may prove to be difficult but with the right tools you may be able to both detect a memory leak and eventually diagnose the problem.Application TopologyThe final performance component to measure in this top-5 list is your application topology.
Because of the advent of the cloud, applications can now be elastic in nature: your application environment can grow and shrink to meet your user demand.
unlike the days when software used to be shipped in boxes and there was no way of knowing how it will perform in production, today almost any metric you can think of can be tracked down and reported.
the problems we’re now dealing with are coming from information overload and scale, rather than not having enough information.
so following up on them becomes even more critical right at those sensitive times following new deployments.
much easier than to prove it wrong when presented to you.
in practice, when a single pageview has tens or more of these requests (usually well over 40), it means that the user is 99.999…% likely to experience a result worse than the median (it’s simple math: 1 – (0.5 ^ 40)).
even if we’re looking at the 95th percentile, since you probably have well over 40 requests per page, most of your users will experience a response even worse than that.
to read more about percentiles and just how misleading data, check out gil tene’s blog   right here  .
it means your machine is under stress.
cpu utilization alone doesn’t cover that, but load average takes the bigger picture into account.
tools to check:  1.   htop   running htop to examine the loads on one of our servers, load average appears on the top right  3. error rates (and how to solve them) there are several different ways to look at error rates, and most developers go with the high level metrics – looking at error rates at the whole application level, total failed http transactions out of the overall http requests for instance.
but there’s an often overlooked in-depth layer to this with immediate implications for your application health: error rates for specific transactions.
showing the number of times a certain method in your code fails and produces a logged error or exception out of the overall times it has been called.
error rate breakdown by specific events in takipi, narrowing down on the root cause for a peak in error volume  but this data doesn’t mean much on its own, right?
the second step after prioritizing the most urgent events that you should address, be it logged errors or exceptions, is to get down to their real root cause and fix them.
takeaway #3:  high level data is not enough to get down to the real root cause of increased error rates.
tools to check:  1.   takipi   zooming on the error analysis down to the specific variables that caused each error  4. gc rate and pause duration a misbehaving garbage collector is one of the main reason that can cause your application throughput and response time to take a deep dive.
so when digging in to find the cause for these symptoms, a common resolution would be that the application was in the middle of a stop the world gc pause.
to analyze this you’ll need to make sure to turn on gc log collection with the appropriate jvm arguments.
this allows you to drill down to understand exactly which error or exception is causing you the most trouble, so you can prioritize them by their impact on business goals.
making sense of all the exceptions and log errors that are flying around.
takeaway #6:  uptime might be a binary indicator but there’s a lot of value in looking at in an aggregate matter to locate the weak spots in your stack.
but we still can’t ignore logs altogether.
if you don’t keep an eye on their size and the process you have in place for keeping them in check – bad things can happen.
when logs get loose, hard drives cry.
it’s a neverending source of havoc.
another way could be just to rollover or truncate them at some point, but then we’re risking information loss since like most developers, we haven’t cut our dependence on logs just yet.
it’s time to rethink the problem and start reducing log sizes.
Internal DependenciesYour PHP application may be utilizing a backend database, a caching layer, or possibly even a queue server as it offloads I/O intensive blocking tasks onto worker servers to process in the background.
For example, if your customer is attempting to purchase items in a shopping cart and in order for the transaction to complete your application must charge their credit card so that you can display a confirmation or error page.
If the credit card was declined, the user is presented with an error message to try again.
Common caching problems include:     Loading too much data into the cache       Not properly sizing the cache  When measuring the performance of a cache, you need to identify the number of objects loaded into the cache and then track the percentage of those objects that are being used.
Or is the cache sized appropriately?A common problem when sizing a cache is not properly anticipating user behavior and how the cache will be used.
If 32M is not enough for APC to be used as an opcode cache, then you run the risk of exchanging data in memory for data on the disk.
Considering disk I/O is much slower than reading from memory, this defeats the purpose of your in-memory cache and significantly slows down your application performance.
Because of the advent of the cloud, applications can now be elastic in nature: your application environment can grow and shrink to meet your user demand.
Highly elastic microservice and serverless architectures mean containers spin up on demand and scale to zero when that demand goes away.
This shift has exposed deficiencies in some of the tools and practices we used in the world of servers-as-pets.
Many of the clients we work with at Real Kinetic  are trying to navigate their way through this transformation and struggle to figure out where to begin with these solutions.
This means many of the tools that were well-suited before might not be adequate now.
Similarly, it's much more difficult to correlate the behavior of a single service to the user's experience  since partial failure becomes more of an everyday thing.
This model also leads to ineffective feedback loops if engineers are not on-call and responsible for the operation of their services-something I've talked about ad nauseum.
In this environment, it is no longer practical to SSH into a machine to debug a problem or tail a log file.
It also probably means we are running an agent for many of these services on each host, and if any of these services are unavailable or behind, our application either blocks or we lose critical observability data.
If these are tightly integrated, this can be difficult to do.
We no longer have to figure out data to send from containers, VMs, and infrastructure, to send it, and to send it.
Anyone who's shipped production code has been in the situation where they're frantically trying to regex logs to pull out the information they need to debug a problem.
It's even worse when we're debugging a request going through a series of microservices with haphazard logging.
In order to monitor systems, debug problems, make decisions, or automate processes, we need data.
What we want to avoid is the sort of murder-mystery debugging  that often happens.
A lone error log is the equivalent of finding a body.
We know a crime occurred, but how do we piece together the clues to tell the right story?
The data we can't get for free should go on the context, typically data that is request-specific.
You can take this as far as you'd like-highly structured with a type system and rigid specification-but at a minimum, get logs into a standard format with property tags.
If these aren't aligned, pain-driven development  creates problems.
The downside of moving away from agent-based data collection is we now have to handle routing that data ourselves.
With the amount of data and the number of tools modern systems demand these days, the observability pipeline becomes just as essential to the operations of a service as the CI/CD pipeline.
We asked, "What are some real-world problems you, or your clients, are solving by migrating legacy apps to microservices?"
Resilience, reliability, scalability, and fault isolation.
The most common real-world problem we hear from customers is how they can stay relevant to their end-users and customers.
You cannot get by with long outages or poor user experience regardless of the vertical you’re in.
The other primary benefit customers see is scale — an elastic environment that allows your business to auto-scale takes the worry out of slow performance during critical events or peak traffic seasons.
This could be a retail outlet during Black Friday/Cyber Monday, or an insurance company during a natural disaster or macro-economic changes that cause a flurry of activity on Wall Street.
Fail fast or scale fast.
They’re also simple to scale independently and responsively, so you can scale out based on demand for a particular service without scaling the entire application.
Managing different, potentially conflicting, component versions.
They've moved from sharded SQL setup to us to collapse two tiers for the scale, performance, and consistency they need.
For one bank, the critical factor was creating a global API so users (consumers and third-party systems) could get the same service anywhere in the world independent of the backend system is connected to.
Data-as-a-service exposing data APIs and running on microservices.
Internal dev teams are tired of working with business teams or the business gets a huge OpEx bill and learns they are doing cloud wrong.
Here’s what we learned:   You may also like: TechTalks With Tom Smith:  VMworld Hybrid Cloud and Multicloud Conversations  PlanningWe see 30,000 customers migrating high-performing applications early and not so great later on.
If you don’t have a plan, you’ll be in trouble.
Listen to the people who struggle to maintain applications.
Hear their pain and then help them with it.
There are three key steps towards moving to a microservices architecture:  1) The first step is to understand the reasons for migration – potential reasons could be that updates to the apps are difficult due to cross-dependencies, scaling of individual parts of the application is difficult, data privacy/security is not managed well, etc.
A well-articulated business case with these reasons is critical to sustaining any migration initiatives.
Build it applicable to the problem you’re trying to solve consistent with the structure of the organization.
For example, it aims for the most limited scope for each microservice, which helps to simplify strategies around performance, scalability, fault tolerance, and maintainability.
This is the result of a more complex software demand.
If you have trouble maintaining a monolith, don’t go to microservices.
Fix whatever problem you have today before starting the transition.
What is the biggest pain point?
All this time real-time pre- and post-production monitoring is even more critical.
OtherIt's difficult to implement new functionality as microservices.
There are four options for migrating:  1) Some customers say it’s too hard to move to microservices so they stay with a monolithic application and will get left behind.
Try to avoid a rip-and-replace approach; the app should be evolved in pieces when deemed necessary for the portion that’s being migrated to an independent service.
It’s very difficult to choose the right path in the middle of so many tools and practices.
If you didn’t, don’t worry you can take a look at below courses to learn your choice of language, though I strongly suggest you to learn at least one of these three major general purpose programming language.
You might be thinking that there is so many stuff to learn, so many courses to join, but you don’t need to worry.
Principles Used To Design Microservice ArchitectureHigh cohesion along with loose coupling.
Isolated data storage for each microservice – This is very important to maintain limited access to data and avoid ‘service coupling’.
In real-world problem solving, expansion and large-scale systems are crucial to the performance of any microservice ecosystem.
These hurdles might look tough but with a set of microservice design patterns and their right implementation, achieving the same is possible.
But, fetching user-owned resources from the variety of microservices presented from a single UI can be very tricky.
With simultaneous execution of hundreds of services it becomes troublesome to pinpoint the root area responsible for the failure in log registry.
User gets alerted if a failure is generated.
Detecting Production failures in terms of business requirements.
API Health CheckMicroservice architecture design promotes services which are independent of each other to avoid any delay in the system.
APIs as we know serve as the building blocks of an online connectivity.
It is often observed that a microservice is up and running yet incapacitated for handling requests.
Today we will look at the decomposition of the microservice design patterns which leave a lasting impact.
Strangler Vine PatternWhile we discuss decomposition of a monolithic architecture, we often miss out the struggle of converting a monolithic system to design microservice architecture.
Without hampering the working, converting can be extremely tough.
And to solve this problem we have the strangler pattern, based on the vine analogy.
But due to loose coupling, accessing this data can be a challenge.
Mainly because different services have different storage requirements and access to data is limited in microservice design.
Due to the challenges and lack of accessibility, a single database per service needs to be designed.
This database has limited access for any outside microservices.
Shared Database per ServiceIn Domain Driven Design, a separate database per service is feasible, but in an approach where you decompose a monolithic architecture to microservice, using a single database can be tough.
This number should be limited to 2 or 3 services.
The problem here lies with reliability.
Command Query Responsibility Segregation (CQRS)In a database-per-service model, the query cannot be implemented because of the limited access to only one database.
To solve this problem and give users the location of the request, a registry needs to be used.
To avoid this issue, when you design microservice architecture, you should use the blue-green deployment pattern.
The answer lies with the use of a metric service.
Running a Health CheckMicroservice architecture design promotes services which are independent of each other to avoid any delay in the system.
But, there are times when the system is up and running but it fails to handle transactions due to faulty services.
To avoid requests to these faulty services, a load balancing pattern has to be implemented.
Over the years of building our monitoring solution we have seen almost every tool in action.The experience received has been eye-opening in regards to how different organizations are either not deploying the right tools for the job or are deceiving themselves with the information exposed.
Depending of the organization, the dysfunctionality can take different forms, for example:Living in ignorance.
The domain has its roots deep in IT and is thus treated as something that the business and product owners do not benefit from directly.In this article I am going to give you an overview of different monitoring solutions that businesses use to track their IT assets.
When you interpret data such as the following:the CPU has been utilized 100% for the past 20 minutes,RAM use has steadily climbed to 90% since yesterday,we are currently utilizing the network at 7 Gbit/s,you cannot really tell if any of the facts above are symptoms of something good or bad.
Maybe the application got stuck in an infinite loop because of a programming error, and is now eating 100% of CPU.
Also, if a particular log record does indicate a problem you will only be aware of it happening if you are explicitly looking for it.As such, building your monitoring on logging tends to be obtrusive – you end up coupling the logging calls tightly to your application code, polluting the business logic with statements that do not necessarily belong there, just for the sake of monitoring.DO: Use logging to:Debug information in business-critical parts of the application.Aggregate and store the logs to be the source of information during troubleshooting.DON’T: Have the log monitoring as:A reliable way to detect poor user experience.To be coupled with your application logic.Improve your monitoring via Plumbr end user monitoringHealth checksThe next type of monitoring solutions is periodically checking the availability of certain services.
Throughout the week, there have been two short downtimes and the latency of the monitored endpoint is constantly between 400 and 600 ms. As this is our very own application that is monitored, there actually were several serious performance issues throughout the week, which cannot be seen with this type of monitoring.The reason for this is that the amount of functionality covered with such health checks tends to be very limited.
It does close to nothing in exposing users who are unhappy with your service either due to bugs in the service, the unavailability of a particular function or just the poor performance of the application.Since Google Analytics is very good at the things that it is good for, its weaknesses are hidden so well that businesses often end up investing in the wrong places.
A typical example would involve tweaking the landing page conversion rates to gain an extra 0.3% of users, while blissfully ignoring the 6% of disappointed users struggling with your application due to performance or availability issues.To be honest, Google Analytics has limited capability in exposing these performance metrics:What you should pay attention to is that this data is captured only from a small sample of users.
In addition, this data is presented deceptively, exposing only averages and a comparison to the baseline.
As you can see in the next section, the averages are not that good of a cornerstone in understanding what is really happening with your users.DO:Deploy web analytics to:Keep a close eye on conversion funnels and product usage patterns.DON’T: Use web analytics as:The only source of information used by the business/product owners.A reliable source data to understand the performance of your application.Improve your monitoring via Plumbr end user monitoringApplication Performance MonitoringThis category of monitoring solutions is called Application Performance Monitoring.
I bet that the application you are building/monitoring/designing exposes many different services.
Numbers without an explanation are still not very useful.
Unless you understand what the remaining 1% consists of, you are in for nasty surprises.Our own recent experience demonstrated that the users who suffered the most and made up the last 1% were our best customers.
Knowing how your priority pass holders or platinum members experience the service might make all the difference in the world.DO: Use an APM to:Expose the end user experience via latency distribution per service.DON’T: Deceive yourself by:Relying on single numbers representing averages or medians as key metrics.Trusting just the application performance metrics instead of monitoring for named user experience.Expecting (most) APM solutions to assist you during root cause detection.Improve your monitoring via Plumbr end user monitoringTake-awayIf you got this far in the post, take some more time and think about how you monitor your own IT assets.
Do you actually know the answers for these questions:How are the end users actually experiencing the application?Which users are suffering the most?Have I over-provisioned my infrastructure?How are my business-critical services performing?If some of the questions are left unanswered, maybe it is time to do something about it.
So I can only encourage you to start introducing the change via deploying the following:Infrastructure monitoring to detect both under- and over-provisioned resources.Log monitoring to harvest additional information during service failuresHealth checks to receive early alerts on availability issuesWeb analytics to understand how the users are converting through funnels and how they are using your siteApplication performance monitoring to expose actual end user experienceThe order of steps depends on your organization, but each category is designed to solve a different problem and exposes different information.
We asked, "What are some real-world problems being solved by the use of containers?"
3) Ad media log files DNS attack compare traffic with history to find log lines in a strange order to shut down the DNS attack.
We keep up the pace without giving up security.
We start with our design process to get the client to focus on the problem to solve and the proper technology to solve the problem.
Financial institutions will have several active data centers running at less than 50% capacity in the event of failure of a data center.
When there’s no failure, there’s a lot of excess capacity that can be used for batch jobs running in containers.
3) Legacy Windows VMs where the underlying OS is no longer supportable, but the app must remain long term.
Verticals are not as interesting as new ways of deploying.
Fragmentation is really a BIG problem.
1) Containers at the edge run Docker everywhere to quickly move immutable containers close to the machines they are predicting failures for.
1) The first is the use of container platforms to reduce cloud costs, whereby instead of running low duty-cycle workloads in individual cloud compute instances (which results in low use of paid resources), enterprises instead use containers to achieve high workload density over the container host instance, thereby acting as a shared resource which allows IT to achieve higher use rates and lower compute costs.
Biggest use case is visibility of images in the registry, see vulnerabilities – pinned, repos not updated.
The Hidden Valley Where Performance Defects Graze, Waiting to be Slaughtered in my   previous blog post  , i talked about the it industry’s desperate need for a steady flow of java developers with performance skills — but instead, all we get is a drip, drip, drip, where very few (if any) java developers vet their code for performance defects before checking it into source code control.
the key to fixing performance defects quickly is to understanding how your code (the trigger) invokes the code in the bottleneck (the current landmark).
this might be a bit controversial, but i’m going to take you to a secret place.
frankly, i’m not sure why it's so secret and why other performance experts have been so hush-hush about this, because this magic place has been around forever — since the first end user, with a beet red face, shouted and cursed at their slow computer.
they thought you would be a bit scared and that you perhaps might not even admit it.
but don’t worry, i’m was a bit scared myself — of java stack traces, of course — massive, complex, hairy, 100 line stack traces, like the ones you get with a java.lang.exception, littered with mostly unfamiliar class and package names.
stack traces also show up in thread dumps and java profilers — to show what triggered the code eating all the cpu.
for example, when you use the jdk’s jstack to capture a thread dump, the current landmark shows the code   currently  on the cpu for every thread at the instant you ran jstack (it just takes an instant).
i captured this stack trace while i was doing some performance tuning and this particular “current” showed more than any other in the thread dumps — so this is a likely candidate for a performance bottleneck, the lowest hanging fruit.
when you’re asking these kinds of questions, you have definitely lost your way to the hidden valley where performance defects graze.
we also learned that immediately blaming the openjdk for the performance problem was a bit premature — instead, we must first locate the trigger to discover our own code’s role in causing the slowdown.
so at last, the trigger landmark itself is the fertile hidden valley, where performance defects graze.
most of the time, you’ll find performance defects here, or in the immediate vicinity.
this works with java profilers (jprofiler, yourkit), thread dumps (from jstack), or application performance management (apm) tools (dynatrace, new relic, appdynamics, glowroot,org, etc…) — because they all use stack traces to display performance metrics.
without the third landmark, there are still obstacles to understanding scary, complicated stack traces.
one single 100-line stack trace can be scary on its own, as shown above.
but it gets worse, i promise you.
what about when they gang up on you, and there are dozens of stack traces in a single thread dump?
seeing all those stack traces in a thread dump once scared me, too.
i would say to myself, “i get paid to program in java — aren’t i supposed to understand what’s going on in there?”  that all changed when i discovered a simple performance secret that sounds a little obvious:  your performance problem is caused by the threads under load.
there is no load.
that’s right, and it’s stupid obvious.
with all three landmarks defined, let’s return to the last scary question we raised:  of all those stack traces [in a thread dump],    which ones are a cause for performance concern?
unfortunately, you won’t find the threads under load carrying little signs saying “i’m under load”.
now that we have all three landmarks defined, don’t stack traces seem a little less scary?
you need a plan for finding performance defects in        any       jvm, not just those few that are carefully instrumented with just the right monitoring.
this forthcoming blog post will show you how defects can be detected in        any       jvm using the entry-trigger-current landmarks along with a fantastic but undocumented technique called “mtdp” — manual thread dump profiling.
App users have a low tolerance for slowness, with a reported 43% of users unhappy if they have to wait longer than three seconds for an app to load.
(App Samurai)It’s not enough to ensure that your mobile app functions properly, but also to test how it behaves on different devices, under heavy user load, different network connections, etcetera.
Visual Playback Editor: Drag and drop test cases into tracks to be executed on various cloud or on-premise locations.
Troubleshooting: Discover how your application responds to various loads with a load test and identify performance bottlenecks before they cause damage in production.
PricingHeadSpin does not offer a free plan.
JMeter allows you to perform various testing activities like performance, load, stress, regression, and functional testing, to get accurate performance metrics against your web server.
What problems will it solve for my business?
AIOps allows your business to reduce the time and effort it spends on performing redundant, repetitive IT operations and allows you to focus on your core offerings.
What Problem Does AIOps Solve for Your Business?
Apart from reducing your productivity, such practices also hurt the morale of people working for your enterprise.
To meet the demands, your business is forced to hire more IT staff and spend money and time upgrading your IT infrastructure.
While this can work in the short term, you’ll have to repeat this process to oblivion as corporate data sees no end to its exponential rise.
Eventually, your business will waste way too much time and money on what doesn’t even constitute your core offering.
With AIOps, you can get rid of problems such as:Increasing investment of time, money, and resources in IT operations: AIOps automates much of your IT workflows to reduce the amount of time and money you invest in manual work.
Overburdening of employees working: Backed on CI/CD infrastructure, AIOps does most of your redundant, repetitive work by itself so your staff it’s overburdened with work!
Slow anomaly detection and inaccurate error tracking: Manually detecting anomalies and tracking down points of error can consume a lot of time, leading to greater chances of reduced product quality and monetary losses for your business.
AIOps quickly identifies anomalies and warns you in advance, then tracks through the errors and finds the core problem!
Traditional IT methods are finding it increasingly difficult to analyze corporate data with accuracy.
The sheer volume of data is making performance analysis difficult within a reasonable time frame.
Better Error Prevention: AIOps detects error patterns and uses proactive forecasting technologies to prevent errors.
Enhanced Productivity and Morale: AIOps significantly cuts down the work burden on your staff.
Improve Mobile App Performance to Enhance the User ExperiencePoor Mobile App Performance: A Horror StoryI can vividly remember one of the horrors I faced due to poor mobile app performance, which occurred while traveling across Europe with nothing but two friends, a backpack, and my iPhone in tow.
After clicking the same submit button over and over again to no avail, I entered a panicked state as I wondered what this meant for me (and my wallet!).
If I did do it all over again, would I end up being charged twice if it had worked the first time?
Whether my fears were founded or not, it was not a pleasant experience.
It goes to show that just one negative experience a customer has with your app can seriously trouble or end your relationship with them.
The findings of a mobile app user survey conducted by Dimensional Research showed that 80% of app users will only attempt to use a problematic app three times or less and 36% said that an app with slow performance issues made them have a lower opinion of the company.
Unfortunately, a user may have a poor experience with your app’s performance, even if the cause of the issue has nothing to do with the app itself.
Users Are FickleWhen a mobile app’s performance doesn’t meet user expectations, there is a very high chance they will abandon it, resulting in a loss of potential revenue.
Ratings can also suffer, which leads to a decline in downloads.
One app maker reported that even a 0.1 drop in an online app rating caused a 5% decline in downloads, while a 0.3 decrease resulted in a 60% drop.
Although not common, 7% of users will write a bad review if an app has errors, crashes regularly, or stops responding, and 8% will give it a poor star rating for the same reasons.
It is no wonder, then, why many tech companies are hiring performance testers to stay competitive.
Also, make sure that the size of the image is right because it’s a waste of bandwidth to rely on the browser to scale a high-resolution image into smaller width and height.
They should also be rigorously tested for performance and monitored frequently.
It then generates a report with everything you should know about your app, its weaknesses, and how you can fix them.
APM helps detect and diagnose deep-level application performance problems to maintain an expected level of service across the board.
Make Your Company’s Culture Performance-CentricAs TechBeacon editor, Todd DeCapua has stressed many times, performance is now everyone’s concern from the engineering department to the marketing department.
In this circumstance, rigorous testing of the application and the website becomes crucial.
The 9 Most Demanded Web App Testing Tools of 20211.
The higher detection rate of threats with low false positives.
Integrated vulnerability management – systematize and manage threats.
It determines the faults and examines whether they are false or real.
Proof-centric scanning technology for dead-accurate vulnerability identification and scan results.
Automatic detection of custom 404 error pages.
It also has traits for risk analysis, integration management, and distributed execution.
QA engineers create completely automated test scenarios with no coding.
We asked respondents, "What are some real-world problems being solved with microservices?"
Here's what they told us:FraudThe biggest and most relevant use case is fraud detection.
Fault detection apps need to access the server.
It looks a lot like ETL but microservices to detect fraud.
With microservices, as long as don’t violate the contract, four variables in one answer out, you can make changes in hours.
Take one service, expose it, you’re able to make changes in hours versus months.
T-Mobile took their payment system, the heart of their business and broke apart the microservices they wanted and started iterating on it.
We store data people are uncomfortable putting in the cloud, clustering horizontally without configuration.
As reliability degrades it needs to be refreshed.
We enable developers to use statistical data to determine where problems lie.
Stop doing boring repetitive things that don’t add value – automate.
Proxy metrics are 1) number of deployments per day, 2) how quickly you can get a code change from Git commits to production, 3) MTTR, 4) how often are you experiencing failures.
This means that when something goes wrong it becomes more difficult to pinpoint the issue.
As we break pieces of the monolith, the mean-time-to-detection of issues starts to drop as it’s easier to detect performance problems, memory leaks, and other issues in smaller units, especially since each release in a microservice has fewer changes in it.
DevOps engineers often get confused with the CI/CD pipeline by automation of individual stages in CI/CD.
Though different tools may automate each complicated stage in CI/CD, the whole software supply chain of CI/CD may still be broken because of manual intervention in between.
Imagine a step where the code committed gets to build directly and it failed during build or deployment.
This becomes a slow and costly process in terms of resource utilization, both machine, and man.
This is a fast check process where the code is checked for any syntactic errors.
Though this stage lacks the feature to check for runtime errors, which is performed at a later stage.
Placing an additional policy checking into an automated pipeline can dramatically reduce the number of errors found later in the process.
BVT checks if all the modules are integrated properly and if the program’s critical functionalities are working fine.
The purpose is to reject a badly broken application so that the QA team does not waste time installing and testing the software application.
Post these checks a UT (unit test) is added to the pipeline to further reduce failures at the production.
There can be many builds getting generated per day, and keeping track of all builds can be difficult.
They perform regression analysis, stress tests to check deviations from the expected output.
Activities that are involved in testing are Sanity tests, Integration tests, Stress tests.
Load and Stress Testing:Load balancing and stress testing are also performed using automated testing tools like Selenium, JMeter, etc., to check if an application is stable and performing well when exposed to high traffic.
This test is typically not run on every single update, as the full stress testing is long-running.
Because so much testing has happened now, failures should be rare.
However, any failure at this point needs to be resolved as quickly as possible so the impact on the end customer is minimized.
Log analyzers will scan torrents of logs produced by underlying middleware and OS to identify behavior and track the source of problems.
Process:The goal of the DevOps team is to release faster and continuously and then continually reduce errors and performance issues.
research shows that the partof european internet users that buys on-line has grown from 40% in 2004 to 84%in 2008. additionally, the large web retailers in my country saw theirrevenue grow in 2009 and in the first part of 2010 just as if the crisis never materialized.
so thequestion is: how can we prevent these performance and availability problems andhow can we assure that a web site is always quick and available?
howperformance problems get to you     frustrationsand loss of revenue    wheninternal applications are slow, this is frustrating for the users.
the customer in turn will be frustrated by the long waiting times andlong phone calls.
disruptionof regular development    slownessproblems most of the time manifest themselves unexpectedly, such as after theintroduction of a new application or new release.
the difficulties which turn up in production put a high pressure onboth the operators as well as the developers to solve the usually difficult tofind problems.
this will have its disruptive effect on the regular developmentof new releases: the development team is only busy firefighting.
however, if the bottleneck turnsout not to be located in the web tier but somewhere else, this investment inmore servers will turn out to be just wasted money.
seven stepsto performance success    it can be avalid choice to run the risk of performance problems in production and dealwith them in a re-active manner.
step 1:define performance requirements    definingthe performance requirements well usually is a neglected activity.
with such vague definitions the confusionstarts.
the goal is unclear and is typically explained very differently by thebusiness and the it department.
these aspects are oftenignored by the non-technical contributors to these requirements.
in addition, each technology orframework comes with its own teething troubles and most of the time uses more resourcesthan its predecessor.
therefore,it is questionable if the chosen new technology and architecture will meet thespecified performance requirements.
by executingthis poc and understanding and using the results of it, the project can early becorrected in the right direction to prevent excessive cost and delay.
step 3:test representatively    slowness ofapplications in development environments is often neglected with the rationalethat faster hardware in the production environment will solve this problem.
alsoreporting features and maintainability of the scripts are both not so great.
this is just like any other software defects and quality assurance: the later in the development process defects are detected, the more costly these defects are.
in case serious performance defects popped up, a crisis team was gathered and we found ourselves in a stressful situation.
there was usually not enough time to fix the defect before the release date, so my recommendation at times was to defer the release date.
in this way, the it department gets early insight into new and changed features, it can adapt its course quicker, back-off early from an unfortunate architecture or approach, minimize surprises and also have lower costs.
however, to be able to take the right measures in case of a fatal incident, it is necessary to be able to pin-point the problem.
this approach results in low memory usage and a low performance overhead, at the cost of some information loss.
this happened while no new release was introduced and no other page became slower.
the troublemaker turned out to be a dao executing a prepared statement with only part of the variables being bind-variables.
with help of jarep, we could look back to where the trend of increasing response time started so when the problems started.
we could also see that this problem was only present at one of the two machines.
with this knowledge and his log book, the operator could remember that on the start date he had experimented with a new jdbc driver to try to solve a memory leak.
problems only appeared slowly during the following weeks.
they had left the new driver in place, which manifested itself as a time bomb later.
when we put back the old driver, the problem just disappeared!
identifying which pages or services do not meet stated requirements and isolating the problem: where is it located, in which layer, in which component.
this can for instance be a missing or wrong index on a database table or the invocation of too many small queries.
if not, then there is something wrong with the hypothesis and we need an alternative hypothesis.
what is missing here is:   measure, don’t guess  .
live monitoring is essential to see the actual performance problems.
step 7: share the responsibility for the whole cha   in  when an incident happens in production, this usually means stress.
a performance problem in production often leads to finger pointing.
summery and conclusions  in this growing on line world with demanding customers it has become crucial that services provided on the web are always available and always fast enough.
this is often challenging to developers and operators: performance problems manifest themselves in various ways, like in frustration, loss of revenue and disruption of development; and just adding hardware is a doubtful solution.
some time ago we at xebia presented our   ejapp top 10  about performance problems.
The portal also exposes monitoring information such as CPU time, number of requests, and data out for web sites.
The role instances deployed in Cloud Services are stateless which means that there is no durability guarantee on any information written to the drives attached to the virtual machine hosting the instance.
This can include the automatic migration of a role instance to another server when problems are found with the physical hardware.
The Windows Azure Portal exposes a gallery of Virtual Machine types that can be deployed directly using the portal or management scripts.
These drives are durable so that any information written to them survives instance failures and self-healing migrations.
The Blob Service supports the storage of files in two types of blob: block blobs and page blobs.
A block blob is targeted at streaming media with the primary access mechanism being a read from the start to the end of the file.
For example, a virtual network can be created to contain a Virtual Machine hosting SQL Server with no public endpoint and a Cloud Service with an HTTP public endpoint exposed by a web role.
It supports 3 types of traffic distribution: geographical in which traffic is directed to the server with the least latency from the current location; active-passive failover where the traffic is failed over to a passive backup service when the active service fails to respond to probes; round-robin load balancing between multiple services.
The Service Bus then welds these connections together allowing two-way communication between the client and the server, both of which could be hidden behind firewalls restricting inbound traffic.
The CodePlex cmdlets are no longer being developed but contain some functionality not yet exposed in the official cmdlets, although that will presumably change.
60 Problem Solving StrategiesOur lives as programmers are a never ending series of one problem after another.
In between these glorious moments we toil through the frustration of one problem after another.
This post will describe a bunch of different problem solving strategies.
Nicely unit tested codeEnforce database constraintsUse an input validation frameworkAvoid unimplemented "else" conditionsUnderstand how to use something in isolation before you use it in the main applicationPepper your code with exceptions for situations you don't expect to happenLogging2.
Think carefully about the problem and often one or two lines of extra output will help to isolate the problem.
Sometimes when an underlying library or product is not telling you why it is failing then increasing the verbosity of the logging can uncover additional clues.
If there’s too much logging to read easily, search through the log files for keywords or error codes.
Sometimes if you are not getting any useful logging then you might need to write it yourself.
For example if you have a complicated data structure to deal with you might include carefully crafted “dump” statements at strategic points in order to get the visibility you need to solve the problem.
This can also be quite handy when dealing with multi-thread problems.
Sometimes half the problem is just “seeing” what’s actually happening.
If the problem is something that you think someone else on the team might know about then ask them.
Don’t be afraid about looking dumb.
The only sin is to keep asking the same question over and over, so when someone tells you the answer that you make a note of it somewhere and don’t ask the same question twice.
Ask dumb questions.
You might think they’re dumb questions but they probably are not.
If you don’t know the answer then it probably isn’t a dumb question.
Explain the problem to a teammate.
Explain the problem to your dog.
It really doesn’t matter who you explain the problem to it’s the act of explaining that gets your brain to analyse the problem from different angles.
Write a problem statement.
Write the most accurate and precise problem statement you can.
Write a problem diary.
Include snippets of code or configuration settings and also any errors produced15.
Keep a record of your problems and solutions.
Have you ever had a problem where you know you've solved it once before but you can't remember how?
What you will also find is that the act of writing the support request will prompt you to think about the problem again and you often solve the problem or come up with new things to try before you even hit the send button.
Likewise sleeping on the problem is also good for letting your brain come to grips with the key aspects of a problem.
The other good thing about taking break from a problem is that it lets your reevaluate how important a problem is.
Ignore the problem.
If you ignore the problem what you find happens is that you will be hypersensitive to related keywords for the next week or so.
Isolation24.
Are you sure you know which line of code is causing the problem.
If you are having problems with a library or product then sometimes it helps to break out the related code into a program separated from the main application.
Problem code is often messy code.
One technique is to dump all of the related code and rewrite it from scratch.
A fresh perspective may avoid the problem altogether.
If you are having problems with one particular technology it might be worth dumping it and switching to a different technology.
Maybe the problem you are dealing with has already be found and fixed.
Maybe the problem is being caused by incompatibilities with other products/libraries you are using and needs to be downgraded to something more compatible.
Have you been able to find a fault in the product, or maybe you just need it to behave differently for your application, patch over it with the problem.
Of course often even it’s the right version of the manual that might be the problem since the code has been updated and the manual hasn’t.
If you do step over the wrong line of code then look for the debugger feature that lets you step back through the code.
Defect Number Hooks.
Have you ever had a problem where something gets fixed one way and then a few weeks later becomes a bug that gets fixed a different way (by someone else).
The solution to this problem is to tag each source code change with the related defect number and keep a more detailed description of why something was changed and who was involved in the decision.
Blame.
If you can’t fix the defect then change the requirement.
Your client might change their mind about fixing IE6 defects when you explain how much it’s going to cost them.
If you can’t find why a particular input file is causing a problem maybe you don’t allow that kind of data into the system.
Maybe the defect becomes a “business validation” error before it’s accepted into the system.
You can find out more about his problem solving strategies here.
When load is your problem, latency is probably not, so it’s OK if individual requests take 50-100ms.
Latency is the killer when scaling performance.
But don’t be fooled by the effect that parallelism has!
Parallelism has no effect on your algorithm’s Big O Notation.
If your algorithm is O(n log n), and you let that algorithm run on c cores, you will still have an O(n log n / c) algorithm, as c is an insignificant constant in your algorithm’s complexity.
The killer is achieve O(1) or quasi-O(1), of course, for instance aHashMap lookup.
is really causing trouble.
You ship to production and your fix has no effect.
Please read this article in the context of there being a problem at the leaf node of an inevitable O(N3) algorithm.
Try to avoid the +operator.
Sure, you may argue that it is just syntax sugar for aStringBuilder anyway, as in:String x = "a" + args.length + "b";… which compiles to 0  new java.lang.StringBuilder [16] 3  dup 4  ldc <String "a"> [18] 6  invokespecial java.lang.StringBuilder(java.lang.String) [20] 9  aload_0 [args]10  arraylength11  invokevirtual java.lang.StringBuilder.append(int) : java.lang.StringBuilder [23]14  ldc <String "b"> [27]16  invokevirtual java.lang.StringBuilder.append(java.lang.String) : java.lang.StringBuilder [29]19  invokevirtual java.lang.StringBuilder.toString() : java.lang.String [32]22  astore_1 [x]But what happens, if later on, you need to amend your String with optional parts?
String x = "a" + args.length + "b"; if (args.length == 1)    x = x + args[0];You will now have a second StringBuilder, that just needlessly consumes memory off your heap, putting pressure on your GC.
Every CPU cycle that we’re wasting on something as stupid as GC or allocating a StringBuilder‘s default capacity, we’re wasting N x O x Ptimes.
There is only one StringBuilder that “traverses” your whole SQL AST (Abstract Syntax Tree)And for crying out loud, if you still have StringBuffer references, do replace them by StringBuilder.
Avoid regular expressionsRegular expressions are relatively cheap and convenient.
branch, they’re about the worst thing you can do.
branch, you must avoid regular expressions at all costs.
If you run this iteration many many times, you want to make sure to avoid creating this useless instance, and write index-based iterations instead.4.
Let’s assume your JDBC driver needs to go through incredible trouble to calculate the value of ResultSet.wasNull().
null : value;}So, this is a no-brainer:TakeawayDon’t call expensive methods in an algorithms “leaf nodes”, but cache the call instead, or avoid it if the method contract allows it.5.
Use primitives and the stackThe above example is from jOOQ, which uses a lot of generics, and thus is forced to use wrapper types for byte, short, int, and long – at least beforegenerics will be specialisable in Java 10 and project Valhalla.
But you may not have this constraint in your code, so you should take all measures to replace:// Goes to the heapInteger i = 817598;… by this:// Stays on the stackint i = 817598;Things get worse when you’re using arrays:// Three heap objects!
Otherwise, you might be wasting a lot of stack frames for something that might have been implemented using only a few local variables.TakeawayThere’s not much to say about this apart from: Always prefer iteration over recursion when you’re deep down the N.O.P.E.
branch) abort every equals()method early, if:this == argumentthis "incompatible type" argumentNote that the latter condition includes argument == null, if you’re usinginstanceof to check for compatible types.
For instance, there is no way these two items can be equal:com.example.generated.Tables.MY_TABLEDSL.tableByName("MY_OTHER_TABLE")If the argument cannot be equal to this, and if we can check that easily, let’s do so and abort if the check fails.
They’re solving a problem step by step, branch by branch, loop by loop, method by method.
It is so much easier to write:SomeSet INTERSECT SomeOtherSetrather than:// Pre-Java 8Set result = new HashSet();for (Object candidate : someSet)    if (someOtherSet.contains(candidate))        result.add(candidate); // Even Java 8 doesn't really helpsomeSet.stream()       .filter(someOtherSet::contains)       .collect(Collectors.toSet());Some may argue that functional programming and Java 8 will help you write easier, more concise algorithms.
That’s not necessarily true.
Although the HTTP and TCP load balancers have feature parity, the available directives and parameters differ somewhat because of inherent differences between the protocols; for details, see the documentation about the Upstream modules for HTTP and TCP.You enable load balancing with two configuration blocks, which we’ll show in their basic form, without optional parameters or any auxiliary features:     The server block defines a virtual server that listens for traffic with the characteristics you define, and proxies it to a named group of upstream servers.
This block is the same in all our examples.
The upstream block names an upstream group and lists the servers that belong to it, identified by hostname, IP address, or UNIX-domain socket path.
The upstream block is where you specify the load-balancing technique, so we’ll be highlighting that in the sections that follow.
As an example, here’s the block for the default method, Round Robin:    upstream backend {    server web1;    server web2;    server web3;} Round RobinRound Robin is the default load-balancing technique for both NGINX Plus and NGINX.
In case of reboot or reconnection, the client’s address often changes to a different one in the /24 network range, but the connection still represents the same client, so there’s no reason to change the mapping to the server.If, however, the majority of the traffic to your site is coming from clients on the same /24 network, IP Hash doesn’t make sense because it maps all of them to the same server.
If all servers aren't equally loaded, traffic is not being distributed efficiently.
Try adjusting the weights, because their absence might be causing the imbalance rather than a problem with the load-balancing technique.
Errors and failed requests – You need to make sure that the number of failed requests and other errors is not larger than is usual for your site, otherwise you’re testing error conditions instead of realistic traffic.
The hashing algorithm evenly divides the set of all possible hash values into “buckets,” one per server in the upstream group, but there’s no way to predict whether the requests that actually occur will have hashes that are evenly distributed.
The web1 server ends up receiving more than twice as many requests as the other servers combined.So it makes sense to use Hash or IP Hash when the benefit of maintaining sessions outweighs the possibly bad effects of unbalanced load.
If you direct all requests for the file to the same server, only the first client experiences a long delay because of the database calls.
If, for example, you have servers in different data centers for purposes of disaster recovery, Least Time tends to send more requests to the local servers because they respond faster.
Recall that Least Connections and Least Time send each request to the server with the lowest “score” (number of connections, or a mathematical combination of connections and time, respectively).
When you assign weights, the load balancer divides each server’s score by its weight, and again sends the request to the server with the lowest value.
You need to understand upon what principles a microservices architecture has been built:ScalabilityAvailabilityResiliencyFlexibilityIndependent, autonomousDecentralized governanceFailure isolationAuto-ProvisioningContinuous delivery through DevOpsAdhering to these principles brings several challenges and issues when pushing your solution or system.
Those problems are common for many solutions.
Decompose by SubdomainDecomposing an application using business capabilities might be a good start, but you will come across so-called “God Classes”, which will not be easy to decompose.
Domain-Driven Design (DDD) refers to the application’s problem space — the business — as the domain.
The problem with 2PC is that it is quite slow compared to the execution time for the operation of a single microservice.
Coexist — leave the existing site where it is for a time.
Bulkhead PatternIsolate elements of an application into pools so that if one fails, the others will continue to function.
This design helps to isolate failures, and allows you to sustain service functionality for some consumers, even during a failure.
Sidecar PatternDeploy components of an application into a separate processor container to provide isolation and encapsulation.
Integration Patterns for MicroservicesAPI Gateway PatternWhen an application is broken down to smaller microservices, there are a few concerns that need to be addressed:There are multiple calls for multiple microservices by different channels.
Proxy PatternAPI gateway we just expose Microservices over API gateway.
If the application is a monolith and trying to break into microservices, denormalization is not that easy.
Each request has a compensating request that is executed when the request fails.
It can be implemented in two ways:Choreography — When there is no central coordination, each service produces and listens to another service’s events and decides if an action should be taken or not.
Performance MetricsWhen the service portfolio increases due to a microservice architecture, it becomes critical to keep a watch on the transactions so that patterns can be monitored and alerts sent when an issue happens.
To avoid code modification configuration can be used.
There are two problems with this: first, the request will keep going to the down service, exhausting network resources, and slowing performance.
Second, the user experience will be bad and unpredictable.
When the number of consecutive failures crosses a threshold, the circuit breaker trips, and for the duration of a timeout period, all attempts to invoke the remote service will fail immediately.
After the timeout expires the circuit breaker allows a limited number of test requests to pass through.
Otherwise, if there is a failure, the timeout period begins again.
This pattern is suited to, prevent an application from trying to invoke a remote service or access a shared resource if this operation is highly likely to fail.
Blue-Green Deployment Pattern avoids this.
You need to understand what principles microservice architecture has been built ScalabilityAvailabilityResiliencyFlexibilityIndependent, autonomousDecentralized governanceFailure isolationAuto-ProvisioningContinuous delivery through DevOps Adhering to the above principles brings several challenges and issues while bring your solution or system to live.
Those problems are common for many solutions.
DDD refers to the application’s problem space — the business — as the domain.
Coexist — Leave the existing site where it is for a time.
Bulkhead Pattern This pattern isolates elements of an application into pools so that if one fails, the others will continue to function.
This design helps to isolate failures, and allows you to sustain service functionality for some consumers, even during a failure.
Sidecar PatternThis deploys components of an application into a separate processor container to provide isolation and encapsulation.
Integration Patterns API Gateway Pattern When an application is broken down to smaller microservices, there are a few concerns that need to be addressed There are multiple calls for multiple microservices by different channels.
Proxy Pattern API gateway just exposes Microservices over API gateway.
But if the application is a monolith and trying to break into microservices, denormalization is not that easy.
Each request has a compensating request that is executed when the request fails.
It can be implemented in two ways: Choreography — When there is no central coordination, each service produces and listens to another service’s events and decides if an action should be taken or not.
Performance Metrics When the service portfolio increases due to a microservice architecture, it becomes critical to keep a watch on the transactions so that patterns can be monitored and alerts sent when an issue happens.
To avoid code modification configuration can be used.
There are two problems with this: first, the request will keep going to the down service, exhausting network resources, and slowing performance.
Second, the user experience will be bad and unpredictable.
When the number of consecutive failures crosses a threshold, the circuit breaker trips, and for the duration of a timeout period, all attempts to invoke the remote service will fail immediately.
After the timeout expires the circuit breaker allows a limited number of test requests to pass through.
Otherwise, if there is a failure, the timeout period begins again.
This pattern is suited to, prevent an application from trying to invoke a remote service or access a shared resource if this operation is highly likely to fail.
Blue-Green Deployment Pattern avoids this.
The downside is that you end up with more applications, a lot of different apps to deploy and manage.
This is why application platforms exist – they mitigate the downside.
It can be difficult to get the paradigm shift of working with constituent parts.
While it’s critical that an individual microservice is monitored and deployed in a responsible way, it’s also critical to keep an eye on the cross-microservice flows that make up most companies’ revenue-generating core business processes.
Not more efficient message brokers.
As such, they should address a domain broadly enough to avoid challenges around “nanoservice” orchestration but bounded enough to avoid unnecessary dependencies.
What are the problems Netflix and Amazon tried to solve and are you trying to solve the same problem?
The architecture of the application became less defined and started to step on itself and became difficult to reason with and change.
Caution teams to question that doing microservices is optimization for a particular problem that comes with a lot of complexity.
Microservices code has 5X the vulnerabilities of monolithic code.
Microservices are interface based, there is no governance behind the scenes.
When writing microservices many assumptions are made around how they are used and consumed that does not hold true.
Isolation and API guarantees are the most important elements of microservices.
Isolation allows developers to iterate quickly and independently.
However, developers must maintain API compatibility so other microservices are not impacted (and create dependencies that slow down iterations).
People have difficulty understanding where ingress and egress points give way to microservices.
Avoid premature decomposition.
The hard part is breaking the data dependencies of state apart.
Splitting the data becomes the hard part.
Caching can become more difficult.
A common mistake is that people don’t understand the boundaries of what a service should be (e.g., service for data access layer).
Lack of understanding around microservices and properly defining boundaries.
Solving a real problem around engineering philosophy and scaling.
It's the manifestation of communication problems.
Further, operations become a critical piece of the equation as we introduce automation of the software supply chain with these services.
How you can group a set of microservices and expose via a gateway.
This does not optimize the process.
This brings about the need to learn common patterns in these problems and solve them with reusable solutions.
Before we dive into the design patterns, we need to understand on what principles microservice architecture has been built:ScalabilityAvailabilityResiliencyIndependent, autonomousDecentralized governanceFailure isolationAuto-ProvisioningContinuous delivery through DevOpsApplying all these principles brings several challenges and issues.
Let's discuss those problems and their solutions.
Decomposition Patternsa. Decompose by Business CapabilityProblemMicroservices is all about making services loosely coupled, applying the single responsibility principle.
b. Decompose by SubdomainProblemDecomposing an application using business capabilities might be a good start, but you will come across so-called "God Classes" which will not be easy to decompose.
It uses subdomains and bounded context concepts to solve this problem.
Note: Identifying subdomains is not an easy task.
c. Strangler PatternProblemSo far, the design patterns we talked about were decomposing applications for greenfield, but 80% of the work we do is with brownfield applications, which are big, monolithic applications.
Applying all the above design patterns to them will be difficult because breaking them into smaller pieces at the same time it's being used live is a big task.
Integration Patternsa. API Gateway PatternProblemWhen an application is broken down to smaller microservices, there are a few concerns that need to be addressed:How to call multiple microservices abstracting producer information.
Who will do the data transformation or field manipulation?
How to handle different type of Protocols some of which might not be supported by producer microservice.
b. Aggregator PatternProblemWe have talked about resolving the aggregating data problem in the API Gateway Pattern.
c. Client-Side UI Composition PatternProblemWhen services are developed by decomposing business capabilities/subdomains, the services responsible for user experience have to pull data from several microservices.
Database per ServiceProblemThere is a problem of how to define database architecture for microservices.
Each microservice should have a separate database id so that separate access can be given to put up a barrier and prevent it from using other service tables.
But if the application is a monolith and trying to break into microservices, denormalization is not that easy.
In this pattern, one database can be aligned with more than one microservice, but it has to be restricted to 2-3 maximum, otherwise scaling, autonomy, and independence will be challenging to execute.
c. Command Query Responsibility Segregation (CQRS)ProblemOnce we implement database-per-service, there is a requirement to query, which requires joint data from multiple services — it's not possible.
d. Saga PatternProblemWhen each service has its own database and a business transaction spans multiple services, how do we ensure data consistency across services?
Each request has a compensating request that is executed when the request fails.
It can be implemented in two ways:Choreography — When there is no central coordination, each service produces and listens to another service’s events and decides if an action should be taken or not.
Observability Patternsa. Log AggregationProblemConsider a use case where an application consists of multiple service instances that are running on multiple machines.
Performance MetricsProblemWhen the service portfolio increases due to microservice architecture, it becomes critical to keep a watch on the transactions so that patterns can be monitored and alerts sent when an issue happens.
Prometheusc. Distributed Tracing ProblemIn microservice architecture, requests often span multiple services.
Then, how do we trace a request end-to-end to troubleshoot the problem?
d. Health CheckProblemWhen microservice architecture has been implemented, there is a chance that a service might be up but not able to handle transactions.
In that case, how do you ensure a request doesn't go to those failed instances?
External ConfigurationProblemA service typically calls other services and databases as well.
How do we avoid code modification for configuration changes?
Service Discovery PatternProblemWhen microservices come into the picture, we need to address a few issues in terms of calling services:With container technology, IP addresses are dynamically allocated to the service instances.
c. Circuit Breaker PatternProblemA service generally calls other services to retrieve data, and there is the chance that the downstream service may be down.
There are two problems with this: first, the request will keep going to the down service, exhausting network resources and slowing performance.
Second, the user experience will be bad and unpredictable.
How do we avoid cascading service failures and handle failures gracefully?
When the number of consecutive failures crosses a threshold, the circuit breaker trips, and for the duration of a timeout period, all attempts to invoke the remote service will fail immediately.
After the timeout expires the circuit breaker allows a limited number of test requests to pass through.
Otherwise, if there is a failure, the timeout period begins again.
d. Blue-Green Deployment Pattern ProblemWith microservice architecture, one application can have many microservices.
How do we avoid or reduce downtime of the services during deployment?
I am stopping now to hear back from you on what microservice patterns you are using.
You can opt for higher throughput with a chance for data loss, or you may prefer a very high data durability guarantee at the expense of lower throughput.
In a nutshell, having only one in sync replica follows the "letter of the law" but not the "spirit of the law."
Having replicas out of sync with the leader is considered a retriable error, so the producer will continue to retry and send the records up to the configured delivery timeout.
If no partition is provided, and the ProducerRecord has a key, the producer takes the hash of the key modulo as the number of partitions.
Previously, if there was no key and no partition present in the ProducerRecord, Kafka would use a round-robin approach to assign messages across partitions.
For the sake of simplicity, let’s assume that your application produced nine records with no key, all arriving at the same time:  As you can see above, the nine incoming records will result in three batches of three records.
Tip #3: Avoid “Stop-the-World” Consumer Group Rebalances by Using Cooperative RebalancingKafka is a distributed system, and one of the key things distributed systems need to do is deal with failures and disruptions — not just anticipate failures, but fully embrace them.
But what happens if one of the applications suffers an error or can’t connect to the network anymore?
Here’s another illustration showing the consumer group protocol in action:  As you can see, Consumer 2 fails for some reason and either can miss a poll or trigger a session timeout.
So as you can see, losing a consumer application for a particular group ID doesn’t result in a loss of processing on those topic partitions.
Each consumer gives up its entire assignment of topic partitions, and no processing takes place until the topic partitions are reassigned — sometimes referred to as a “stop-the-world” rebalance.
To compound the issue, depending on the instance of the ConsumerPartitionAssignor used, consumers are simply reassigned the same topic partitions that they owned prior to the rebalance, the net effect being that there is no need to pause work on those partitions.
It could be revoking topic partitions that are no longer assigned or adding new topic partitions.
The bottom line is that eliminating the "stop-the-world" approach to rebalancing and only stopping the topic partitions involved means less costly rebalances, thus reducing the total time to complete the rebalance.
Even long rebalances are less painful now that processing can continue throughout them.
To start the console producer, run this command:    Shell    x                                   1            kafka-console-producer --topic  \             2                                    --broker-list <broker-host:port> After you execute the command, there’s empty prompt waiting for your input — just type in some characters and hit enter to produce a message.
Dump LogSometimes when you’re working with Kafka, you may find yourself needing to manually inspect the underlying logs of a topic.
However, records aren’t stored in one big file but are broken up into segments by partition where the offset order is continuous across segments for the same topic partition.
Remember, you never want to go into the filesystem and manually delete files.
Offset: the offset you want the delete to start from, moving backward to lower offsets.
The low_watermark value of 10 indicates the lowest offset available to consumers.
Because there were only 10 records in the example topic, we know that the offsets ranged from 0 to 9 and no consumer can read those records again.
But adding artificial data to the key poses two potential problems.
The only way to catch integration problems is monitoring.
These problems, like a mobile carrier experiencing an outage, may be due to our errors or to external conditions; but they should nevertheless be discovered as early as possible.The infrastructureDatadog is the only data-collection service that passed the stress tests of SLL, our solution architect.
The server collects data locally and periodically uploads it to Datadog in bursts, where you can access it via a web application or via APIs in case you want to call it from your build.The UDP protocol is aligned with the goals of metric collections: a silent server that decouples the sending of metrics from the rest of the business logic:UDP packets are just lost if no process is there listening to them, no errors are raised if the server crashes or is not running or installed for some reason for instance in development machines).The monitoring code, which you write, should be decoupled and asynchronous as much as possible.
The part that talks over the network is already externalized in the DataDog server, but you don't want the user to wait because you have to send some strange number.So the internal part (sending via UDP) is performed in Listener objects that implement the Observer pattern.
These object still have to be wrapped in all-encompassing try/catch constructs so that any errors in the monitoring part never influence the business logic.
Againg, you don't want a payment to fail because of an exception in how monitoring DateTime objects are built.For PHP we built a SilentListener class to wrap all of our object:class SilentListener{  private $wrapped;  public function __construct($wrapped)  {      $this->wrapped = $wrapped;  }  public function __call($method, $args)  {      try {          call_user_func_array(array($this->wrapped, $method), $args);      } catch (Exception $e) {          $this->log($e);      }  }}SLLAn exampleIn some countries, we receive payments through mobile-originated messages (MO), a fancy word for saying SMS sent by the end user.
So a simple way to monitor if we are receiving payment or if the server is exploded is to upload a metric counting them every time we receive one (pseudo-JSON format to show you the data):{  counter: 1}However, we can be more precise than this: an external outage or an integration problem may happen to a lower level than the whole application.
The data you’ll gain through the IMMUNIO integration with Datadog allows you to differentiate between attacks on your site, such as Account Takeover attacks, and periods of higher-than-normal traffic.To get all the details on the integration and to see a full demo illustrating how you can use IMMUNIO within Datadog to analyze and stop an Account Takeover attack, check out the full IMMUNIO/Datadog webinar.If you’re an existing Datadog user and you like what you see, now’s the time to sign up: we’re offering Datadog users a three-month free trial of IMMUNIO for their first application if you sign up by Aug. 30.
Update the sbt version to 0.13.7 or higher in the build.properties file and update your Scala version to 2.11.8 or higher (I have not tried this with a lower version of Scala than this) in build.sbt.
Add the following library dependency and configurations for Cinnamon in your build.sbtlibraryDependencies += Cinnamon.library.cinnamonAkka //Akka based application monitoring via Cinnamonlazy val app = project in file(".")
We just need the same thing for design, deployment, virtualization, serverless, DNS, SDK, documentation, and the other critical stops along a modern API life cycle.I'll keep profiling the APIs for the service providers in my lifecycle research until I get more of the DevOps-aggregate API definition mapped out.
A single point of failure can stop the entire process, but identifying it is becoming increasingly difficult.
Using these metrics, one can find out whether or not to increase or decrease the number and size of nodes in the cluster.
Running pods - the number of pods running will show you if the number of nodes available is sufficient and if they will be able to handle the entire workload in case a node fails.
The following information can be monitored: the number of instances a pod has at the moment and how many were expected (if the number is low, your cluster may be out of resources), how the on-progress deployment is going (how many instances were changed from an older version to a new one), health checks, and some network data available through network services.
Container metrics are available mostly through Cadvisor and exposed by Heapster which queries every node about the running containers.
For example, a database application will probably expose metrics related to an indices' state and statistics concerning tables and relationships.
An e-commerce application would expose data concerning the number of users online and how much money the software made in the last hour, for example.
As nodes are destroyed, the pod is also terminated.
In this case, there is not a general solution; each tool will have its own software for cluster monitoring.
For example, creating a label for database intensive pods will enable the operator to identify if there is a problem in the database service.
Dynatrace does not distinguish between a simple Linux host and Kubernetes nodes, so you can't expect some useful Kubernetes metrics exposed by default like with Datadog.
Topic 3 - The first time I saw the service, there was this cool “look-back correlation” capability that would align problems with activities that happened around that time (eg.
performance problem; new code deployed).
Sometimes it’s hard to get teams to all use the same collaboration environments.
The 7 Categories Of Engineering ManagementIan Nowland, the SVP of Core Engineering at DataDog, joins the Dev Interrupted podcast to discuss how he takes his ego out of being a manager and the seven categories he uses when coaching his teams.
Interestingly, 2% of Datadog customers abandoned Docker within a year.
The number is based on the number of hosts that are using Docker, which may seem misleading, but in order to be considered an adopter they needed to be running more containers than their SMB counterparts.
It is odd is that Apache was not in the top ten of the most common technologies to use with Docker, while NGINX was, but this may be because of Datadog's enteprise-focused clientele.Docker hosts often run four containers at a time on each host.
According to the research, "this seems to indicate that Docker is in fact commonly used as a lightweight way to share compute resources; it is not solely valued for providing a knowable, versioned runtime environment.
If you abandoned Docker, why?
If you deploy on Heroku, StatusPage will let you build a custom-branded status page to communicate: The overall status of your platformThe status of individual components and services within your platformPublic metrics about your service like API uptime, error rate, or user activityA running log of incidentsMetric data can be pulled from performance analysis services, including New Relic, Librato, TempoDB, Pingdom and Datadog.
It is, in essence, a trivial countdown application.
By setting it to 0, you avoid synchronization and get a similar result to a single node.
Crafting an OpenAPI Spec allows me to define each of these companies APIs, and easily articulate what it is that they do — minus all the bullshit that often comes with the businesses side of all of this.
Moreover, tracing without limits will be added to its application performance monitoring (APM) and distribution tracing services to enable DevOps team to find all the traces without live sampling along with storing traces that match bad user experiences.
According to Raj Mehta, CEO of Raj Technologies, many development projects are slowed down by manual steps and undefined communication methodologies and there is too much room for error.
He opined that the lack of skilled professionals cannot bring workflow automation, so they need to become experts in designing workflows before they deploy automation.
There are many elements including project templates, resource management, demand management, policies, and scripted actions should be considered.
DevSecOps startup ShiftLeft raises $20 million for code analysis software that automatically patches vulnerabilities.
People can have good and bad ones, even when using exactly the same website or mobile application.
When was the last time you submitted a report for a software error?
Fortunately, these wasted interactions are preventable using Application Performance Monitoring tools.
Why You Need Application Performance Monitoring ToolsPart of the problem is that different operating systems, browsers, connection speeds, devices and locations mean that any number of combinations could be interacting with your application.
Local testing is useful but fails to identify the vast amount of variables that users can encounter when trying to use your app.
How do you fake millions of users interacting with your application and get an accurate view of what may happen when you deploy into production?
Mainly, Raygun focuses on errors, crashes, and performance issues that users encounter.
One of the features that saves the most time is smart error grouping, which collects errors underneath a single root cause so you don't get flooded with notifications.
Another time-saving feature of Raygun is the ability to identify authenticated users, so you get a view of which specific users encountered problems, and to what extent.
The unique profile will have a list of every error or crash that user has encountered, which browser they use, and all the devices they use to access your app.
Key FeaturesFull-text search and filteringTrack multiple applications using any language and frameworkError, crash, and performance tracking in one platformUnlimited end usersDiscover critical bugsAdd comments and mention team members on issuesVersion/deployment trackingAttach tags, custom data objects, and user dataJavaScript source mapsAutomatic error reportingSeamless integration that takes minutesIntelligent error groupingError notifications via email, Slack, HipChat etcFull stack traces2.
With a ton of insights into how the elements of our application interact, New Relic helps tune the experience for users and identify potentially disastrous issues before they become problems.
Though New Relic is not as strong on the error and crash diagnosis as other dedicated crash reporting and error tracking solutions, you can pair it up with a tool like Raygun to give you the best of both worlds.
Key FeaturesApp availability monitoring, alerting, and notificationProduction Thread Profiler featureAutomatic application topology mappingPlatform pluginsHistograms and percentilesPerformance data API accessReal-User response time, throughput, and breakdown by layerJVM performance analyzerError detection, alerting, and analysisDatabase call response time and throughputCode-level diagnostics, transaction tracing, and stack traceApp response time, throughput, and breakdown by componentSlow SQL and SQL performance detailsApdex scoreReal-User breakdown by web page, browser, geographyCustom dashboardsTrack individual business transactionsX-Ray sessions for business transactionsCross-application tracing for distributed appsAvailability, scalability, deployment reports3.
So, if your app is painfully slow for customers located there, you'll need to know about it.
For this reason, Real User Monitoring (RUM) tools are essential for identifying performance problems and their environments.
Pingdom also monitors your website for downtime, alerting you right away when a disaster happens, and it goes offline.
Key FeaturesEnd User MonitoringApplication health dashboardReal-time business transaction monitoringReports and visibility into your applicationVisualize and manage your entire appOperational dashboardsDetect business impact and performance spikesAnalyze impact of agile releasesIsolate bottlenecks in your applicationDeep code-level diagnostics in productionIdentify root cause with complete code diagnosticsDynamic scaling in the cloudAutomatic business transaction discoveryDiagnose the root cause of the problems 90% fasterDiscover and visualize your application topology and businessTroubleshoot performance and availability issuesSet up proactive alerting to find problemsMonitor 24/7 what matters most-your key business transactionTroubleshoot bottlenecks 90% fasterMonitor hybrid environments with Java, .NET, PHP, and Node.jsOverviewIn conclusion, each of the performance monitoring tools above offers something slightly different, but are all built around the same goal - seeing what your users are doing when they encounter problems.
Application Performance Monitoring tools present the symptoms of the problem in a clear and visual way to aid in diagnosis and ultimate resolution.
Ultimately, performance problems are a huge contributor to dissatisfactory software experiences.
Talk about your architecture and why you’d want to tackle such a large problem space?
The Cloudcast #301: SRE and Infrastructure Operations [Podcast]Description: Brian talks with Rob Hirschfeld (@zehicle, Founder/CEO of @RackN) about the concepts of SRE (Site Reliability Engineering), the challenges of maintaining infrastructure software, emerging tools, and the next-generation of operations.
Time was slowly passing and civilizations prospered, but it’s still difficult to find info on how to make Spring Boot work with Micrometer CloudWatch.
TL;DRIf you are not interested in why and just want to make Spring Boot work with AWS CloudWatch, do the following:Add dependencies (build.gradle)dependencies {    compile('org.springframework.boot:spring-boot-starter-actuator')    compile('org.springframework.cloud:spring-cloud-starter-aws')    compile('io.micrometer:micrometer-registry-cloudwatch:1.0.6')}Set the required properties:management.metrics.export.cloudwatch.namespace=m3trics2management.metrics.export.cloudwatch.batchSize=20That’s it.
And this one just takes the batchSize from the property or sets the default value to 10000, not caring about AWS restriction.
"Properly" can be defined during the creation of the test, and what happens with that data can vary from product to product, but typically includes features such as screenshots of failures, waterfall latency to determine where page load slow-downs derive, or even fully synthetic workflows such as logging in as a user and executing a transaction.
While it's possible to roll your own synthetic monitoring through localized testing, there’s no replacement for a solid product that uses last-mile monitoring to report exactly what your customers are seeing—and from an unbiased source.
There's no reason to use a service that begins at a low cost but results in exorbitant costs over time as you grow.
If it is too difficult, your team may not even use the service that you’re paying for in the first place.
Unfortunately, there are many variables in determining “difficulty” as a Key Performance Indicator.
Here we'll give a score of 1-5 (5 being the best) based on difficulty in finding the options to create, edit, and delete basic synthetic tests.
AccuracyIf alerts are constantly firing they can breed a sense of distrust and complacency or “Alarm Fatigue” that commonly plague operations and developers alike.
If no alarm fires, was your site ever really monitored at all?
While this is a more subjective analysis, it’s no less important.
This will pass or fail on being able to easily and intuitively navigate to an alert, and identify the source of the failures.
Notification Channel Integrations (Slack, Discord, Email, etc.)
It may not be important to dial an engineer for Jira failures over a weekend, but it’s very important to get all hands on deck ASAP when the entire site is down.
APM IntegrationsFinally, we’ll be considering the difficulty to triage incidents.
We will inject faults by partially shutting down services for 20 minutes on a single container, and then entirely for 30 minutes across all containers.
Afterward, we will determine how long it takes each service to catch the failure and alert.
By shutting off services on various hosts, we can simulate a code deployment or application failure that should be enough to take services out of the load balancers configured in DigitalOcean and Cloudflare.
Based on our design, we should be extremely resilient to such faults and agile in our rollouts.
However, as seasoned admins, we know that there is still a limit to even this level of distribution and resiliency, and it is our fail-safe last-mile monitoring that alerts us to critical customer impact.
Because each product has vastly different timings allowed for trial accounts, we'll level the playing field by setting each service to 15-minute threshold alerts and multiple failures before alerting where applicable.
Each product should be able to tell us the moment it detects the site is no longer reachable during our deployments.
Updown.IOProductPrice/moSetup easeAccuracyOperabilityNotification ChannelsAPM IntegrationUpdown.io$.075 4Fail6NoA relatively small French company, Updown's goal is to provide an inexpensive and user-friendly tool with a simple, slick interface.
The score for configuration notifications took a hit due to the out-of-sight, out-of-mind location of the options, and the limited number of options available.
UptimeRobotProductPrice/moSetup easeAccuracyOperabilityNotification ChannelsAPM IntegrationUptimeRobot$.255Fail10NoWith simple monitoring, simple setup, and a free-tier option to boot for entry-level monitors, UptimeRobot is a low-barrier option for quickly implementing site monitoring.
It was difficult to find the escalation points as they were nested in the global configurations.
Metrics and ResultsThe escalations all fired very quickly without any false positives.
Unfortunately, diving into the ale product detected ert itself was somewhat vague as I couldn't see where the failures occurred.
Due to the lack of functionality available to understand the nature of the failure, this tool is best used as a trigger to the real work rather than a finder of smoking guns.
However, it lost some points due to its very busy interface.
But after the initial color shock, the workflow in the navigation pane was simple to understand.
Metrics and ResultsWhile the dashboard itself was concise in its information display and clear in its messaging, I did not receive any alerts for the failed test.
It was unclear as to whether this was due to incorrect configuration or due to the short duration of the tests (<30 minutes).
But without a trigger to draw my attention to this alert, the tool wasn't very helpful.
Overall ImpressionThe user interface is very busy and the color scheme is distracting.
The fact that it appears so simple to configure makes it somewhat disappointing that there weren't any escalations performed.
The product lost some points with accuracy.
Although both events were marked as "alerting", I didn’t receive any Pagerduty escalations, which caused a delay in troubleshooting.
By selecting "Integrations" in the sidebar, I was able to set up the escalation methods and configure Pagerduty, Slack, and Discord through the webhook interface.
Metrics and ResultsPingdom caught one prolonged outage but missed the partial outage triggered shortly before it.
I did receive quick and relevant notifications through Slack and Discord with an easy-to-follow root cause analysis as to what was wrong.
By digging through the alert, I was able to get a clear picture as to what the monitors were alerting on and from where allowing me a direct picture of where the problem might be and letting me understand the nature and impact of the failures.
The only barrier I encountered in setting up my notification channels was digging through all the options to find the integration I wanted to use.
It was abundantly clear that the breadth of offerings in DataDog’s catalog could be integrated and finely tuned to drive not only triage, but automated mitigation with features like code deployment kill switches and API triggered workflows.
However, without careful considerations, its cost can quickly overrun budgets.
Add in the integrations available with the rest of the SolarWinds APM suite, and they are on par with the offerings from DataDog, but at a lower cost.
To check your Java version, run the following command:java -versionThe output on my system is as follows:java version "1.8.0_161"Java(TM) SE Runtime Environment (build 1.8.0_161-b12)Java HotSpot(TM) 64-Bit Server VM (build 25.161-b12, mixed mode)If Java 8 is not installed, then please download it from the Oracle website and follow the instructions for installation.
Monitoring Using Spring Boot 2.0, Prometheus, and Grafana (Part 2 — Exposing Metrics)In part two, we will be enabling the metrics endpoints.
In this part, we will be working on the same application to add settings and configurations, which will enable us to expose endpoints for metrics.
pom.xml : In pom.xml, add the dependency for micrometer-core and micrometer-prometheus-registry by adding following snippet.xml<!-- Spring boot actuator to expose metrics endpoint --><dependency>    <groupId>org.springframework.boot</groupId>    <artifactId>spring-boot-starter-actuator</artifactId></dependency><!-- Micormeter core dependecy  --><dependency>    <groupId>io.micrometer</groupId>    <artifactId>micrometer-core</artifactId></dependency><!-- Micrometer Prometheus registry  --><dependency>    <groupId>io.micrometer</groupId>    <artifactId>micrometer-registry-prometheus</artifactId></dependency>application.properties: unable the actuator and Prometheus endpoints to be exposed by adding below properties.proper#Metrics related configurationsmanagement.endpoint.metrics.enabled=truemanagement.endpoints.web.exposure.include=*management.endpoint.prometheus.enabled=truemanagement.metrics.export.prometheus.enabled=trueThat is all you need to do to enable the metrics.
For operations, there are no solutions.
There’s a lack of expertise of what to look for and how to look for it.
The framework is integrated with Jenkins and there is a variety of trend graphs (percentage of failed requests, mean response time, etc.)
They're the superheroes because they solve some unsolvable problems in a lot of ways.
The speaker went through problems with BASIC off , DIGEST off and J2EE ... all of these things which we use in our products.
At the end we're left with, “what is the answer?” Well, the answer is that for each of these, there are different problems.
We have to understand where some things are broken, know that those things aren't perfect, and design for that imperfection.
This will help you identify issues in container images such as vulnerable packages and embedded secrets during the build process where you can choose to automatically fail the builds that don’t meet your security policy.
Monitor containers – Visibility into running containers themselves is as critical as the images they’re instantiated from.
By implementing the above, you should also be able to generate detailed vulnerability and configuration assessment reports to meet compliance requirements.
this echoes the trends we see at electric cloud, with an increasing demand for getting teams started more quickly with docker and other container platforms, incorporating   container-based and microservices-based applications  into their overall software delivery strategy – alongside traditional releases or legacy apps.
These situations are disruptive, stressful, and can result in downtime that impacts operations (and profits) of the entire organization.
What’s even more problematic is that outrages result in 545 hours of staff productivity losses per annum.
This way, you can maximize your uptime and develop better disaster recovery plans.
Highly-customizable alerts systems that can include email, text, slack alerts, IFTTT posts, service restarts, and web alarms.
Monitoring allows the IT pros to develop insights regarding potential issues, especially as an organization grows and puts more stress on the current systems.
Detect Issues Early OnUsing proactive monitoring tools means that you will receive alerts before an issue becomes a disaster.
Small discrepancies and early warning signs will allow your IT folks to anticipate potential issues and ward them off.
This is a far more productive approach than facing the disasters after they occur and trying to put out fires.
Plan for Upgrades and IT BudgetingAs organizations grow, the stressors on current infrastructure grow as well.
With downtime comes a loss of productivity on the part of end-users.
Order fulfillment is stalled; logistics suffers; customer-service is negatively impacted, etc.
If these things occur too often, business losses occur.
We spent three days together to try to solve this problem.
If the metrics fall outside a defined deviation, the canary fails.
Once the canary stage is completed, both the canary and the baseline server groups are destroyed, and the pipeline continues either as a success or a failure.
Here's an example of a pipeline that passed, so it goes on to do the staging deployment, and here's one that failed.
If the deployment fails, it actually shows you a DataDog dashboard of the metrics that caused the failure.
Too many metrics will cause too much noise and false failures.
My answer to him was, "Dude, stop watching it.
He was so trained to sitting there, and watching the deployment, and waiting for it to succeed so he could see if it failed.
Metrics pipelinesMetrics Server and APIOriginally the metrics were exposed to users through Heapster which queried the metrics from each of Kubelet.
The metrics-server was introduced to replace Heapster and use the Kubernetes API to expose the metrics so that the metrics are available in the same manner in which Kubernetes API is available.
Aggregation LayerOne of the key pieces which enable exposing the metrics via the Kubernetes API layer is the aggregation layer.
The metrics will be exposed at /apis/metrics.k8s.io as we saw in the previous section and will be used by HPA.
Before we can expose the endpoint using Kubernetes API aggregation we need to convert the metrics to a suitable format.
If you are not familiar with the landscape it is easy to get confused and lost.
The services also publish their health along with the number of total, slow & failed transactions detected during this period.Apparently the BitcoinController.mining() service is performing rather badly, with 25% of the transactions being too slow and 25% failing altogether.
enter your query in the “search” field — in this case, i’m looking at an apache access log that reports a 504 error: once you’ve narrowed down the type of log message on which you want to be alerted, click the “create alert” button.
No vendor pitches are allowed.
are some of the key productsNetwork: With each of the containers running different processes, there is a need to manage and, at times, isolate which container services can access which other services.
Products like Flannel, Weaveworks, and Calico are some of the products in this space.Monitoring/Auditing/Logging: With thousands of containers running, monitoring/auditing/logging each of the containers become a tough problem.
But I’m not sure that’s always going to be so.
And since we are no closer to understanding the world than we were in Newton’s time (or so it seems)….”Right, and even if we did model the whole world, we’d need another system to model all the software we’ve written so we know whether it’s running correctly, and so we can keep it running correctly.
You want to be able to act in no time and post-mortem an outage with handy, detailed reports.
To quote Simon Hørup Eskildsen’s recent blog “Why Docker is Not Yet Succeeding Widely in Production”:One example of an area that could’ve profited from change earlier is logging.
In 1.7, experimental support for out-of-process plugins was merged, but – to my disappointment – it didn’t ship with a logging driver.
This is convenient for ops engineers who might need to search through dead containers’ logs.Use Case 3: Streaming Logs into Data Processing BackendsIf you want to do analytics on your raw container logs, you can also send all Docker container logs to HDFS via the HDFS output plugin.
Use both education and enforcement controls to prevent the use of untrusted images, which may introduce vulnerabilities and configuration issues into your environment.
Manage Secrets Putting secrets in the container image exposes it to many users and processes and puts it in jeopardy of being misused.
The secret should only be accessible to the relevant containers, and should not be stored on a disk or be exposed at the host level.
Once the container is stopped, the secret disappears.
Vulnerability Scanning Prevent images with known vulnerabilities from running in your production environment by using vulnerability scanning tools.
Incorporate vulnerability scanning “quality gates” in your delivery lifecycle to prevent vulnerable containers from deploying.
For the purpose of this post, we will look at a Flask application, in flask_app.py:from flask import Flask@app.route('/test/')def test():    return 'rest'@app.route('/test1/')def test1():    1/0    return 'rest'@app.errorhandler(500)def handle_500(error):    return str(error), 500if __name__ == '__main__':    app.run()Requests to the application for “/test/” receive a text “rest” as a response, and requests to the application for “/test1/” cause an exception that results in the 500 Internal Server Error.
This, in turn, is returned as a proper, HTTP 500 response with the error returned as a response text.
Our next step is to add this endpoint to our web application to expose the calculated metrics in a format understood by Prometheus by using the Prometheus Python client.
from prometheus_client import Counter, HistogramREQUEST_COUNT = Counter(    'request_count', 'App Request Count',    ['app_name', 'method', 'endpoint', 'http_status'])REQUEST_LATENCY = Histogram('request_latency_seconds', 'Request latency',    ['app_name', 'endpoint'])The first argument while creating the objects above is the name of the metric, followed by a description of what it refers to.
The third argument is a list of labelsassociated with the metric.
For example, request_count{http_status="500"} will only show the requests that were unsuccessful with a 500 HTTP status code.
Setting Up Statsd Bridge for PrometheusIf you haven’t terminated the services we started earlier, press Ctrl+C in the window where we started the service and that will terminate the services and stop the containers.
Next, we will use docker-compose once again to start our web application, the statsd exporter, and our Prometheus server:$ cd flask_app_statsd_prometheus$ docker-compose -f docker-compose.yml -f docker-compose-infra.yml upIf you encounter errors such as Cannot create container for service webapp, you can remove it using docker rm webapp and then attempt to run docker-composeagain.
We first saw an approach using the native Prometheus client, but considering the limitation it suffers from, we used a straightforward second approach to eliminate that limitation.
Since container orchestration is critical to deploying, scaling, and managing your containerized applications, monitoring Kubernetes needs to be a big part of your monitoring strategy.
Container environments don't operate like traditional ones.
Orchestrating your containerized services and workloads through Kubernetes brings order to the chaos, but remember that your environment is still decentralized.
You will give yourself a fighting chance if you centralize your logs and metrics.
You still need to debug problems and monitor cluster activity, even when services are coming and going.
The trick is to grab the logs and metrics before they're gone.
You will need to collect logs and monitor for errors, failures, and performance issues at each layer - the pod, the container, and the controller manager - of your environment.
I may have simply failed to think of it, so omission shouldn’t be considered a negative opinion!
We started with physical boxes - we had to buy and maintain these boxes, and no matter how much we used, we had to pay for all of it.
So, if your function takes 500ms to run, you pay for 500ms to run it once.
Benjamin Wilms just announced the next BIG release of Chaos Monkey for Spring Boot, including Micrometer metrics, customized attacks on your methods at runtime, and much more!
Caching can be used and customized with an acceptable eviction policy based on business requirements.
It’s difficult to identify issues between microservices when services are dependent on each other and they have a cyclic dependency.
Testing — This issue can be addressed with unit and integration testing by mocking microservices individually or integrated/dependent APIs which are not available for testing using WireMock, BDD, Cucumber, integration testing.
It will trace all microservices communication and show request/response, errors on its dashboard.
Critical Alerting for When Your Tools Are Out of ControlWe have all heard stories of DevOps woe.
Some tales are sad.
Some tales just leave you thinking, what the heck were developers thinking?
However, the developers never thought to test what they were spinning up or to put critical alerting in place for when things went wrong.
And that is where our tale of woe begins.
Tale Number 1: Automation Destroyed the WorldWhat the tool was supposed to do has long since been forgotten but the horror and nightmares it caused will not go away so soon.
At the start-up I am referring to here, every protocol was seemingly ignored when this new tool was written and deployed.
This is bad because code that can do anything will do everything.
Additionally, when the servers were no longer needed, the code would scale down by killing oldest instances first.
However, the tool didn’t check if the code was working on the newest servers before destroying the old ones.
The problem was that it was that it was not fully thought out.
If not, the original tool would do nothing.
The second tool checked if the environment was not healthy.
In addition to validation requests, code also needs critical alerts to notify developers and Ops for when it doesn’t work as planned.
Tale Number 2: Automation Quickly Balloons Your Amazon BillIn this second tale of DevOps horror and woe, another New England start-up had a group of its developers create a tool for spinning up infrastructure.
That is to say, the tool had no dependencies.
Again, the tool was designed to have no dependencies and indeed it did not.
The problem with having no dependencies is that you can build everything and there is a cost to building infrastructure.
No dependencies can start costing your team a lot of money very quickly.
The lack of control on this tool enabled it to create lots of unnecessary infrastructure.
The lack of supervision also allowed for the creation of thousands of CloudWatch metrics.
AWS charged the company anytime it made more than two million API calls per month.
The only recognition of how far things had gone out of control was only the bill came at the end of the month and the team realized their error.
Furthermore, developers were now given their own account that is separate from Testing and Production that does not create new infrastructure that impacts Ops.
Finally, the company instigated weekly Dev and Ops meetings to ensure that each side is aware of the others’ pain points.
A Cautionary Tale for Developers Everywhere: Use Critical AlertingWhile much fault in both tales can be found with the developers, the scenarios are not unique.
Most DevOps engineers can probably think of instances where imperfect code was pushed into production.
The cautionary tale here lies in that there was no mechanism or alerting platform to recognize the problems.
Prayer is not a strategy for effective DevOps.
Instead, you need:You need to have mechanisms in place to alert you when things go wrong.
Monitoring is no longer just for ops teams.
It can let you know when things are heading in a dangerous direction, sometimes enabling intervention prior to a failure point.
When problems do occur, monitoring your infrastructure tells you quickly if it’s an app problem or infrastructure problem.
If your service is a dependency to others and your instance is down, it’s critical that the development team have this information.
Application MonitoringStabilityStability monitoring measures overall error levels and helps you understand when application errors have reached a critical level that could impact your users’ experience.
This is especially important since buggy applications are frustrating and considered unreliable by end users.
In the past, logs were used to track application errors, and engineers would investigate them when they received customer complaints.
This was a painful experience for both engineering teams trying to piece together what caused a bug, and also for the user who experienced the bug and was responsible for reporting it.
Since then, engineering teams have standardized on error monitoring software that automatically captures and alerts on errors, but this still requires lots of engineering overhead to triage the list of errors continually.
Stability monitoring adds logic to the process of monitoring for errors.
It provides a definitive metric that lets engineering teams know when errors are impacting stability to the point that they must spend time debugging.
Slowdowns in request processing from both the server-side and client-side can result in a poor user experience.
Performance bottlenecks in your code base can produce downstream effects on multiple services, sometimes leading to a cascading failure.
Setting up KPIs for any service where higher CPU or memory usage could result means you are notified closer to the source of any problem occurring.
The downside of RUM is that the data can be noisy as each user’s internet speed and system influence the metrics collected.
Synthetic user monitoring is frequently overlooked in the monitoring toolset available to engineers.
One downside to SUM is that it requires setting up the behavioral scripts to capture the information.
Without carefully monitoring key metrics like uptime, network load, and resource usage, you’ll be blind to where to spend development efforts or refine operation practices.
In architecture with physical servers, information on hardware health—like CPU temperatures and component uptime—can also be helpful to avoid server failure.
Metrics and ExamplesDevelopment MetricsThe better the development process, the less the burden on the test and operation teams and the less test and production errors of the application.
Technical Debt: It refers to the time spent resolving the errors in the code.
Faulty units are usually found with static analysis tools, and estimations are given automatically by the tool.
Here, the total value of the time given to correct these issues refers to technical debt.
The higher the complexity value, the more difficult it is to read or maintain a code.
Each code block such as if, case, for increments the Cyclomatic Complexity value by one.
Applications with high duplication value are difficult to change.
Tools: Sonarqube, Cast...Code Review: We can check the written codes through static analysis, but in order to be limited in the rules and to minimize the possibility of overlook, another developer or team is required to review and comment.
Fast typing allows us to see steps that are overlooked or difficult to read instantly.
In addition, the more frequent feedback is given with the continuous and sufficient number of tests, the sooner the team becomes aware of possible errors and takes precautions.
The risk calculation can be given as "error probability x severity" or directly as "error cost".
This process is difficult to be manual and operated continuously.
Number of Test Runs: The purpose of the tests is to give feedback to the team about possible functionality and / or non-functional errors without your customers noticing.
The sooner the feedback is given, the faster the action is taken and the growth of the error is prevented.Tests run every two weeks are slow to give feedback.
So if there is more error than you expected, you may not have enough time to solve it.
Test Data Management: Running tests with fixed data is not enough to catch errors.
Leaving these processes to the initiative of the people may cause any step in the process to be skipped or disruption when the person with the initiative leaves the team.
This gives both continuous feedback to the business units and the choice to take early action on possible errors.
Deployment Time: Long deployment times result in low feedback.
Tools:New Relic, Dynatrace, AppDynamics, Riverbed, Datadog...Error Rate: It specifies the error rate on the server side of the application.
If this error is high, it indicates that the application is constantly getting errors on the server side.
Errors are expected to be minimized by addressing them with APM.
For example, you can examine ordering scenarios end-to-end and identify if there are any problems preventing the process from completing.
Rendering Time: Even though the application response time is of sufficient performance, the processing time of the responded page on the browser may be long or receive errors.
In this case, we may be missing that the user viewed the page late.
The most common problems are:  Large in size and non-optimized images Too many DOM iterationsCSS selectors and file sizes are longer than usualOver-library dependencyThe project is not designed to be mobile-friendlyLoops whose algorithm is not developed correctlyLegacy technologies that are not updatedTools: New Relic, Dynatrace, AppDynamics, Riverbed, Datadog.
Crash Rate: It indicates the rate of errors that will cause the application to close on mobile applications.
These errors are desired to be minimized.Tools: Firebase Crashlytics, Countly, New Relic.
It is calculated taking into account response times and errors.
(Good response time x 1.0) + (Acceptable response time x 0.5) + (Unacceptable Response time x 0.0) + Bad Transactions x 0.0 and their sum divided by the total number of transactions.Tools: New Relic, Dynatrace, AppDynamics, Riverbed, Datadog.
To the left of the dashboard is Technical Debt and Unit Test Coverage information from Sonarqube.
At the bottom, there is Average Response Time and Error Rate information from New Relic.
The purpose of the metrics monitoring is not to create work pressure on the team; on the contrary, it is to talk about the place where the team is and is expected to reach with numerical values.
Don't Give Up on Agile ProcessesSince transition to agile processes requires a cultural transformation, it will not be easy.
A developer who is having trouble reading his own code may have trouble reading another developer's code.
With these tools, you have the chance to see the experience of end users in your application, the errors they encounter and the response times of the system in real time.
Set Objectives and KPIsIt is very important to have objectives and KPIs that determine how we are and where we will go, with the motto "You can't improve if you can't measure".
There is no direct meaning of the maturity level here.
So fast forward to modern-day containerization, we have this company, Docker, that solves a lot of problems.
This means apps run the same no matter where they are and what machine they are running on.
Because we are running these virtual machines, which are basically isolated desktop environments inside of a file, the Hypervisor is what’s going to understand how to read that file.
In this, we have the infrastructure and OS, but no Hypervisor.
Containerization basically kills the, "It works on my machine but not theirs" drama.
to the last line in Dockerfile if possibleUse .dockerignore fileDon't install debugging tools to reduce image sizeAlways use resource limits with Docker/containersUse Swarm mode for small applicationDon't blindly trust downloads from the DockerHub!
See more at "DockerHub Breach Can Have a Long Reach"Make a Docker image with tuned kernel parametersUse alpine imagesDocker Is All About SpeedMatrix of hellSource: DockerContainers are the next once-in-a-decade shift in infrastructure that we all need to take part in.
That really is hell.
This problem has been fixed by Docker.
The On-Premises Edition is designed forcustomers with isolated infrastructure who require an internally deployedsolution, while leveraging the same real-time visibility to quicklytroubleshoot problems and reduce costly downtime.
Anyvariation, or “drift” in configuration across host groups is also immediatelyvisible to enable teams to fix minor issues before they escalate into downtimeevents, saving hours of detective work and remediation.Infrastructure search and AssertionsLive-state inventory data can be instantly searched to findvulnerable packages, to identify every version of an open-source package thatis deployed, or find hosts that are running a specific service.
Safari has been slow to provide performance data and need to respond more quickly or people will stop using the browser due to its poor performance.
JIRA for problem resolution tracking.
Stop wasting time, look at the right stuff and automate monitoring.
We want to understand what the user was doing when they experienced the problem.
One of the hard core differences between companies is the attitude of the people you show a graph to.
Listening to Web AttacksThere are numerous ways to monitor attack and anomaly activity with Signal Sciences.
Visualization of injection attacks.
One way to passively monitor for events is with sound.
If you can imagine, as you go about your busy day, hearing recognizable tones emitting from your computer that indicate your web site is under attack.
Perhaps among these tones are sounds representing blocked requests, which tell you the attack attempts are being blocked.
It calls the Signal Sciences API (time series endpoint) to retrieve attack and anomaly events over a period of time and will play a sound for each type of event.
Large enterprises want to move to the cloud without compromising security and compliance.
This is the point of these companies that focus on keeping AWS consumers secure from external and internal threats.
These companies struggle with hiring people with the right skills.
This missing group made sense, because they most likely exhibit in their own industries, such as in gaming conventions, instead of coming to a generalized cloud computing event.
They wouldn’t be able to completely focus on their core business, which could weaken their competitive position.But relying on business partners does introduce a measure of risk: if Amazon Web Services suffers an outage, it’s not just Amazon.com that will experience downtime.
Countless other businesses that run their sites on AWS would suffer an outage, too.Because of the inherent risk we assume when using their parties we need a way to work around and mitigate the impact a third-party’s failure will have on our business.
Sure, your partners may have a policy of notifying customers (you) of an outage, but this can occur well after an outage begins extending the amount of time your service is unavailable for your customersHaving redundancy with your third-party services and testing them against each other will help you limit the risk of one provider’s failure affecting your operations and customer experience.
Taking this principle to heart by designing around failure is the only logical response.
Knowing how your systems will function when one of your partner’s is down will help you proactively alleviate pain when it happens in the wild.
Failure testing will:Provide your engineering team with a holistic, top-down view of your operations.Put on display the dependencies in your system architecture (i.e., the third-party services you rely on to keep things up and running)Let you probe those dependencies in a structured, rigorous way.End-to-End Testing at PagerDutyAt PagerDuty, end-to-end testing is a pivotal piece of our reliability story.
If a provider is down, our customers may not receive an alert to know that there is a critical incident in their system.
To deploy the project, you only have to perform just one command and there is no need to install a lot of applications.
Based on Issue Tracking Tools Survey 2016 by JetBrains Youtrack is not so popular as Jira.
Sifting through the logs to find the roots of the problem can be a time-consuming and laborious task.
The ML algorithms trained on historical data are able to identify the “normal” behavior of the systems, operators and agents — and alert the DevOps specialists when some “abnormal” activity takes place, be it the beginning of a malicious DDoS attack, an attempt of unauthorized access or the loss of connection to some part of the system.
Prescriptive Analytics: The Way DevOps Systems Should EvolveAs you can see, all the stages of the evolution of the DevOps instruments were aimed at minimizing the time needed to investigate the origin of the problem and the human effort needed to rectify the situation.
For example, there are DevOps tools able to analyze the current state of the infrastructure on the go, watch for triggers (growing number of sessions, signs of upcoming server overload, etc.
The late majority of businesses is struggling with their transition to DevOps culture and performing their digital transformation.
Essentially, you want to be made aware of a problem before your users are.
The collection of monitoring data is essentially a solved problem: there's a plethora of tools (Datadog, Nagios, and Prometheus, just to name a few) that offer insight into how your systems and applications are performing.
Where it gets interesting (and wherein lies the challenge), is connecting that data with the systems and tools you rely upon.
And, modern infrastructures are increasing in velocity, and increased velocity further exacerbates the problem of connecting disparate data from the bounty of tools at our disposal.
A need to solve a problem introduced by decisions you had no influence over (the square peg), a requirement to solve the problem using some existing tool (the round hole).
You have a multitude of tools for monitoring your infrastructure, but no way for them to speak to each other.
We consume monitoring events (e.g., availability and performance data) and provide a simple set of building blocks (or core “primitives”) including event filters, event payload mutators, event handlers, and more.
These simple building blocks enable users to model workflows and automate them using Sensu.
Also, no one needs any more service alerts on infrastructure they intentionally spun down.
Applying filters to your monitoring ensures that dev servers crashing at 2am don't wake you up and that minor faults can wait until you have had your morning cup(s) of coffee.
Are you finally ready to migrate from OpenTSB to InfluxDB, but you fear the migration headaches and plugin rewrites?
It’s not only easier, but the possibilities for what you can monitor (and how) are virtually endless.
Docker Security Threats and Hardening GuidelinesWhat’s a Docker?
Stolen Sensitive Secrets: It's always a bad idea to store API keys and passwords of critical infrastructure unencrypted.
Run the application with a least privileged user.
Poisoned Docker Images: “Not all Docker images are malware free."
Obtaining Docker images from untrusted sources would eventually be malware infected or might have outdated versions of software which might have known-vulnerabilities.
Breaking out of a container: It is highly likely an attacker could break out of a container to gain access onto the host if the Docker is misconfigured with weak ACL, SETUID/SETGID binaries etc.
Always run a Docker with a least privileged user.
Here’s Why This Would WorkDefragging  SETUID/ SETGID binaries  -Setuid/setgid  binaries are the endpoints for the attackers to privilege escalate to root, i.e leading to a complete compromise of the host.
It is quite possible to over exhaust resources, hence the fellow Dockers would starve out, which would lead to denial of service.
Kernel Exploit: It is crucial to not use old/obsolete kernels on Dockers as they would induce the risk of using exploits that may lead to a kernel attack that would cause instability or privilege escalation, which would end up having the host compromised.
Some ways to mitigate the attack can be :Try to allow required kernel resources from the VM level or have some kind of memory limiter in place for not over exhausting resources.
Always run a Docker with a least privileged user.
A container is a self-contained (it’s right there in the name) execution environment with its own isolated network resources.
If an alteration leads to problems in a new image, then you can simply go back to the previous one.
Occasionally, containers might even be run within virtual machines, warping the space time continuum and confusing partisans of both approaches.
There are particular concerns about containers and security, as the varied images provide more points of entry for attackers and a container’s direct access to the OS kernel creates a larger surface area for attack than would be found in a hypervisor controlled virtual machine.
The need for multi-account AWS environment isn’t something that Amazon ignores.
When configurations of the environments don’t comply with the required policies, warnings are triggered, and you are informed immediately.
You always know the kind of information security and policy violations you are dealing with and you know exactly who to address to get the issues fixed.
That said, the AWS Control Tower still doesn’t stop there.
Further Reading AWS DevOps: Introduction to DevOps on AWS Top 3 Areas to Automate in AWS to Avoid Overpaying Cloud Costs
The precise numbers aren't clear, but it looks to be almost 75% of the pie-chart.
It sets up a scenario that should be very uncommon, field injection, and demonstrates how adversely this affects the goals of testing.
The number of nodes should be odd in order to support votes during downtime/network cut.
The minimal number should be 5, as a lower number (such as 3) will result in high stress on the machines during node failure (replication factor is 2 in this case, and each node will have to read 50% of the data and write 50% of data).
You can take this numbers down, but it will affect your backup and recovery strategy (see details about recovery from failure using hints).
-local: Run only on the local data center to avoid downtime of both in any case.
The higher your peaks, the more stress you should put on your system during off peak hours.
GC ConcurrentMarkSweep: 9 (warning), 15 Error.
Terraform does not support wait conditions.
When using an update policy, CloudFormation will perform a rolling update, including a rollback in case of a failure.
Terraform does not support rolling updates for Auto Scaling Groups out-of-the-box.
Handle Existing ResourcesBesides using input parameters, there’s no way to access existing resources with CloudFormation.
Personally, I prefer CloudFormation when the evaluation based on the differences of the tools does not produce a clear winner.
The Challenge of Finding ClickHouse Monitoring ToolsClickHouse itself provides no built-in monitoring solution other than exposing metrics in various database tables that you can query yourself.
It does generate metrics that you can collect but offers no feasible way to keep track of ClickHouse alerts or interpret them.
In addition, many of the major commercial monitoring, log analysis, and APM tools available today, such as Datadog and Splunk, lack ClickHouse integrations.
Still, you can't responsibly or effectively use ClickHouse without monitoring it.
Executing a query such as the one below can alert you to any problems within your system, such as locked tables, extended queue sizes, or inactive replicas.
SELECT    database,    table,    is_leader,    is_readonly,    is_session_expired,    future_parts,    parts_to_check,    columns_version,    queue_size,    inserts_in_queue,    merges_in_queue,    log_max_index,    log_pointer,    total_replicas,    active_replicasFROM system.replicasWHERE is_readonly OR is_session_expired OR (future_parts > 20) OR (parts_to_check > 10) OR (queue_size > 20) OR (inserts_in_queue > 10) OR ((log_max_index - log_pointer) > 10) OR (total_replicas < 2) OR (active_replicas < total_replicas)The above code shows a query to identify problems within your database.
An export server, such as this ClickHouse exporter, written in Go, can be used to query the system tables mentioned earlier in this article and expose the statistics through an HTTP, which can be ingested by your Prometheus instance.
Most of the monitoring tools that support ClickHouse at all lack official integrations with ClickHouse from their vendors, and in many cases, the number of metrics that they can collect is limited.
Yet, there is a lack of tooling around leveraging this data.
Such attributes enables technical metrics such as 90th percentile latency broken down by endpoint and discovering any outliers.
For example, the below chart shows unique API users broken down by marketing channel.
Monitor when a new account activates or has struggles and preempt any hard discussions.
Compliance RiskAnalytics systems can potentially track PII or sensitive information that is subject to regulation such as GDPR and CCPA.
Having the correct processes and workflows in place to expire and remove data is critical when storing such data.
In such cases, extra burden is placed on engineering and data science teams to generate reports.
With the rise of containers and ephemeral infrastructure, it is even more critical to have all the data available for the applications.
The monitoring as of today is usually done across three axes: metric monitoring, logs, and distributed tracingStop reverse engineering applications and start monitoring from the inside.
Tracing in CassandraApache Cassandra is fault tolerant and highly scalable database system (source).
Kube-advisor simple tool, from Azure team, that scans pods for missing resource and limits requests.
Drag and drop EventListener Node from the pallet to the canvas and open the EvenListener node settings and configure the following.
Drag and drop the Datamapper node from the palette to the canvas.
Drag and drop EventListener Node from the pallet to the canvas and open the EvenListener node settings and configure the following.
JSON    xxxxxxxxxx            1                       33                                              1             {             2                “OrderID”: “APQ-00001”,             3                “TxnID” : “0948939-AWHDH”,             4                “Item” : {             5                “Name” : “ABC Laptop”,             6                “Type” : “Mini-5”,             7                “Qty” : 1             8                },             9                “Address” :{             10                    “Unit” : “9”,             11                    “Street” : “TTK Street”,             12                    “Road” : “Tower Road”,             13                    “State” : “NSW”,             14                    “Country” : “Australia”,             15                    “PIN” : 94839             16                }             17               }After you invoke the mobile channel API and when you search in Splunk with the TxnID value as the search parameter.
Despite all the testing in CI, code still breaks because tests just can’t detect 100% of errors.
If CI could detect 100% of errors, the term hot fix would not exist.
DevOps teams must deal with logged exceptions, swallowed exceptions, and uncaught exceptions (defects) so they turn on the logs, error trackers, and APM tools.
Swallowed exceptions are caught and handled, but no data is emitted so the error is essentially invisible.
Let’s call a spade a spade here, not all developers are equal and sometimes a bad developer can make things worse and the logs, error trackers, and APM tools can’t magically make them better at coding.
Sentry, Rollbar, Raygun, Bugsnag, Airbrake, and others for error tracking.
Despite all of it, developers can struggle to find and fix errors.
We can get lost in the logs, APM tools, and error trackers due to a lack of context.
OverOps provides complete context to resolve every error and can even Slack the developer who wrote the code.
The cloud migration trend was accelerated by a significant drop in cloud platform pricing in 2013, led by Amazon’s AWS.
We don’t have any storage available and the EMC storage we ordered is still stuck in customs…” The frustration drove us to look for more agile alternatives and eventually led us to migrate to the cloud.
How much are you going to pay for Lambda or for BigQuery?
Basically, you have NO idea upfront: it depends on your usage.
The FinOps team is charged with evaluating the business need, usually based on extensive resource tagging.
The problem is even greater with Infrastructure-as-Code since developers actually write and maintain the infrastructure in their git repositories.
A simple “git push” can lead to a major cost degradation, but since developers don’t have the tools to take ownership of the process, they usually don’t take it into account.
To summarize, forget about old-school cloud cost management.
The cloud has moved from CapEX to OpEX, demanding new solutions.
Elastic Block Store: This service provides persistent block storage for ECS tasks (workloads running in containers).
For a vertically integrated stack, task definitions can specify one tier which exposes an HTTP endpoint.
This endpoint can in-turn be used by another tier, or exposed to the user.
Load-balanced services detect unhealthy pods and remove them.
Load BalancingPods are exposed through a service, which can be used as a load-balancer within the cluster.
CloudWatch alarms can be used to auto-scale ECS services up or down based on CPU, memory, and custom metrics.
Block StorageTwo storage APIs:The first provides abstractions for individual storage backends (e.g.
Modifying the storage resource used by the Docker daemon on a cluster node requires temporarily removing the node from the cluster.Kubernetes offers several types of persistent volumes with block or file support.
Disadvantages of KubernetesLack of single vendor control can complicate a prospective customer’s purchasing decision.
Netflix, for example, even takes it a step further by introducing failure into their testing process.
In The Seven Deadly Sins of DevOps, I wrote about how you should not do DevOps.
In fact, failure is something you should strive for.
This blog will give you a sense for how you can plan to fail strategically.
Embrace DevOps and Fail Fast (Fail to Be Great)How do you succeed by failing?
Simply put, it’s by building failure into the testing process.
Think of it as controlled failure whereby you think strategically about where the system is likely to fail and undergo stress.
At its core, DevOps is about shifting from a fear of failure to a desire to a fail-fast and move forward.
If zero failure is part of your corporate mantra, then your employees are afraid to innovate.
That is because, at its core, innovation is about failing and failing many times before you succeed.
I am not saying that your goal should be to fail.
Agile processes harness change for the customer’s competitive advantage.” By living up to this part of the Manifesto, you understand that some project and coding developments will fail either due to change in product specifications or due to a streamlining of the process.
Build Failure Into the ProcessNetflix has the Chaos Monkey as part of its Simian Army to introduce failure into the process when it is not expected so that engineers are trained to deal with things breaking and not working as planned.
Netflix believes that the best way to avoid major failures is to fail constantly.
Chaos Monkey introduces scheduled failure that allows simulated failures to occur at times when they can be closely monitored.
In this way, it’s possible to prepare for major unexpected errors or outages rather than just waiting for outages to occur and seeing how well the team manages.
That alone is a reason to shift left.
Rather than recommending a specific tool, I will instead say that it is important to pick a set of tools that causes the least amount of friction, confusion, and breakdowns in communication between project stakeholders.
Use the Cloud to Lower Your Cost of FailureSimilar to the way Linux brought open-source to developers everywhere, cloud infrastructure is enabling companies to lower the cost of development.
By using AWS EC2, teams only pay for the memory they need rather than purchasing large racks that remain idle.
One expert writes:A wonderful thing happens because suddenly, the fear of failure evaporates.
Just as with a public cloud, if a new application initiative fails, you simply shut it down — no harm, no foul.
The cost of failure is inexpensive, which fosters risk taking, which in turn creates a culture of acceptance of failed initiatives.
creates waste and the cloud cleans it up.
By moving further towards the cloud, DevOps furthers its mission of failing fast at a lower cost.
Introduce Alerting to Fail FasterAlso, be sure to include strong Application Performance Monitoring tools (APM) and feedback loops using tools such as Slack to communicate when tests fail.
When significant failures do occur (and they inevitably will), it is important to have OnPage integrated into monitoring systems such as DataDog or SolarWinds to ensure that critical notifications are delivered prominently and promptly to the on-call engineer’s smartphone.
In last week’s blog, we talked about how the lack of critical alerting allowed for AWS bills to grow in ways that engineers couldn’t explain.
What is unacceptableis that there were no alerts in place to notify the DevOps teams when thousands of dollars were being spent on API unnecessary calls.
Provide DocumentationKnow that when systems or code fails, there is documentation to fall back on to explain how the system is supposed to work.
No communal memory gets imprinted and this makes it difficult to keep focused on what has failed in the past.
Moreover, the team realizes that mistakes are inevitable and that they are something the team can learn from.
Technical Debt: Avoiding Technical BankruptcyThis is part two of my piece on technical debt.
The first explored what technical debt is and what it means for your product.
Now, it’s time to talk about how to avoid these issues and how to tackle that debt in a more mature product.
It looks to be fine from far away, but the technical debt beneath the surface could sink the product or even the organization.
Avoiding Technical BankruptcyHere are a few recommendations based on past experiences.
Modularize your system architecture and take a firm stance on technical debt in new components or libraries in the application.
Ingrain quality in your culture to avoid reckless and inadvertent technical debt.
Addressing Tech DebtThere are three common approaches that teams take to try to address technical debt.
A Technical Backlog Have a technical backlog of many Sprints full of technical debt, addressing those problem areas in the system.
Fixing Debt While Adding Value Each new feature you work on could get tech debt work included in the scope so your product matures as you add new features.
It’s hard to estimate how long it will take to deliver that new feature while including a rework.
Fix the Upfront PainThere is no exact formula to fixing tech debt so it is important to hear feedback from the team about what they think is going on.
Your engineers who interact with the system day to day will know where to start, so talk to them and find what the main points of contention and pain are.
Try to get the team to estimate the effort in fixing that pain and frontload the work.
The value delivered always needs has to be higher than the cost and effort involved to do that work in fixing the pain.
There is no point in that.
Low test coverage makes things hard to change — you break stuff without noticing.
Messy code is hard to maintain is slow and makes adding new features slow.
Never touch code without ensuring it has unit tests covering it.
This saves time and risk of human error.
You can automatically break the CI build if quality degrades which helps keep standards high.
No longer do deploying or changing part of the system involve changing everything else (because they isolated services).
If you have a problematic monolithic application, you could start rewriting or breaking out functionality into microservices.
Closing OffThat’s a wrap with technical debt.
Usually, if those agents are down or have a problem communicating with the backend, then the SAS provider will alert you about it.
Network AccessUsually, there will be restricted network access between a monitoring application server and its agents running in production environments.
During the DC-only era, the options to implement reliable meta-monitoring were limited.
Single Data CenterWhen all your applications are in one DC or colo and you are not using a SAS-based monitoring service, the options available to you are limited.
It also means that if you are using cloud-based monitoring services for both general monitoring and log aggregation and indexing, there is no meta-monitoring requirement.
You only need to make sure that the SAS provider can actually notify you of any failures on their backend or with their monitoring agents hosted in your environment.
Without much cloud support available for monitoring, it is important that your monitoring infrastructure and processes cover all the requirements of meta-monitoring to make the overall monitoring system highly reliable.
I'm a fan of Brendan Gregg's USE method for monitoring overall system health and identifying errors and bottlenecks.
To my chagrin, my hacked together monitor did not reveal any anomalies or resource constraints that were the cause of the timeouts.
That's when I noticed something weird; several times, one of the nodes will detect the other as being down for a period of time.
I had found my problem!
I'm able to have my new max heap size set to slightly above that recommendation because I switched my memtables to storing part of its data off-heap (offheap_buffers) which greatly reduced heap memory pressure.
The proliferation of new services, requirements, and devices in diverse geographic locations has made visibility into the entire network critical.
You need to be able to see where all of your data is residing to understand how performance is, or is not, being optimized.
However, it’s important to identify and focus on key business metrics, or else you run the risk of being overwhelmed with data.
While more tools are coming online, some providers are enabling disparate tools to provide an integrated view to the client, which results in greater visibility into the entire pipeline and faster time to problem resolution.
There continues to be a lack of knowledgeable professionals that know distributed computing and parallel processing.
Understanding the product, load, load tests, and performance graphs is critical.
In the future, performance and monitoring tools will automatically react to issues and know the difference between mitigating and fixing problems.
They’ll be able to do this by collecting more data and identifying a dynamic system to determine what the problem may be before it affects the customer.
Just as data is used to solve problems, it can also be used to change the way performance testing is done and measured.
The biggest concerns about performance and monitoring today are the lack of collaboration, identification of KPIs and how to measure them, and expertise.
Don’t assume the model you have in your mind is correct and know you’re going to get it wrong.
Once a problem is identified and remediation proposed, there is a need to test and validate that the change has completely fixed the problem.
Docker Swarm vs Kubernetes: Pros and ConsHave you ever found yourself stuck choosing between Docker vs. Kubernetes?
Docker and Kubernetes- both offer containers to deploy and isolate software within the same infrastructure.
But, does this make Kubernetes a bad choice?
However, it is not easy-to-use and is despised by many developers.
When it comes to installation and setup, it can give developers a hard time.
Another key reason why Kubernetes is not easy to setup and install is the planning that is required before you start implementing.
Not everything can be automated, and that makes Kubernetes hard to manage.
You can use any of the following ways to do so:Logging: Kibana (ELK) or ElasticsearchMonitoring: Grafana, Heapster, or InfluxFor Docker, there is no in-built library or process for monitoring or logging.
Docker offers good features, but limited by its API.
Docker’s market is relatively weaker compared to Kubernetes.
Kubernetes is hard to setup and configure.
It is growing slowly, but it is hard to speculate whether it will ever beat Kubernetes.
Both on-premises and public cloud infrastructure have their own difficulties, and it's important to take the Kubernetes architecture into account.
Big clusters put a higher burden on the master nodes, and they need to be sized appropriately.
It's recommended to run at least three nodes for etcd, which allows a single node failure.
Adding more nodes will protect against multiple node failures simultaneously (5 nodes/2 failures and 7 nodes/4 failures), but each node added can decrease Kubernetes' performance.
For master nodes, run two protects against failure of any one node.
For both the etcd cluster and Kubernetes master nodes, designing for availability across multiple physical locations (such as Availability Zones in AWS) protects the Kubernetes environment against physical and geographical failure scenarios.
While some instance types are explicitly a bad idea (for example, VMs with partial physical CPUs assigned or with CPU oversubscription), others might be too expensive.
Kubernetes allocates an IP block for pods, as well as a block for services.
Depending on the pod network type — overlay or routed — additional steps have to be taken to advertise these IP blocks to the network or publish services to the network.
Fluentd uses disk or memory for buffering and queuing to handle transmission failures or data overload and supports multiple configuration options to ensure a more resilient data pipeline.
It’s a big problem and finding the balance between making sure everything is captured and not overloading everybody that’s responding to these issues is a key DevOps transformational issue.
When launching new products or services monitoring is often the last thing to be considered and often overlooked.
If they’re not putting them at the right thresholds, or not tweaking them the right way it creates a lot of noise, and that’s a very common problem that can be avoided with the right planning architecture.
Giving them data on the health of parts of their application is meaningless to them.
We are often asked to look at conversion rates, page response time, cart abandonment, and things like that.
Using a Single ConsoleA common problem is that engineers have to look at multiple screens to measure infrastructure and other data.
If it doesn’t impact the users, they’re probably not going to care.
If we are not adding new alerts, we may miss something.
But if we are not making sure the alerts are cleaner and more relevant, then we are going to wind up in the alert fatigue situation.
Explaining Eventbridge Amidst the HypeA bridge over troubled services.
That means lower operational costs, infinite scalability, and the AWS guaranteed reliability.
Considering that EventBridge allows event-driven communication between integrated partners and AWS services and accounts, one of the major benefits of hooking everything up with EventBridge is that we no longer need to worry about setting up complex web-hooks.
Not setting up complex webhooks means reducing burdens such as security, network reliability, and coupling of interacting modules.
The branching logic that springs out of the rule-based routing means we no longer need to rely on a singular ingress point that comes with webhooks.
No more issues of reliability and uptime as these are delegated away to AWS.
Additionally, it does not ensure order.
Of course, there are some limitations to EvenBridge, such as the number of EventBridge connections per account being limited to 100 and only 750 invocations per second before we see throttling kick in.
ITOps: Benchmarks for 2018How Do You Measure Up When An Incident Strikes?
Unfortunately, we also saw that for all the Chaos Monkeys and strides towards improved response to alerting, there is still a significant lack of progress.
With automated alerting, teams can immediately receive notification of issues and quickly identify potentially severe issues before they magnify in scope.
But alerts are frequently ineffective.
As our survey showed, this lack of effectiveness is because teams are inundated with alerts and become desensitized to them.
According to our survey, over 2/3rds  of the respondents reported that critical incidents are sent to a team rather than a specific individual.
Over time, this pattern leads to engineers losing sensitivity to the notifications.
Typically, one expects that whether they respond to an email immediately or an hour later should present no difference.
According to our survey, over 80% of IT teams are alerted to critical incidents via email.
For critical incidents, email is less than ideal as it allows critical incidents to get buried under a pile of other emails.
Email provides no way for critical issues to rise to the top of the pile.
If over 40 alerts are sent to you and your team every day, it becomes very hard to prioritize alerts and determine which should be handled first.
While our survey did show that just shy of 59% receive a manageable number of alerts, 41% are inundated.
At the same time, the most frequent ways to escalate critical responses was through email or SMS.
Unfortunately, this is not the case.
The problem with this result is more than just a missed opportunity.
Yet by foregoing investments in these BI tools, teams are failing to investigate their processes and methods that would improve their team and minimize alert fatigue.
Additionally, alerts are actionable and come with instructions regarding what the problem might be.
Instead, we got increasing vendor lock-in and a handful of cloud players so large that if a data center experiences problems, significant sections of the internet go offline.
The past decade has taught us that relying on one provider is a bad idea, and we should use a mixture of public and private platforms and switch between them as required for operational or financial reasons.
What features it offers and how you use them is a little unclear and lurks behind a sign-up form.
If you are interested in Kubernetes but don't want install and manage it yourself, then look no further than my roundup of Kubernetes managed hosting options.
The Span is the primary building block of distributed tracing.
):Img.5.
As the tracing provides you the ability to get a view into the application communication layer and recognize potential issues, when the JFR is attached to the microservice JVMs, you can directly analyze the potentially suspicious code.
):Img.6.
Up until last year, the show was growing 100% in attendance year over year, so to see only 37.5% increase in attendance was a bit disappointing.
There is no way that containers will be rolled out at scale by IT Ops in large enterprises (especially for existing applications) if the only mechanism for moving applications to containers and managing changes to them over time is Dockerfiles and Docker Compose files.
I can just imagine the poor IT guys who will then have to manage all these apps that have been containerized in a non-uniform way getting beat down by this #MuscleMemory.
Many companies vying for a piece of this pie were touting containers for legacy applications, but when you asked how they converted existing applications to containers, the answer was a confused look and, “Well… you give us your Docker Compose file and...” Unfortunately, this is not a great answer for large enterprises with hundreds (even thousands) of existing applications in ongoing development, nor for smaller ISVs that don’t have skills in container technology like Docker.
When that failed, of course, I persisted and insisted that I wanted to see the demo promised on the sign.
He finally confessed that they had no such product or features and the marketing guys just came up with the sign to lure attendees in.
There were plenty of these folks around as well who are capitalizing on the current complexity and confusion around containers.
Some claim they are on the verge of a meteoric rise to dominate the data center.
The use of containers to deploy applications — particularly those employing a microservices architecture — is no different.
It makes it difficult to easily trace the data path from one end (the client) to the other (the app).
It’s very hard to imagine deploying any mission critical production workloads to any other platform than Linux.
The containers are and will be the new Linux for running any critical applications and workloads.
This will also enable all CSPs to scale their networks rapidly and in completely automated ways as the demand increases.
When something does go wrong, or not according to plan, tracking down the cause is trickier than with "traditional" applications.
It's a common problem in any architecture, but compounded with distributed systems with multiple components, instances, network speeds and competition for system resources.
What if elements of your cluster experience downtime, and data loss in transmission?
None of these log sources provide any method of reading or storing output beyond writing to standard output and error streams.
TracingTaking logging a step further, tracing allows you to follow the execution of an application component, helping you drill down into what went wrong and where.
Then if an application state is ever in doubt, and you need to debug what happened, you replay the events leading up to it to ascertain at what state the application should be.
We’ve known that distributed systems are hard.
From a theoretical perspective, the difficulty mostly arises from two key areas: consensus and partial failure.
Nearly all distributed systems research attempts to grapple with this problem in some way.
Given that distributed consensus is hard, how does this problem manifest in the context of microservices?
This approach both does and doesn’t solve the consensus problem.
Partial FailureConsider an HTTP request serviced by a monolith.
If there is a problem, be it a software bug or hardware failure, the entire monolith crashes – every failure is a total failure.
Now suppose one of those microservices fails.
Partial failure has been described as an unqualified good thing.
Furthermore, they’re CPU and memory intensive, making them painfully slow on a laptop.
Note, check out Kelda, and specifically our whitepaper for a detailed description of this problem.
make the problem tractable, but it’s still hard.
Microservices Make Sense, SometimesFrom a technical perspective, microservices are strictly more difficult than monoliths.
This means that teams can move quickly without waiting for the lowest common denominator to get their code QA’d and ready for release.
Still, these systems are, themselves, extremely complicated, proving my point: distributed consensus is hard.
A customer doesn’t care if a single request hits instance A or instance B, but they may stop using your service if a feature is confusing or no value created.
Where do users drop off in my adoption funnel?
via cohort retention analysisWhich users are having a bad customer experience or likely to churn?
Infrastructure metrics include:How is my error rate and memory utilization trending over time?
What happened in the last 5 minutes when a service failed?
A downside of time-based metrics is that it’s next to impossible to look at user behavioral trends by looking at multiple events together.
There is no concern with the user id being high-cardinality.
Of course, with user-centric data stores, a negative is that expiring data or moving specific time periods to beefier nodes is harder.
A second downside is building a pipeline to store data in a user-centric way is more complicated.
Fortunately, there are a variety of tools for each aspect of SRE: monitoring, SLOs and error budgeting, incident management, incident retrospectives, alerting, chaos engineering, and more.
Without logging latency, availability, and other reliability metrics throughout your system, you’ll have no way of knowing where to invest your development efforts.
Monitoring can be broken down into four main categories:Resource monitoring: reports on how servers are running with metrics such as RAM usage, CPU load, and remaining disk space.
Network monitoring: reports on incoming and outgoing traffic which can be broken down into the frequency and size of specific requests.
SLOs and Error BudgetingOnce monitoring is in place, there’s no better way to put that data to work than building SLOs and error budgets around them.
The inverse of the SLO is the error budget: the amount of room left on the SLO before exceeding the threshold.
As your SLO and error budget will be key decision-making tools in development decisions, find tools that can display changes over time.
Incident ManagementFailure is inevitable.
In an SRE mindset, incidents aren’t failures or setbacks, but unplanned investments in reliability.
This also informs SLOs, as follow-up actions can include increasing monitoring in certain trouble areas to get early warnings of future issues before they become customer-facing.
Chaos EngineeringChaos engineering is a disciple practiced for testing resilience.
Chaos engineering tools such as Gremlin and Chaos Monkey simulate outages, intense server loads, or other crises that could jeopardize reliability.
These experiments take place in small replica environments with no consequence to the live build of the service.
To be effective, a chaotic engineering tool will need to affect systems as if it was a real external threat.
Your chaos engineering tool should give you meaningful results across experiments.
Chaos engineering provides an opportunity for incident responders to build experience using and refining procedures.
When we broke out Stitch as a separate platform, we didn't want to use PHP – it doesn't have great support for datatypes and functions for data processing, and its performance isn't optimized for that use case.
It connects to Slack and PagerDuty to alert us in the event of a problem.
A Complete Guide To Develop A Cloud-Based ApplicationThe term cloud computing is all the rage at present.
You can also store apps’ data on the device offline.
Proper Market ResearchProper and thorough market research to understand your customers’ pains is a must to do a task.
In a few days, your app will either be approved or rejected.
In the past, we've asked to add some additional metrics to their system, but the system isn't that easy to configure with additional metrics.
Some of the features that appealed to us were:No reliance on distributed storage; single server nodes are autonomous.
IT departments have sold the transition to the cloud as a self-service haven where development teams no longer suffer from the multi-day-ticket hell of days past and are freed to move at their own pace.
When done properly, automation reduces a team’s workload, reduces errors (or rapidly creates them), and provides living documentation of a team’s workflow.
Since much of the drive towards automation comes from the migration to the cloud, a platform designed by and for operators, this is not surprising.
Fortunately, we have a solution to the current crisis brought on by the application of 2008 operations approaches to 2018 DevOps problems: development, the other half of our favorite portmanteau.
And lest the monorepo crusaders—you know who you are—look down upon the microservices practitioners and scoff at the troubles beset them by their dozens of repositories, let’s not forget that monorepos, whose builds and deployments are typically more complicated than microservice repositories, are no better served by untestable YAML and shell scripts.
The only difference is that when you make a mistake in an untestable monorepo build script, you break everyone’s build.
He will be missed.
Customer responsivenessLimited knowledge wasteFeedback loopExamples of Issue-Tracking Tools:Atlassian’s JiraJira is a proprietary issue-tracking product developed by Atlassian that allows bug tracking and Agile project management.
It focuses on query-based issue searching — with autocompleting, manipulating issues in batches, customizing the set of issue attributes, and creating custom workflows.
Asset controlLimit transportation wasteEmpower teamsExamples of SCM Tools:GitGit is a distributed version-control system for tracking changes in source code during software development.
JFrog offers high availability, replication, disaster recovery, scalability, and works with many on-prem and cloud-storage offerings.
Fast feedbackReduce defect waste and waiting wasteExamples of CI Tools:JenkinsJenkins is a free and open-source automation server.
With AWS CodePipeline, you only pay for what you use.
There are no upfront fees or long-term commitments.
It also performs monitoring, failure recovery, and software updates with zero-to-minimal downtime.
It has to handle failures by doing automatic failovers, and it needs to be able to scale containers when there’s too much data to process/compute for a single instance.
ZooKeeper helps Marathon to look up the address of the Mesos master — multiple instances are available to handle failure.
The Kubernetes scheduler’s task is to watch for pods having an empty PodSpec.
Fast recoveryResponsivenessTransparencyLimited human involvement during incidentsExamples of Monitoring/Logging Tools:ELK StackThe ELK Stack is a collection of three open-source products — Elasticsearch, Logstash, and Kibana.
The nodes expose these over the endpoints that the Prometheus server scrapes.
Reduce knowledge wasteIncrease new-hire productivityLimit repeat mistakesExamples of Knowledge-Sharing Tools:GitHub PagesGitHub Pages is a static site-hosting service that takes HTML, CSS, and JavaScript files straight from a repository on GitHub, optionally runs the files through a build process, and publishes a website.
In addition to most of the updates, there is normally no need to re-deploy or reconfigure Cloudprober due to the automatic aim discovery.
The Cloudprober Docker image size is low, containing only a statically compiled binary, and it requires a very small amount of CPU and RAM to run even a large number of probes.
LitmusCloud-Native Chaos EngineeringLitmus is a cloud-based chaos modeling toolkit.
Litmus provides tools to orchestrate chaos on Kubernetes to help SREs discover vulnerabilities in their deployments.
SREs use Litmus to conduct chaos tests first in the staging area and finally in development to discover glitches and vulnerabilities.
Features:Developers can run chaos tests during application development as an extension to unit testing or integration testing.
For CI pipeline builders: To run chaos as a pipeline stage to find bugs when the application is subjected to fail paths in a pipeline.
If specified criteria are violated, it will trigger notifications.
No dependency on distributed storage; single server nodes are autonomous.
Kube-monkeyKube-monkey is a Kubernetes cluster implementation of Netflix's Chaos Monkey.
PowerfulSealPowerfulSeal injects failure into Kubernetes clusters, helping you to recognize issues as quickly as possible.
It enables scenarios that portray complete chaos experiments to be created.
On the other hand, SREs recognize that some amount of failure is inevitable.
An error budget is an SRE concept that quantifies the amount of downtime your infrastructure can have before you are in breach of an SLO (service level objective).
This allows SREs to leverage risk for the benefit of the product rather than futilely attempting to eliminate risk and potentially becoming a bottleneckReducing ToilMuch of SRE concerns itself with removing toil.
This sometimes also includes automating those jobs that are repetitive and time-consuming.
Dealing With Failure: Understanding SLOs and Blameless PostmortemsSysAdmins are familiar with the RCA(Root Cause Analysis) process — when a failure occurs the root cause is identified, and a solution is put in place.
Good SRE practices insist on keeping people in the loop when a failure occurs, including your customers.
Bridging the gap between these various stakeholder perspectives may require conflict resolution skills.
Cloud, Containers, and Container Orchestration: Cloud and container services make something that was previously difficult to automate — physical hardware — manageable via standardised APIs.
This crossover brings dividends to the organisation as they find solutions to recurrent problems without investing in more manpower and hardware.
Continuous delivery enables applications to released quickly, reliably & frequently, with less risk.
Containers provide lightweight virtualization by dynamically dividing a single server into one or more isolated containers.
Here, we have grouped tools and solutions as per the problem they solve.
Culture: Adopting cloud-native practices needs a cultural change where teams no longer work in independent silos.
As you may have noticed in the above infographic there are several projects, tools, and companies trying to solve similar problems.
With such a litany of tools to choose from, there is no “right” answer to what tools you should adopt.
No single tool will cover all your needs and will be deployed across a variety of development and Operational teams, so let’s break down what you need to consider before choosing what tool might work for you.
DatadogDatadog lets you collect all the unused metadata that makes your programs slow such as slow database queries, thrown exceptions, unmanaged error logs and cache misses, and growing upstream services.
Apache CassandraCassandra is the go-to tool when it comes to critical data, with its proven fault tolerance and linear scalability, Cassandra ensures your database will always maintain a high level of scalability and availability.
All data encryption keys are stored in the local device storage to avoid unwanted external access.
Does Monitoring Still Suck?
which problems at which company sizes.
First, in some ways, monitoring still sucks, in ways we’ll explain in more detail below.
Second, the pain is actually going to get worse as more companies move to microservices.
There were a lot of different tools in the “Other” category, but no particular tool stood out.
Spammy AlertsIf there was one consistent complaint from all the companies we spoke to, it was about overenthusiastic alerting.
The problem is only getting worse as companies scale out on more servers or run microservices on continuously changingcloud environments.
However, most ofthe monitoring tools they used suffered from poor usability and dated UIs, so the collected data was siloed away for the eyes of the operations team alone.
Unfortunately, it’s clear that current monitoring tools have not been designed around this microservice- centric model, and most suffer from poor usabilityand adoption in teams outside operations.
Conclusion There are many more monitoring tools available in the four years after “#monitoringsucks” became a DevOps meme, but our research shows that many organizations are still struggling with monitoring.
While it’s not important who claimed the moral victory of coming closest, it’s important to remember that no one got paid (read: I lost).Why Does This Matter?The Cloud is all abuzz with immutable infrastructure, blue/green deployments, and treating servers like cattle instead of pets.
Replacing a database server is both scary and requires a significant amount of data streaming, which is lengthy and costly.
Sadly, this is the most traveled road.As Verizon’s 2015 Data Breach Investigations Report (DBIR) reminded us, the most commonly exploited vulnerabilities are the ones that were disclosed months or years before.
Also, the rate of exploitation of a known vulnerability scales much more rapidly as it gets included in common automated scanning tools.
However, do not rely on CVE ratings alone.
The DevOps culture spawned so many tools to make this a trivial exercise for any type and size of organization, so you no longer have excuses.How Frequently Do People Upgrade Software?Not frequently.
While this might be for packages internal to the organization, such as their custom software, it’s still unsafe and lazy behavior since it’s relatively simple to internally sign software packages.
Either way, this is a dream come true for a bad actor because they don’t even need to circumvent cryptographic signatures, they can just poison the repository or convince your server to talk to a malicious repository.Those on Debian based distributions were five times more likely to run apt-get dist-upgrade than apt-get upgrade.Red Hat based users were far more likely to upgrade their entire system blindly instead of specifying specific packages.
Their online versus offline status was measured on March 17th.The total number of registered and online agents continues to grow rapidly month over month (awesome job sales team!
), while the percentage of offline agents stays relatively flat if you exclude the incomplete month of March.Unfortunately when an agent goes offline we do not know the exact reason.
An average lifespan of about 30 days for these offline agents suggests a monthly refresh rate of infrastructure.
We can track this because Threat Stack notes when an agent is registered and the last time that it sent data.The two interesting trends these averages show are that agents are typically registered mid-month and are seen for the last time at the end of the month.
March looks strange because it is not a complete month of data, but March’s Online Average Life would suggest it’s trending as expected.It appears that the actual drop off of agents is relatively linear when you look at the average age of online agents filtered by a minimum age.For agents registered between Oct ‘15 and Mar ‘16.
March looks like a break out and that everything is going horribly wrong, except that we just learned that agents are more likely to churn at the end of the month.
That churn is likely replaced by new agents which would last longer, rapidly increasing the percentage of online agents registered in March going into April.While the spike in March is certainly interesting, it is largely due to an incomplete month’s worth of data.So, the good news is that environments are churning some their infrastructure, but unfortunately we cannot be sure why.
Tracking drift of AWS AMI’s is not reliable enough because it is not necessary when performing all types of upgrades.The bad news is that this analysis shows that there is a large population of critical or high risk systems (pets) that are not patched, left vulnerable for extended periods of time.P.S.
For example, an operator might run apt-get install vim=2:7.4.052-1ubuntu3 -y which is indistinguishable from a software installation, so it was not counted as an upgrade (if it were an emacs install we’d immediately know it was a bad actor).Dry runs and repository checks were filtered out.The percentage of organizations upgrading their software was calculated by the number of organizations observed performing upgrades, divided by the number of organizations that had agents running that distribution on that day regardless of their age.
From Wikipedia, "Docker uses the resource isolation features of the Linux kernel … to allow independent containers to run within a single Linux instance, avoiding the overhead of starting and maintaining virtual machines."
Management of all those microservices is where organizations will start to feel pain, and that pain is compounded when they have to integrate with traditional architectures and delivery pipelines.
Docker Compose mitigates a lot of pain, but some organizations require more.
In fact, they need tools that not only support microservices and legacy architectures, but also a strategy for transitioning at an appropriate pace.UrbanCode Deploy is the ultimate DevOps framework.
Note that deployment processes may fail if these properties are not defined.The two components in this application must also be linked using container links.
An additional caveat I noticed is the "fpm" versions of library/wordpress work a bit differently, so avoid those for now.
But there are also two often overlooked components that Cassandra provides: data variety and data complexity.
Can Handle Massive DatasetsIf there were any questions about whether or not Cassandra is capable of handling large data sets, there is no need to look any further than the companies using it.
Homogeneous EnvironmentUnlike some of the legacy distributed systems, Cassandra does not require outside support for synchronization.
Since Cassandra also operates in a peer-to-peer fashion, there is no master-slave or sharding setup and all nodes in the ring are equal.
Additionally, there is only one machine type that an administrator needs to worry about.
Highly Fault TolerantCassandra employs many mechanisms for fault tolerance.
Since Cassandra is masterless, there is no single point of failure.
Easy-to-Integrate Core ApplicationsA lot of work has been done on data manipulation and parsing systems to integrate with Cassandra.
This is made possible by Cassandra taking advantage of Java MBeans and exposing them to the client.
Given the sizable number of organizations and people who are a part of the ecosystem and the Cassandra community as a whole, there is no shortage of articles, documentation, and people willing to help.
not sure if this is a good or bad feeling.
i’ve never been comfortable only looking at a piece of a giant process, i always like to know how things are working together and what realities can be exposed in each aspect of a business.
also making use of the   weekly newsletter  i run didn’t hurt sign ups either.
another problem presented itself.
i decided to take the crazy step of talking to my user list.
what shocked me even more was the number of users who decided to subscribe afterwards.
so 2/3 isn’t that bad.
i need a cause, often i am left unfulfilled with how my time is being spent.
i am going to work on a   real  problem and dedicate my entire year to it, open ended i guess, but imagine what it could bring.
The content spanned different monitoring scopes such as metrics, distributed tracing, chaos engineering, governances/compliance, efficiency engineering, monitoring/observability tools, and more.
Chaos Engineering TrapsNoraJones | @nora_js | VideoChaos engineering is definitely not a bull-in-a-china-shop type of situation.
Its intention is not to ensue chaos without careful consideration of the potential consequences and remediation on the systems which you wreck havoc.
Since real chaos experiments happen in production, rather than sandboxes, it's important to always approach chaos engineering in a deliberate and cautious way.
Minimizing the blast radius, implementing safety, and understanding when things are not going well is difficult," says Nora on the art of creating a chaos plan.
Since there is no prescriptive formula for chaos experiments, it's imperative that the engineers architecting and conducting the experiments have a really good understanding of the system, if not experts within the discipline.
That's why Nora advises that there should always be an unbiased facilitator involved, and basically everyone else that might help fill the holes in the knowledge of the overall system/process (you're actually wasting time by not involving them!).
Pattern recognition can actually be detected when going through the process of automating chaos experiments.
Once these automated tests are conducted, vulnerabilities within the system are brought to the surface with as much contextual information as possible.
In conclusion, the goal of chaos engineering isn't to simply find vulnerabilities through tooling (Nora actually suggests that the number of vulnerabilities found is not a useful statistic).
Rather, the goal of chaos engineering is to push forward on a journey of resilience through the vulnerabilities that are found.
Of course, the family fixed the bug and lowered the threshold to account for changes in temperature, but that's not the extent of what they learned together.
He advocates that we should all find the "feather for your bear trap" and bridge the gap between people and what you do, because it will take you down rabbit holes and targets of learning that you weren't even expecting.
Using multiple, disconnected solutions however, can often lead to isolated environments.
Although service meshes provide features to extract metrics and to control traffic, their usage is limited when it comes to addressing larger, more complex and emergent behaviors that operators of microservices at scale face.
Criticisms of its comparatively large memory footprint subsequently led to the development of Conduit, a lightweight service mesh specifically for Kubernetes, written in Rust and Go.
Citadel’s features are important in a microservices architecture where “man-in-the-middle” attacks need to be defended against with encryption and in auditing what activities took place when, and by whom.
Metrics Storage: Prometheus is the component that stores all of the metrics exposed by Linkerd so it can be used to generate dashboards.
It requires no sidecars or agents and integrates cleanly into any existing environment.
We also examined how service mesh capabilities fail to address operator concerns.
Instead, we got increasing vendor lock-in, and a handful of cloud players so large that if a data center experiences problems, significant sections of the internet go offline, or thousands of applications and their data might be vulnerable to exploitation.
And by reducing your dependence on one supplier you can spread your risk if one provider fails to patch or is vulnerable for other reasons.
An application is only as secure as its weakest point, and adding more complexity introduces more potential vulnerabilities.
The more network connections you introduce, the more opportunities for lag, latency and "moving parts" that you need to debug in case of a problem.
The dashboard also provides information on the state of Kubernetes resources in your cluster, and any errors that may have occurred.
Because the performance of this infrastructure dictates your application performance, it's a critical area.
There's another important reason to study these metrics: they define the behavior of the infrastructure on which the applications run, and they can serve as an early warning sign of potential issues.
* BTW - If you don't want to go through the trouble setting up and managing Prometheus on your own, check out our Managed Prometheus solution for multi-tenant, out-of-the-box Prometheus monitoring with 99.9% SLA on any environment.
Kubernetes simplifies this by separating supply and demand.
This became alarmingly clear when the massive series of DDoS attacks hit Dyn in 2016, and much of the websites and applications on the Internet became inaccessible to huge swaths of users across North America and Europe.
The DNS lookup and resolution process should take milliseconds, but if something goes wrong along the way, your browser will lag, be unable to access the site, or worse, get hijacked and redirected to another (potentially malicious) site.
“All these changes can lead to “tremendous risk for the installed base whose businesses depend on those services,” says Brian Zeman, COO at NS1 in a recent blog on the Oracle announcement.
There are a host of problems that can go wrong within the multi-step DNS process.
This means you can’t detect issues fast enough (or at all), and also cannot properly troubleshoot the issue.
It allows you to look at the performance of all the nameservers involved and detect any errors along the entire DNS resolution chain.
It also gives you a window into the database records used by DNS servers (from MX to CName Records), which enables the diagnosis of the specific cause(s) of an error such as misconfiguration, DNS Cache Poisoning, or insecure zone transfers.
As we’ve recently been discussing, it is also critical to monitor your Service Level Agreements (SLAs), and not simply rely on the vendor’s assessment of their service, which might not bring the accountability you require as a buyer.
Furthermore, you can’t monitor DNS solely using inferred DNS metrics by monitoring HTTP URLs, as you are not truly monitoring the DNS service and the vendor infrastructure providing the service.
It allows you to trace queries through the complicated web of DNS hierarchy, pinpoint the issues, and solve any problem with your vendor, whoever they may be amidst these changing times.
DNS is the first interaction a customer has with your brand; it’s the most critical, and at the same time, the most fragile because of UDP.
Still, there is little doubt that the general usage trend is pointing upwards.
A fundamental problem for applications developed using a serverless architecture is maintaining the state of a function.
This presents a difficulty for developers, as functions cannot use other functions’ knowledge without having a third-party tool to collect and manage it.
As concurrent servers are no longer required, resource usage is significantly decreased, and the cost is calculated according to execution time.
Amazon charges for execution in 100 ms units.
But Microsoft hasn’t stopped there and is now attempting to address the needs of less technical users, involving them in the process by making serverless simpler and more approachable for non-coders.
Google is still missing some important integrations with storage and other cloud services that help with business-related triggers, but this isn’t the problematic part.
Google restricts projects to having less than 20 triggers.
On top of all the missing functionality and limited capabilities, Google charges the highest price for function triggering.
Interrupting the execution of an entire app to examine its instructions step-by-step is not ideal when running production code.
We could discover how one microservice was performing, but it would be hard to determine how it affected the other components.
Is tracing, therefore, limited to monolithic applications?
You can see which requests perform badly and where exactly they struggle.
Being able to identify the source of any problems quickly may be crucial to ensuring the system’s stability.
change is feared throughout most organizations of any type, so the adoption of new methodologies can quite challenging.
nowadays, businesses are expected to quickly deliver flawless applications that focus on user experience, but without the right tools, applications, and behavior, this seemingly simple task can turn into a complicated mess.
ultimately, faulty delivery translates into missed business opportunities.
paas offerings may require little to no client-side hosting expertise and include preconfigured features in simple frameworks.
examples of vendors and tools:   heroku  ,   google app engine  ,   aws elastic beanstalk    saas   : if you’re familiar with google and facebook, you’ve already been exposed to software as a service (saas).
if problems are brought to light in testing or qa, the software has to be recoded or go even further back in the development process.
testing is implemented early on and often so that developers can fix problems and make adjustments while they build, providing better control over their projects and reducing a lot of the risks associated with the waterfall methodology.
integration and delivery   continuous integration (ci)   – developers integrate code into a shared repository multiple times a day and each isolated change to the code is tested immediately in order to detect and prevent integration problems.
docker  containers linux containers are lightweight virtualization components that run isolated application workloads.
examples of vendors and tools:   github  ,   bitbucket  ,   jfrog  ,   artifactory    github   bug tracking a bug tracker is a system that aggregates and reports software bugs and defects.
examples of vendors and tools:   new relic  ,   appdynamics  ,   datadog   infrastructure monitoring –  tools in this category automatically detect and alert about degradations in underlying physical or virtual resource performance and availability.
Either the fix is so small that you can just do it in addition to your other work, or development is so painful that making changes has ground to a halt.
In this first post, we’ll dig into how Remy made this tough decision, and got buy-in from the rest of the company.
Once we added instrumentalization to our tools, we started to understand the scale of the problems.
Moving to the cloud is expensive but when we were able to put it side by side with the wasted engineering time, the decision was easy for us.
If this is unclear, I would start with that.
They will generate inconsistency and the developer experience will suffer over time.
In terms of which metrics to pick, there is no general recommendation.
It’s important to understand how developers work, understand how frequently they perform critical tasks, and instrument the tools that they use.
Seeing a drop would mean that the team might not be working on the most relevant projects.
If you experience some flaky tests or deployment errors in the pipeline, it would negatively impact the key results.
Over time, some OKRs will be exhausted, so consider renewing them over time.
For example, if your survey always has the same questions, developers will eventually stop responding.
Are there any warning signs people should look out for in order to know their developer productivity is suffering?
ReferencesSee Blimp commands and usage in the DocsRead 5 common Docker Compose mistakesRemy DeWolf’s MediumBy: Kevin Lin
But for those just getting started with OPA, this difference isn't always clear.
Use of OPA is not tied to Kubernetes alone; neither is Kubernetes is mandatory for using OPA.
All violations are stored in the status field of the failed Constraint.
But no request will be denied even if the Constraint fails.
Whereas, Gatekeeper currently does not support mutating admission control scenarios.
It took me a full day of adding logging and enabling debugging in production to find my mistake (that I forgot to push to the array).
Maybe you found an error and you want to know how many times it occurred over the last seven days.
If the error message is something like “System X failed because Z > Y” — where X, Y, and Z are all changing between each error message — then it will be difficult to classify those errors as the same.
To solve this, use a general message for the actual log message so you can search by the exact error wording.
For example: “This system failed because there are more users than there are slots available.” Within the context of the log message, you can attach all the variables specific to this current failure.
ContextRarely does a single log message paint the entire picture; including additional context with it will pay off.
Some examples are INFO for general system state and probably happy-path code, ERROR for exceptions and non-happy-path code, WARN for things that might cause errors later or are approaching a limit, DEBUG for everything else.
Only in rare cases or demo projects will your services be completely isolated from other services.
This way, when there is an error four of five levels deep in your system, you can trace that request back to when the user first clicked the button.
If you see an error in InfluxDB, then you can use the transaction ID to find any related messages in Elasticsearch.
You may also be able to detect errors or slowness in your downstream systems by using throughput monitoring.
Especially when you’ve got a bunch of developers all pushing to a single code base, it is difficult to realize when you’ve impacted the response times of another endpoint.
It is accessible for both Mac and Windows OSs with no additional configuration or setup.
Cloud computing enables some of the largest companies in the world, such as Airbnb, Netflix, and Uber, to reinvent and dominate their industries.
Understanding and choosing the correct cloud-native technologies are critical for increasing development velocity and spending less time and money developing and maintaining tooling and infrastructure.
Additionally, cloud-native tools rely heavily on abstractions, making them more generic and allowing teams to run their services without agreeing on a shared runtime across the company.
Resources always adapt to the current demand, which saves money over traditional, statically scaled resources.
Companies that leverage the full suite of tools can often deliver faster, with less friction, and lower development and maintenance costs.
Engineering teams can store container images in a container registry, which in most cases, also provides vulnerability analysis and fine-grained access control.
Manual configuration, however, makes it hard to keep track of changes.
SecretsSecret management is essential for cloud-native solutions but is often neglected at smaller scales.
Ultimately, organizations that ignore secret management could increase the surface area for credential leakage.
CertificatesSecure communication over TLS is not only best practice but a must-have.
What is the result of the operations (success, failure, or status codes)?
Without alerts, you don't get notified of incidents, which in the worst case means that companies don't know that there have been problems.
When performance issues arise, teams can see what service errors are occurring and how long each phase of the transaction is taking.
If you think you can prevent failure, then you aren't developing your ability to respond.
“If you are not prepared to be wrong, you’ll never come up with anything original.” — Ken RobinsonCloud-Native: What It Is and How It All StartedAt a high level, cloud-native architecture implies adapting to the many new possibilities empowering innovation that paves the way for digital transformation.
Released in 2014, Kubernetes stands out among many container orchestration systems and winning the container orchestration war.
In 2001, Amazon had a problem: the large, monolithic “big ball of mud” that ran their website, a system called Obidos, was unable to scale.
The CEO, Jeff Bezos directed this problem into an opportunity.
In between, as the team at Amazon grew, they faced severe communication problems, and hence Bezos instructed the team leads to have two-pizza teams.
To avoid the communication overhead that can kill productivity as we scale software development, Amazon leveraged one of the most important laws of software development – Conway’s Law.
Engineering Culture at Google, The Cloud-Native WayAt Google, we see that writing code correctly is taken very seriously, reviewing code is taken very seriously, and testing code is taken very seriously.
Fully exploiting the power of cloud computing that offers on-demand, limitless computing power, whether in the private or public cloud.
What Problem Are We Solving?
We were still a small team, and we didn't want to dedicate a ton of time to DevOps-y, infrastructure problems yet.
Lambda let us compartmentalize our code into a series of microservices (a service for logging, a service for OAuth key renewals, a service for SMS/email alerting if integrations error out, etc.
Why Lambda Didn't Work for UsWe had a number of issues, ranging from speed to SQS size limits and lack of process isolation in Lambda, that caused us to reconsider its effectiveness as our integration runner.
Process Isolation.
Concurrent invocations are isolated (they run in distinct containers within Lambda).
That's especially a problem if you let customers write their own code, like we do for our customers' integrations.
It turns out that if one customer writes some bad code into their integration - something like this, global.XMLHttpRequest = null;, then subsequent integration runs on that same Lambda that depend on the XMLHttpRequest library error out.
We toyed with forcing our Lambdas to cold-start every time (which was a terrible idea for obvious reasons), and we tried spinning up distinct Node processes within chroot jails.
Neither option panned out - spinning up child Node processes in a Lambda took 3-5 seconds and partially defeated the purpose of being in Lambda in the first place.
chroot and isolate Node processes from one another, solving the process isolation issues we were seeing in Lambda.
Now, ECS hasn't been perfect - there are some trade-offs.
An alerting service that sends SMS or email notifications to users if their integrations error.
Docker SwarmSwarm is Docker's answer to a developer's problem of how to orchestrate and schedule containers across many servers.
Link: https://aws.amazon.com/ecs/Cost: Amazon ECS comes at no additional cost.
Pay only for the AWS resources (e.g.
Link: https://azure.microsoft.com/en-us/services/container-service/Cost: Pay only for the virtual machines, and associated storage and networking resources used.
It can also act as a container orchestration tool to provide fault recovery for containerized workloads.
Marathon automatically handles hardware or software failures and ensures that an application is "always on."
BuddyBuild, test, and deploy apps in no time.
The platform requires no installation, configuration or server maintenance and it integrates seamlessly with BitBucket, Heroku, GitHub, and others to automate code building, testing, and deployment using Docker containers.
Machine learning analytics also means the quick discovery and future prediction of threats and anomalies before they can become an issue and affect end-users.
New RelicAn industry leader, New Relic is a pure SaaS-based performance management solution which allows developers to diagnose and fix application performance problems in real-time.
The tool generates and collects container metrics such as network statistics, resource isolation parameters, and a complete history of resource usage.
ClairClair is an open source project designed to identify and analyze vulnerabilities in Docker and appc application containers.
Clair regularly ingests container vulnerability metadata from a customized and configured group of sources in order to identify threats in container images, including those upstream.
Link: https://www.aquasec.com/Cost: Pricing is a combination of selected software plan charges plus Azure infrastructure costs for the necessary virtual machines36.
Unfortunately, supply has not kept up with demand and there seems to be a shortage of engineers focused on the ingestion and management of data at scale.
Part of the problem is the lack of education focused on this growing field.Currently, there seems to be no official curriculum or certification to become a Data Engineer or Data Architect (that we know of).
Because it’s basic supply and demand and data engineers and data architects are cleaning up!
While the salaries for data architects average around $112k nationally, the main path to this strategic, coveted position (and salary) means cutting your teeth as a data engineer and working your way up or making a lateral job move.What does it take to start getting a piece of this action?
Don’t worry: we did research on that, too!
How to Choose the Right DevOps ToolsLet’s be straight: no tool in the world will magically make you DevOps (or agile, or lean).
Although many tools touch all phases of the development cycle in one way or another, no single tool we know of plays the primary role in each phase.
And don’t forget about integrations.
Something weird about the class path?
So why not extend that thinking to IT infrastructure?
This can be difficult to apply to systems because they are always changing.
Because branch-and-merge workflows are all the rage (and deservedly so!
), tools that take the pain out of running CI in a multi-branch environment are the key to maintaining testing rigor without sacrificing dev speed.
Risk is a fact of life in software, but you can’t mitigate what you can’t anticipate.
DeployRelease DashboardsOne of the most stressful parts of shipping software is getting all the change, test, and deployment information for an upcoming release into one place.
Find something that gives you full visibility on branches, builds, pull requests, and deployment warnings in one place.
Automated DeploymentThere’s no magic recipe for deploy automation that will work for every application and IT environment.
Create utility methods or scripts to avoid duplicated code.
Provisioning tools like Puppet and Chef reduce the pain in standardizing environments.
Look for a chat tool that is extensible and integrates with monitoring tools so you never miss an important service degradation alert.The best-loved companies extend communication outside their own four walls.
Incident, Change, and Problem TrackingThe key to unlocking collaboration between teams is making sure they’re viewing the same work.
Are they linked and traceable to software problems?
Look for tools that keep incidents, changes, problems, and software projects on one platform so you can identify and fix problems faster.
Highly elastic microservice and serverless architectures mean containers spin up on demand and scale to zero when that demand goes away.
This shift has exposed deficiencies in some of the tools and practices we used in the world of servers-as-pets.
Many of the clients we work with at Real Kinetic  are trying to navigate their way through this transformation and struggle to figure out where to begin with these solutions.
This means many of the tools that were well-suited before might not be adequate now.
Similarly, it's much more difficult to correlate the behavior of a single service to the user's experience  since partial failure becomes more of an everyday thing.
This model also leads to ineffective feedback loops if engineers are not on-call and responsible for the operation of their services-something I've talked about ad nauseum.
In this environment, it is no longer practical to SSH into a machine to debug a problem or tail a log file.
It also probably means we are running an agent for many of these services on each host, and if any of these services are unavailable or behind, our application either blocks or we lose critical observability data.
If these are tightly integrated, this can be difficult to do.
We no longer have to figure out data to send from containers, VMs, and infrastructure, to send it, and to send it.
Anyone who's shipped production code has been in the situation where they're frantically trying to regex logs to pull out the information they need to debug a problem.
It's even worse when we're debugging a request going through a series of microservices with haphazard logging.
In order to monitor systems, debug problems, make decisions, or automate processes, we need data.
What we want to avoid is the sort of murder-mystery debugging  that often happens.
A lone error log is the equivalent of finding a body.
We know a crime occurred, but how do we piece together the clues to tell the right story?
The data we can't get for free should go on the context, typically data that is request-specific.
You can take this as far as you'd like-highly structured with a type system and rigid specification-but at a minimum, get logs into a standard format with property tags.
If these aren't aligned, pain-driven development  creates problems.
The downside of moving away from agent-based data collection is we now have to handle routing that data ourselves.
With the amount of data and the number of tools modern systems demand these days, the observability pipeline becomes just as essential to the operations of a service as the CI/CD pipeline.
You can easily recover from a disaster and move your environment to another place—from on-prem to the cloud or from one cloud provider to another.
You avoid vendor lock-in.
You can also reduce risk by repetition.
Gone are the days when you need to reboot a server because your app is having problems.
Now, if you’re having issues with your app, you just kill the bad container and create another fresh one in seconds.
But what if something’s wrong with the new release?
If you’re practicing continuous integration, you’ll stop everything, make the correction, and deploy again.
In the meantime, just use the previous container version image and deploy again to avoid any interruptions.
I don’t blame you—I bet you’re thinking of that Windows service you’ve been running forever.
Or maybe you’re thinking of that old app that you don't even dare to look at, let alone touch.
You don’t have to worry about more infrastructure.
Instead of adding big servers, you can have small servers and add just the blocks you need, little by little.
But you won’t be able to use all resources, leading to waste.
No.
If you’re using vSphere, for example, you can rest assured that if something goes wrong with the host, that VM will be moved quickly to another one without any downtime.
What if the problem is inside the application because it’s using so many resources?
In vSphere, you do have the ability to auto-scale, but it’s not something that comes built in, sadly.
This doesn’t mean that you won’t need to worry anymore about security in your applications (for example, in SQL injections).
It means that if for some reason, the attacker is able to gain access to the container, you can limit the damage that can be done.
If for some reason an attacker gains access to the container, the attack surface is limited.
But now, you don’t have to worry about that.
Next time you see that something bad is happening to a container, you can just kill it and create another one.
So if you thought that containers were the new kids on the block, think twice.
It’s very difficult to choose the right path in the middle of so many tools and practices.
If you didn’t, don’t worry you can take a look at below courses to learn your choice of language, though I strongly suggest you to learn at least one of these three major general purpose programming language.
You might be thinking that there is so many stuff to learn, so many courses to join, but you don’t need to worry.
The lack of visibility is a critical problem that is worsening as the instance lifespan reduces, components span private, and public clouds and external service dependency increase.
Figure 1a is a grouping of multiple instances and its “resolution” is low.
Golden signals such as latency, error rates, throughput and saturation metrics should be captured and displayed for nodes and edges.
Operations teams can then define the Service-level Objectives (SLOs) for the critical services and set alerts/paging for them.
This can greatly reduce the well-known problem of alert fatigue.
Application maps also highlight potential single points of failures and congestion hotspots.
Practically, sampling is enabled in production and heavy recording rules are often avoided unless root cause analysis is being performed.
This can be tricky when calls are made to legacy services or OSS components.
Also tricky when different languages are used, e.g.
End to end trace is often misleading as it does not capture the load on services (i.e.
Though with modern microservice patterns this is less of a problem as fewer API endpoints exist on services compared to monolithic applications.
Hard to track specific business transactions end to end without automatic trace ID correlation.
Worse, an exhaustive market analysis might never finish, but due diligence is critical given the average lifespan of integration code.
While Java has messaging standards like JMS, it’s not helpful for non-Java applications that need distributed messaging, which is severely limiting to any integration scenario, microservice or monolithic.
Nearly the opposite of RabbitMQ, Kafka employs a dumb broker and uses smart consumers to read its buffer.
Requirements and Use CasesMany developers begin exploring messaging when they realize they have to connect lots of things together, and other integration patterns such as shared databases are not feasible or too dangerous.
One of those use cases it describes is messaging, which can generate some confusion.
Kafka is a durable message store and clients can get a “replay” of the event stream on demand, as opposed to more traditional message brokers where once a message has been delivered, it is removed from the queue.
For example, a 3-node Kafka cluster the system is functional even after two failures.
However, if you want to support as many failures in Zookeeper you need an additional 5 Zookeeper nodes as Zookeeper is a quorum based system and can only tolerate N/2+1 failures.
Of course, message per second rates are tricky to state and quantify since they depend on so much including your environment and hardware, the nature of your workload, which delivery guarantees are used (e.g.
persistent is costly, mirroring even more so), etc.
The queue is backed by a single Erlang lightweight thread that gets cooperatively scheduled on a pool of native OS threads - so it becomes a natural choke point or bottleneck as a single queue is never going to do more work than it can get CPU cycles to work in.
New automation products and services now dominate the marketplace.
Both men focus on maximizing the flow of resources (inventory) through the systems while reducing defects.
Below are some familiar pain points of the traditional model for deploying software.
During application outages, it is often difficult to identify who needs to be involved or where to find the expertsMultiyear development projects that does not deliver incremental valuePerception of slow IT that serves as justification to go around ITInformation security is seen as a roadblockInfighting between teams – with an "us versus them" mentalityResources cannot focus on new opportunities because they are tied up in multi-year projectsTeams are terrified to make changesWaiting for new environments to be provisioned (environment, testing, etc.)
Teams work long hours moving code between environments, testing new features and codes, developing tests, fixing defects, etc.
While code is waiting to be shipped, revenue is lostDevOps ModelWith DevOps, we can apply those same manufacturing principles to deploying and managing software.
DevOps builds lean manufacturing principles such as: focusing on customer value attention on time, eliminating waste, shared learning, reducing cycle time, avoiding batching, finding bottlenecks, and identifying and elevating constraints.
PlanningI cannot overstress the importance of transparency in DevOps.
CI helps teams test more frequently to discover and address defects (bugs) earlier before they become larger problems.
Continuous DeliveryWith the traditional model, there is a high probability of errors during the deployment process.
Installation steps may not be accurate and executed in multiple environments and many things can go wrong because the process is manual.
It is typically a very stressful process, which leads to the slow delivery of new functionality to users.
The release pipeline may contain steps similar to manual steps such as ssh to a server, copying files, stopping and starting services and deploy to specific environments (test, dev, prod, etc.).
MeasurementIn DevOps, you must continually try to optimize the software delivery processes and as such, it is critical you collect performance, process, and people metrics as often as you can – if you cannot measure, you cannot improve.
The following table indicates the key challenges:CategoryDescriptionTechnologyFull-scale environment is not available for performance testingProcessLimited time available for environment setup, load test script and data setup to execute full-scale load testProcessUnstable builds: Since development and testing activities go simultaneously break-off time is needed within a Sprint.
It is possible to detect anomalies, draw patterns from sprint level test results, and find defects using ML techniques.
Continuous Performance Testing StrategyIt has been observed that 70-80% of performance defects actually do not require a full-scale performance testing in a performance lab.
These defects can be found using a signal user or small scale load test in the lower environment if a good APM tool is used to profile and record execution statistics at each stage of SDLC.
The target of this shift left approach is to find and fix all code related performance defects as development is in process.
These should be lower of insolation or component level test.
Performance analysis, data trend, abnormally detection, and cutting down performance analysis by more than 50% if we use machine learning algorithm.
In Agile methodology, since full-scale performance test is not done all the time in a performance lab, production performance testing is highly critical to make sure the system can handle user traffic without any issues.
The percentage of the user set is increased gradually if there are no issuesIf there are issues found, either we do not increase the load further or new features will be pulled back.
Stop the test if real user experience degrades below the predefined threshold.
Need for detailed triage meeting in WAR rooms is minimized by tracing the sessions, pinpointing exact issuesConsistent metrics to analyze performance issues across the environment and it is accessible to development, testing teams, business users all the time.
The following diagram depicts how APM tools help in every Agile lifecycle stage:Performance Risk AssessmentUser story performance risk assessment not only helps identify key user stories which should be performance tested but also helps decide which user stories can be given lower priority.
The following table indicates the parameters that decide the probability and impact of failure.
The higher the risk factor, the higher the priority:Probability/ImpactParameterUser Story # 1 User Story # 2 Factor#1Degree of change from the previous sprint11Factor#2Stringent Performance SLA54Factor#3No.
of Users impacted43Factor#4Public Visibility and impact on the brand43Weighted ImpactImpact of Failure42Risk FactorRisk Factor208Right SkillsetIn Agile manifesto, performance engineers need to be a good developers with data science tools expertise to apply ML and automate tasks, analyze, and derive useful insights from data.
Having one code-base makes the application difficult to manage and scale.
This software being updated is no longer a monolithic application, because parts of it can be updated piece by piece.
Time does not just increase when a release goes to production.
Microservices normally do not share databases.
Thus, the barrier to adopting new technologies, frameworks, or languages is lowered.
There is no language or technology lock-in.
If one service fails, then its failure does not have a cascading effect.
Inter-service communication can be very costly if it is not implemented correctly.
This can create problems (e.g.
Our vision for 2.0 is to collapse the TICK Stack into one cohesive and consistent whole, which combines the time series database, the UI and dashboarding tool, and the background processing and monitoring agent behind a single API.
In order, it represents four components for solving problems with time series data: Telegraf, our data collector, InfluxDB, our time series database, Chronograf our UI for visualization, and Kapacitor, our service for processing and monitoring.
Said another way: many data problems are actually time series data problems and we’re going to help people solve those.
Over that time we’ve learned more about how people are using the platform and what types of problems they frequently encounter.
We’ve frequently heard users mention that TICKscript was difficult to learn and difficult to debug, but for the users that made the journey, they frequently think of it as a killer app.
Most of these features represent lower level API work.
We plan to cut weekly versioned alpha builds starting in the first week of February.
This will set us up for years of continued innovation with Flux as a core building block.
Thomas and his mentor had made a mistake in their work, demonstrating that even an experienced staff member was unable to memorize everything.
What is a Low Context Culture?
In a low context culture:Communication is explicit; you are told the rules, knowledge tends to be codified, public, external, and accessible.
Examples of a low context culture include airports and sports with established rules.
The Need for Low Context DevOps According to Thomas, the DevOps environment should strive to be low context "you should spend more time working and less time frustrated with roadblocks and information gaps."
If you are changing projects all the time, you might be faced with this problem regularly, and dynamic companies should involve moving from projects.
Ticket systemBug tracking systemMonitoring/observabilityCI/CD pipeline systemContainer/artifact repositoryDocumentation repositoryUbiquitous documentation - Documenting as you work means you'll have documentation when you need it like when you're fixing an error as part of 3 am pager duty.
Documentation is easy to do with a deep link/URL can include:   In error messagesIn CI/CD control panel restrictionsIn alert messagesThomas advises the mindset of "my code is the documentation" and suggests instead that people need to "record tech debt or it won't be fixed."
For those who hate documentation, he suggests templates that do much of the work and that teams include documentation updates in work estimates - "don't think of documentation as something extra but part of the project itself."
He also notes that there's no need to reinvent the wheel, and you can find where engineers already write and repurpose - such as email, chatrooms, IM, Stack Overflow, and repurpose these into your work.
"The number one thing to avoid is dogma - don't look at how another org has implemented things as the only way to do it."
"No one can sell you DevOps, it's a journey and a process with no end, and you should embrace that."
Matthew explains that the company was experiencing a range of problems as the team rapidly expanded that led to their realignments, such as pain points in inter-team communication, monolith software, and a single code repository.
Each of these elements serves as crucial building blocks towards the coveted end-to-end pipeline.
However, we’re still missing a crucial piece of the puzzle.
This mistake is compounded when working with a team of data scientists to create a product.
CI/CD With BuddyTo patch this missing puzzle piece, we introduce the concept of Continuous Integration / Continuous Deployment — abbreviated as CI/CD.
You might be wondering — how is this going to stop my development pipeline from breaking?
Let’s examine each element in turn:Rigorous Testing: After installing the relevant environment, you can run unit-testing through a full suite of tools, including Cypress, Split Testing, and tests written natively in Python, PHP, Django, and more.
Just by pushing to the master-branch or using a recurrent trigger, the pipeline will automatically start, run tests and update your cloud instance if there are no errors.
If there are any errors in integrating changes, you can be notified and go straight in to fix the issue.
We can either solider on (not recommended for obvious reasons), or stop the pipeline where it broke and send a notification that something went wrong, or try running different commands.
Identifying where something has broken is perhaps the most frustrating part of fixing a broken process.
Proactively setting error-handling behavior and notifications as a priority will help keep frustrations at a minimum going forward, when some element inevitably breaks.
If you are following along with the tutorial to this stage and faced errors, check that the Python version is exactly3.7, because that is required for this particular app’s dependencies.
Adding Testing FunctionalityTesting is a central element in software development, but it is unfortunately not prioritized or taught in most data science curriculums.
In this case, error handling setup becomes particularly important, as Buddy can share notifications if some tests fail.
Adding Notification FunctionalityAdding in notifications is critical to ensuring we know where breaks in the pipeline occur, or which tests have failed.
From the pipeline overview, click on the “Actions Run On Failure” section, where we can decide what actions will run if there is an error anywhere in the pipeline.
For our purposes, it will be sufficient to set this up using environmental variables that will indicate which execution or test broke the pipeline execution.
$BUDDY_PIPELINE_NAME gives us the name of the pipeline that is broken$BUDDY_EXECUTION_ID gives us the unique identifier of the instance of the pipeline that created an error, including the$BUDDY_FAILED_ACTION_LOGS will give an extensive overview of the logs of what went wrong, which is convenient because it helps in diagnosing any issues that pop up.
With the use of additional environmental variables, you can make this output incredibly specific to allow you to hone in on the error.
This will not happen if the EC2 instance is stopped or interrupted.
In the event of any errors or snags along with the execution, we’ll receive an email highlighting exactly what went wrong.
Limited file size upload.
ConsYou gotta pay more to get more functionality.
Inability to upload spreadsheets on dashboards.
ConsA bit harder to push unresolved merge conflicts.
Be careful with merge operations; bad merge can be painful to reverse.
Hard to migrate repositories to other services.
Lack of authorization rule as separate users or groups cannot be assigned to separate views or jobs.
ConsIt can be difficult to configure.
No analytics on the end-to-end deployment cycle.
ConsLower storage limit.
No calendar integration.
No Gantt charts to visualize the project in landscape mode.
ConsLimited functions in the mobile app.
Email notifications may get confusing.
Inability to merge calendars.
ConsAutomated tests and manual test cases are difficult to keep in sync.
FeaturesIt provides error tracking that completes feedback loops to detect errors as soon as new code is deployed.
ConsLimited capabilities for filtering slack notifications.
Front-end error tracking can get noisy as it also detects errors provoked by browser extensions, bots, and non-supported browsers.
User Review“A fast and integrated solution to keep user generated errors under control.”FigmaFigma is a cloud-based design tool with real-time collaboration for digital projects.
Drag and drop accessible libraries.
ConsNo Timestamp feature to track iterations.
ConsNot a free application, but it has a free trial.
Leave Management.
ConsDifficult to navigate through the payroll information.
User Review“One stop for all the company management stuff.”LocoLocalise.biz (aka Loco) is an online translation management service.
The platform security is not that great and needs some work.
It helps in the early detection of bugs that might cause much more damage to the system later.
ConsThe number of requests is limited in the enterprise version.
It is a difficult task to move a folder from one collection to another.
Finds critical code issues before they become roadblocks.
Debugging requires knowledge of C.User Review“I’m using Phalcon for almost all my business projects, because of the low time needed to load the data.”DatadogDatadog is a monitoring platform for cloud applications.
ConsLimited reports and analytics.
FeaturesIt can be used as a VPN Client, AV client, and host of vulnerability scanners.
Integrated firewall, VPN, and vulnerability detection.
Identify and remediate vulnerable or compromised hosts across your attack surface.
ConsIt works on Windows and Mac, but there’s no Linux version.
No alerts show up when issues are detected.
No automatic update feature.
ConsNo option to run code directly.
Lags a bit when connected to the server.
Licensing is costly.
Might lag at times.
Automatic error reporting.
Error debugging.
ConsThe Project Explorer is not that good.
No theme customization.
No print option.
FeaturesEasy to use with drag and drop.
ProsThousands of drag and drop programming functions for rapid application development.
Have to pay to use the most wanted features.
User Review“Tool for the quick integration & automation of technology solutions through low code creation.”Cloud9Cloud9 is a cloud-based software dev tool.
ConsLimited GUI customizations.
FeaturesBuild a user interface with drag and drop.
Costly when compared to other alternatives.
It’s a bit costly, even for smaller projects.
User Review“A powerful platform with tons of options but a confusing interface and unclear pricing.”Amazon Web Services(AWS)AWS is a cloud-computing platform that provides infrastructure as a service (IaaS), platform as a service (PaaS), and packaged software as a service (SaaS) offerings.
ConsDue to the large number of services offered, it can be confusing for beginners.
ConsNot that good for large projects.
Essential DevOps Skills: Part IAnyone involved in hiring DevOps engineers will realize that it is hard to find prospective candidates who have all the skills listed in the job description.
The DevOps jobs seekers have similar uncertainties bothering them as they wonder what skills they have to highlight or get trained on before approaching a prospective employer.
With many tools and processes being linked to the DevOps practice and the fact that any technology could be related to DevOps somehow, it becomes very confusing to differentiate the essential skills from the desired or the unrelated.
I don’t think that there is any single recipe to sort out this problem.
ComponentsThe application is based on Akka 2.4.11 and Scala 2.11.8 and has these components:REST endpoint exposed through Akka HTTP.
Circuit breaker to insulate the application from failures on Redis and the third party SOAP endpoint.
However, when it comes to tuning the application, it is necessary to move to a lower level of abstraction in order to analyze the different threads that run the application.
The following table shows the thread pools found after taking a thread dump of the application:The application is configured to use a router of actors whose number can be changed in between tests.
The role of the blocking block is to ensure that parallelism level is maintained despite the blocking operation.
That is why the call to the third party inside a blocking block is so critical.
On the other hand, Redis driver uses its own thread pool, so there is no risk of blocking threads shared with other operations.
All blocking calls must be delegated to some other thread pool.
It will be configured to generate responses with 1 second delay (http://wiremock.org/docs/simulating-faults/).
Redis runs on a Docker container on localhost and its latency is of the order of a few milliseconds (what makes the third party’s latency the dominant factor when it comes to considering blocking operations)ToolsThis section describes the different tools used to run and monitor the tests.
#!/bin/bashextension=tdumpif [ $# -eq 0 ]; then    echo >&2 "Usage: jstackSeries <pid> [ <suffix> [ <count> [ <delay> ] ] ]"    echo >&2 "    Defaults: suffix = "dump", count = 10, delay = 60 (seconds)"    exit 1fipid=$1          # requiredsuffix=${2:-dump} # defaults to "dump"count=${3:-10}  # defaults to 10 timesdelay=${4:-60} # defaults to 60 secondsecho $pid $suffix $count $delaywhile [ $count -gt 0 ]do    jstack -l $pid >jstack.$suffix.$pid.$(date +%H%M%S).$extension    sleep $delay    let count--    echo -n "."
donedispatcher=akka.actor.default-dispatcherrediscala=rediscala.rediscala-client-worker-dispatcherglobal=ForkJoinPool-2-workercommon=ForkJoinPool.commonPool-workerapache=default-workqueuestatsd=StatsD-poollog=logbackecho "" > ./tmp.txtfor f in *$suffix*.tdumpdoecho "===========> $f" >> ./tmp.txtecho "$dispatcher: $(grep "$dispatcher" $f | wc -l)" >> ./tmp.txtecho "$rediscala: $(grep "$rediscala" $f | wc -l)" >> ./tmp.txt#scala.concurrent.ExecutionContext.Implicits.globalecho "$global: $(grep "$global" $f | wc -l)" >> ./tmp.txt#java.util.concurrent.ForkJoinPool.commonecho "$common: $(grep "$common" $f | wc -l)" >> ./tmp.txtecho "$apache: $(grep "$apache" $f | wc -l)" >> ./tmp.txtecho "$statsd: $(grep "$statsd" $f | wc -l)" >> ./tmp.txtecho "$log: $(grep "$log" $f | wc -l)" >> ./tmp.txtdoneecho "===========  FILES ===========" > result.$suffix.txtgrep ".$extension" ./tmp.txt >> result.$suffix.txtecho "===========  START ===========" >> result.$suffix.txtgrep "$dispatcher" ./tmp.txt >> result.$suffix.txtecho "=======================" >> result.$suffix.txtgrep "$rediscala" ./tmp.txt >> result.$suffix.txtecho "=======================" >> result.$suffix.txtgrep "$global" ./tmp.txt >> result.$suffix.txtecho "=======================" >> result.$suffix.txtgrep "$common" ./tmp.txt >> result.$suffix.txtecho "=======================" >> result.$suffix.txtgrep "$apache" ./tmp.txt >> result.$suffix.txtecho "=======================" >> result.$suffix.txtgrep "$statsd" ./tmp.txt >> result.$suffix.txtecho "=======================" >> result.$suffix.txtgrep "$log" ./tmp.txt >> result.$suffix.txtecho "===========  END ===========" >> result.$suffix.txtrm ./tmp.txtThread Dump AnalyzerIn order to examine the content of the thread dumps in detail, a tool like this comes in handy.
As discussed above, this is down to the use of the statement blocking  to enclose the blocking call to the SOAP endpoint.
However, when changing to 40 concurrent clients, the application hits the limit of 25 threads imposed by the camel connector thread pool.
Although the HTTP and TCP load balancers have feature parity, the available directives and parameters differ somewhat because of inherent differences between the protocols; for details, see the documentation about the Upstream modules for HTTP and TCP.You enable load balancing with two configuration blocks, which we’ll show in their basic form, without optional parameters or any auxiliary features:     The server block defines a virtual server that listens for traffic with the characteristics you define, and proxies it to a named group of upstream servers.
This block is the same in all our examples.
The upstream block names an upstream group and lists the servers that belong to it, identified by hostname, IP address, or UNIX-domain socket path.
The upstream block is where you specify the load-balancing technique, so we’ll be highlighting that in the sections that follow.
As an example, here’s the block for the default method, Round Robin:    upstream backend {    server web1;    server web2;    server web3;} Round RobinRound Robin is the default load-balancing technique for both NGINX Plus and NGINX.
In case of reboot or reconnection, the client’s address often changes to a different one in the /24 network range, but the connection still represents the same client, so there’s no reason to change the mapping to the server.If, however, the majority of the traffic to your site is coming from clients on the same /24 network, IP Hash doesn’t make sense because it maps all of them to the same server.
If all servers aren't equally loaded, traffic is not being distributed efficiently.
Try adjusting the weights, because their absence might be causing the imbalance rather than a problem with the load-balancing technique.
Errors and failed requests – You need to make sure that the number of failed requests and other errors is not larger than is usual for your site, otherwise you’re testing error conditions instead of realistic traffic.
The hashing algorithm evenly divides the set of all possible hash values into “buckets,” one per server in the upstream group, but there’s no way to predict whether the requests that actually occur will have hashes that are evenly distributed.
The web1 server ends up receiving more than twice as many requests as the other servers combined.So it makes sense to use Hash or IP Hash when the benefit of maintaining sessions outweighs the possibly bad effects of unbalanced load.
If you direct all requests for the file to the same server, only the first client experiences a long delay because of the database calls.
If, for example, you have servers in different data centers for purposes of disaster recovery, Least Time tends to send more requests to the local servers because they respond faster.
Recall that Least Connections and Least Time send each request to the server with the lowest “score” (number of connections, or a mathematical combination of connections and time, respectively).
When you assign weights, the load balancer divides each server’s score by its weight, and again sends the request to the server with the lowest value.
The goal of Distributed Tracing is to pinpoint failures and causes of poor performance across various system boundaries.
To solve this problem, OpenTracing and OpenCensus projects were started.
Concepts and TerminologySpan — Span is the building block of a trace.
If no         context is found, then a new span is started.
Acmeshop ArchitectureLet's say the end-user experiences poor performance.
This can then be used to go to those services and start looking at their logs and other stats to figure out the actual problem with the application.
This is only for local testing    var options = {      uri: endpoints.usersUrl + "/users/" + req.params.id,      method: 'GET',      json: true,      headers: req.headers    };    // Leverages request library    request(options, function(error, response, body) {        if (error) {          return next(error);        }        if (response.statusCode == 200) {            console.log('printing from within request')            res.writeHead(200)            res.write(JSON.stringify(body))     // Section 3            userSpan.setTag(opentracing.Tags.HTTP_STATUS_CODE, response.statusCode)            userSpan.log({              'event': 'request_end'            });            userSpan.finish();            res.end();        }        else {            res.status(response.statusCode);            res.write(JSON.stringify(response.statusMessage))  // Section 4            userSpan.setTag(opentracing.Tags.HTTP_STATUS_CODE, response.statusCode)            userSpan.setTag(opentracing.Tags.ERROR, true);            userSpan.log({              'event': 'error',              'message': response.statusMessage.toString()            });            userSpan.log({              'event': 'request_end'            })            userSpan.finish();            res.end();        }    }); // end of request}); // end of methodIn the first code block, you can notice that we start a span.
In the second code block, we are making an HTTP request to the user service.
In the third and fourth code blocks, there were tags and log fields added.
You may also add tags to indicate an ERROR by using userSpan.setTag(opentracing.Tags.ERROR, true);Messages can also be logged along with the span by using    userSpan.log({      'event': 'error',      'message': response.statusMessage.toString()    });Augmenting additional data along with the span can increase the network and storage costs.
func initJaeger(service string) (opentracing.Tracer, io.Closer) {// Uncomment the lines below only if sending traces directly to the collector// tracerIP := GetEnv("TRACER_HOST", "localhost")// tracerPort := GetEnv("TRACER_PORT", "14268")agentIP := GetEnv("JAEGER_AGENT_HOST", "localhost")agentPort := GetEnv("JAEGER_AGENT_PORT", "6831")logger.Infof("Sending Traces to %s %s", agentIP, agentPort)cfg := &jaegercfg.Configuration{Sampler: &jaegercfg.SamplerConfig{Type:  "const",Param: 1,},Reporter: &jaegercfg.ReporterConfig{LogSpans:          true,LocalAgentHostPort: agentIP + ":" + agentPort,// Uncomment the lines below only if sending traces directly to the collector//CollectorEndpoint: "http://" + tracerIP + ":" + tracerPort + "/api/traces",},}tracer, closer, err := cfg.New(service, config.Logger(jaeger.StdLogger))if err != nil {panic(fmt.Sprintf("ERROR: cannot init Jaeger: %v\n", err))}return tracer, closer}// Set tracer to Global tracerfunc main() {  // Start Tracer  tracer, closer := initJaeger("user")stdopentracing.SetGlobalTracer(tracer)  // Start the serverhandleRequest()  // Stop Tracerdefer closer.Close()}Extract the Context and Start Span in User Service (Go)Once the request is received by the service, it is re-directed to GetUser Function, as shown below.
func GetUser(c *gin.Context) {var user UserResponsetracer := stdopentracing.GlobalTracer()userSpanCtx, _ := tracer.Extract(stdopentracing.HTTPHeaders, stdopentracing.HTTPHeadersCarrier(c.Request.Header))userSpan := tracer.StartSpan("db_get_user", stdopentracing.ChildOf(userSpanCtx))defer userSpan.Finish()userID := c.Param("id")userSpan.LogFields(tracelog.String("event", "string-format"),tracelog.String("user.id", userID),)if bson.IsObjectIdHex(userID) {error := collection.FindId(bson.ObjectIdHex(userID)).One(&user)if error != nil {message := "User " + error.Error()userSpan.LogFields(tracelog.String("event", "error"),tracelog.String("message", error.Error()),)userSpan.SetTag("http.status_code", http.StatusNotFound)c.JSON(http.StatusNotFound, gin.H{"status": http.StatusNotFound, "message": message})return}} else {message := "Incorrect Format for UserID"userSpan.LogFields(tracelog.String("event", "error"),tracelog.String("message", message),)userSpan.SetTag("http.status_code", http.StatusBadRequest)c.JSON(http.StatusBadRequest, gin.H{"status": http.StatusBadRequest, "message": message})return}userSpan.SetTag("http.status_code", http.StatusOK)c.JSON(http.StatusOK, gin.H{"status": http.StatusOK, "data": user})}Here, the request is extracted by usinguserSpanCtx, _ := tracer.Extract(stdopentracing.HTTPHeaders, stdopentracing.HTTPHeadersCarrier(c.Request.Header))This provides the traceID from the parent.
NOTE: In the end, if you use Opentracing with Jaeger agents (as we have described in the article above), you can pick essentially ANY tracing UI you need (open source or commercial)Tracing Is DifficultIt's difficult to get tracing correct, especially when adding it to existing applications.
well,   if  you want to find out why a user encountered a problem, why an error occurred, and why an application crashed.
pdoexception thrown with message "sqlstate[hy000]: general error: 1 no such table: tblmovies".
we’ll start off, in no particular order, with raygun.
raygun’s offering is broken down into two primary sections: crash reporting and real user monitoring.
crash reporting aims to help find, understand, and fix the exceptions which cause applications to crash.
for every exception that is sent to raygun, it is broken down and organized so that it shows, among other things:  when it occurred   the page that it happened on   the exception’s message   the country of origin  the exceptions can be filtered based on a range of criteria, including browser, operating system, host, and machine name.
pricing raygun’s pricing is broken down based on what your subscription includes.
you can access either the entire platform or crash reporting or real user monitoring.
plans start as low as $19 per month and range as high as $1,499 per month.
looking more closely, airbrake replaces an application’s default error handler with an open source error and exception notifier.
it catches the standard types of errors, including 404s, and 500s, login problems, and other application-specific errors.
the dashboard data includes such information as:  the number of times an exception has occurred   the environment in place at the time the error occurred   a full backtrace   parameters in place at that moment   how many times an exception occurred since the last deploy   the exceptions that a deploy fixed  exceptions can be filtered by an environment, prioritized by error count, environment, and users.
these start as low as $49 a month and go up to $249 a month.
looking at the core functionality, exceptions that are sent to sentry can be interrogated based on a range of criteria including:  the environment   browser   operating system   the route that triggered the exception   when the exception occurred   the software release   exception severity   server name   when it was last seen   number of active users affected by the exception   error type (http error, 500, and 404)   the number of times the exception has occurred   whether it’s resolved or not  sentry also supports a concept   called breadcrumbs  .
level:    the level may be any of error, warning, info, or debug.
given that, you can pay nothing and host it yourself, or you can outsource it to sentry.
what’s particularly handy about rollbar is that you can trace problems often back to the commit which introduced them, as it has full integration with   github  ,   bitbucket  , and   gitlab  .
A Crash Course on Serverless APIs with Express and MongoDBI don't think that's a crash course.
TL;DRYou can severely hurt my feelings and jump to the section you’re interested in, or just keep reading.
Fire up a terminal in there.
Shocker!
Lastly, we also have the serverless-offline plugin for offline testing.
However, the real power lies in the Serverless HTTP module.
Lastly, we need Serverless Offline for testing our app locally.
You should also think about adding try-catch blocks around the await operators.
$ sls offline start --skipCacheInvalidationNote: In the root directory of your project run sls and you should see a list of commands.
If you configured it correctly, sls offline and sls offline start should be available.
// package.json{  "name": "a-crash-course-on-serverless-apis-with-express-and-mongodb",  "version": "1.0.0",  "description": "",  "main": "app.js",  "scripts": {    "offline": "sls offline start --skipCacheInvalidation"    // right here!
},  "keywords": [],  "author": "Adnan Rahić",  "license": "ISC",  "dependencies": {    "body-parser": "^1.18.3",    "express": "^4.16.3",    "helmet": "^3.12.1",    "mongoose": "^5.1.7",    "serverless-http": "^1.5.5"  },  "devDependencies": {    "serverless-offline": "^3.20.2"  }}Once added, you can run the command npm run offline instead.
$ npm run offlineYou’ll see the terminal tell you a local server has started on port 3000.
That won’t cut it.
MonitoringThe issue of bad overview and not enough insight into what’s happening in your app is a real problem with serverless architectures.
Using serverless architectures can be scary.
If you missed any of the steps above, here’s the repository with all the code.
The tribal warfare needs to stop.
Vendor lock-in is another key point you need to think about, even though I'd argue it's not that big of an issue.
Containers are isolated, stateless environments.
Sadly, this introduces more cost.
After all, you're paying for the resources all the time, no matter whether you have traffic or not.
Not everything is that bad, though.
Last of all, with containers your team will have the same development environment no matter which operating system they're using.
Mainly because you can, with little to no fuss, refactor existing monolithic applications to container-based setups.
That's not entirely true.
Meaning, you use various services to create business logic without caring about any servers.
The application is even completely shut down if there's no traffic at all.
You only pay for the resources you use.
No usage, no costs.
There are no operating system updates to install, no security patches, no worries, because the provider handles it for you.
Because your application shuts down entirely if there's no traffic, it can be incredibly cheap.
But everything is not that great.
You will have to live with defined limits for processing power and memory, pushing you to write more efficient code because of the risk to overload your functions, if they grow too large.
This can also cause the dreaded nightmare called latency.
Regarding latency, FaaS solutions suffer from what are called cold-starts.
If this is a problem, you should reconsider using FaaS.
No Dockerfiles or Kubernetes configurations.
No need to have a server run all the time for a background task that runs every once in a while.
The magic of the Serverless Framework lies in the automated process of creating resources and deploying code all in one step.
All of them explain the benefits of both technologies and why the petty squabbles between the container and serverless tribes make no sense whatsoever.
It takes away the common problems associated with vendor and platform-specific architecture and implementation.
Loose Coupling of Directories: SAML doesn't require user information to be maintained and synchronized between directories.
The identity provider bears this burden.
Issuer: The name (identity) of the service providerInResponseTo: The ID of the SAML request that this response belongs toRecipient: The name (identity) of the service providerAside: SAML Authentication With Auth0With Auth0, SAML authentication is dead simple to implement.
You can ignore the rest of the fields for now.
(the angle bracket and slash icon)Locate the box with the "SAML2 WEB APP" label and click on the circle toggle to turn it green.
In this GraphConnect presentation, Dippy Aggarwal walks you through the basics of Docker containerization and why Docker alone isn’t enough to handle today’s challenges of scale and multi-cluster deployments.
Containers allow you to avoid “runs on my machine” issues and allows you to package your machine as a standardized unit.
You package your application with all its dependencies, application codes and run-time environments and ship it as a container — which offers the benefits of portability and isolation.
When you start talking about large numbers of containers, which can scale up into the millions, you encounter challenges that come with distributed systems such as scaling, replication, fault tolerance and container communication.
While this avoids fragmentation, you will lose a lot of information if that node fails where you have deployed containers.
By default, Docker Swarm follows the spread strategy so it will just launch your container on the node which has the least number of containers.
But all the containers in a single pod will live and die together.
The idea behind service is the concept that customers don’t want to worry beyond a single access location; they just want one endpoint they can access.
The terms are given in no alphabetical order.
BBastion host — a special server used to access private networks and withstand hacker attacks.
Back-end — the program engine that the user has no direct access to.
Cloud computing — a dominating IT paradigm of accessing over the Internet the networks of virtual servers for collecting, processing and storing data, running apps and managing other resources.
Configuration drift — the undesirable result of updating various servers independently, leading to different software configurations and states.
Unfortunately, it cannot boast the same functionality as Kubernetes and is literally out of use as of mid-2018.
It works under the serverless computing billing scheme — you specify what needs to be done and pay for the resources consumed, without any manual cluster configuration.
MTTR — Mean Time To Recovery — the average expected time for a failed system component to become operational again; the main parameter of failure recovery scenarios, system stress-testing and performance checks.
Final Thoughts on 100 DevOps Terms From IT SvitThe list of DevOps terms above is in no way complete or comprehensive.
We always tend not to violate any SLAs, violating any part of the SLA can have many consequences.
If a service fails to meet the terms defined in an SLA, it risks brand reputation damage and revenue losses.
Worst of all, a company may lose a customer to a competitor due to its inability to meet a customer’s service-level requirements.
Defect rates: Despite good efforts in system development, no system is 100% perfect.
We should counts or percentages of errors in major flows.
Production failures such as server error, database query error, connection error, network errors and missed deadlines can be included in this category.
Choosing a set of metrics could be non-trivial in the beginning, also we can be in the dilemma of how much we should monitor.
We’ll use the application.properties file to handle enabling, disabling and customization.
We need to also use Spring boot actuator as this is going to expose the Prometheus endpoint.
dependencies {  implementation 'org.springframework.boot:spring-boot-starter-data-jpa'  implementation 'org.springframework.boot:spring-boot-starter-web'  compileOnly 'org.projectlombok:lombok'  annotationProcessor 'org.projectlombok:lombok'  providedRuntime 'org.springframework.boot:spring-boot-starter-tomcat'  implementation 'io.micrometer:micrometer-registry-prometheus'  implementation 'org.springframework.boot:spring-boot-starter-actuator'  // https://mvnrepository.com/artifact/com.h2database/h2  compile group: 'com.h2database', name: 'h2', version: '1.4.200'  testImplementation('org.springframework.boot:spring-boot-starter-test') {  exclude group: 'org.junit.vintage', module: 'junit-vintage-engine'  }}We can enable Prometheus export by adding the following line to the properties file.
Even though we have added this line in properties, we can’t browse Prometheus endpoint, since this is disabled by default, we can expose that using management endpoint, include Prometheus in the list.
A sample application.properties can have these lines# Enable prometheus exportermanagement.metrics.export.prometheus.enabled=true# Enable prometheus end pointmanagement.endpoints.web.exposure.include=prometheus# enable percentile-based histogram for http requestsmanagement.metrics.distribution.percentiles-histogram.http.server.requests=true# http SLA histogram bucketsmanagement.metrics.distribution.sla.http.server.requests=100ms,150ms,250ms,500ms,1s# enable JVM metricsmanagement.metrics.enable.jvm=trueNow, if we run the application and browse to the page http://locahost:8080/actuator/prometheus, this will display hell lot of data.
The above data displays HTTP request detail, exception=None means no exception occurred if any then we can use that to filter how many requests have failed due to that exception, method=GET HTTP method name.
—   WikipediaProfiling is very helpfull in diagnostic system problems like how much time an HTTP call takes, even if it takes N seconds, then where all this time has been spent, what’s the distribution of N seconds among different database queries, downstream service calls, etc.
Profiler code in every file and likely in every function that requires profiling would increase the complexity and can become a complete mess; though we can avoid this mess using Aspect-Oriented Programming (AOP).
Timed annotation is useless without TimedAspect bean, as we’re redefining the Timed annotation so we’ll also define TimedAspect class as per need like logging, mass profiling (profile all methods in a package without adding any annotation on any method or class), retry, etc.
@Target({ElementType.ANNOTATION_TYPE, ElementType.TYPE, ElementType.METHOD})@Retention(RetentionPolicy.RUNTIME)public @interface MonitoringTimed {  /** All fields are same as in {@link io.micrometer.core.annotation.Timed} */  String value() default "";  String[] extraTags() default {};  boolean longTask() default false;  double[] percentiles() default {};  boolean histogram() default false;  String description() default "";  // NEW fields starts here  boolean loggingEnabled() default false;}This annotation is not useful without the timed aspect class, so a new class MonitoringTimedAspect will be defined with all required details, this class would have a method to profile any method based on the processing joint object and MonitoringTimed object and another one to profile method based on the MonitoringTimed annotation.
method_timed_seconds_count{class="com.gitbub.sonus21.monitoring.service.StockManager",exception="none",method="addItems",} 4.0method_timed_seconds_sum{class="com.gitbub.sonus21.monitoring.service.StockManager",exception="none",method="addItems",} 0.005457965method_timed_seconds_max{class="com.gitbub.sonus21.monitoring.service.StockManager",exception="none",method="addItems",} 0.00615316MonitoringTimed can be further customized, like the number of retries can be added to support retry in the case of failure, log function arguments in case of failure so that it can be analyzed later why it’s failed.
Even in the worst case where our collection network is down, the SDKs will queue locally and not disrupt the application.
A VM with extremely low memory usage can be either be downsized or have additional services allocated to that VM to consume additional memory.
When looking at memory usage, you should also look at the number of page faults and I/O ops.
Usually, your end to end RPM will be much lower than an advertised RPM, which serves more as an upper bound for a simple “Hello World” API.
Your RPM may vary depending on the day of the week or even hour per days especially if your API is geared for other businesses that exhibit lower usage during nights and weekends.
Because problematic slow endpoints may be hidden when looking only at aggregate latency, it’s critical to look at breakdowns of latency by route, by geography, and other fields to segment upon.
For example, you may have a POST /checkout endpoint that’s slowly been increasing in latency over time which could be due to an ever-increasing SQL table size that’s not correctly indexed; however, due to a low volume of calls to POST /checkout, this issue is masked by your GET /items endpoint which is called for more than the checkout endpoint.
6: Errors Per MinuteSimilar to RPM, Errors per Minute (or error rate) are the number of API calls with non 200 families of status codes per minute and are critical for measuring how buggy and error-prone your API is.
To track errors per minute, it’s important to understand what type of errors are happening.
500 errors could imply bad things are happening with your code whereas many 400 errors could imply user errors from a poorly designed or documented API.
You can further drill down into seeing where these errors come from.
Many 401 Unauthorized errors from one specific geo region could imply bots are attempting to hack your API.
API Product MetricsAPIs are no longer just an engineering term associated with microservices and SOA.
The role of ensuring the right features are built lies on the API product manager, a new role that many B2B companies are rushing to fill.
For example, February may have only 28 days whereas the month before has a full 31 days causing February to appear to have lower usage.
Not surprisingly, these are the same power users that generally bring your company the most revenue and organic referrals.
This means its critical to track what your top 10 customers are doing with your API.
In general, product churn is a leading indicator of subscription churn since customers who don’t find value in an API may be stuck with a yearly contract while not actively using the API.
11: Time to First Hello World (TTFHW)TTFHW is an important KPI for not just tracking your API product health, but your overall developer experience aka DX.
It may also mean your API is not flexible enough when it comes to filtering and pagination.
ConclusionFor anyone building and working with APIs, its critical to track the correct API metrics.
For example, product managers shouldn’t worry about CPU usage just like infrastructure teams shouldn’t worry about API retention.
Many companies are struggling to achieve the flawless canary deployment strategy due to lack of Automation skills.
Once all the health check is passed, when there are no complaints, then the complete customers will be routed to the new version of the application and the old version will be deleted.
Image 4 shows that 100% traffic is completely routed to the new application version (v1.1) whereas there is no traffic at all to the older app version (v1.0).
Spinnaker is a pure Continuous Deployment tool, we should avoid comparing with Jenkins or any other CI tool.
By default, Kubernetes will serve the traffic in Round Robin method if there is no other external Load Balancer is configured.
There is no pipeline script needed for this stage.
It is composed of microservices which can make deploying and managing the platform difficult and require a lot of compute power to run.
If your company is mature in its infrastructure capabilities than this tool makes wonder but might not be right for smaller groupsAvoid using the ad-hoc “edit” features, Spinnaker provides quick ways to edit your deployed Manifests in the infrastructure screen.
This is done to provide you a quick fallback when mitigating a broken rollout, or to increase the number of pods serving traffic.
Windows based systems finds it difficult to onboard to Spinnaker.
In times when our users are very demanding regarding speed and uptime (particularly in web-based applications), APM tools are very useful for detecting and diagnosing performance-related problems in production environments, without significantly adding overhead to resource consumption.
Typically, APM tools expose information about response times, load, transactions, resources consumption, network data and other performance metrics of a system application and infrastructure.
Each reader will decide if the advantages and disadvantages here apply for her/his specific situation, based on what they are looking for in an APM tool or in her/his personal experience using software with these characteristics.
On the other hand, it has a free Lite version, but it presents really poor data and has only 2 hours of data retention.
Flexible low prices.
Disadvantages:It can be only used for AWS components and therefore for applications running only on Amazon servers.
I found some scripts made by third parties to get metrics for non-AWS servers but they aren't an "official" solution and I haven't tested them.
The user interface is not really friendly for data analysis making it difficult to perform data correlation.
No transaction tracing.
No memory usage metrics by default.
Their number of visits is limited to 100k.
Disadvantages:The user interface is a little complex, especially for users without experience on this tools.
CA APM GUI:Conclusion and ComparisonAs you are probably thinking, the conclusion of the post is that there is no definitive APM tool to choose over the others.
Before APM, developers would primarily leverage code profilers, which were essentially limited to pre-production phase of software or application lifecycle.
Since the overhead introduced by code profilers was unacceptable for production environments, the first generation of APM tools such as those from Wily Technologies and Dynatrace were not widely used for always-on production monitoring use cases.
However, APM is extremely limited when it comes to monitoring a new breed of applications called “data-first” applications.
This infrastructure could be either in a public cloud, such as Amazon Web Services or an on-premises datacenter.APM for Data-First ApplicationsAPM tools come up woefully short when applied to the data-first world given their inability to look beyond the code layer.
The first problem is that they can’t see into the inner workings of distributed data frameworks.
For example, detecting an Apache Kafka broker as a remote call is not good enough.
To measure app health, it is vital to track the consumer lag on a per topic basis where topic represents the app level data partition.Similarly, alerting whenever there are under-replicated or offline partitions is critical to monitoring overall Kafka cluster health.
However, this provides no additional value given critical performance metrics are already available on these frameworks via common endpoints such as JSON and JMX.
Finally, there are overhead and transaction explosion concerns to worry about.The second problem is the lack of correlated troubleshooting in data-first applications.
Since distributed transaction tracing is not possible, there is no correlated, end-to-end performance view that can serve as the foundation for troubleshooting.
OpsClarity applies data science constructs such as anomaly detection and event correlation to rapidly troubleshoot issues using common concerns such as throughput, latency, error rate, back pressure etc.While OpsClarity can certainly co-exist with an APM tool in a data-first application, we see customers choosing to cover the application code layer by writing custom metrics to the OpsClarity Custom Metrics API.
Microservices With Observability on KubernetesAre you looking to implement observable microservices but clueless as to how to make them work with Kubernetes?
Observability - Third Pillar - MetricsTo implement the third pillar, i.e., metrics, we can use the APM Services dashboard where Latency, Throughput, and Error rates are captured.
At this point, you can retrieve the list of services that were started on your cluster:     Shell    xxxxxxxxxx            1                                              1            kubectl cluster-info    Shell    xxxxxxxxxx            1                                              1            Kubernetes master is running at https://127.0.0.1:53519             2            KubeDNS is running at https://127.0.0.1:53519/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy             3                         4            To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
The Operator exposes the service with a static IP address.
The stop analyzer type breaks out the field based on characters like period and slash.
Check the Use specific JVM arguments box to add the following parameters:-Dcom.sun.management.jmxremote-Dcom.sun.management.jmxremote.port=54321-Dcom.sun.management.jmxremote.authenticate=false-Dcom.sun.management.jmxremote.ssl=false4) Run the JobsAfter the jobs have been built, and settings changed, we can run them locally.
Make sure you stop the Talend Spark Process first, so that no logs are written while you are attempting this.
curl -s -XDELETE http://localhost:9200/logstash-2017.03.30/curl -s -XPUT http://localhost:9200/logstash-2017.03.30/ { "mappings": { "jmx": { "properties": { "metric_path": { "type": "text", "analyzer": "stop" } } } } }Reset KafkaTo reset Kafka, you must stop the program and remove the log files.
Each microservice should be run in an environment isolated from the other microservices, so they do not share any resources like data sources or log files between them.
Grepping the logs is not the right solution for that problem.
Each microservice should be run in an environment isolated from the other microservices, so they do not share any resources like data sources or log files with them.
Grepping the logs is not the right solution for that problem.
Architecture and exposed services are the same as in the previous sample.
There is no an upper limit for the number of microservices in your system.
Incident Detection, Response & Forensic Analysis: Performing a root cause analysis and generating an incident report that provides a detailed analysis of an attack attempt or an ongoing attack that helps enterprises take appropriate remedial action immediately.
Threat Intelligence: This isn’t part of traditional SIEM.
Incident Detection, Response & Forensic Analysis: Elastic endpoint provides visibility and advanced threat detection.
Apart from the user-defined rules, the pre-built rules are added for better threat detection and response.
Also proactively it blocks malware and ransomware.
As stated earlier, though threat analytics isn’t part of SIEM by definition, it has become an unsaid expectation for SIEM solution to have anomaly detection as part of it.
In fact, with a lot of firms moving towards solutions such as Google Cloud Platform (GCP), Azure & Zoom to support remote collaboration, Elastic Security 7.10 has come up with key capabilities to detect and prioritize the threats across solutions: https://www.elastic.co/blog/whats-new-elastic-security-7-10-0-correlation-cloud-visibility-detectionThe most critical element is to understand the domain, business problem, influencing factors, and building appropriate rules to identify threats.
Why You Need to Rethink Using Log Analytics for APMMonitoring is complicated, with multiple tools in use and many more available to us for little or no cost.
Oftentimes the obstacles become apparent, as these tools are not well-suited to handle high volume metrics, nor do they have the troubleshooting workflows or out of the box views to make user troubleshooting more productive.
This is even something Gartner has called out, and has been seen many times with limited outcomes.
They're a major part of monitoring an application, but are generally missing from Observability or tracing systems.
People new to looking at memory metrics often panic thinking that having no free memory means the server doesn’t have enough RAM.
If it’s all used and there is very little or no buffered & cached memory, then indeed the server is low on RAM.
This example shows a logging system with more writes than reads:Disk I/O – read vs. write operationsThe operating system settings for disk I/O are a base for all other optimizations – tuning disk I/O can avoid potential problems.
On Windows one can simply disable virtual memory.Define the heap memory for Elasticsearch by setting the ES_HEAP_SIZE environment variable (-Xmx java option) and following these rules:Choose a reasonable minimum heap memory to avoid ‘out of memory’ errors.
Typically, one does not want to allocate more than 50-60% of total RAM to the JVM heap.
When some of these Memory Pools, especially Old Gen or Perm Gen, approach 100% utilization and stay there, it’s time to worry.
If there’s too much garbage collection activity, it could be due to one of the following causes:One particular pool is stressed, and you can get away with tuning pools.JVM needs more memory than has been allocated to it.
In this case, you can either lower your requirements or add more heap memory (ES_HEAP_SIZE).
Lowering the utilized heap in Elasticsearch could theoretically be done by reducing the field and filter cache.
In practice it would have a negative impact to query performance by doing so.
For example, in a summarized view of JVM Memory over all nodes a drop of several GB in memory might indicate that nodes left the cluster, restarted or got reconfigured for lower heap usage.——-Summary DashboardJVM Pool Size (left) and Garbage collection (right)7.
Through our Elasticsearch consulting practice we have seen many cases where request latency is low and then suddenly jumps as a consequence of something else starting to misbehave in the cluster.
A word of caution – query latency that Elasticsearch exposes are actually per-shard query latency metrics.
Remember that by default (because of how costly it is to build it) field data cache is unbounded.
To avoid nasty surprises consider limiting the size of field data cache accordingly by setting the “indices.fielddata.cache.size” property and keeping an eye on it to understand the actual size of the cache.Field cache size (yellow) and field cache evictions (green)Another thing worth configuring are circuit breakers, to limit the possibility of breaking the cluster by using a set of queries.
Build Your Own Error Monitoring ToolIn this tutorial, I’ll describe how you can create your own error watcher.
Why Use a Simpler Error Monitoring Tool?
If your application is on production or will be in near future, you should look for some kind of error monitoring tool.
Otherwise, you’ll have a really big problem.
And as a developer, let me tell you that looking for errors manually in your production environment isn’t cool.
Find the Cause of the Problem Before Your Customers NoticeFor example, let’s say your application is doing some background processing that isn’t visible at first glance to the end user.
The process fails at one of the background steps.
If you have an error monitoring tool, you’ll have the possibility to fix the bug before your customers notice it.
Now, imagine that your team gets a notification right away when the error appears — you can now skip that time-consuming part.
If you’d like to send an alert message to a Slack channel or email someone about a critical error, you’ll need to use “semi-paid” X-Pack.
We will query Elasticsearch for recent logs containing error log level using our custom Node.js Elasticsearch Watcher.
Dockerized ELK StackSetting up Elasticsearch, Logstash, and Kibana manually is quite boring, so we’ll use an already Dockerized version.
Below, you can see the Logback appender configuration:<appender name="stdout" class="ch.qos.logback.core.ConsoleAppender">        <encoder>            <pattern>%d{yyyyMMdd HH:mm:ss.SSS} [%thread] %-5level %logger - %msg%n</pattern>        </encoder></appender>Here are the sample logs:20180107 12:03:26.353 [pool-48-thread-1] DEBUG com.example.email.EmailPollingManager - Executing mail polling20180122 11:12:09.541 [http-nio-8081-exec-73] ERROR com.example.service.importer.GenericCsvImporter - Could not parse csv.
If your logs are created as a user with ID 1000, you won’t notice the problem and you can skip this step.
One of them is Filebeats, which kills the pain of log access.
The file permission problem will, of course, be present if you want to run the dockerized version of the Filebeats, but that’s the cost of virtualization.
However, as stated before, it’s not free.
It’s just a scheduled job which checks if there’s something to send, so it shouldn’t be hard, right?
This library takes two arguments when creating a new instance of a watcher:Connection configuration: It defines connection parameters to Elasticsearch.
Error handler (optional): Task which will be executed when an error appears.
require('dotenv').config();const express = require('express');const watch = require('./watcher');const app = express();app.listen((err) => {    if (err) {        return console.log('something bad happened', err);    }    watch();});The meat.
In the example, we’re looking for entries with Error log level which appeared in last 30 seconds.
In the predicate field, we’ll define the condition of hits number at greater than 0 since we don’t want to spam Slack with empty messages.
There’s no magic here.
In the title, you can find information about the class of the error and the timestamp.
const request = require('request');const RED = '#ff0000';const sendMessage = (message, channels) => {    console.log('Sending message to Slack');    const cb = (err, response, body) => {        if (err) {            console.log('Error appeared while sending message', err);        }        console.log('Message sent', body);    };    const sendRequest = (message) =>        request({url: process.env.SLACK_INCOMING_WEBHOOK_URL, method: "POST", json: message}, cb);    if (channels) {        channels.forEach(channel => {            message.channel = channel;            sendRequest(message);        });    } else {        sendRequest(message);    }};const send = (data) => {    const mapHitToAttachment = (source) => (        {            pretext: `*${source.loglevel}* ${source.customTimestamp}`,            title: `${source.class}`,            text: `\`\`\`${source.msg}\`\`\``,            color: RED,            mrkdwn_in: ['text', 'pretext']        }    );    const message = {        text: "New errors!
I think the next step would be to aggregate errors.
In the current version, you’ll get errors one by one in your Slack channel.
I’ll leave it to you as homeworkYou need to analyze the pros and cons of setting up all of this by yourself.
It may be quite surprising, but the answer is no.
It is exposed on port 5000 and reads configuration from logstash.conf file:docker run -d --name logstash --net es -p 5000:5000 -v ~/logstash.conf:/usr/share/logstash/pipeline/logstash.conf docker.elastic.co/logstash/logstash:6.7.2Finally, we can run Kibana, which used just for displaying logs:$ docker run -d --name kibana --net es -e "ELASTICSEARCH_URL=http://elasticsearch:9200" -p 5601:5601 docker.elastic.co/kibana/kibana:6.7.2After starting my sample application that uses spring-boot-logging library, the logs from POST requests are displayed in Kibana as shown below:Each entry with response log contains X-Correlation-ID, X-Request-ID, X-Response-Time, and X-Response-Status headers.
Pay as you go: Power up your Agileload by renting from 50 to 10000 virtual users.
End-to-end diagnostic features: The tool not only monitors the front-end but also the back-end to detect the reason for performance degradations.
JMeter allows you to perform various testing activities like performance, load, stress, regression and functional testing, in order to get accurate performance metrics against your web server.
This also means you won’t hit any trouble when using different local machines and cloud servers to run and create your tests.
In case the results exceed a threshold, it is possible to automatically mark the test(s) as failed.
Furthermore, when someone goes wrong and an application is behaving abnormally, we need to identify the root cause of the problem quickly so that we can remedy it.
It then needs to bundle all of that information together into a digestible format and alert you to the problem.
Finally, depending on your application and deployment environment, there may be things that you can tell the APM solution to do to automatically remediate the problem.
Thus we can refine our definition of APM to include the following activities:  The collection of performance metrics across an entire application environment The interpretation of those metrics in the light of your business applications The analysis of those metrics against what constitutes normalcy The capture of relevant contextual information when abnormalities are detected Alerts informing you about abnormal behavior Rules that define how to react and adapt your application environment to remediate performance problems Why is APM Important?
First let’s consider how we detect problems.
How do you know if this is a real problem or a normal slowdown for your application at this hour and day of the week?
Finally, detecting the problem is one thing, how do you find the root cause of the problem?
Some challenges in manual instrumentation include: what parts of my code do I instrument, how do I analyze it, how do I determine normalcy, how do I propagate those problems up to someone to analyze, what contextual information is important, and so forth.
Plus you have introduced a new problem: you have introduced performance monitoring code into your application that you need to maintain.
There are other technical options, but what I find most often is that companies are alerted to performance problems when their custom service organization receives complaints from users.
I don’t think I need to go into details about why this is a bad idea!
Next let’s consider how we identify the root cause of a performance problem without an APM solution.
Most often I have seen companies do one of two things:  Review runtime logs Attempt to reproduce the problem in a development / test environment  Log files are great sources of information and many times they can identify functional defects in your application (by capturing exception stack traces), but when experiencing performance issues that do not raise exceptions, they typically only introduce additional confusion.
You may have heard of, or been directly involved in, a production war room.
These war rooms are characterized by finger pointing and attempts to indemnify one’s own components so that the pressure to resolve the issue falls on someone else.
The bottom line is that these meetings are not fun and not productive.
Alternatively, and usually in parallel, the development team is tasked with reproducing the problem in a test environment.
Furthermore, if you are able to reproduce the problem in a test environment, that is only the first step, now you need to identify the root cause of the problem and resolve it!
Pay as you go: Power up your Agileload by renting from 50 to 10000 virtual users.
End-to-end diagnostic features: The tool not only monitors the front-end but also the back-end to detect the reason for performance degradations.
JMeter allows you to perform various testing activities like performance, load, stress, regression and functional testing, in order to get accurate performance metrics against your web server.
This also means you won’t hit any trouble when using different local machines and cloud servers to run and create your tests.
In case the results exceed a threshold, it is possible to automatically mark the test(s) as failed.
The only thing is that they had missing functionalities.
info: sends the request body and headers, as well as the response body and headers for samplers that resulted in errors.
As of version 2.3.0, the plugin now supports a new mode:4. error: will only send the data of the failing samplers to the ElasticSearch engineContinuous Integration (CI) SupportIf you’re using this plugin in your CI environment, you’ll probably want to know the performance trend of your application.
Furthermore, when someone goes wrong and an application is behaving abnormally, we need to identify the root cause of the problem quickly so that we can remedy it.
It then needs to bundle all of that information together into a digestible format and alert you to the problem.
Finally, depending on your application and deployment environment, there may be things that you can tell the APM solution to do to automatically remediate the problem.
Thus we can refine our definition of APM to include the following activities:  The collection of performance metrics across an entire application environment The interpretation of those metrics in the light of your business applications The analysis of those metrics against what constitutes normalcy The capture of relevant contextual information when abnormalities are detected Alerts informing you about abnormal behavior Rules that define how to react and adapt your application environment to remediate performance problems Why is APM Important?
First let’s consider how we detect problems.
How do you know if this is a real problem or a normal slowdown for your application at this hour and day of the week?
Finally, detecting the problem is one thing, how do you find the root cause of the problem?
Some challenges in manual instrumentation include: what parts of my code do I instrument, how do I analyze it, how do I determine normalcy, how do I propagate those problems up to someone to analyze, what contextual information is important, and so forth.
Plus you have introduced a new problem: you have introduced performance monitoring code into your application that you need to maintain.
There are other technical options, but what I find most often is that companies are alerted to performance problems when their custom service organization receives complaints from users.
I don’t think I need to go into details about why this is a bad idea!
Next let’s consider how we identify the root cause of a performance problem without an APM solution.
Most often I have seen companies do one of two things:  Review runtime logs Attempt to reproduce the problem in a development / test environment  Log files are great sources of information and many times they can identify functional defects in your application (by capturing exception stack traces), but when experiencing performance issues that do not raise exceptions, they typically only introduce additional confusion.
You may have heard of, or been directly involved in, a production war room.
These war rooms are characterized by finger pointing and attempts to indemnify one’s own components so that the pressure to resolve the issue falls on someone else.
The bottom line is that these meetings are not fun and not productive.
Alternatively, and usually in parallel, the development team is tasked with reproducing the problem in a test environment.
Furthermore, if you are able to reproduce the problem in a test environment, that is only the first step, now you need to identify the root cause of the problem and resolve it!
Here are some of the common use cases where Elasticsearch or the ELK stack (Elasticsearch, Logstash, Kibana) can be used:Keep track of the number of errors in a web application.
You can create dashboards for developers which shows application errors and API response times and what not.
A sample Logstash pipeline configuration: input {        file {             path => "/path/to/your/logfile.log"        }   }   filter {        if [request] in ["/robots.txt"] {             drop {}        }   }   output {        file {             path => "%{type}_%{+yyyy_MM_dd}.log"        }   }  Let us consider a basic use case of Logstash before moving to the other components of our ELK stack.
It can store very large datasets (Petabyte Scale)It can efficiently search across a very large datasetIt scales horizontally and can tolerate node failures.
Elasticsearch ArchitectureLet's review the Elasticsearch architecture and key concepts that are critical to the EFK stack deployment:Cluster: Any non-trivial Elasticsearch deployment consists of multiple instances forming a cluster.
The data is sharded and replicated so that a given number of data nodes can fail, without impacting availability.
As Elasticsearch can use replicas, the individual processes can terminate immediately, without the risk of data loss.
Moreover, not only is it important to be able to define RUM and APM — it’s also critical to understand the similarities and differences between each type of software monitoring process.
Real-user testing is not a passive technique and does not directly relate to real-user monitoring.
For example, some Java monitoring tools include lightweight JVM profilers aimed at profiling applications in productions without creating a lot of overhead and slowing them down.
Crash ReportingAnother key advantage of RUM is that it can help with crash reporting.
When you perform RUM, your end-users typically interact with a client-side frontend that may crash or experience other stability issues independent of the backend.
Not to mention, we raised fatal alerts whenever a server went down.
The cloud changed our view of the world because no longer did we take a system-level view of the behavior of our applications, but rather we took an application-centric view of the behavior of our applications.
The former APM monitoring model of raising alerts when servers go down would drive you nuts.
You also have to ask yourself where your expertise lies.
If you are a rock star at building an eCommerce site but have not invested the years that APM vendors have in analyzing the underlying technologies to understand how to interpret performance metrics then you run the risk of leaving your domain of expertise and missing something vital.
This depends on how complex your applications are and how downtime or performance problems affect your business.
If your company makes its livelihood by selling its products online then downtime can be disastrous.
If the site is slow then she’ll just move on to the next vendor in her list, which means you just lost the sale.
This is a hard metric to quantify, but if your web site is slow then it may damage customer impressions of your company and hence lead to a loss in confidence and sales.
The APM solution observes your application to determine normalcy and, when it detects abnormal behavior, it captures contextual information about the abnormal behavior and notifies you of the problem.
The default operating system limits on mmap counts is likely to be too low, which may result in out of memory exceptions.
It might be able to tell you that your site is down or slow, but you would have no idea why.
Tracking and monitoring application error rates.
Google Analytics AlertsThe problem with most of these strategies is they don’t tell you WHY your app is down or slow.
They are merely alerting mechanisms to tell you there is a problem.
Knowing about the problem is no doubt very important and these tools can help.
That said, solving the problem is where all the hard work lies.
Solving the problem takes a lot more insight, data, and tools.
It is an umbrella term used to describe a wide range of tools that alert you about potential application problems.
Unfortunately, they typically only tell you that a problem exists and don’t understand the full context of what is going on across your entire environment.
It has no context that the site might be running across a dozen load-balanced servers.
Error rates.
The only way to identify slow SQL queries, long-running web service calls, and other common problems is to profile the code itself.
SummaryI would argue that application performance monitoring is part of application performance management.
There is no real debate of application performance monitoring vs application performance management.
If you run into any issues with docker, you can start fresh with: docker container ls -a | cut -c1-12 | xargs docker container rm --force docker images | cut -c69-80 | xargs docker rmi docker system prune -aNOTE: This will destroy all docker containers, images, and networks, so use at your own risk.
If the database is clustered: no.
When the generator has almost finished, a warning shows in the output:WARNING!
it offers an in-browser widget with no backend required that is automatically injected to the monitored web page.
meaning that it aims to explain how every transaction gets executed, trace the flows between the components and (bad joke ahead) pinpoints problematic areas and potential bottlenecks.
pinpoint also lets you see the request count and response patterns so you’ll be able to identify potential problems.
you can view critical details that include cpu usage, memory/garbage collection, and jvm arguments.
to get started, all you need to do is drop the .jar file into the web-inf/lib folder or by including a small new section in the web.xml file.
to get started with glowroot, you need to download and unzip the main installation file and add -javaagent:path/to/glowroot.jar to your application’s jvm arguments.
glowroot’s dashboard   bottom line:  if clean and simple is what you’re looking for, no doubt you’d want to check out glowroot over the other tools here.
kamon is distributed as a core module with all the metric recording and trace manipulation apis and optional modules that provide bytecode instrumentation and/or reporting capabilities to your application.
the only problem is that once you get that haystack in which the problem was found, you have to start digging around looking for the actual needle that caused it.
Distributed Tracing With Zipkin and ELKWhile logs can tell us whether a specific request failed to execute or not and metrics can help us monitor how many times this request failed and how long the failed request took, traces help us debug the reason why the request failed or took so long to execute by breaking up the execution flow and dissecting it into smaller events.
In our case, I want to pass some environment variables and specify Elasticsearch as the storage type (this example assumes a locally running ELK Stack), and so I will use the following commands to download and run Zipkin: curl -sSL https://zipkin.io/quickstart.sh | bash -sjava -DSTORAGE_TYPE=elasticsearch -DES_HOSTS=http://127.0.0.1:9200 -jar zipkin.jarIf all goes as expected, you should see this output in your terminal: 2018-05-02 09:59:27.897  INFO [/] 12638 --- [  XNIO-2 task-1] o.s.web.servlet.DispatcherServlet        : FrameworkServlet 'dispatcherServlet': initialization completed in 108 msOpen Zipkin at http://localhost:9411:Of course, we have no traces to analyze in Zipkin yet, so our next step is to simulate some requests.
With error stack traces received from Graylog, engineers understand the context of any issues in the source code.
Proxy ErrorThe proxy server received an invalid response from an upstream server.
Reason: Error reading from remote serverWe used our APM (Application Performance Monitoring) tool to examine the problem.
We logged in to this problematic AWS EC2 instance.
In the output generated by this command, we noticed the following interesting messages to be printed repeatedly:[4486500.513856] TCP: out of memory -- consider tuning tcp_mem[4487211.020449] TCP: out of memory -- consider tuning tcp_mem[4487369.441522] TCP: out of memory -- consider tuning tcp_mem[4487535.908607] TCP: out of memory -- consider tuning tcp_mem[4487639.802123] TCP: out of memory -- consider tuning tcp_mem[4487717.564383] TCP: out of memory -- consider tuning tcp_mem[4487784.382403] TCP: out of memory -- consider tuning tcp_mem[4487816.378638] TCP: out of memory -- consider tuning tcp_mem[4487855.352405] TCP: out of memory -- consider tuning tcp_mem[4487862.816227] TCP: out of memory -- consider tuning tcp_mem[4487928.859785] TCP: out of memory -- consider tuning tcp_mem[4488215.969409] TCP: out of memory -- consider tuning tcp_mem[4488642.426484] TCP: out of memory -- consider tuning tcp_mem[4489347.800558] TCP: out of memory -- consider tuning tcp_mem[4490054.414047] TCP: out of memory -- consider tuning tcp_mem[4490763.997344] TCP: out of memory -- consider tuning tcp_mem[4491474.743039] TCP: out of memory -- consider tuning tcp_mem[4491859.749745] TCP: out of memory -- consider tuning tcp_mem[4492182.082423] TCP: out of memory -- consider tuning tcp_mem[4496318.377316] TCP: out of memory -- consider tuning tcp_mem[4505666.858267] TCP: out of memory -- consider tuning tcp_mem[4521592.915616] TCP: out of memory -- consider tuning tcp_memWe were intrigued to see this error message: “TCP: out of memory — consider tuning tcp_mem”.
It means out of memory error is happening at TCP level.
We had always taught out of memory error happens only at the application level and never at the TCP level.
Problem was intriguing because we breathe this OutOfMemoryError problem day in and out.
We never thought there would be a problem at the device driver level, that too in, stable Linux operating system.
Being stumped by this problem, we weren’t sure how to proceed further.
We restarted the EC2 instance to a put-off immediate burning fire.
Restarting the server cleared the problem immediately.
ConclusionHere are a few conclusions that we would like to draw:Even the modern industry-standard APM (Application Performance Monitoring) tools aren’t completely answering the application performance problems that we are facing today.
Memory problems don’t have to happen in the code that we write, it can happen even at the TCP/Kernel level.
i covered some of these new features in   this blog post  , but one announcement i missed out on was amazon x-ray.
it can thus be used to not only monitor the performance of the requests but also to identify bottlenecks and errors.
after a while, stop the requests and open the   service map  tab in the console.
for example, say we want to narrow down the list of service requests to only those with errors:service("xray-demo.htuaqjkx9a.us-east-1.elasticbeanstalk.com") {error = true} we can see that   post  requests originating from the signup page are triggering 409 errors.
to troubleshoot the root cause, we’re going to open one of the available traces: what we see is a timeline of the request — and we can see that calls to dynamodb are failing, for some reason.
Geek Reading April 10, 2015More analysis on some research leads today’s articles.
Companies can pay for the infrastructure that they need for any given hour of the day, scaling up to meet high user demand during peak hours and scaling down during off peak hours.
Stolen CPU is not as diabolical as it sounds, it is a relative measure of the cycles of a CPU that should have been available to run your processes, but were not available because the hypervisor diverted cycles away from your instance.
The key to identifying noisy neighbors is to observe the metrics over time pay particular attention to the stolen CPU time when your virtual machine is idle.
If you see discrepancies in the stolen time when your CPU is idle then that indicates that you are sharing the same hardware with other customers.
For example, when your CPU usage is idle and the stolen CPU percentage is 10% once and then another time the stolen CPU usage is 40% then chances are that you are not simply observing hypervisor activity, but rather are observing the behavior of another virtual machine.In addition to reviewing the operating system CPU utilization and stolen percentage, you need to cross reference this with Amazon’s CloudWatch metrics.
If the CloudWatch CPUUtilization metric is at 100% for your EC2 instance and your operating system is not reporting that you’ve reached 100% of your ECU capacity (one core of your physical CPU type / 1.0-1.2 GHz) and the CPU stolen percentage is high, then you have a noisy neighbor that is draining compute capacity from your virtual machine.
If the operating system reports that we’re at 30% CPU utilization with a high stolen CPU usage and CloudWatch reports that we’re at 100% usage, then we have identified a noisy neighbor.Figure 1 presents an example of the CloudWatch CPU Utilization as compared to the operating system CPU utilization and stolen percentages, captured graphically over time.
CloudWatch vs OS CPU MetricsOnce you’ve identified that you have a noisy neighbor, so how should you react?
You have a few options, but here’s what we recommend:Observe the behavior to determine if it is a systemic problem (is the neighbor constantly noisy?
)If it is a systemic problem then move.
While this might not be the best choice if you live in an apartment building, in Amazon it is simple: start a new AMI instance, register it with your ELB, and decommission the old one.
App Insights does not support 2.0 and they don’t have any plans to do so.
Key Limitation: Finding Slow SQL QueriesApplication Insights provides reporting down to the server and database name being accessed.
There is no way to see how long individual queries take, which are the slowest, or which are called the most often.
There is also no way to set up monitoring for a specific SQL query.
Look for missing gaps of time in your requests.
Limitations and Missing Exceptions FeaturesApp Insights is missing some key features around exceptions.
No way to see exceptions across all applications.
No way to collect all “first chance” exceptions.
Asynchronous HttpClient Calls Are Not SupportedAlmost all new ASP.NET development is now done in async and uses the common async/await pattern.
Unfortunately, it does not support the recommended way to make HTTP calls with ASP.NET.
LimitationsApp Insights does not support some other notable key Azure dependencies, including Service Bus and Redis.
It will tell you the average time and % of failures.
I would also suggest going to “Failures” from the menu to look for dependency failures and exceptions.
Monitoring LimitationsNo way to monitor a specific request (URL/action).
No way to monitor a specific SQL query.
No way to monitor % of requests having errors.
No built-in concept of alert “escalation.”No concept of alert severity like warning vs. critical.
No Support for the Concept of Multiple EnvironmentsWithin an individual application, there is no high-level way to separate out and report data by environment (i.e., Dev, QA, Prod).
This works but then there is no reporting across them.
They are completely isolated.
The reason it does not support a wide array of third-party dependencies is primarily because they don’t do code profiling that would let them instrument the libraries.
No custom profiling.
They can help you plan for upgrades before outdated systems begin to cause failures.
Unfortunately, APM tools do not analyze log files and are unable to detect security attacks.
If you rely on any one of these tools alone when an incident occurs, you’ll always miss some key piece for the resolution.Monitoring Tools Require Even More MonitoringEven if you’ve adopted all of these tools for monitoring, it can get chaotic when an incident occurs.
This means that you end up looking around frantically and jumping back and forth between tools, causing a lot of frustration for your teams as well as your customers.
Your MTTRs will be longer since you face an overload of data from your entire toolset.
Further, these processes can be automated, saving your team a lot of time and reducing overall MTTR.All your monitoring tools offer their own set of unique features, but with them comes a lot of confusion if not managed well.
The world’s largest video streaming service, Netflix experienced one of its worst incidents in company history.
In fact, Netflix already was aware of the possible risk of their system and was experimenting with a more resilient architecture termed Isthmus, albeit for a different purpose.
However, what is missing, or at least what has been lost, is the theory and clear understanding of the motive behind implementing such an architecture.
Back to the BasicsThings fail.
The main cause of this failure, when looking at it from a broad perspective, is the need for scaling services and the increasing velocity in their development.
The probability of failure increases when both scalability and velocity increase and this is just the phenomena observed in the field and validated through academic literature.
The active/active architectural concept, which as mentioned Netflix and others are looking towards, provides measures to mitigate the consequences of the inevitable failure.
Therefore, it can be seen that the concept is built on the premise that failures are inevitable, and so aims to tackle downtimes instead.
The goal here is that the Mean Time To Resolve (MTTR) is low enough that it is insignificant in how it affects the consumer’s perception of the service’s availability.
Nevertheless, the problem that the creators of the active/active architecture have to wrestle can be divided into two points:How to route customer traffic to available services or resources and being aware of disrupted service or resources.
That is because theoretically, everything can fail.
Not only compute services but also resources such as datastores, event buses, routing services, and other such resources can all fail, and should be replaced with either copies or other like service lightening the blow of the failure.
Redundancy and Request RoutingBefore addressing the traffic routing problem, let us revisit the idea of redundancy.
This may seem counterintuitive and against the intrinsic principles of software such as DRY, but is critical to the idea of active/active systems.
Two or more processing nodes available where many of these nodes are redundant acting as standby for when the primary node experiences disruptions.
After all, for the customer, they must be as the experience of using the platform must go unhindered when the primary serving node fails.
It is evident why this is a crucial block for an active/active architecture.
Any node apart from the primary servicing node can begin receiving requests upon the failure of the primary node.
This is because serverless compute services such as AWS Lambda when torn down do not retain the existing state, and thus when being utilized, we must always be mindful of the stateless nature.
This is where the notion of data replication kicks in, but there is an inherent barrier that we need to address and that is the CAP theorem.
Alas, as tech has demonstrated before, there is no such thing as a silver bullet.
The need to go faster while maintaining the stability of the product, under the umbrella of DevOps, warrants the need for architectures such as active/active, albeit the difficulties that arise in its implementation.
With so much flux and complexity across a cloud-native system, it's important to have robust monitoring and logging in place to control and manage the inevitable chaos.
In the case of critical system errors, we typically see accelerated growth in the log volume.
If the log storage hits storage limits, you lose the latest logs, which are essential to fix system errors.
Every attacker will try to avoid or delete his traces in log files.
If the attacker has access to your infrastructure, sending logs off-site, e.g., using a logging SaaS will help keep evidence untampered.
5. Review & Constantly Maintain Your LogsUnmaintained log data could lead to longer troubleshooting times, risks of exposing sensitive data or higher costs for log storage.
If logs contain only some error codes or 'cryptic' error messages it can be difficult to understand.
On top of such audit logs, you should define alerts on logs in order to be notified quickly on any suspicious activity.
To practice automation at every step of the development pipeline, you need the visibility to know where issues are introduced, and what the main sources of these issues are - faulty code, dependency issues, external attacks, insufficient resources, or something else.
Without the appropriate testing and logging, you'll be left with runaway releases and deployment hell.
Container Capabilities Lag Hype by Two YearsThanks to Ranga Rajagopalan, CTO and Co-founder, and Chandra Sekar, V.P.
The process of managing and configuring load balancers, security, visibility, and performance between clients was challenging and delayed application rollouts.With container-based applications, apps that were once static and immovable are now broken down into light, manageable parts, or containers.
Q: What are some real-world problems being solved by the orchestration and deployment of containers?
A: The actual capabilities lag the hype by a couple of years.
Each microservice should be run in an environment isolated from the other microservices so it does not share resources such as databases or log files with them.
Grepping the logs is not the right solution for that problem.
Architecture and exposed services are the same as in the previous sample.
Top 10 Open-Source Monitoring Tools for KubernetesEngineers list monitoring as one of the main obstacles for adopting Kubernetes.
Not surprisingly, when asked, engineers list monitoring as one of the main obstacles for adopting Kubernetes.
After all, monitoring distributed environments have never been easy and Kubernetes adds additional complexity.
Fluentd uses disk or memory for buffering and queuing to handle transmission failures or data overload and supports multiple configuration options to ensure a more resilient data pipeline.
Both Fluentd and Fluent Bit are also CNCF projects and Kubernetes-native - they are designed to seamlessly integrate with Kubernetes, enrich data with relevant pod and container metadata, and as mentioned - all this with a low resource footprint.
Pros: Huge plugin ecosystem, performance, reliability Cons: Difficult to configurecAdvisorcAdvisor is an open-source agent designed for collecting, processing, and exporting resource usage and performance information about running containers.
research shows that the partof european internet users that buys on-line has grown from 40% in 2004 to 84%in 2008. additionally, the large web retailers in my country saw theirrevenue grow in 2009 and in the first part of 2010 just as if the crisis never materialized.
so thequestion is: how can we prevent these performance and availability problems andhow can we assure that a web site is always quick and available?
howperformance problems get to you     frustrationsand loss of revenue    wheninternal applications are slow, this is frustrating for the users.
the customer in turn will be frustrated by the long waiting times andlong phone calls.
disruptionof regular development    slownessproblems most of the time manifest themselves unexpectedly, such as after theintroduction of a new application or new release.
the difficulties which turn up in production put a high pressure onboth the operators as well as the developers to solve the usually difficult tofind problems.
this will have its disruptive effect on the regular developmentof new releases: the development team is only busy firefighting.
however, if the bottleneck turnsout not to be located in the web tier but somewhere else, this investment inmore servers will turn out to be just wasted money.
seven stepsto performance success    it can be avalid choice to run the risk of performance problems in production and dealwith them in a re-active manner.
step 1:define performance requirements    definingthe performance requirements well usually is a neglected activity.
with such vague definitions the confusionstarts.
the goal is unclear and is typically explained very differently by thebusiness and the it department.
these aspects are oftenignored by the non-technical contributors to these requirements.
in addition, each technology orframework comes with its own teething troubles and most of the time uses more resourcesthan its predecessor.
therefore,it is questionable if the chosen new technology and architecture will meet thespecified performance requirements.
by executingthis poc and understanding and using the results of it, the project can early becorrected in the right direction to prevent excessive cost and delay.
step 3:test representatively    slowness ofapplications in development environments is often neglected with the rationalethat faster hardware in the production environment will solve this problem.
alsoreporting features and maintainability of the scripts are both not so great.
this is just like any other software defects and quality assurance: the later in the development process defects are detected, the more costly these defects are.
in case serious performance defects popped up, a crisis team was gathered and we found ourselves in a stressful situation.
there was usually not enough time to fix the defect before the release date, so my recommendation at times was to defer the release date.
in this way, the it department gets early insight into new and changed features, it can adapt its course quicker, back-off early from an unfortunate architecture or approach, minimize surprises and also have lower costs.
however, to be able to take the right measures in case of a fatal incident, it is necessary to be able to pin-point the problem.
this approach results in low memory usage and a low performance overhead, at the cost of some information loss.
this happened while no new release was introduced and no other page became slower.
the troublemaker turned out to be a dao executing a prepared statement with only part of the variables being bind-variables.
with help of jarep, we could look back to where the trend of increasing response time started so when the problems started.
we could also see that this problem was only present at one of the two machines.
with this knowledge and his log book, the operator could remember that on the start date he had experimented with a new jdbc driver to try to solve a memory leak.
problems only appeared slowly during the following weeks.
they had left the new driver in place, which manifested itself as a time bomb later.
when we put back the old driver, the problem just disappeared!
identifying which pages or services do not meet stated requirements and isolating the problem: where is it located, in which layer, in which component.
this can for instance be a missing or wrong index on a database table or the invocation of too many small queries.
if not, then there is something wrong with the hypothesis and we need an alternative hypothesis.
what is missing here is:   measure, don’t guess  .
live monitoring is essential to see the actual performance problems.
step 7: share the responsibility for the whole cha   in  when an incident happens in production, this usually means stress.
a performance problem in production often leads to finger pointing.
summery and conclusions  in this growing on line world with demanding customers it has become crucial that services provided on the web are always available and always fast enough.
this is often challenging to developers and operators: performance problems manifest themselves in various ways, like in frustration, loss of revenue and disruption of development; and just adding hardware is a doubtful solution.
some time ago we at xebia presented our   ejapp top 10  about performance problems.
Consider what’s happened with augmented reality (AR) in this year alone.
Defining the scope of problems in execution, runtime architecture, and communications.
Tools for going deeper into the components identified as sources of the problems.
At the intersection of technology and finance, the role of the CIO has become the locus for all the most critical data analytics.
Attaching and detaching microservices from the main functionality of the application is never as easy in practice as it is in theory.
Struggles with team cohesion.
Many developers have developed their skills in isolation and may have difficulty aligning their work habits with a tighter team structure.
Everyone suffers when there is internal friction between functionality and security.
Internal DependenciesYour PHP application may be utilizing a backend database, a caching layer, or possibly even a queue server as it offloads I/O intensive blocking tasks onto worker servers to process in the background.
For example, if your customer is attempting to purchase items in a shopping cart and in order for the transaction to complete your application must charge their credit card so that you can display a confirmation or error page.
If the credit card was declined, the user is presented with an error message to try again.
Common caching problems include:     Loading too much data into the cache       Not properly sizing the cache  When measuring the performance of a cache, you need to identify the number of objects loaded into the cache and then track the percentage of those objects that are being used.
Or is the cache sized appropriately?A common problem when sizing a cache is not properly anticipating user behavior and how the cache will be used.
If 32M is not enough for APC to be used as an opcode cache, then you run the risk of exchanging data in memory for data on the disk.
Considering disk I/O is much slower than reading from memory, this defeats the purpose of your in-memory cache and significantly slows down your application performance.
Because of the advent of the cloud, applications can now be elastic in nature: your application environment can grow and shrink to meet your user demand.
App users have a low tolerance for slowness, with a reported 43% of users unhappy if they have to wait longer than three seconds for an app to load.
(App Samurai)It’s not enough to ensure that your mobile app functions properly, but also to test how it behaves on different devices, under heavy user load, different network connections, etcetera.
Visual Playback Editor: Drag and drop test cases into tracks to be executed on various cloud or on-premise locations.
Troubleshooting: Discover how your application responds to various loads with a load test and identify performance bottlenecks before they cause damage in production.
PricingHeadSpin does not offer a free plan.
JMeter allows you to perform various testing activities like performance, load, stress, regression, and functional testing, to get accurate performance metrics against your web server.
Furthermore, we need to be able to differentiate between problems in our application and problems in dependencies.From a business transaction perspective, we can identify and measure external dependencies as being in their own tiers.
For each iteration within the infinite loop, the event loop executes a block of synchronous code.
Node.js – being single-threaded and non-blocking – will then pick up the next block of code, or tick, waiting in the queue as it continue to execute more code.
Although it is a non-blocking model, various events that potentially could be considered blocking include: Accessing a file on disk Querying a database Requesting data from a remote webserviceWith Javascript (the language of Node.js), you can perform all your I/O operations with the advantage of callbacks.
Thus, if the event loop is blocked by code from transaction X, the slowdown of the execution may impact the performance of transaction Y.This non-blocking, single-threaded nature of Node.js is the fundamental difference in how code execution may potentially impact all the requests within the queue and how with other languages it does not.
Memory leaks are caused when application code reserves memory in the heap and fails to free that memory when it is no longer needed.
If you choose to ignore memory leaks, Node.js will eventually throw an error because the process is out of memory and it will shut down.In order to understand how GC works, you must first understand the difference between live and dead regions of memory.
Everything else is considered dead and is targeted for cleanup by the GC cycle.
The V8 GC engine identifies dead regions of memory and attempts to release them so that they’re available again to the operating system.Upon each V8 Garbage Collection cycle, hypothetically the heap memory usage should be completely flushed out.
Unfortunately, some objects persist in memory after the GC cycle and eventually are never cleared.
Eventually, the memory leak will increase your memory usage and cause your application and server performance to suffer.
As you monitor your heap memory usage, you should pay attention to the GC cycle before and after.
Specifically, you should pay attention to a full GC cycle and track the heap usage.
As a general rule, you should be concerned if heap usage grows past a few GC cycles and does not clear up eventually.Once you’ve identified that a memory leak is occurring, you options are to then collect heap snapshots and understand the difference over time.
Performing a heap dump may be taxing on your application so once a memory leak has been identified, diagnosing the problem is best performed on a pre-production environment so that your production applications are not impacted on performance.
Diagnosing memory leaks may prove to be difficult but with the right tools you may be able to both detect a memory leak and eventually diagnose the problem.Application TopologyThe final performance component to measure in this top-5 list is your application topology.
Because of the advent of the cloud, applications can now be elastic in nature: your application environment can grow and shrink to meet your user demand.
Highly elastic microservice and serverless architectures mean containers spin up on demand and scale to zero when that demand goes away.
This shift has exposed deficiencies in some of the tools and practices we used in the world of servers-as-pets.
Many of the clients we work with at Real Kinetic  are trying to navigate their way through this transformation and struggle to figure out where to begin with these solutions.
This means many of the tools that were well-suited before might not be adequate now.
Similarly, it's much more difficult to correlate the behavior of a single service to the user's experience  since partial failure becomes more of an everyday thing.
This model also leads to ineffective feedback loops if engineers are not on-call and responsible for the operation of their services-something I've talked about ad nauseum.
In this environment, it is no longer practical to SSH into a machine to debug a problem or tail a log file.
It also probably means we are running an agent for many of these services on each host, and if any of these services are unavailable or behind, our application either blocks or we lose critical observability data.
If these are tightly integrated, this can be difficult to do.
We no longer have to figure out data to send from containers, VMs, and infrastructure, to send it, and to send it.
Anyone who's shipped production code has been in the situation where they're frantically trying to regex logs to pull out the information they need to debug a problem.
It's even worse when we're debugging a request going through a series of microservices with haphazard logging.
In order to monitor systems, debug problems, make decisions, or automate processes, we need data.
What we want to avoid is the sort of murder-mystery debugging  that often happens.
A lone error log is the equivalent of finding a body.
We know a crime occurred, but how do we piece together the clues to tell the right story?
The data we can't get for free should go on the context, typically data that is request-specific.
You can take this as far as you'd like-highly structured with a type system and rigid specification-but at a minimum, get logs into a standard format with property tags.
If these aren't aligned, pain-driven development  creates problems.
The downside of moving away from agent-based data collection is we now have to handle routing that data ourselves.
With the amount of data and the number of tools modern systems demand these days, the observability pipeline becomes just as essential to the operations of a service as the CI/CD pipeline.
Barely a month went by without some news story breaking of dozens, or even hundreds, of millions of records being accessed by bad actors.
This means that if a client wants support, they are forced to use a version that by design will lack new features and productivity, or go to a software release model that forces the client to upgrade every three months or so to maintain supportability.
The third and final reason is the most troubling, leaving clients scrambling to migrate their workloads, and that’s security patching.
Anurag Kahol, CTO and co-founder, Bitglass: Cloud database disasters: Misconfigurations of cloud databases will continue to plague enterprises around the world and will be a leading cause of data breaches in 2020.
Tomer Shiran, CEO and co-founder, Dremio: Goodbye performance benchmarks, hello efficiency benchmarks: Escalating public cloud costs have forced enterprises to re-prioritize the evaluation criteria for their cloud services, with higher efficiency and lower costs now front and center.
This has created a new battleground where cloud services are competing on the dimension of service efficiency to achieve the lowest cost per compute, and 2020 will see that battle heat up.
It is common to have these kinds of problems whenA particular SQL query is slowThe SQL database server is downExternal HTTP web service calls are failingNoisy neighbors within the cloud are causing issuesCode-Level Performance ProfilingIf you wish to know why your application is slow, throwing errors, or has bugs, you’ve got to get right down to the code level.
Deciding why it doesn’t work is tough, and generally very exhausting.
Application Log DataWhenever anything goes wrong in production, the first thing you’ll hear a developer say is "send me the logs."
Application ErrorsThe last thing we need is for a user to contact us and tell us that our application is giving them an error or simply blowing up.
Errors are the primary line of defense for locating application issues.
We want to find and fix the errors, or at least understand them, before our customers call to inform us — most of them won’t even call to inform you.
We would suggest fixing alerts for brand new exceptions as well as monitoring overall error rates.
Any time you are preparing for production, you should be watching your error dashboards to see if any new issues have arisen.
Odds are, you’ll realize some sort of new error that you will then quickly hotfix.
A straightforward JavaScript error or slow loading file might utterly disfigure your application.
Application Services – Application functionality is exposed through lightweight, REST style services that expose JSON.
The Message Broker also decouples these services, allowing the Integration Services to be taken offline for maintenance while the Nanotrader application remains online.4.
Static resources are served from vFabric Web Server.B) The Application Services leverage the enterprise proven Spring Framework to expose REST style services to the browser.
The framework also choreographs multi-step transaction with SQLFire.E) SQLFire provides a low latency, highly scalable in-memory database.
APM’s lightweight code agents provide code level diagnostic information allowing teams to quickly identify and correct problematic code.
The following table indicates the key challenges:CategoryDescriptionTechnologyFull-scale environment is not available for performance testingProcessLimited time available for environment setup, load test script and data setup to execute full-scale load testProcessUnstable builds: Since development and testing activities go simultaneously break-off time is needed within a Sprint.
It is possible to detect anomalies, draw patterns from sprint level test results, and find defects using ML techniques.
Continuous Performance Testing StrategyIt has been observed that 70-80% of performance defects actually do not require a full-scale performance testing in a performance lab.
These defects can be found using a signal user or small scale load test in the lower environment if a good APM tool is used to profile and record execution statistics at each stage of SDLC.
The target of this shift left approach is to find and fix all code related performance defects as development is in process.
These should be lower of insolation or component level test.
Performance analysis, data trend, abnormally detection, and cutting down performance analysis by more than 50% if we use machine learning algorithm.
In Agile methodology, since full-scale performance test is not done all the time in a performance lab, production performance testing is highly critical to make sure the system can handle user traffic without any issues.
The percentage of the user set is increased gradually if there are no issuesIf there are issues found, either we do not increase the load further or new features will be pulled back.
Stop the test if real user experience degrades below the predefined threshold.
Need for detailed triage meeting in WAR rooms is minimized by tracing the sessions, pinpointing exact issuesConsistent metrics to analyze performance issues across the environment and it is accessible to development, testing teams, business users all the time.
The following diagram depicts how APM tools help in every Agile lifecycle stage:Performance Risk AssessmentUser story performance risk assessment not only helps identify key user stories which should be performance tested but also helps decide which user stories can be given lower priority.
The following table indicates the parameters that decide the probability and impact of failure.
The higher the risk factor, the higher the priority:Probability/ImpactParameterUser Story # 1 User Story # 2 Factor#1Degree of change from the previous sprint11Factor#2Stringent Performance SLA54Factor#3No.
of Users impacted43Factor#4Public Visibility and impact on the brand43Weighted ImpactImpact of Failure42Risk FactorRisk Factor208Right SkillsetIn Agile manifesto, performance engineers need to be a good developers with data science tools expertise to apply ML and automate tasks, analyze, and derive useful insights from data.
This application is managing a list of employees and exposes REST APIs for adding an employee and retrieving existing employees.
That’s actually harder to achieve than it looks, and it’s also critical to understand the current performance profile of the application.
Simply put, the problem with using += to append Strings is that it will cause an allocation of a new String with every new operation.
Avoid RecursionRecursive code logic leading to StackOverFlowError is another common scenario in Java applications.
Avoid Creating and Destroying Too Many ThreadsCreating and disposing of threads is a common cause of performance issues on the JVM, as thread objects are relatively heavy to create and destroy.
The first step is to determine predictable memory requirements by answering following questions:How many different applications we are planning to deploy to a single JVM process, e.g., the number of EAR files, WAR files, jar files, etc.?
These numbers are difficult to estimate without some real-world testing.
However, the current generation of garbage collectors has mostly solved that issue and, with proper tuning and sizing, can lead to having no noticeable collection cycles.
JDBC PerformanceRelational databases are another common performance problem in typical Java applications.
Architectural ImprovementsCachingMemory prices are low and getting lower, and retrieving data from disk or via a network is still expensive.
Scaling OutNo matter how much hardware we throw at a single instance, at some point that won’t be enough.
How Containers Helped New Relic to Scale [Webinar]We’re not afraid to admit it.
Our success was killing us.New Relic offers great SaaS-based APM software, but we were struggling to provide fast response time as we grew to serve trillions of event queries a day.
The other was a Java data collection pipeline.Over time, however, as our customer base grew, as our feature set grew, and as our company grew, we started to experience the kinds of problems familiar to many growing companies.
Service quality suffered because of communication problems.
As Karl Matthias and Sean P. Kane write in their book Docker: Up & Running, Shipping Reliable Containers in Production:It is hard and often expensive to get communication and processes right between teams of people, even in smaller organizations.
Docker SwarmSwarm is Docker's answer to a developer's problem of how to orchestrate and schedule containers across many servers.
Link: https://aws.amazon.com/ecs/Cost: Amazon ECS comes at no additional cost.
Pay only for the AWS resources (e.g.
Link: https://azure.microsoft.com/en-us/services/container-service/Cost: Pay only for the virtual machines, and associated storage and networking resources used.
It can also act as a container orchestration tool to provide fault recovery for containerized workloads.
Marathon automatically handles hardware or software failures and ensures that an application is "always on."
BuddyBuild, test, and deploy apps in no time.
The platform requires no installation, configuration or server maintenance and it integrates seamlessly with BitBucket, Heroku, GitHub, and others to automate code building, testing, and deployment using Docker containers.
Machine learning analytics also means the quick discovery and future prediction of threats and anomalies before they can become an issue and affect end-users.
New RelicAn industry leader, New Relic is a pure SaaS-based performance management solution which allows developers to diagnose and fix application performance problems in real-time.
The tool generates and collects container metrics such as network statistics, resource isolation parameters, and a complete history of resource usage.
ClairClair is an open source project designed to identify and analyze vulnerabilities in Docker and appc application containers.
Clair regularly ingests container vulnerability metadata from a customized and configured group of sources in order to identify threats in container images, including those upstream.
Link: https://www.aquasec.com/Cost: Pricing is a combination of selected software plan charges plus Azure infrastructure costs for the necessary virtual machines36.
Highlight and cross-correlate problems.
Reroute users and then fix the problem.
Access, evaluate, and fix without the building being on fire.
Manage the impact of failure.
Know how to identify a dynamic system to correlate what the problem may be before it affects the customer.
One day, you’ll be able to put APM on auto-pilot to identify the root cause of the problem and proceed to fix the problem.
For example, it might not be immediately clear that the application usage is higher on weekends and evenings and lower during the night (or vice versa!).
No matter how fast a DB is, keeping things in memory will be faster.
No matter what caching technology you use, it will eventually need to read from the disk or write something to the disk.
Go for the parts that perform badly.
Monitoring and triaging this type of applications is difficult and orchestration tools only provide rudimentary health monitoring (mainly to identify failed nodes and scale them up).
Deploying Kafka With the ELK StackFollowing a production incident, and precisely when you need them the most, logs can suddenly surge and overwhelm your logging infrastructure.
We cannot afford to have our logging infrastructure fail at the exact point in time when it is most required.
This missing group made sense, because they most likely exhibit in their own industries, such as in gaming conventions, instead of coming to a generalized cloud computing event.
Monitoring is one of OpsGenie’s critical internal business continuity operations.
If you implement your system on AWS Lambda and want to monitor your Lambda functions, almost no monitoring tool (at least for Java) tells you what is going on in your Lambda function.
; it doesn’t expose internal metrics.
You still have no idea what is going on inside.
But then you have seen that some Lambda invocations take a few minutes and some unlucky ones fail with timeout.
Here, the only data you have is the invocation time and you have no idea which part of the code during invocation takes most of the time.
Then what is the next part to blame?
This is one of the worst cases you might be faced with for your Lambda function in the production environment.
Here, the problem might be about regex operation which causes catastrophic backtracking for some problematic regex and input texts but you have no metric to verify that is the problem.
In this case, you should be a magician developer to find the problem by guessing with static code analyze.
But before we go to production with our new Lambda architecture, the problem mentioned above was one of the key challenges we needed to handle.
Currently, we are using both of NewRelic Insights and ElasticSearch for similar requirements because they have advantages and disadvantages over each other.
But then we switched to TCP-level communication via SSL sockets (based on Netty) in GELF format due to performance overhead of application layer (HTTPS) communication and intermittent SSL handshake errors when using HTTPS.
You may think that since we can see the latency of the Repository layer via AWS XRay, DynamoDB metrics are not the problem; it is obvious that the normalization is the problematic one.
You need concrete metrics to blame a specific part of your system.
Method arguments, return values and errors thrown from a method can be audited.
This is required, especially for AWS Lambda environments, because the agent cannot be specified as a VM argument to a Lambda function.
GC Stats: Minor/Major GC counts/durations, GC CPU percentages, etc …Thread Stats: Current thread count, peak thread count, total created thread count, metrics (CPU/user time, blocked time, waited time) of the most CPU-intensive threads.
No matter how many servers you have, you can use this to know if your web application was online and available.
After some troubleshooting, you may find that your IIS Application Pool is stopped for some reason, causing your site to be offline.
Sometimes, an IIS Application Pool will crash and stop due to various fatal application errors, issues with the user the app pool is running under, bad configurations, or other random problems.
It is possible to get it into a state where it won’t start at all due to these type of problems.
One weird thing about app pools is they can be set to “Started” but may not actually be running as w3wp.exe if there is no traffic to your application.
In these scenarios, w3wp.exe may not be running, but there is no actual problem.
ASP.NET Error Rate Counters.NET CLR Exceptions – # of Exceps Thrown: This counter allows you track all .NET exceptions that are thrown even if they are handled and thrown away.
A very high rate of exceptions can cause hidden performance problems.
ASP.NET Applications – Errors Unhandled During Execution/sec: The number of unhandled exceptions that may have impacted your users.
ASP.NET Applications – Errors Total/Sec: Number of errors during compilations, pre-processing and execution.
This may catch some types of errors that other Exception counts don’t include.
Note: Some Windows Performance Counters are difficult to monitor because of the process name or ID changes constantly.
Advanced error monitoring and tracking.
Truthfully, I've done a poor job keeping up with new trends in development and the Software Development Life Cycle (SDLC).
Setting up IDEs was a challenge, application packaging/deployment was a wild “artform,” and a lot of things could (and did) go wrong.
Gone are the days of hunting jars/dependencies to drop into a project to try to compile it.
And gone are the days of dependency hell in large projects when components needed different versions of the same package.
“A backing service,” according to the Twelve-Factor App, “is any service the app consumes over the network as part of its normal operation.”“The code for a twelve-factor app makes no distinction between local and third-party services,” says the Twelve-Factor App.
I remember dealing with sticky sessions and load balancing … it now seems like so much wasted time.
If a production server fell out of rotation, our system faced considerable risk.
To aggravate the situation, major components of the applications were stateful and kept (in memory) user information and state, so any server or process shutdown—graceful or not—caused disruptions for our users.
How New Relic can help with this factor:As with factor #8, you can use New Relic Insights to track instances and processes as they’re created and killed.
Let’s put it this way: If you wanted to call this a software crime, I’d have to plead “guilty as charged.”According to the Twelve-Factor App, the lack of parity can manifest itself as three types of gaps: time, personal, and tools.
[They] have no fixed beginning or end, but flow continuously as long as the app is operating.”Today, it’s virtually impossible to think about running production-grade apps without it.
Sometimes I wonder how much time I wasted maintaining all of that code.
Unfortunately, even today, there are still applications and technologies that can’t be instrumented and instead rely heavily on logs for monitoring.
These units are independent of each other, so the application can scale by adding more units (called also “instances”).The XAP PU is the unit of scalability and failover.
The PU supports Java , .Net, and C++.
In such a case , XAP will orchestrate the PUs provisioning to have the right deploy, healing, and scaling order.Separation of Concerns XAP PU promotes developing and deploying applications composed of autonomous, self-contained units—very similar to microservice architecture.
The PU concept provides functional separation of concern, where the deployed PUs are mostly independent, each having its own lifecycle scaling behavior, while exposing well known public interface via remote methods or a web service.As microservices, the PUs are fine-grained units of execution.
For example, a PU may encapsulate a web service (Web PU) that exposes a well-defined functionality with a well-known entry point, packaged as one unit of execution.
It may also have a space instance (in-memory data grid node); this will make it a stateful PU, where it can preserve its state.
A stateful PU is usually deployed with a backup instance, which makes it highly-available to survive failures without any data loss.
A PU as a microservice should adhere to being an atomic business entity, which must implement everything to achieve the desired business functionality.
For optimized cross microservices communication, a map-reduce approach should be used.Cloud DeploymentAn application deployed on a cloud platform, when handling a cloud instance failure, produces higher operational costs and system design complexities.
This often leads to a general lack of confidence in cloud platforms, and eventual disagreement among the technical team when considering the application for cloud deployment.
XAP inherently delivers this functionality as each PU includes all the third-party components and resources it needs to be self-sufficient.
This allows XAP to deliver continuous availability with zero downtime for the deployed PU where Cloudify handles the full automation of the PU lifecycle, leading to true consistency, durability, and scalability.CommunicationMicroservices communicate with each other through language and platform-agnostic application programming interfaces (APIs).
This could be a web PU or a remote Space service that is acting as a front-end to the users.
Stateless PUs can hold a local cache or local view that hold a copy of a state where its golden copy is preserved within another stateful PU.
A stateless PU is usually deployed using a cluster topology without backups, as there is no state to maintain.
Stateful PUs would be any PU that embeds a Space component, usually deployed with primary-backup cluster topology.
These are usually collocated with each microservice instance (stateful PU) for maximum performance and minimal network utilization, which avoids expensive serialization.Proactive processing usually performs map-reduce across the entire data grid nodes.
This allows users to collocate different objects to reside within the same partition, avoiding extra remote calls when accessing multiple related space objects and the usage of distributed transactions.Partitioning is fundamental to any stateful system and it’s the primary reason why XAP provides scalability when it comes to data access and data processing.
The PU supports this via hot deploy process.
When this process completes, all the instances of a deployed PU will be refreshed running the latest business logic.Global HTTP SessionMicroservices is about breaking the application into manageable self-sufficient services.
With large scale web applications, users often get to the point where their monolithic application has broken down into multiple logical components as the effort to test the entire web application with every change is prohibitive.For Java application (J2E), this means multiple different ear or war files that need to be deployed where all need to share the same HTTP session.
XAP, as a JVM-based technology, offers its own container, called GSC, to act as the runtime environment of the deployed PU.
XAP GSC does not include the operating system, but the PU lifecycle management infrastructure.
XAP PU can have unit tests developed that will mimic any data grid cluster topology leveraging the same pu.xml used when deploying the PU into production environment.The IPUC for Java users has been designed allow users running their PU within their IDE and also to be used with Unit Tests.
This allows users to simulate a complete PU (specific PU instance or a complete cluster) to be provisioned within the single JVM.
When testing microservices-based applications, this makes the Unit testing very simple.The XAP Maven Plugin provides similar functionality as it allows users to build, deploy, and run a PU.GatewayWith microservices, an API gateway is used to orchestrate the cross-functional microservices that may reduce round trips for web applications.API gateways are primarily used to Insulate the clients from how the application is partitioned into microservices and to Insulate the clients from the problem of determining the locations of service instances.
Fewer requests also means less overhead and improves the user experience.With XAP, an API gateway will be implemented as a web PU exposing Rest API deployed into XAP.
The web PU can scale dynamically to increase/decrease its capacity in runtime without any downtime.Best-of-BreedMicroservices compels developers to choose best-of-breed languages, frameworks, and tools to write parts of applications.
The hot deploy utility will allow you to refresh existing running PU that includes your business logic with a new version without un-deploying the PU.
For a stateful PU this is critical as it allows you to update any collocated business logic (i.e.
This write-behind activity would allow the application to operate if the underlying database is slow or even if the database failed or is under maintenance activity.
XAP supports this capability both for stateful and stateless PU.High AvailabilityWith microservices, the culture and design principles should embrace failure and faults, similar to anti-fragile systems.
Upon failure of a data grid node, a new PU instance running the missing data grid node will be provisioned, keeping the capacity of the deployed stateful PU intact.Semantic MonitoringSemantic monitoring can provide an early warning system of something going wrong that triggers development teams to follow up and investigate.
This includes the grid and its hosting machines, the PU, and its stateful (space) or stateless artefacts.
This provides a simple resolution path when problems are identified.
You may also push metrics available into external monitoring tools such as CA APM to enterprise-level monitoring.ConsistencyMicroservice architectures emphasize transaction-less coordination between services, with explicit recognition that consistency may only be eventual consistency and problems are dealt with by compensating operations.
Many of these are classic concepts that do not require special changes to the regular development and deployment cycle you may have.
Customer responsivenessLimited knowledge wasteFeedback loopExamples of Issue-Tracking Tools:Atlassian’s JiraJira is a proprietary issue-tracking product developed by Atlassian that allows bug tracking and Agile project management.
It focuses on query-based issue searching — with autocompleting, manipulating issues in batches, customizing the set of issue attributes, and creating custom workflows.
Asset controlLimit transportation wasteEmpower teamsExamples of SCM Tools:GitGit is a distributed version-control system for tracking changes in source code during software development.
JFrog offers high availability, replication, disaster recovery, scalability, and works with many on-prem and cloud-storage offerings.
Fast feedbackReduce defect waste and waiting wasteExamples of CI Tools:JenkinsJenkins is a free and open-source automation server.
With AWS CodePipeline, you only pay for what you use.
There are no upfront fees or long-term commitments.
It also performs monitoring, failure recovery, and software updates with zero-to-minimal downtime.
It has to handle failures by doing automatic failovers, and it needs to be able to scale containers when there’s too much data to process/compute for a single instance.
ZooKeeper helps Marathon to look up the address of the Mesos master — multiple instances are available to handle failure.
The Kubernetes scheduler’s task is to watch for pods having an empty PodSpec.
Fast recoveryResponsivenessTransparencyLimited human involvement during incidentsExamples of Monitoring/Logging Tools:ELK StackThe ELK Stack is a collection of three open-source products — Elasticsearch, Logstash, and Kibana.
The nodes expose these over the endpoints that the Prometheus server scrapes.
Reduce knowledge wasteIncrease new-hire productivityLimit repeat mistakesExamples of Knowledge-Sharing Tools:GitHub PagesGitHub Pages is a static site-hosting service that takes HTML, CSS, and JavaScript files straight from a repository on GitHub, optionally runs the files through a build process, and publishes a website.
change is feared throughout most organizations of any type, so the adoption of new methodologies can quite challenging.
nowadays, businesses are expected to quickly deliver flawless applications that focus on user experience, but without the right tools, applications, and behavior, this seemingly simple task can turn into a complicated mess.
ultimately, faulty delivery translates into missed business opportunities.
paas offerings may require little to no client-side hosting expertise and include preconfigured features in simple frameworks.
examples of vendors and tools:   heroku  ,   google app engine  ,   aws elastic beanstalk    saas   : if you’re familiar with google and facebook, you’ve already been exposed to software as a service (saas).
if problems are brought to light in testing or qa, the software has to be recoded or go even further back in the development process.
testing is implemented early on and often so that developers can fix problems and make adjustments while they build, providing better control over their projects and reducing a lot of the risks associated with the waterfall methodology.
integration and delivery   continuous integration (ci)   – developers integrate code into a shared repository multiple times a day and each isolated change to the code is tested immediately in order to detect and prevent integration problems.
docker  containers linux containers are lightweight virtualization components that run isolated application workloads.
examples of vendors and tools:   github  ,   bitbucket  ,   jfrog  ,   artifactory    github   bug tracking a bug tracker is a system that aggregates and reports software bugs and defects.
examples of vendors and tools:   new relic  ,   appdynamics  ,   datadog   infrastructure monitoring –  tools in this category automatically detect and alert about degradations in underlying physical or virtual resource performance and availability.
Here's more of what they told us about big data:Michael Berthold, Founder and CEO, KNIMEBecause of the significant shortage of data science experts, we’ll see a surge in adoption of easy-to-use analytic applications.
Harnessing the continually evolving power of machine learning, predictive analytics can help organizations forecast failure, change, and bottlenecks in all manner of applications from products to the factory floor.
This involves treating security logs and edge device sensor data to the standard big data analysis rigamarole in order to derive insights about network attacks and penetrations.
In the ERP space specifically, the focus will be the automation of repetitive business processes.
Business users and data scientists are not going to be content with waiting for constrained resources to provide access and analytics.
As a result, companies will have to abandon smaller, strictly regulated markets as they strive to understand the technology needs and requirements to support these new regulations.
These enterprises aren’t getting the insight they need from data, and as a consequence are missing business opportunities, while also exposing their organizations to the risk of security breaches.
They’re letting 60-80% of it drop on the floor because they can’t handle the volume, the number of new data sources, or the speed at which incoming data is growing.
Unfortunately, statements referring to the past are not often completely correct.
It is not scientific research, and not much information is available overall—so a lot of important information may still be missed.
Deploying across multiple machines was more difficult, and the cost of rollback was significant, especially for Commercial Off-The-Shelf (COTS) software that might be deployed by hundreds or even thousands of customers.
"Fix-it-later was a viable approach in the 1970s, but today, the original premises no longer hold - and fix-it-later is archaic and dangerous.
The original premises were:Performance problems are rare.
Isolation of systems inside a larger pool of resources.
First of all, there is no instrumentation on the OS level, and even resource monitoring becomes less reliable due to the virtualization layer.
Second, systems are not completely isolated from the performance point of view, and they could impact each other, and we mostly have multi-user interactive workloads, which are difficult to predict and manage.
What Lies Ahead?
If I click on a search result from Google and it doesn’t load within a couple seconds, I will no doubt just click on a different result.
We also know that premature optimization is a bad thing.
Optimizing Image UsageI struggle with this problem a lot.
Lastly, if you have pages that are long with a lot of images, you may want to consider lazy loading your images.
Optimizing Server PerformanceNo matter which programming language you are using, there are several common issues that cause performance problems.
Many performance problems are due to slow SQL queries, hidden errors, and other issues.
Identify Top Application ErrorsYour application is probably throwing all sorts of exceptions that you don’t even know about.
This is a common problem in code that does a lot of data parsing in “try-catch” blocks and just ignores any exceptions being thrown.
That’s why more service providers are evolving to adopt big  data network analytics platforms that can perform more accurate anomaly  detection for DDoS attacks, as well as provide in-depth analytics  capabilities.
There are two factors to why big data is more accurate in detecting DDoS  attacks.
In  traditional, appliance-based systems, you have to make a number of analytical  compromises due to limited resources.
So let’s say a host IP  is being hit by a DDoS attack, but it’s coming in via multiple routers, then  instead of seeing a large bump of network-wide traffic going to that host, the  appliance detection algorithm will see a small bump of traffic across several  routers—all of which may not trigger any alert.
A big data approach doesn’t have the computing constraints, so it can  always look at network-wide traffic, and so it will naturally notice attacks  that would otherwise get missed.
This means that instead of having a static set of IP addresses that  you’re baselining (or worse a big set of them that where accuracy is diluted  heavily by averaging across the set), the system continuously adjust the set of  IPs are “interesting” based on how much total traffic they’re receiving within  a given segment of time.
DZone:  What are “best practices” for reducing the  incidence of, and the harm caused by, DDoS attacks?AF:  Well, first I  think it’s important to say that DDoS is something that you can’t really  prevent, since it’s an attack from outside of your network that can be coming  from a variety of motivations.
So that  means that in terms of reducing the harm, the first step is to be educated.
Many IT leaders are still coming to grips with the fact that  DDoS is an industrialized threat, that it is trivial to purchase DDoS attacks  from a broad variety of dark providers.
A huge number of businesses report being attacked multiple times.
Yet, many IT teams are still attempting to  deal with DDoS using inappropriate tools such as firewalls or Intrusion  Prevention Systems (IPS) which utilize stateful tracking of connections and  therefore are susceptible to resource exhaustion when faced with volumetric  attacks.
And third, something that our customers have been focusing  on more recently - especially after the recent wave of Mirai-based IoT attacks  - is to focus inwards and detect and adapt to attacks originating from your  network.
They consume time, resource,  and impact performance, and with more modern big data-based approaches, it’s  finally possible to find them - even with hundreds of thousands or millions of  locally infected nodes.
And - if you threaten their infrastructure  they’ll rate-limit or shut you off, usually without consultation.
And also ask and understand what happens if  you are attacked while on their infrastructure.
Developers have had cloud-friendly  application performance management (APM) tools for a while now, but network  performance monitoring has been stuck in the pre-cloud era.
From a DDoS protection  perspective, I’m thinking about a service provider that started using our  analytics platform and noticed that they were seeing many attacks that their  legacy DDoS protection platform wasn’t catching.
That meant that customers didn’t have to  experience severe service impacts from attack traffic, and their end users got  improved service quality, and engineers could sleep at night.
I have faced several problems while trying to use the Debian package and would not recommend that method.
./bin/elasticsearchYou journey to this point should have been smooth, but in case you see errors related to the permissions and a stack trace similar to below:org.elasticsearch.bootstrap.StartupException: java.lang.RuntimeException: can not run elasticsearch as rootCheck to make sure you aren't using sudo to start Elastic, which won't work.
To retrieve data about a specific bean, you can add parameters to the URL that represent the name of the bean and attribute you want:http://localhost:8080/manager/jmxproxy/?get=java.lang:type=Memory&att=HeapMemoryUsageOverall, this tool can be useful for a quick check, but it’s limited and unreliable; therefore, it’s not recommended for a production instance.
When the system can no longer create new objects due to lack of memory, the JVM will throw an exception.
Also, you should note that a constant increase in memory usage without a corresponding activity level is indicative of a memory leak.
Generally, it’s difficult to set a minimum absolute value for the available memory, and you should instead base it on observing the trends of your particular application.
The maxTime attribute shows the longest time to process a request, while processingTime represents the total time for processing all the requests:The disadvantage of viewing this MBean directly is that it includes all the requests made to the server.
To isolate the HTTP requests, you can check out the “HTTP hits per minute” graph of the JavaMelody interface.
In order to use both JavaMelody and Prefix, you have to disable the gzip compression of the JavaMelody monitoring reports, to avoid encoded everything twice.
In case there is a SQL error, Prefix will show you this as well.
For example, if someone attempts to add a user with an existing email address, this causes a primary key constraint violation:The tool shows the SQL error message, as well as the script that caused it.
Error RatesErrors are a sign that your application is not performing as expected, so it’s important to monitor the rate at which they occur.
Let’s introduce an error in the example application by writing an incorrect name for the JNDI data source, and see how the performance tools behave.
JavaMelody provides a “% of HTTP errors” chart which shows what percentage of requests at a given time resulted in an error:This shows you that an error occurred, but it’s not very helpful in identifying the error.
For this latter purpose, you can turn to Prefix again – which highlights HTTP requests that ended with an error code:If you select this request, Prefix will display details regarding the endpoint that was accessed and the error encountered:According to this, the error happened when accessing the /users endpoint, and the cause is “MyyyDataSource is not bound in this context,” meaning the JNDI data source with the incorrect name was not found.
This can refer to ensuring that the application responds to requests without significant delays, in addition to identifying any potential errors or memory leaks in your code.
Heartbeat (beta) was introduced in Elastic Stack 5.0 back in October and is meant for “uptime monitoring.” In essence, what Heartbeat does is probe services to check if they are reachable or not — it’s useful, for example, to verify that the service uptime complies with your SLA.
And the data itself:Note: YAML files are notorious for being extremely syntax-sensitive.
Same note as before: YAML files are notorious for being extremely syntax-sensitive.
SummaryIf you are using an advanced system monitoring and alerting tool such as Nagios, you most likely have no immediate use for Heartbeat.
What we see it that the logging methods, like an error, warn, info all call to a method forcedLog which gets the logging level and the logging message.
firstAfterBody is called right after the method body is performed, so in case of a timer sensor, we could simply stop the timer here again.
But an even simpler approach is to just ignore these two methods, and only go for the secondAfterBody.
We are expecting the// method: Priority.forcedLog (String, Priority, Object, Throwable)      String level = parameters[1].toString();      if (!checker.shouldCapture(level)) {        return;      }      try {        LoggingData data = new LoggingData();        data.setLevel(level);        data.setMessage(parameters[2].toString());        data.setPlatformIdent(idManager.getPlatformId());        data.setSensorTypeIdent(idManager.getRegisteredSensorTypeId(sensorTypeId));        data.setMethodIdent(idManager.getRegisteredMethodId(methodId));        data.setTimeStamp(new Timestamp(System.currentTimeMillis()));        coreService.addMethodSensorData(sensorTypeId, methodId, null, data);      } catch (IdNotAvailableException e) {        if (LOG.isDebugEnabled()) {          LOG.debug("Could not save the timer data because of an unavailable id. "
We check if we need to capture and return if the level is too low.
Before firing up this agent with a sample application, lets have a brief look at the LoggingData class.package info.novatec.inspectit.communication.data;import info.novatec.inspectit.communication.MethodSensorData;public class LoggingData extends MethodSensorData {private static final long serialVersionUID = 6428356462914363539L;private String level;private String message;// getters and setters removed        // hashcode removed        // equals removed@Overridepublic String toString() {return "LoggingData [level=" + level + ", message=" + message + "]";}}One thing to note is that this class needs to be put into the Commons project.
To try this out I simply place loggings into the Calculator:// Constructorpublic Calculator() { Logger log4JLogger = Logger.getLogger("My Sample Logger"); BasicConfigurator.configure(); log4JLogger.error("some error string"); log4JLogger.debug("some debug string");[...]}I set the configuration to capture only loggings of level INFO and highermethod-sensor-type log4jlogger info.novatec.inspectit.agent.sensor.method.logging.Log4JLoggingSensor MIN minlevel=INFOsensor log4jlogger org.apache.log4j.Category forcedLogSo I expect to see the first logging of the calculator, but not the second logging.
As this level is lower than INFO, the hook does not capture the information.
Host: 127.0.0.1 Port: 10992015-02-20 15:36:03,275: [inspectIT] 917    [           main] INFO  nection.impl.KryoNetConnection - KryoNet: Connecting to 127.0.0.1:10992015-02-20 15:36:03,315: [inspectIT] 957    [           main] INFO  izer.schema.ClassSchemaManager - ||-Class Schema Manager started..00:00  INFO: Connecting: /127.0.0.1:10992015-02-20 15:36:04,466: [inspectIT] 2108   [           main] INFO  nection.impl.KryoNetConnection - KryoNet: Connection to the server failed.
Calling the server for an identifier for a method would block the agent quite some time and thus impose overhead into the application.
Thus, if your stack is not Java only, I guess it’s hard to consider inspectIT as the option as some parts of your system would be untraceable, thus losing the complete end-to-end picture that we aim for.
More DataHowever, soon you will discover that with no additional hacking, the data provided by both tools is not enough for meaningful performance diagnosis.
inspectIT can solve this problem in no time, because it offers a UI-based configuration interface that allows you to quickly bound measurement points to any Java method, thus you will get the duration of the method executions together with your tracing data.
The sampling rate approach is based on the Google Dapper paper that concludes that if a problem exists in a system with high-throughput, then the same problem will surface multiple times and would be part of one of the captured traces: "New Dapper users often wonder if low sampling probabilities – often as low as 0.01% for high-traffic services – will interfere with their analyses.
Our experience at Google leads us to believe that, for high-throughput services, aggressive sampling does not hinder most important analyses.
inspectIT, at the moment, does not provide the sampling rate feature; it collects all the traces and is thus more suitable for services with lower volume.
Anyhow, both tools have nothing less than a sunny future.
The 6 Most Common Performance Testing Mistakes, and How to Fix ThemAs a performance testing consultant for the last 10 years, you could say that it's second nature for me to look for performance patterns.
One of the patterns I have observed over my career is that, regardless of the project size, company, or the technology being used, the same types of performance testing mistakes get made over and over and over.
Unfortunately, however, those shortcuts can lead to costly performance testing mistakes and oversights.
Inadequate User Think Time in ScriptsHitting your application with hundreds or thousands of requests per second without any think time should only be used in rare cases where you need to simulate this type of behavior – like a Denial of Service attack.
What are the transactions that have a high business cost if they were to fail under load?
Setting Up Inadequate Infrastructure MonitoringYour execution results like throughput, transaction response times, and error information aren’t overly helpful unless you can see how your target infrastructure is coping with the scenario.
It's a common problem — I have heard many testers ask why their response times are taking minutes instead of seconds.
The problem can lie either in the load generation or the target application infrastructure.
How do you solve this problem?
This enables you to view system resource utilization while running your tests, ensuring that no bottlenecks are present on the load generation side.
Using Hard Coded Data in Every RequestUsing the same data in your HTTP request for every user is not a realistic usage scenario.
Here’s common scenario:A load test runs with a target number of users, and the tester see that response times and error rates are within acceptable ranges.
Expected throughput however, is lower than anticipated.
How can this be when your load testing platform is reporting very few transaction related errors?
Once you have a test with transaction response times, error rates, and throughput rates all in the expected zone, are you in the clear?
This might not be much of an issue with a single transaction, but multiply this over 1,000 concurrent users and it can severely impact your system's responsiveness.
To pinpoint if you are overloading a load injector, look for error logging, out of memory exceptions, and CPU or network utilization stats.
One of the patterns I have observed over my career is that, regardless of the project size, company, or the technology being used, the same types of performance testing mistakes get made over, and over, and over.
Unfortunately, however, those short-cuts can lead to costly performance testing mistakes and oversights.
Inadequate User Think Time in ScriptsHitting your application with hundreds or thousands of requests per second without any think time should only be used in rare cases where you need to simulate this type of behavior (perhaps a Denial of Service attack?).
What are the transactions that have a high business cost if they were to fail under load?
Setting Up Inadequate Infrastructure MonitoringLoad generation isn't the only important part of a performance testing scenario.
The execution results gained from a scenario such as throughput, transaction response times, and error information isn't overly helpful unless you can see how your target infrastructure is actually coping with the scenario.
It's a common problem - I have heard many testers ask why their response times are taking minutes instead of seconds.
The problem can lie either in the load generation or the target application infrastructure.
So how do you solve this problem?
This enables you to view system resource utilization while running your tests, ensuring that no bottlenecks are present on the load generation side.
Ignoring System or Script Errors Even Though Response Times and Throughput May Look FineWhen running a load test - there are several things to keep an eye on to ensure you are running a valid test.
Take the following example that can be seen quite often:A load test runs with a target number of users and the user observes response times and error rates to be in acceptable ranges.
However, expected throughput is lower than expected.
How can this be when Flood is reporting very little transaction related errors?
From the logs, we can see a number of script related issues that are causing the script replay to exit out of a substantial amount of iterations due to script errors.
This will significantly impact your target throughput and is the primary source for observed throughput to be lower than expected.
This might not be much of an issue with a single transaction but multiply this over 1,000 concurrent users and it can severely impact your system's responsiveness.
Overloading Load GeneratorsThe last of our 6 common performance testing mistakes is the overloading of load generators due to one or more of the following:Too many concurrent users on a single load injection node.
error messages in the JMeter execution logs.
Error LoggingErrors are often represented as I/O Exceptions reported in the Test Execution logs similar to the following:o.a.j.p.h.s.HTTPHC4Impl$6: I/O exception (org.apache.http.NoHttpResponseException) caught when processing request to {s}-> https://flooded.io:443: The target server failed to respondo.a.j.p.h.s.HTTPHC4Impl$6: I/O exception (java.net.SocketException) caught when connecting to {s}-> https://flooded.io:443: Broken pipe (Write failed)
How to Monitor Ceph Using Instana [Code Snippet]Many companies use Ceph for their distributed on-prem storage system and it provides a robust platform that eliminates single point of failure scenarios.
When everything is working properly, Ceph is a solid and reliable platform - but what happens when you have a Ceph Cluster outage or poor performance from your Ceph cluster?
Complete Application Monitoring requires the ability to monitor Ceph so that you will be alerted to these performance problems and have the information required to fix issues quickly.
Ceph Performance MonitoringDue to customer demand, Instana has released a Ceph sensor that monitors your clusters and pools, and provides a number of pre-built health signatures to alert you of potential problems with your Ceph system.
com.instana.plugin.ceph:  ceph-executable-path: '' # default path is /usr/bin/ceph Once this is handled, Instana will automatically attach to Ceph, monitor the key indicators, and alert when any health signatures are violated.
Whenever there is a storage related problem you will know exactly what services are impacted.
Limiting the number of visible Applications or Services to those that are relevant to a specified group, ensures engineers are not overwhelmed by the noise of applications and services they're not responsible for.
That does not mean that there is no room for a new startup to disrupt the market.
Grafana Plugin for Instana APMApplication monitoring users are a demanding group.
Using the Instana APM Plugin for GrafanaUsing official plugins in Grafana is dead simple.
this functionality is excellent for quickly identifying and resolving incidents, but what if for some reason a problem occurs that is not automatically identified?
looking for trouble — finding performance problems on your own   the answer is     yes    , it’s easy to use instana’s new application perspectives capabilities to find performance problems on your own.
every new application perspectives screen provides charts and tables that bring the worst performers to the top.
let’s look at an example…   the animation above shows the following sequence:    choose your application perspective to analyze     select a       poor performing service      from the “top services” table      select a       poor performing endpoint      from the “top endpoints” table      select “analyze” to see all the poor performing traces in that endpoint      analyze the slow traces to see why they are slow     in this case, you can see that our payment transactions are waiting on an external payment provider (note: we artificially induced slowness into the external service call to paypal for this example).
Backend storage and processing,fully managed by Instana — no need to waste time deploying, configuring, and managing the backend monitoring systemWe expect as the OpenTracing community evolves and matures there will be a number of applications, libraries, and plugins developed which interact natively with the Jaeger and Zipkin compatible clients.
This will enable our customers to leverage these predefined traces and spans with no extra effort required.
Application Perspectives Advanced Use Cases: Client Feature TestingWith traditional monitoring tools, it's either extremely difficult or impossible to isolate and compare the performance of individual application features as required when introducing new client functionality.
When doing client-side testing in production, developers are challenged with observing the performance of APIs, databases, and other subsystems while trying to distinguish between requests that originate from a specific client and all other requests.
Example Microservice Environment without Application PerspectivesIn most application monitoring tools, a complex set of filters would be required to isolate the performance of your application solely based on an HTTP header of an inbound request.
Instana has made defining and observing such a use case trivial with Application Perspectives.
In Instana, this is trivial when using application perspectives combined with metadata that can be used to determine if our client application is generating requests from the aforementioned feature toggle.
This comes very handy for short-lived jobs which might not be able to pull the data before they are finished and destroyed.
Specifically, for each container it keeps resource isolation parameters, historical resource usage, histograms of complete historical resource usage and network statistics.
Its architecture is the most common one as it consists in several layers:      A REST layer that exposes endpoints for CRUD (Create, Read, Update, Delete) press releases.
As a matter of fact, while the Node Exporter handles lots of server-centric metrics, it may lack this kind of very specific metrics.
But despite its impressive look, this GUI is more or less useless for anything then a single server.
Check that all the services are started and that there are no exceptions raised.
The Press Release REST layer exposes a Swagger interface on the behalf of which you can exercice and test the API.
Once you have exercised for a while the API via the Swagger interface, you probably get tired after a few minutes.
There is also no data collected about the application services being built by Jenkins.
Obviously, this is where you can check to see if any build jobs are failing and investigate why that might be happening.
This high granularity data ensures that you will be able to identify any resource spike that causes problems in your build pipelines.
KaaS relieves you of the burden of building and maintaining a Kubernetes cluster.
Services and their endpoints are automatically discovered and their KPIs automatically calculated: Rate, Errors, and Duration.
Error messages are automatically captured and indexed against each endpoint and rolled up to Service and Application level.
Indexing error messages against each endpoint facilitates immediate access to additional diagnostic context without the need to switch tools.
There is no averaging or sampling as data is presented.
All contextual information, including any error messages, are included in the trace.
Along with this functionality, we delivered a set of curated dashboards that are used to quickly identify any requests that are slow or throwing errors.
You need to understand the concept of traces and calls in order to take full advantage of the data within Instana and solve difficult performance problems.
Troubleshooting Performance Problems — From Application Perspective to AnalyzeApplication Perspectives offers a set of curated dashboards that make it easy for anyone to identify problematic services, endpoints, traces, or calls.
When you've identified a problem you can immediately jump to Analyze to explore the traces and calls in the exact context you drilled down from.
From there you can filter by Application, Service, Endpoint, Type, Technology, Latency, Erroneous, or More - which gives you access to every tag available within Instana.
It's difficult to understand how/where K8s has deployed your environmentIt's difficult to understand how events in Kubernetes are related to the performance and availability of your servicesIt's difficult to know when K8s itself is brokenThis is especially true for the teams that write the applications.
It also alerts you when K8s is unable to deploy components due to resource limitation or other issues.
Instana screenshot showing K8s event and reason for the problem.
These labels are critical to mapping the monitoring data to your actual Kubernetes configurations.
Is Kubernetes Broken?
If any of those components fail you want to be alerted and fix them right away.
What if you have a critical bit of code you want more details on, such as timing, runtime data and error messages?
However, there may be occasions when a little extra insight is needed to optimize a critical code block.
The PHP sensor now has that capability with custom spans, adding the ability to shine a little light into the darkest corners of code execution.
Analyzing the potentially lag inducing endpoint is as simple as clicking on the “Analyze Calls” button on the upper right-hand side, sorting by latency, and digging into the calls which matter - in this case, the slow ones (fig.
When Network Timing Is EverythingWhen observing distributed applications, networking and external service dependencies can become a major contributor to performance degradation.
Beyond the hype of the last couple of years, it must seem to a lot of people that Node.js is dead since not every conference is packed with Node.js talks anymore.
This is not as easy as it might sound, especially looking at source code already written, but also new programming work.
Another common issue occurs when a problem crops up and we begin our analysis.
If we manually added monitoring code, you can bet that some important metrics were forgotten and will need to be added for the next time there is a problem.
That said, Instana, by only adding two lines of code, is able to gather performance metrics, distributed traces, and service dependencies in a fully automatic fashion, no matter if you use MySQL, PostgreSQL, rabbitMQ or anything else.
No changes are required if you are using minikube.
You also know exactly where every transaction is slow or throwing errors and what impact that has on the rest of the system.
Once you’ve arrived at your Application Dependency Map you can change the visualization of each service to represent throughput, error rate, or latency.
Figure 2: Changing the visual representation of services based upon throughput, error rate, or latency.
When you mouse over a service icon you are presented with Total Calls, Error Rate, Avg Latency, and alerts over the selected time period.
Use Case 1: TroubleshootingWhen I’m troubleshooting any problem with an application, the first thing I do is look at the Dependency Map.
Figure 6: Shop service in an alert state showing 1 warning issue.
Baseline the performance and error profiles of each application and service — If performance is poor and the services throw a bunch of errors, you want to make sure you don’t move the same problems from inside your data center out to a cloud computing environment.
Rebuild portions of your applications whose architecture is not clou or microservice-friendly — this will save a lot of pain if planned for ahead of time instead of reacted to after the migration.
Test cloud applications and services using loading patterns gathered from your on-prem installation — fix performance bottlenecks and errors.
Send a portion of your production workload to your new cloud applications and services — testing in production with real users will expose major issues.
secondly, after reinstating the service, they need to figure out and fix the exact root cause, to ensure the problem does not occur again.
it can take hours or days to identify the root cause of an issue, and often, the reason is left unidentified and lurking in the background waiting to reappear.
some example heath signatures are:    high cpu - host sensor     high garbage collection - jvm sensor     high memory usage - docker sensor     unschedulable pod - k8s sensor    the health signatures will trigger an issue with either warning or critical severity.
for every service the kpis of throughput, latency and error rate are monitored (a.k.a.
red - rate, errors, duration).
when the ai detects a potential problem, if the service is not customer facing, an issue is triggered.
automatic root cause identification   when a problem occurs with one particular microservice in an application it will introduce a ripple effect and have an impact on other microservices, which will also affect other microservices, etcetera.
a traditional monitoring solution would just fire off every event to the operator's inbox and leave them to figure it out.
this is a well-known problem and there are other solutions available that attempt to quell these storms.
additionally, the constant change present in a microservices application means that there is the burden of continually trying to keep the aggregation rules synchronized.
at instana we don't like doing tedious repetitive manual configuration, that's why we have stan, our ai-powered assistant, to automate all the boring bits.
from the roll-up of events, it is clear that the elasticsearch datastore became unstable due to the loss of node two, this was the root cause.
now the operator knows that if they fix the elasticsearch problem all the other problems will disappear.
a monitoring solution should decrease the burden, not increase it by firing out a shedload of events since this is just data, not information.
At the moment the agent is not supported on Mac if you are running the application locally via docker-compose.
Is there an endpoint throwing an excessive number of errors?
No user setup is required.
From here, you can troubleshoot the root cause of any slowness or errors.
It started with Jonah Kowall, a former Gartner APM analyst, and today VP of market development at AppDynamics, writing about Misunderstanding "Open Tracing" for the Enterprise.
His criticism is that OpenTracing is not a broadly adopted open standard and explains that Enterprises have highly heterogeneous environments that require automated, agent-based tracing.
Alois Reitbauer, Chief Technology Strategist at dynaTrace, reacts in his blog A CTO's strategy towards OpenTracing that OpenTracing is not a standard and agrees with Jonah about the need for standardization of TraceContext.
In microservices architectures, understanding dynamic dependencies using topology and graph analysis is more difficult than in traditional architecture.
Instana's AI leverages our unique Dynamic Graph which provides the context to pinpoint to the root cause of problems and ultimately arrive at accurate causation.
We predict that there will be no ultimate reason to drive to a single tracing standard.
Inbound proxy architectureProxies and MicroservicesReverse proxies are a critical component of microservices applications.
The ability to trace transactions across multiple function invocations will be the key to understanding where any bottlenecks or errors are occurring.
Instana's AI-powered backend processes all traces as they arrive and automatically calculates the service KPIs: latency, throughput and error rate.
Anomalies in these KPIs will raise an Incident; the manual burden of constantly configuring thresholds and alerts is not required.
Unfortunately, end to end tracing into Lambda is not yet possible due to the lack of a standard on how to pass trace context between different tracing systems.
At Instana, we believe this focus is wrong for a couple of reasons and came to the conclusion that the first point is much more important, especially in highly distributed microservices environments.
Callstacks are used by runtimes of all programming languages and they can usually be seen printed out as “stacktrace” when errors occur, as they allow "tracing back" to what calls led to the error.
For example an error message might say "Apple is not a number".
Combined with the callstack it is possible to narrow down where in a complex system the error actually occurred.
The message alone is usually insufficient, as the NumberUtil algorithm might be used in many places.Thread.run()  HttpFramework.service()    HttpFramework.dispatch()      ShoppingCart.update()        ShoppingCart.updateCart()          ShoppingCart.parseQuantity()            ShoppingCart.convertParams()              NumberUtil.convert()  <-- Error: "Apple is not a number"Developers can use this callstack to trace back from the error to the relevant business method (ShoppingCart.parseQuantity()) to get a better understanding why the error occurred.Distributed TracingWhat happened when service oriented architectures appeared ten years ago, was that the callstack was broken apart.
The "ShoppingCart" logic now might reside on server A, while "NumberUtil" resides on server B.Suddenly, the error trace on server B was only containing the short callstack of the parse error, and on server A a new error was produced, telling that something went wrong on server B, but not stating what the problem was.
Instead of a single error callstack which allowed troubleshooting the problem, one now ended up with two callstacks, with two errors.
Even worse there was no connection between the two, sometimes making it impossible to have access to both at the same time.Server A:Thread.run()  HttpFramework.service()    HttpFramework.dispatch()      ShoppingCart.update()        ShoppingCart.updateCart()          ShoppingCart.parseQuantity()            ShoppingCart.convertParams()              RestClient.invokeConversion() <-- Error: UnknownServer B:Thread.run()  HttpFramework.service()    HttpFramework.dispatch()      NumberUtil.convert()  <-- Error: "Apple is not a number"This is where distributed tracing came into the picture.
The idea behind distributed tracing was to fix this problem by connecting the two error call stacks with each other.
Such a mechanism is usually called "correlation".Thread.run()  HttpFramework.service()    HttpFramework.dispatch()      ShoppingCart.update()        ShoppingCart.updateCart()          ShoppingCart.parseQuantity()            ShoppingCart.convertParams()              RestClient.invokeConversion()                Thread.run()                  HttpFramework.service()                    HttpFramework.dispatch()                      NumberUtil.convert()  <-- Error: "Apple is not a number"Much better: With some added decoration where the remote call actually takes place and on which servers which parts of the callstack were executed, this allows for again finding out that the "ShoppingCart" was the context of the error, and the "NumberUtil" caused the shopping cart activity to fail.Measuring PerformanceWhile the examples so far illustrated error tracing, APM tools use the same mechanism also for taking and presenting performance measurements.
We saw that because of that focus, most customers of existing APM solutions actually do not use the tool for operations and monitoring, but only use it for troubleshooting performance issues or errors.
They are not easy to read for users of the APM tool and do not provide information to correlate performance and availability of a system to an overall health.
This way we understand how a system of services works together and understand the impact to traces and applications if one of the components fails.
In case of errors or slow performance we provide a detailed context, so that all the data needed for troubleshooting this case is available - we don’t collect this data (including callstack) for every trace - as this is a very invasive task that can cause overhead and side effects in the code itself.
Google and Twitter both face highly distributed service architectures and they exactly faced the problems many microservice users have today.
It is either unchangeable runtime code, framework code or third party code.Interaction of Services and Components matters.Most problems can be resolved when the failing component or service has been identified.
While there can be internal errors, service availability and interaction is the most important to monitor.
The underlying data model of an application monitoring and/or analytics tool will either set you up for success or doom you to failure as you manage your cloud, container, and microservices applications.
Example components:Physical: Servers, network adapters, disksVirtual: Containers, processes, VM's, JVM'sLogical: Clusters, transactions, services, applicationsIf your data model is missing components (like containers, data caches, or orchestration tools), you will only have partial information to work with and the conclusions you draw from this data will be incomplete.
If the data is stale (not accurate due to rapid change), you also risk drawing the wrong conclusions which wastes time and money.
Warning, the following information is simplified to keep this blog post relatively short and digestible.
Users connect to the web server with either a dumb or a rich client (web browser), while much of the application logic and processing occurred on the server side with little to no computing on the client side.
You had to account for everything already present in a 3-tier application but then add in the ability to model message buses, API calls, and distributed transactions (requests that flow across multiple services).Scale of typical environment: 100’s – 1000’s of components.Rate of change: Medium (days)At this point, all hell is about to break loose as virtualization technologies are really becoming mainstream and are being applied in various forms including cloud and containers.
Microservices, Containers and Dynamic Application Models: Similar to SOA but broken down into automatically deployed services that are connected via lightweight communications (often RESTful APIs).
We'll discuss why it's not okay to get notifications 5-10 minutes after a problem has started, even though that is still the norm in many organizations.
Ensuring the performance of our applications is critical.
Google discovered that an artificial delay as little as 400ms introduced to their search response would result in users conducting 0.2 to 0.6 percent fewer searches.
This behavior translates into lost revenue for most businesses.
It may seem counter-intuitive but a slow web service has proven to be a more frustrating experience for users than one which is down.
If the system is repeatedly slow, they will often leave frustrated and seek out competitors who can solve their problem without unresponsive or slow responses.
The most common strategy when building systems which can scale is to avoid design patterns which prevent scaling out in the future.
Spending too much time address scaling problems which may or may not happen is considered premature optimization and this should be avoided at all costs.
This requires that monitoring tools identify these services and understand their role in the overall context—without relying on manual tagging.Polyglot: Using the right tool for the problem is one of the paradigms of modern applications.
An understanding of an application’s health implies the need to know how everything works together and the entire surrounding context.The current generation of monitoring tools have no understanding of an environment—they only have the concept of single components with metrics and traces (business transactions) for understanding health and performance.
The interpretation and understanding of how all these components work together to find the root cause of problems is left to the user:Understanding what components are affected by an issue.Understanding what metrics to look at.Interpreting and getting the right information out of the metrics/data.Correlate metrics and data to find the root cause of the problem.Some tools even provide “war rooms” for collaboration as normally whole teams of experts are needed to find the root cause of a problem.As a simple example, a trace could show that the performance of a request was bad because of a slow Elasticsearch query.
A change can be a degradation of health (which we call an “Issue”), a configuration change, a deployment or appearance/disappearance of a process, container or server.To make this a real-world example I will describe how we would model and understand a simple application that uses an Elasticsearch cluster to search for a product using a web interface.
An index is physically structured in shards that are distributed to the ES nodes in the cluster.We add the index to the graph to understand the statistics and health of the index used by applications.To get a little bit further we assume that we access the Elasticsearch index with a simple Spring Boot application.Now the graph includes the Spring Boot application.As our sensor for the Java application will inject some instrumentation for tracing distributed transactions, Instana will automatically “see” if the Spring Boot application accesses an index of Elasticsearch.One the screen above, you can see the transaction including a waterfall chart showing the calls to different services and below the details of one service call to an Elasticsearch index including performance and payload data.We inserted this trace and its relationship to the logical components into the graph and track statistics and health on the different traces.Using this Graph, we can understand different Elasticsearch issues and show how we analyze the impact on the overall service health.Let’s assume that we have two different problems:I/O problem on one host causing read/write on index/shard data being slow.Thread pool in one Elasticsearch node is overloaded so that requests are queued as they cannot be handled until a thread is free.In this case, the Host (1) starts having I/O problems and our health intelligence would set— —the health of the host to yellow and fire an issue to our issue tracker.
As the performance is badly affected by this, our engine marks the status of the node as red.
Now the performance of the product search transactions is effected and our performance health analytics will mark the transaction as yellow (6) which also affects the health of the application (7).As the application and the transaction are effected, our incident will actually fire with a yellow status saying that the product search performance is decreasing and users are affected, showing the path to the two root causes:  the I/O problem and the Threadpool problem.
As seen in the screenshot, Instana will show the evolution of the incident and the user can drill into the components at the time the issue was happening—including the exact historic environment and metrics at that point of time.This shows the unique capabilities of Instana:Combining physical, process and trace information using the Graph to understand their dependenciesIntelligence to understand the health of single components but also the health of clusters, applications, and tracesIntelligent impact analysis to understand if an issue is critical or notShow the root cause of a problem and give actionable information and contextKeeps the history of the graph, its properties, metrics, changes and issues and provide a “timeshift” feature to analyze any given problem with a clear view of the state and dependencies of all componentsFinding root cause in modern environments will only get more challenging in the coming years.
In this article, we'll explore application performance Issue #8, "Performance analysis expertise for new technologies is hard to find making it difficult to troubleshoot problems."
As companies migrate to cloud, container and microservices architectures they are adding new technologies faster than at any point in the history of IT, and it's causing problems.
Poor monitoring coverage and not knowing which KPIs are important.
Longer MTTR due to lack of experience and expertise.
Unfortunately, sometimes complex ideas get oversimplified in translation."
The reality is that most of us don't get to troubleshoot production issues caused by new technology every day (if you do then you're either practicing chaos engineering or your apps are in desperate need of an intervention).
Knowing good or bad states for these KPIs - a simple example of this is "iowait time on Unix systems should not average more than 20% per 5 second interval."
Being able to identify known problems for each technology - such as identifying a split brain condition in Elasticsearch clusters which causes data integrity issues.
Understanding the impact of KPIs on system state - A simple example would be knowing that a disk filling up is undesirable and leads to performance and stability problems.
Unfortunately, many of the characteristics listed above are missing from most monitoring tools that I've used, seen, or heard about.
The incident investigation and problem resolution phases contribute about 1 hour of time in this example.
A 2017 web poll about downtime costs of over 800 companies conducted by Information Technology Intelligence Company (ITIC) showed the following:98% reported that 1 hour of downtime costs at least $100,000 US ($16,665/10 minutes)81% reported that 1 hour of downtime costs at least $300,000 US ($50,000/10 minutes)33% reported that 1 hour of downtime costs at least $1.5 million US ($250K/10 minutes)These are staggering numbers, but not surprising given the ongoing digital transformation and cloud migrations occurring in every company across every sector.
What if you could cut 30-60 minutes off of each incident on average?
If we're all going to use it, what's the problem?
For the rest of us, Kubernetes is really, really intimidating.
The reality is that there is very limited expertise in K8s at this point.
The complexity doesn't have to translate into a painful management experience.
Cloud ProvidersThe usual suspects are present: Amazon Web Services Lambda, Google Cloud Functions, Microsoft Azure Function Apps and recently IBM has entered the space with a hosted version of OpenWhisk.
This level of automation makes moving functions from one provider to another less painful.
However, functions are not truly portable as there is currently not any standard for function entry points, returning data or for the libraries that will be available at runtime.
Event GatewayWhile each cloud provider has their own API Gateway they do not typically provide much convenience for multiple provider solutions nor ease of portability.
The Future for ServerlessIt is still a wild frontier out there with many offerings and no real standards.
Observability vs. MonitoringObservability is a hot topic at the moment, stirring a lot of debate principally around the argument that observability is different than monitoring.
It does not make any sense to continually push out changes without knowing if they make things better or worse.
Without meaningful analysis, you've fallen short of the whole purpose of creating observability and performing monitoring in the first place.
The most difficult form of observability is distributed tracing within and between application services.
Modern application delivery has shifted to CI/CD, containerization, microservices, and polyglot environments creating a new problem for APM vendors and for observability in general.
New software is deployed so quickly, in so many small components, that the production profilers of the SOA generation have trouble keeping pace.
They have trouble identifying and connecting dependencies between microservices, especially at the individual request level.
This strategy MIGHT be acceptable for SOA applications, but is completely unacceptable in the microservices world.
The problem is so pervasive that the Cloud Native Computing Foundation (CNCF) has multiple open source observability projects in either the Incubation or Graduated phase.
These monolithic applications were hard to maintain - changes had to be tightly managed as they were risky with many potential side effects.
As with most super powers, Cloud Native also comes with some nasty side effects: Higher complexity and the pain of cultural change.
Every change has the potential to introduce new errors or performance issues, and these must be identified quickly to repair them either by applying a fix or by rolling back to the last working version of the microservice.
These new challenges require a fresh approach to performance monitoring and problem detection.
Why Traditional Tools Struggle to Monitor Cloud Native ApplicationsTraditional monitoring strategies DO NOT WORK with Cloud Native applications for the following reasons:Manual instrumentation is not efficient   Writing tracing/monitoring as code takes real effort and time - this works against rapid release strategyFrequent CI/CD releases makes it difficult to keep manual monitoring accurate and up to dateHaving to write monitoring as code slows down release cyclesHumans can't keep up with constant change to re-configure monitoringManual health rules rapidly go out of date (deteriorate, stale) - Because of CI/CD, and especially in orchestrated environments (Kubernetes, OpenShift, Mesos, Swarm), the rules break down and no longer correlate properly.
The only problem is that the shift is happening faster than the production tooling can adapt.
Alerts take too long to trigger making the impact on business too costly.
Performance analysis expertise for new technologies is hard to find making it difficult to troubleshoot problemsMonitoring data exists in too many silos, causing inefficiency and errors as the IT organization troubleshoots and optimizes their applications.
We'll go with a low estimate of 5 different infrastructure monitors per host.
It's basically an impossible task and leaves you exposed to the risk of business impact without knowing until customers complain.
Automatic, Continuous Discovery, and MappingThe solution to this problem is to have your monitoring tool continuously discover everything about your infrastructure and applications and adjust itself automatically to your changing environment.
In my next blog post, I'll discuss issue #3 which deals with lack of data granularity and sampling.
This is a major issue that can lead to false results when troubleshooting or wrong conclusions by AI systems.
The following table indicates the key challenges:CategoryDescriptionTechnologyFull-scale environment is not available for performance testingProcessLimited time available for environment setup, load test script and data setup to execute full-scale load testProcessUnstable builds: Since development and testing activities go simultaneously break-off time is needed within a Sprint.
It is possible to detect anomalies, draw patterns from sprint level test results, and find defects using ML techniques.
Continuous Performance Testing StrategyIt has been observed that 70-80% of performance defects actually do not require a full-scale performance testing in a performance lab.
These defects can be found using a signal user or small scale load test in the lower environment if a good APM tool is used to profile and record execution statistics at each stage of SDLC.
The target of this shift left approach is to find and fix all code related performance defects as development is in process.
These should be lower of insolation or component level test.
Performance analysis, data trend, abnormally detection, and cutting down performance analysis by more than 50% if we use machine learning algorithm.
In Agile methodology, since full-scale performance test is not done all the time in a performance lab, production performance testing is highly critical to make sure the system can handle user traffic without any issues.
The percentage of the user set is increased gradually if there are no issuesIf there are issues found, either we do not increase the load further or new features will be pulled back.
Stop the test if real user experience degrades below the predefined threshold.
Need for detailed triage meeting in WAR rooms is minimized by tracing the sessions, pinpointing exact issuesConsistent metrics to analyze performance issues across the environment and it is accessible to development, testing teams, business users all the time.
The following diagram depicts how APM tools help in every Agile lifecycle stage:Performance Risk AssessmentUser story performance risk assessment not only helps identify key user stories which should be performance tested but also helps decide which user stories can be given lower priority.
The following table indicates the parameters that decide the probability and impact of failure.
The higher the risk factor, the higher the priority:Probability/ImpactParameterUser Story # 1 User Story # 2 Factor#1Degree of change from the previous sprint11Factor#2Stringent Performance SLA54Factor#3No.
of Users impacted43Factor#4Public Visibility and impact on the brand43Weighted ImpactImpact of Failure42Risk FactorRisk Factor208Right SkillsetIn Agile manifesto, performance engineers need to be a good developers with data science tools expertise to apply ML and automate tasks, analyze, and derive useful insights from data.
(Spoiler: There is no sense.
)Things do however quickly become more difficult when tracing asynchronous executions.
If a request is received by one thread but is answered from within another thread, it does no longer suffice to only trace entries and exits.
And the same way that debugging asynchronous execution is difficult, this is quite a bit of work for us too.
It is obvious that the easiest way of avoiding garbage is to avoid object allocation altogether.
However, object allocation in itself isn’t too bad either.
But since our agent runs mostly isolated from the application threads and at first, this did at first not make sense to me.When digging deeper, I found that our analysis of user objects triggered some additional escapes of objects but the impact was minimal.
Each worker thread of a fork join pool applies work stealing and might grab tasks out of the queue of any other task.
This is not true for all collection algorithms but for many of them such as for all default collectors of HotSpot.
In order to avoid this, I started to pool the objects I was using.
Traditionally, pooling was used to avoid the costs of allocation which became cheap in our days.
But as a matter of fact, the problem that I describe applies to a large number of Java applications.
Just as in the above case, the garbage collection algorithm does not group objects by application as it has no notion of this deployment model.
Therefore, object allocations by two isolated applications that share a container do interfere with the anticipated collection patterns of one another.
As a matter of fact, I think they are a bad idea for most applications.
And even if isolated applications ease development, you quickly pay the price in operations.
I am just mentioning this to avoid a misinterpretation of the moral of the above experience.What this experience taught me was that deploying several applications in a single Java process can be a bad idea if those applications are heterogeneous.
Yet, many enterprise frameworks still advertise all-in-one solutions for tackling such problems which should not share a process to begin with.
Otherwise, you might end up with collection patterns that you did not anticipate when developing, running, and testing your applications in isolation.
Nevertheless, you will likely be forced into upgrading some older technologies or outright replacement for others.
I cannot overstate how important it is to automate your build and delivery pipeline.
It can be challenging for most organizations to fathom leveraging these techniques let alone begin to implement them.
The point here is automating change management must be striven for in any organization leveraging microservices but sometimes we’re faced with the fact that we’re not at the point where these approaches are democratized enough for trivial implementation.
We’re also pulling out all the stops, offering more speaker sessions, a bigger expo set-up and a themed superhero “after-party” for conference-goers to let loose at, following a full day of activities.
And the business typically has little to no interaction at all with operations but they communicate with the app dev organization to provide the business requirement that the developers will code to.
Public and private cloud computing have become common in even the slowest moving large enterprises while container adoption and microservices architectures are dominating the discussions for new or re-architected applications.
The platform engineering team no longer needs to manage server refreshes, operating system upgrades, or any of the time wasting tasks of the past.
The last consideration with this new application delivery organization is whether or not to create a centralized monitoring service.
It has become well understood that when you release new application code so frequently you must be able to observe the impact of those releases so that you can quickly roll back if there is a major unintended bad CX.
The reality for most companies today lies somewhere between the traditional IT delivery organization and the new application delivery organization.
It's utterly ridiculous to think that the divorce rate in Maine is a direct result of how much margarine is consumed per capita.
In the world of cloud and container-based applications, context and experience can both be very difficult to come by.
Partial Visibility Creates Blind SpotsYour cloud and container-based applications are quite complex.
Any one of these conditions creates blind spots and major problems when you are trying to figure out the root cause of performance or stability issues.
Is there another issue lurking, waiting to bite you during a customer impacting event?
Is it the liquid secretion that lubricates our eyeballs or is it what happens when you pull on two ends of paper too hard?
(Login, search, checkout, pay bill, check balance, etc.)
It's often overlooked and does not get collected or associated with the relevant components.
And even if a human might interpret it as such, a monitoring tool or any sort of machine observing this metric has absolutely no chance to know the semantics of it.
Looking at the man page, the description of the metric leaves you alone with the interpretation of its operational meaning, for example judging if it's good or bad, when to ignore it, and when it will have an impact and on what parts of the system.
Any single team that manages an application or service has a finite amount of expert knowledge and can only develop expertise in new technologies at a limited pace.
There is constant change in these environments and it can wreak havoc on your IT management platforms.
We'll explore the problem and uncover the solution.
The ROI of Real-Time AI Driven Incident Monitoring and Prediction: Nine New Application Issues, Part 5Losing TimeThere's a dirty little secret in the legacy monitoring world.
It can take 5-10 minutes for an alert to fire in most monitoring tools.
That's 5-10 minutes before you have any indication that there is a problem.
In this post, we'll explore one of the issues that have the most significant impact on lost revenue - Issue #7: "Alerts take too long to trigger making an impact to business too costly."
Let's take a look at major steps in the incident management process:Incident begins - nobody in IT knows about it yet :(Find out about a problem from an alert (or worse, from an end user)Incident prioritization - is this a high enough priority to work on right now?
The purpose of monitoring tools is to either shorten this entire incident lifecycle or to avoid it altogether.
Because revenue loss due to downtime is significant across every industry.
Time is MoneyA 2017 web poll about downtime costs of over 800 companies conducted by Information Technology Intelligence Company (ITIC) showed the following:98% reported that 1 hour of downtime costs at least $100,000 US ($16,665 / 10 minutes)81% reported that 1 hour of downtime costs at least $300,000 US ($50,000 / 10 minutes)33% reported that 1 hour of downtime costs at least $1.5 million US ($250K / 10 minutes)These are staggering numbers, but not surprising given the ongoing digital transformation occurring in every company across every sector.
Can you afford to put up with an extra 10-minute delay?
On EVERY problem?
Avoid issues altogether or cut as much time as possible between the moment an incident starts and the moment service has been restored.
Countless applications make it into production every day with little to no monitoring in place.
You'll want as much correlated data across the full technology stack as possible to detect and isolate issues early.
The granularity of your data will impact your ability to detect and isolate problems.
The ability to collect and analyze data as fast as possible will have a major impact on reducing the time it takes to detect and isolate problems.
Using AI has the following benefits:   Predictive analytics - possibly avoid problemsIncident correlation and aggregation - avoid alert stormsAutomated root cause analysis with full causation (not simple correlation)Remediation recommendations using expert adviceAll of the tips above, when used together, significantly reduce downtime due to incidents.
Stream processing and AI have combined to dramatically reduce the time it takes to detect, isolate, and remediate production issues.
In my next post, I'll discuss issue #8 - "Performance analysis expertise for new technologies is hard to find, making it difficult to troubleshoot problems"In the meantime, expand your technical skills with Stan's Robot Shop sample microservice application or try Instana in your environment to see how quickly you can monitor, detect, isolate and resolve problems.
Such a board allows us to keep track of experiments running longer than our two-week sprint.Agile is hard because it requires you to question yourself every single day (Kaizen), move the conversation from features to purpose (remember Daniel Pink’s Drive), and apply systems thinking to (almost) everything.How did your development workflow evolve over time?We have been on the road for more than four years.
Anything missing you want to point out?Agile is hard because it requires you to question yourself every single day (Kaizen), move the conversation from features to purpose (remember Daniel Pink’s Drive), and apply systems thinking to (almost) everything.
Keep it up!Staying lean is a demanding endeavor, as many organizations know.
QuickstartDownload.jarrun.jarsee/use GUIObvious errors:truncation of textspelling errors in tooltipsspelling errors in options   e.g.
And I could test it blind, but what if I get more information first?
International C…No generation.
No log output.
It is hard to hide the source if it is on your machine.
If not then there might be something wrong with the generation code.
There are the simple models of “client -> http -> server,” and we use a proxy to mess with the messages; or send messages directly to the server.
This is where I send through the HTTP form messages that a GUI would POST, as though it were an API, even if there is no API, and simulate GUI testing without the GUI.
Write the @Test methods to explore functionality and test itA Quick Investigation to Reveal ‘Tester Thoughts’But, what we’ll do is quickly investigate the “International C…” error we found earlier.
This bug would have been very hard to find from the GUI.
Or you can hex dump the binary and you’ll often find the strings tucked away at the end of the file.
Looking at this list of chars I can then go and search the web to find out the rules for ‘email friendly’ and compare them to this set of chars and see if any are missing, or if we have any extra.
Since I can see more than 26 chars in the lower case set, I have to wonder if the random generation routine uses the full length of the string, or might there be an off by one error?
A quick code review later and…I get distracted immediately because I can see that generateRandomString uses a contains to decide what string set to use, and concatenates them together.
The loops all use pre increment, which confuses my code reading since I’m really not used to that.
In theory, we know exactly how long the string should be so StringBuilder could be initialized to the length, rather than 100, that would reserve enough memory for the full counterstring at the start of the process and might be faster, it might also trigger an out of memory error early in the generation rather than late.
No paid plans or upgradesmoosend.com — Mailing list management service.
No provisioning.
No deploymentzapier.com — Connect the apps you use, to automate tasks.
This is because log collection, transformation, and storage are costly.
It thus increases complexity and takes more time while troubleshooting problems.
Distributed tracing helps to get insight into the individual operation and pinpoint the areas of failure caused by poor performance.
Span: It is a primary building block of a distributed trace.
As you have seen earlier, there was no trace id label on the Loki log.
Incorrect value or missing value may lead to the following error:Re-apply the hotrod-deployment manifest to incorporate the changes made.
The answer to the perplexing problem is Distributed Tracing.
The specification allows the vendors such as Jaeger and Zipkin to implement their unique distributed tracing functionality and enables the users to avoid vendor lock-in from a particular tracer implementation.
You can use it also to customize other features such as ignoring specific requests, adding tags to spans, and modifying the spans created when an error occurs.
With the rise of containers and ephemeral infrastructure, it is even more critical to have all the data available for the applications.
The monitoring as of today is usually done across three axes: metric monitoring, logs, and distributed tracingStop reverse engineering applications and start monitoring from the inside.
Tracing in CassandraApache Cassandra is fault tolerant and highly scalable database system (source).
The downside is it also decreases error and bottleneck detection capabilities, making it much harder to detect why and where a request failed.
The collected data then provides information on which service a request failed or slowed down.
The downside is that it doesn’t offer scalability, nor is it reliable.
It, however, does not scale as well as Jaeger since all services are built in the same artifact.
However, it does not support multiple Zipkin instances, so scalability is limited.
There are also no agents/sidecars.
Grafana TempoAnnounced in 2020, Grafana Tempo is a new contender designed to address specific issues with other distributed tracing tools, particularly the use of large servers and difficulties with data sampling.
Despite being a new project, it offers nothing special for Kubernetes deployment.
This way, you can avoid putting large configuration properties inside the application at runtime and offloads all data ingestion to another service.
As for Zipkin and Grafana Tempo, neither provides a standardized solution for Kubernetes.
This can make it difficult to switch to another tracing tool (though most are cross-compatible).
Remember that Jaeger requires ports 14268 and 16686 for exposing the HTTP collector and the user interface.
The Jaeger HTTP collector requires an instance of HttpSender that takes the collector endpoint address as an argument.
Note that Tye used the binding information to expose the container ports that we configured to the host.
The argument --debug * will attach a debugger to all the services.
You can replace the argument value * with the name of a service to attach the debugger to the process running that service.
We could use this distinction to allow only applications to talk to our Jaeger Collector, while blocking applications from, say, accessing the Jaeger Query UI.
Non-authenticated requests will be blocked, while requests with valid tokens should reach the collector.
We’ll obtain a valid token by running the following command:curl \    -X POST \    -u instrumented-application:THE_SECRET \    http://YOUR_IP:8080/auth/realms/jaeger/protocol/openid-connect/token \    -d 'grant_type=client_credentials'In a production environment, you might have a long-lived OAuth token obtained by a bootstrap script or supplied by a tool like Ansible, perhaps destroying “old” pods from time to time, getting a new token each time.
Then, try stopping your application.
No traces should arrive at the server now, indicating that the Jaeger Auth Proxy blocked the request.
For such cases, controlling who sends data to the collector is not only useful but required.
In most cases, developers just worry about instantiating a tracer and letting the instrumentation libraries capture interesting spans.
The code repository with this example is available on GitLab, but here’s the most relevant part:package com.example.demo;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import io.jaegertracing.Configuration;import io.opentracing.Scope;import io.opentracing.Tracer;import io.vertx.core.AbstractVerticle;import io.vertx.core.Vertx;public class MainVerticle extends AbstractVerticle {    private static final Logger logger = LoggerFactory.getLogger(MainVerticle.class);    public static void main(String[] args) {        Vertx.vertx().deployVerticle(new MainVerticle());    }    @Override    public void start() throws Exception {        String serviceName = System.getenv("JAEGER_SERVICE_NAME");        if (null == serviceName || serviceName.isEmpty()) {            serviceName = "vertx-create-span";        }        System.setProperty("JAEGER_SERVICE_NAME", serviceName);        Tracer tracer = Configuration.fromEnv().getTracer();        vertx.createHttpServer().requestHandler(req -> {            try (Scope ignored = tracer.buildSpan("operation").startActive(true)) {                logger.debug("Request received");                req.response().putHeader("content-type", "text/plain").end("Hello from Vert.x!
Needless to say, no more work happens in the “main” thread for this span.
It’s worth noting that, from this point and on, we stop referring to our span as “span”: for almost all cases after this, it is treated as a fully fledged trace, that happens to be composed by a single span.
When an application experiences a slowdown and its "data flow" goes through several different microservices, pinpointing the exact location of a slowdown may be difficult for a developer.
This is all well and good, but when a problem occurs a developer must go through a lot of logs (produced by the selected logging framework, such as Fluentd or Logstash – in KumuluzEE you can use the KumuluzEE Logs extension to simplify logging) to find the problem.
If a problem is severe (such as a complete crash), a solution is usually discovered quickly or even handled automatically by some container orchestration tool (such as Kubernetes).
Tougher problems are the slowdowns in which the application still works but in a limited capacity.
Right now, we have no data.
Simulated lag is added to this request (random delay).
The same does not apply to outgoing requests.
Locate the Resource.java file (src/main/java/com/kumuluz/ee/samples/opentracing/tutorial/delta) and add the following line:@GETpublic Response get() {    throw new RuntimeException("Something went wrong here.
Adding Custom Data to SpansIf we look back to our project structure, we added some simulated lag to our application in the beta microservice.
Basically, we added a random delay from 1 to 1000 milliseconds to the request.
After that, we can access the current span with tracer.activeSpan and add our delay to it.
For example, we did not include integration with a MicroProfile Config extension and KumuluzEE config frameworks, which would allow additional tracing configuration such as ignoring tracing on JAX-RS endpoints and changing the way that spans are named.
A trace tells you when one of your flows is broken or slow along with the latency of each step.
However, traces don't explain latency or errors.
Metrics allow deeper analysis into system faults.
Traces are also specific to a single operation, they are not aggregated like logs or metrics.
The situation is likely worse for unofficial clients.
The official clients are better across the board, but the unofficial libraries are worse off.
However, you don't want to have to instrument everything.
Zipkin is not part of a wider ecosystem, it's an isolated project part of a pre-container world.
If Jaeger doesn't fit, then go with Zipkin.
The Jaeger Client typically sends spans via UDP to the agent, avoiding the TCP overhead and reducing the CPU and memory pressure upon the instrumented application.
With that in mind, the Jaeger Agent should be deployed as close as possible to the instrumented application, reducing the risks inherent to UDP delivery.
Backend storage and processing,fully managed by Instana — no need to waste time deploying, configuring, and managing the backend monitoring systemWe expect as the OpenTracing community evolves and matures there will be a number of applications, libraries, and plugins developed which interact natively with the Jaeger and Zipkin compatible clients.
This will enable our customers to leverage these predefined traces and spans with no extra effort required.
Unfortunately, WildFly 14 shipped with a bug that prevents the spans from reaching our Jaeger server.
apply plugin: 'war'group = 'io.jaegertracing.examples.wildfly'version = '1.0-SNAPSHOT'description = "JavaEE Traced Store"sourceCompatibility = 1.8targetCompatibility = 1.8repositories {    mavenLocal()    mavenCentral()        }dependencies {    providedCompile 'javax:javaee-api:8.0'    providedCompile 'org.eclipse.microprofile.opentracing:microprofile-opentracing-api:1.1'}At this point, we should be able to build a Java Web Archive (WAR) for our project: run ./gradlew war and check your build/lib directory.
// src/main/java/io/jaegertracing/examples/wildfly/OrderEndpoint.javapackage io.jaegertracing.examples.wildfly;import javax.ejb.Stateless;import javax.ws.rs.POST;import javax.ws.rs.Path;@Stateless@Path("/order")public class OrderEndpoint {    @POST    @Path("/")    public String placeOrder() {        return "Order placed";    }}Once we build and the deployment completes, we can call this endpoint via curl:./gradlew warcp build/libs/javaee-traced-store-1.0-SNAPSHOT.war $WILDFLY_HOME/standalone/deployments/# watch the WildFly logs and wait for the deployment to finishcurl -X POST localhost:8080/javaee-traced-store-1.0-SNAPSHOT/orderYou should be able to see one trace in the Jaeger UI for the service javaee-traced-store-1.0-SNAPSHOT.war, containing one span.
Let’s inject it there and call the new method from our bean:// src/main/java/io/jaegertracing/examples/wildfly/OrderEndpoint.javapackage io.jaegertracing.examples.wildfly;import javax.inject.Inject;import javax.ws.rs.POST;import javax.ws.rs.Path;@Path("/order")public class OrderEndpoint {    @Inject    AccountService accountService;    @POST    @Path("/")    public String placeOrder() {        accountService.onNewOrder();        return "Order placed";    }}To see what it looks like now in a trace, let’s build, deploy, and run it once more:./gradlew warcp build/libs/javaee-traced-store-1.0-SNAPSHOT.war $WILDFLY_HOME/standalone/deployments/# watch the WildFly logs and wait for the deployment to finishcurl -X POST localhost:8080/javaee-traced-store-1.0-SNAPSHOT/orderAt this point, we should now see a second trace for our service, but with two spans.
So, this user might be able to query data related to tenant “A,” but access to production data related to tenant “B” is forbidden.
This is ideal for scenarios where each instance of the target application stack is executed on its own hosts (bare metal, VM, agent as a sidecar in a pod), or where tenancy is a business concern on the target application stack: for instance, when your application is multitenant but tracing data is not shared with the tenants.
This way, no change is required to the tracer or to the target application.
In other words: As each tenant has their own agents and collectors, a rogue or misbehaving tenant will not cause UDP packages to be dropped or the collector to slow down.
A chargeback feature is harder to implement in this scenario, as it’s not easy to calculate the exact computing usage of the agent/collector on a per-tenant basis.
Similarly, it might be hard to calculate the storage requirements, as everything is on the same keyspace.
Arguably, though, scenarios where this feature is required could still use one instance per tenant, ignoring Jaeger’s native multitenancy capabilities.
Or rather, it’s easy to get it wrong.
In our context, several scenarios of multitenancy could be satisfied by applying it at the agent/collector level, but the complexity of doing that might not justify the benefits, especially if we consider that each component has only a few tens of megabytes of overhead “per tenant”.
The goal of Distributed Tracing is to pinpoint failures and causes of poor performance across various system boundaries.
To solve this problem, OpenTracing and OpenCensus projects were started.
Concepts and TerminologySpan — Span is the building block of a trace.
If no         context is found, then a new span is started.
Acmeshop ArchitectureLet's say the end-user experiences poor performance.
This can then be used to go to those services and start looking at their logs and other stats to figure out the actual problem with the application.
This is only for local testing    var options = {      uri: endpoints.usersUrl + "/users/" + req.params.id,      method: 'GET',      json: true,      headers: req.headers    };    // Leverages request library    request(options, function(error, response, body) {        if (error) {          return next(error);        }        if (response.statusCode == 200) {            console.log('printing from within request')            res.writeHead(200)            res.write(JSON.stringify(body))     // Section 3            userSpan.setTag(opentracing.Tags.HTTP_STATUS_CODE, response.statusCode)            userSpan.log({              'event': 'request_end'            });            userSpan.finish();            res.end();        }        else {            res.status(response.statusCode);            res.write(JSON.stringify(response.statusMessage))  // Section 4            userSpan.setTag(opentracing.Tags.HTTP_STATUS_CODE, response.statusCode)            userSpan.setTag(opentracing.Tags.ERROR, true);            userSpan.log({              'event': 'error',              'message': response.statusMessage.toString()            });            userSpan.log({              'event': 'request_end'            })            userSpan.finish();            res.end();        }    }); // end of request}); // end of methodIn the first code block, you can notice that we start a span.
In the second code block, we are making an HTTP request to the user service.
In the third and fourth code blocks, there were tags and log fields added.
You may also add tags to indicate an ERROR by using userSpan.setTag(opentracing.Tags.ERROR, true);Messages can also be logged along with the span by using    userSpan.log({      'event': 'error',      'message': response.statusMessage.toString()    });Augmenting additional data along with the span can increase the network and storage costs.
func initJaeger(service string) (opentracing.Tracer, io.Closer) {// Uncomment the lines below only if sending traces directly to the collector// tracerIP := GetEnv("TRACER_HOST", "localhost")// tracerPort := GetEnv("TRACER_PORT", "14268")agentIP := GetEnv("JAEGER_AGENT_HOST", "localhost")agentPort := GetEnv("JAEGER_AGENT_PORT", "6831")logger.Infof("Sending Traces to %s %s", agentIP, agentPort)cfg := &jaegercfg.Configuration{Sampler: &jaegercfg.SamplerConfig{Type:  "const",Param: 1,},Reporter: &jaegercfg.ReporterConfig{LogSpans:          true,LocalAgentHostPort: agentIP + ":" + agentPort,// Uncomment the lines below only if sending traces directly to the collector//CollectorEndpoint: "http://" + tracerIP + ":" + tracerPort + "/api/traces",},}tracer, closer, err := cfg.New(service, config.Logger(jaeger.StdLogger))if err != nil {panic(fmt.Sprintf("ERROR: cannot init Jaeger: %v\n", err))}return tracer, closer}// Set tracer to Global tracerfunc main() {  // Start Tracer  tracer, closer := initJaeger("user")stdopentracing.SetGlobalTracer(tracer)  // Start the serverhandleRequest()  // Stop Tracerdefer closer.Close()}Extract the Context and Start Span in User Service (Go)Once the request is received by the service, it is re-directed to GetUser Function, as shown below.
func GetUser(c *gin.Context) {var user UserResponsetracer := stdopentracing.GlobalTracer()userSpanCtx, _ := tracer.Extract(stdopentracing.HTTPHeaders, stdopentracing.HTTPHeadersCarrier(c.Request.Header))userSpan := tracer.StartSpan("db_get_user", stdopentracing.ChildOf(userSpanCtx))defer userSpan.Finish()userID := c.Param("id")userSpan.LogFields(tracelog.String("event", "string-format"),tracelog.String("user.id", userID),)if bson.IsObjectIdHex(userID) {error := collection.FindId(bson.ObjectIdHex(userID)).One(&user)if error != nil {message := "User " + error.Error()userSpan.LogFields(tracelog.String("event", "error"),tracelog.String("message", error.Error()),)userSpan.SetTag("http.status_code", http.StatusNotFound)c.JSON(http.StatusNotFound, gin.H{"status": http.StatusNotFound, "message": message})return}} else {message := "Incorrect Format for UserID"userSpan.LogFields(tracelog.String("event", "error"),tracelog.String("message", message),)userSpan.SetTag("http.status_code", http.StatusBadRequest)c.JSON(http.StatusBadRequest, gin.H{"status": http.StatusBadRequest, "message": message})return}userSpan.SetTag("http.status_code", http.StatusOK)c.JSON(http.StatusOK, gin.H{"status": http.StatusOK, "data": user})}Here, the request is extracted by usinguserSpanCtx, _ := tracer.Extract(stdopentracing.HTTPHeaders, stdopentracing.HTTPHeadersCarrier(c.Request.Header))This provides the traceID from the parent.
NOTE: In the end, if you use Opentracing with Jaeger agents (as we have described in the article above), you can pick essentially ANY tracing UI you need (open source or commercial)Tracing Is DifficultIt's difficult to get tracing correct, especially when adding it to existing applications.
Give it a try and don’t forget to give us your feedback!
this is to stress that keycloak does   not  need to be running on the same openshift cluster as our jaeger backend.
you might want to fine tune this in a production environment, otherwise, you might be open to an attack known as     "unvalidated redirects and forwards."
run the following command from the same directory you stored the   yaml  file from the previous steps, referenced here by the name   jaeger-production-template.yml  :oc process -f jaeger-production-template.yml | oc create -n jaeger -f - during the first couple of minutes, it’s ok if the pods   jaeger-query  and   jaeger-collector  fail, as cassandra will still be booting.
all incoming requests go through this sidecar and all features available in keycloak can be used transparently, such as 2-factor authentication, service accounts, single sign-on, brute force attack protection, ldap support, and much more.
The Span is the primary building block of distributed tracing.
):Img.5.
As the tracing provides you the ability to get a view into the application communication layer and recognize potential issues, when the JFR is attached to the microservice JVMs, you can directly analyze the potentially suspicious code.
):Img.6.
If we forget to register a specific tracer instance, then the tracing feature would use NoopTracer.
No changes to the application or business logic!
WARNING: This is not a production application!
Return to operator overview tab and select:12- Change the suggested YAML definition with the following:13- Check the deployment under operator instances tab:14- Expose Prometheus server:oc expose svc prometheus-operatedObservability Labs: Step 3 - Sonatype NexusIn order to continue this lab, you must provide a Sonatype Nexus instance in the microservices namespace.
export current_project=microservicesexport MAVEN_URL=http://$(oc get route nexus3 --template='{{ .spec.host }}')/repository/maven-group/export MAVEN_URL_RELEASES=http://$(oc get route nexus3 --template='{{ .spec.host }}')/repository/maven-releases/export MAVEN_URL_SNAPSHOTS=http://$(oc get route nexus3 --template='{{ .spec.host }}')/repository/maven-snapshots/# deploy nutritionist-api (spring boot 2 API)# oc delete all -lapp=nutritionist-apioc new-app openjdk-8-rhel8:latest~https://github.com/aelkz/microservices-observability.git --name=nutritionist-api --context-dir=/nutritionist-api --build-env='MAVEN_MIRROR_URL='${MAVEN_URL} -e MAVEN_MIRROR_URL=${MAVEN_URL}oc patch svc nutritionist-api -p '{"spec":{"ports":[{"name":"http","port":8080,"protocol":"TCP","targetPort":8080}]}}'# we will also be using the same service monitor defined in the main APIoc label svc nutritionist-api monitor=springboot2-apioc expose svc/nutritionist-api -n ${current_project}Now all APIs are exposed to Prometheus:And tracing working as expected.
Jaeger key features:High Scalability – Jaeger backend is designed to have no single points of failure and to scale with the business needs.
All Jaeger backend components expose Prometheus metrics by defaultGrafana Grafana is an open-source metric analytics and visualization suite.
this is a common trick to get jax-rs endpoints to be managed as ejbs, so that they can be invoked via jmx or get monitoring features.
whether or not to trace all beans is a per-deployment decision, so, no   ejb-jar.xml  is provided by the integration.
we don’t want to block the parent transaction while interacting with those systems, so, we make this an asynchronous ejb.
while this might cause some confusion, we believe this is the right semantic for this use case, as it allows for a complete tracing picture, without any explicit tracing code, apart from passing the context around.
Having invocation counts, error rates, logs, and even stack traces at your fingertips is very compelling.
When performance data is segmented by actor, piecing together a cohesive story is an exercise in frustration.
var express = require('express');var router = express.Router();var createError = require('http-errors');var request = require('request-promise');var opentracing = require('opentracing');router.get('/ot-gen', function(req, res, next) {  // create our parent span, using the operation name "router"  let parentSpan = opentracing.globalTracer().startSpan('router');  // assign the count passed in our request query to a variable which will be used in our for loop below  count = parseInt(req.query.count);  promises = [];  if (count) {    for (let c = 0; c < count; c++) {      promises.push(new Promise(() => {    // create our child spans for each request that will be made to Lambda        let childSpan = opentracing.globalTracer().startSpan('service-egress', { childOf : parentSpan });    // create an empty carrier object and inject the child span's SpanContext into it        var headerCarrier = {          'Content-Type': 'application/json'        };        opentracing.globalTracer().inject(childSpan.context(), opentracing.FORMAT_HTTP_HEADERS, headerCarrier);    // make our outbound POST request to our Lambda function, and inject the SpanContext into the request headers        request.post(process.env.lambda-url, { headers: headerCarrier, body: { 'example': 'request'}, json: true }).then((response) => {      // append some contextual information into span for use later in LightStep          childSpan.logEvent('remote request ok', { response });      childSpan.setTag('Destination', 'xxx.us-east1.amazonaws.com/production/');        })        .catch((err) => { // if there is an error in the Lambda response, then we attach an error boolean tag, and the error message received to the span, which we can then access later in LightStep             childSpan.setTag('error', 'true');            childSpan.logEvent('remote request failed', {                error   : err            });        })        .then(() => {// finish our child span            childSpan.finish();        });      }));    }  } else {    // basic route functionality    next(createError(400, 'Count not provided'));  }  // end the response so it doesn't stay open while we wait for Lambda to return our responses  res.end();  // after all requests have been made, end the parent span  Promise.all(promises).then(parentSpan.finish());});module.exports = router;As you can see, I’ve made a route here which allows us to instruct Express to send a variable number of requests (passed as a query parameter) to our Lambda function.
If there is an error in the request, we attach the error along with a boolean KV pair which tells our backend that there is an error present on the active span.
We’re ending our parent span here after the Lambda requests leave our app, but we aren’t waiting for the responses.
There’s one extra step when tracing HTTP requests on Lambda: exposing the HTTP headers in the event.
var opentracing = require('opentracing');var lightstep   = require('lightstep-tracer');opentracing.initGlobalTracer(new lightstep.Tracer({    access_token: process.env.lightstep-token,    component_name : 'lambda_function',    // along with other considerations, we remove any initial reporting delay in the library initialization to remove unnecessary overhead.
In our example, we are ingesting data within the same region as our function to reduce any delay to a negligible level.
To make this automatic, many users add the OpenTracing logging calls to their existing error handling or logging libraries, which adds exception messages directly to the individual operation.
This view, enabled through the lightweight data model of OpenTracing and the low overhead of the LightStep client library, is the fastest and most context-rich way to understand a distributed performance story.
Pragmatic Tracing for Distributed SystemsIntroduction  A distributed system is one in which the failure of a computer you didn't even know existed can render your own computer unusable.
Just a single neuron acting as a proto-eye doesn’t cut it, but a full-fledged eye does.
Therefore, your new systems cannot be monitored as they were during a lower complexity era.
Usually, log event levels are modeled after syslog levels, DEBUG, INFO, WARNING, ERROR, and CRITICAL being used the most in software.
only ERROR level or higher.
In practical terms, this means you’re writing log level INFO or greater to stderr, but to the file, you’re dumping all logs.
Usually, alarms are tied to some metric; that is, whenever the specified metric is outside given bound perform an action, it will auto-heal or notify the operator.
Another human metric would be pain levels in your left leg.
Usually, it’s near 0, but over a certain threshold, you’re vividly aware of it – that is, an alarm is raised.
For computer systems, the usual metrics are related to throughput, error rate, latency, and resource (CPU/GPU/network) utilization.
X times per second, it stops your program and checks which line is being executed on the machine.
Despite them being parallel (though they’re not necessarily), you shall hit a tail latency problem.
The biggest downside is working with the visualizer.
As a newcomer, I had a hard time finding how can I filter datapoint in categories, by name, and overall advance use cases beyond the basic scroll and see.
Distributed TracingAll’s fine and dandy on a single node, but the trouble starts with distributed systems.
The problem is how to connect/correlate traces coming from multiple nodes.
Here’s a screenshot from Jaeger frontend for searching and looking at your traces: Since writing against a specific backend could be a hard vendor lock-in, there have emerged two client vendor-neutral APIs – OpenTracing and OpenCensus.
OpenCensus ExampleThis subsection will describe basic OpenCensus building blocks.
error, http.statuscode, …).
func HandleXXX (w http.ResponseWriter, req *http.Request) {    ctx := req.Context() // ...}This chapter could be summarized as an OpenCensus crash course/cheat sheet/in 5 minutes.
Istio Tracing and Monitoring: Where Are You and How Fast Are You Going?The Heisenberg Uncertainty Principle states that you cannot measure an object's position and velocity at the same time.
If it's in a location, then it has no velocity.
That sounds pretty broad, but in fact, one of the basic rules of tracing is that data are dumped into a tracing data store without regard to formatting.
Using the Jaeger UI, we can view traces, see how far and deep they go, and get an idea of where performance might be lagging.
Depending on your language (Java, for example) and framework (Spring Boot), you could implement all this with no changes to your source code.
Seriously, that’s it.
:)Hello, WorldOnce again, it is time to say Hello to this cruel World.
Install OpenTelemetryOk so if you know python that was all very boring.
Just to unpack it a bit, there are three critical packages, beyond the launcher itself, which are worth understanding as they explain how OpenTelemetry is structured.
This package only contains interfaces, no implementation.
No Code RequiredThe biggest, most important note is that we added OpenTelemetry to our service, but didn’t write any code.
The most important details to add are application-level attributes critical to segmenting your data.
For example, a projectID allows you to differentiate between errors that are affecting everyone connecting to a service, vs errors that are localized to a handful of accounts.
Generally, this means you shouldn't be creating spans in your application code, they should be managed as part of the framework or library you are using.
10               # WARNING: failing to end a span will create a leak.
Recording ErrorsOk, one final bit.
Python    xxxxxxxxxx            1                       16                                              1            from opentelemetry import trace             2            from opentelemetry.trace.status import StatusCode             3                         4            @app.route("/hello")             5            def hello():             6              try:             7                  1 / 0             8                         9              except ZeroDivisionError as error:             10                  span = trace.get_current_span()             11                  # record an exception             12                  span.record_exception(error)             13                  # fail the operation             14                  span.set_status(StatusCode.ERROR)             15                         16               return "Hello World\n"Uncaught exceptions are automatically recorded as errors.
Top 10 Open-Source Monitoring Tools for KubernetesEngineers list monitoring as one of the main obstacles for adopting Kubernetes.
Not surprisingly, when asked, engineers list monitoring as one of the main obstacles for adopting Kubernetes.
After all, monitoring distributed environments have never been easy and Kubernetes adds additional complexity.
Fluentd uses disk or memory for buffering and queuing to handle transmission failures or data overload and supports multiple configuration options to ensure a more resilient data pipeline.
Both Fluentd and Fluent Bit are also CNCF projects and Kubernetes-native - they are designed to seamlessly integrate with Kubernetes, enrich data with relevant pod and container metadata, and as mentioned - all this with a low resource footprint.
Pros: Huge plugin ecosystem, performance, reliability Cons: Difficult to configurecAdvisorcAdvisor is an open-source agent designed for collecting, processing, and exporting resource usage and performance information about running containers.
Spring Reactive Microservices: A ShowcaseIntroduction and ScopeThe Servlet Specification was built with the blocking semantics or one-request-per-thread model.
Reactive programming is trending at the moment but trivial "hello-world" examples with Mono & Flux cannot simply capture the demands of building 12-factor apps for production usage.
A mix is also possible in case we have some endpoints and services which cannot become reactive for a number of reasons such as: blocking dependencies with no reactive alternatives or we may have an existing legacy app which we want to migrate gradually etc.
Of course, these will be mocked for simplicity.
22                     // If an error occurs in one of the Monos, execution stops immediately.
23                     // If we want to delay errors and execute all Monos, then we can use zipDelayError instead              24                     Mono<Tuple2<CustomerDTO, String>> zippedCalls = Mono.zip(customerInfo, msisdnStatus);              25                           26                     // Perform additional actions after the combined mono has returned              27                     return zippedCalls.flatMap(resultTuple -> {              28                           29                         // After the calls have completed, generate a random pin              30                         int pin = 100000 + new Random().nextInt(900000);              31                           32                         // Save the OTP to local DB, in a reactive manner              33                         Mono<OTP> otpMono = otpRepository.save(OTP.builder()              34                                 .customerId(resultTuple.getT1().getAccountId())              35                                 .msisdn(form.getMsisdn())              36                                 .pin(pin)              37                                 .createdOn(ZonedDateTime.now())              38                                 .expires(ZonedDateTime.now().plus(Duration.ofMinutes(1)))              39                                 .status(OTPStatus.ACTIVE)              40                                 .applicationId("PPR")              41                                 .attemptCount(0)              42                                 .build());              43                           44                         // External notification service invocation              45                         Mono<NotificationResultDTO> notificationResultDTOMono = webclient.build()              46                           .post()              47                           .uri(notificationServiceUrl)              48                           .accept(MediaType.APPLICATION_JSON)              49                           .body(BodyInserters.fromValue(NotificationRequestForm.builder()              50                                 .channel(Channel.AUTO.name())              51                                 .destination(form.getMsisdn())              52                                 .message(String.valueOf(pin))              53                                 .build()))              54                           .retrieve()              55                           .bodyToMono(NotificationResultDTO.class);              56                           57                         // When this operation is complete, the external notification service              58                        // will be invoked.
If an error occurs in one of the Monos, the execution stops immediately.
If we want to delay errors and execute all Monos, then we can use zipDelayError instead.
Validate OTPBusiness RequirementGiven an existing OTP id and a valid pin previously delivered to user's device:fetch the corresponding OTP record from the DB by querying "otp" table by idif found, fetch information of maximum attempts allowed from configuration table "application", otherwise return errorperform validations: check if maximum attempts exceeded, check for matching pin, if OTP has expired etc.
if validation checks fail, then return error, otherwise update the OTP status to VERIFIED and return successin case of error we need to finally save the updated counter of maximum attempts and OTP status back to the databaseWe assume here that we can have OTPs associated with applications and we can have different time-to-live periods, different number of maximum attempts allowed etc.
Notice that for such simple queries no implementation is neededWe then use the switchIfEmpty and Mono.error methods to throw an Exception if no record found.
As the name says, it acts as an error handler so we can put in there related actions.
It's like a finally clause but for errors.
and concurrently (in parallel):fetch the corresponding OTP record from the DB by querying "otp" table by idif not found or its status is no longer valid (e.g.
EXPIRED), return errorif found, proceed re-sending it to the customer via multiple channels and simultaneously via the notification-serviceReturn to the caller the OTP sentSolutionYou can check the implementation here:     Java      xxxxxxxxxx             1                         23                                                  1             return otpRepository.findById(otpId)              2                             .switchIfEmpty(Mono.error(new OTPException("Error resending OTP", FaultReason.NOT_FOUND)))              3                             .zipWhen(otp -> {              4                                 // perform various status checks here...              5                                             6                                 List<Mono<NotificationResultDTO>> monoList = channels.stream()              7                                         .filter(Objects::nonNull)              8                                         .map(method -> webclient.build()              9                                                 .post()              10                                                 .uri(notificationServiceUrl)              11                                                 .accept(MediaType.APPLICATION_JSON)              12                                     .body(BodyInserters.fromValue(NotificationRequestForm.builder()              13                                                         .channel(method)              14                                                         .destination(Channel.EMAIL.name().equals(method) ?
Its primary target is to be a simple, limited, opinionated object mapper.
However, when we are using File Appenders for logging then we have an issue since this I/O operation is blocking.
neverBlock – Setting it to true will prevent any blocking on the application threads but it comes at the cost of lost log events if the AsyncAppender’s internal buffer fills up.
For example, tracing database calls with R2DBC is not yet supported.
Sometimes it may be hard to detect blocking code in Reactor thread.
And this is because we don't need to use block to make things blocking but we can unconsciously introduce blocking by using a library which can block the current thread.
The only thing you need for this is to include the following dependency:     XML      xxxxxxxxxx             1                                                  1             <dependency>              2               <groupId>io.projectreactor.tools</groupId>              3               <artifactId>blockhound-junit-platform</artifactId>              4               <version>1.0.4.RELEASE</version>              5             </dependency>Keep in mind that if go with Java 11 and above, the following JVM argument is needed for the tool to work properly:-XX:+AllowRedefinitionToAddDeleteMethodsIntegration TestingIn our sample project we show an example Integration Test covering our most "complicated" endpoint which is the one that generates an OTP.
We use HoverFly for mocking responses of the two "external" services (i.e.
This client can connect to any server over HTTP, or to a WebFlux application via mock request and response objects.
Moreover, we need to "bind" it with HoverFly so when it is invoked to return the mocked response we want.
Async SOAPNowadays most of the systems we integrate with expose REST endpoints.
The most significant difference (4 times faster than blocking Servlet) comes when underlying service is slow (500ms).
Also, it does not create a lot of threads comparing with Servlet (20 vs 220).
4 Things Cloud-Native Java Must ProvideJava is still the pervasive development language among enterprise developers, even though it is not developers' preferred cloud-native runtime and is falling behind other languages, according to GitHub's Octoverse.
Each company may or may not be using different type of own automation scripts/tools to monitor these areas and also very difficult to quickly find the issue type in production environment.
Canary DeploymentCanary release is a technique that is used to reduce the risk of introducing a new software version in production by gradually rolling out the change to a small subgroup of users, before rolling it out to the entire platform/infrastructure and making it available to everybody.
Kiali ArchitectureKiali ComponentsKiali has composed of two components:Kiali application (back end): This component runs in the container application platform and communicates with the service mesh components, retrieves and processes data, and exposes this data to the console.
Kiali needs to retrieve Istio data and configurations, which are exposed through Prometheus and the cluster API.
Kiali uses this Prometheus data to determine the mesh topology, display metrics, calculate health, show possible problems, and so on.
Prometheus is an Istio dependency and a hard dependency for Kiali, and many of Kiali’s features will not work without Prometheus.
Are any of the microservices in an unhealthy state?
It shows you which services communicate with each other and the traffic rates and latencies between them, which helps you visually identify problem areas and quickly pinpoint issues.
Kiali provides graphs that show a high-level view of service interactions, a low level view of workloads, or a logical view of applications.The graph also shows which services are configured with virtual services and circuit breakers.
Some of the important wizards as Weighted Routing Wizard, Matching Routing Wizard, Suspend Traffic Wizard.
Risk (Vulnerability)A vulnerability was found in Kiali allowing an attacker to bypass the authentication mechanism.
All are vulnerable.
The vulnerability lets an attacker build forged credentials and use them to gain unauthorized access to Kiali.
Additionally, it was found that Kiali credentials were not being validated properly.
MicroProfile: Your Cloud-Native Companion for Enterprise Java [Video]Writing microservices within Jakarta EE is technically possible, but you miss a few goodies for the distributed environment you are running in.
Examples of configuration, tracing, metrics, and fault tolerance are shown.
Working Around the Removal of XML APIs When Working With MicroProfile and Spring AppsUnfortunately, JEP 320 removes some handy XML processing classes as part of the removal of older Java EE and CORBA Modules, specifically JAX-WS (Java API for XML-Based Web Services) and JAXB (Java Architecture for XML Binding), which are common for reading and managing XML configuration files, among other functions.
Flight Recorder (JEP 328) takes the analysis one step further and dials into the JVM to track performance bottlenecks and errors in the running code.
Building Fault Tolerance and Health Checks Into Your Microservice Without CodeEven with all the facilities that I've shared so far for managing code and testing, things can still go wrong in production.
Fault tolerance is a way to manage what happens if a microservice is failing inside a larger infrastructure of multiple processes and microservices, and what automated action to take based on predetermined rules.
For example, to enable health checks in a MicroProfile app, just add this segment to the configuration section of the application's pom.xml file: <config> <thorntail-v2-health-check> <path>/health</path> </thorntail-v2-health-check></config> Once fault tolerance and health checks are enabled, they can be monitored and automated responses to predetermined events can be triggered with separate code.
For more details, check out the Health Check project and the Fault Tolerance project.
Both projects are cutting edge and very competitive, making it a tough choice to select one.
A service typically offers service discovery, load balancing, failure recovery, metrics, and monitoring.
ResilienceCircuit breaking, Retries and Timeouts, fault-injection, delay injection.
No circuit breaking and no delay injection support.
ConclusionService meshes are becoming an essential building block in cloud-native solutions and microservice architectures.
When choosing technology as complex and as critical as a service mesh, think about more than just the technology – consider the context in which it will be used.
Microservices With Observability on KubernetesAre you looking to implement observable microservices but clueless as to how to make them work with Kubernetes?
Observability - Third Pillar - MetricsTo implement the third pillar, i.e., metrics, we can use the APM Services dashboard where Latency, Throughput, and Error rates are captured.
Distributed traces capture and propagate critical details about transactions as they cross microservices boundaries.
Which customer is sending bad requests?
akka {    actor {        provider = "cluster"    }    remote {        log-remote-lifecycle-events = off        netty.tcp {            hostname = "127.0.0.1"            port = 0        }    }    cluster {        seed-nodes = [            "akka.tcp://ClusterSystem@127.0.0.1:2551",            "akka.tcp://ClusterSystem@127.0.0.1:2552"]        # auto downing is NOT safe for production deployments.
akka {    actor {        provider = "cluster"    }    remote {        log-remote-lifecycle-events = off        netty.tcp {            hostname = "127.0.0.1"            port = 0        }    }    cluster {        seed-nodes = [            "akka.tcp://ClusterSystem@127.0.0.1:2551",            "akka.tcp://ClusterSystem@127.0.0.1:2552"]        # auto downing is NOT safe for production deployments.
Here, we cut to the chase and just run the two Docker instances.
No code changes.
No annotations.
Cloud computing enables some of the largest companies in the world, such as Airbnb, Netflix, and Uber, to reinvent and dominate their industries.
Understanding and choosing the correct cloud-native technologies are critical for increasing development velocity and spending less time and money developing and maintaining tooling and infrastructure.
Additionally, cloud-native tools rely heavily on abstractions, making them more generic and allowing teams to run their services without agreeing on a shared runtime across the company.
Resources always adapt to the current demand, which saves money over traditional, statically scaled resources.
Companies that leverage the full suite of tools can often deliver faster, with less friction, and lower development and maintenance costs.
Engineering teams can store container images in a container registry, which in most cases, also provides vulnerability analysis and fine-grained access control.
Manual configuration, however, makes it hard to keep track of changes.
SecretsSecret management is essential for cloud-native solutions but is often neglected at smaller scales.
Ultimately, organizations that ignore secret management could increase the surface area for credential leakage.
CertificatesSecure communication over TLS is not only best practice but a must-have.
What is the result of the operations (success, failure, or status codes)?
Without alerts, you don't get notified of incidents, which in the worst case means that companies don't know that there have been problems.
When performance issues arise, teams can see what service errors are occurring and how long each phase of the transaction is taking.
Are any of the microservices in an unhealthy state?
Kiali does not allow to customize the views/graphs, however you can customize the dashboards and views in Grafana if you have a requirement.
In the below capture, there are 11 applications throwing Errors, 7 applications are in a Degraded state and 78 applications are in a Healthy state.
Building a Real-Time Anomaly Detection Experiment With Kafka and CassandraAnomaly detection is a cross-industry method for discovering unusual occurrences in event streams — it’s applied to IoT sensors, financial fraud detection, security, threat detection, digital ad fraud, and plenty of other applications.
Such systems inspect streaming data to check for anomalies or irregularities and send alerts upon detection to process those exceptions and determine if they do, in fact, represent a security threat or other issue.
Kafka enables fast and linearly scalable ingestion of streaming data, supporting multiple heterogeneous data sources, data persistence, and replication by design to eliminate data loss even when some nodes fail.
Like Kafka, Cassandra also provides linear scalability and maintains data even during failures.
For each event that arrives via the consumer, the Cassandra client writes the event to Cassandra, reads historic data from Cassandra, runs the anomaly detection algorithm, and makes the decision as to whether the event carries a high risk of being an anomalous one.
In contrast, our experiment does provide real-time anomaly detection and blocking, with a detection latency averaging just 500 milliseconds.
Cloud ProvidersThe usual suspects are present: Amazon Web Services Lambda, Google Cloud Functions, Microsoft Azure Function Apps and recently IBM has entered the space with a hosted version of OpenWhisk.
This level of automation makes moving functions from one provider to another less painful.
However, functions are not truly portable as there is currently not any standard for function entry points, returning data or for the libraries that will be available at runtime.
Event GatewayWhile each cloud provider has their own API Gateway they do not typically provide much convenience for multiple provider solutions nor ease of portability.
The Future for ServerlessIt is still a wild frontier out there with many offerings and no real standards.
Caching can be used and customized with an acceptable eviction policy based on business requirements.
It’s difficult to identify issues between microservices when services are dependent on each other and they have a cyclic dependency.
Testing — This issue can be addressed with unit and integration testing by mocking microservices individually or integrated/dependent APIs which are not available for testing using WireMock, BDD, Cucumber, integration testing.
It will trace all microservices communication and show request/response, errors on its dashboard.
Its requirements can include discovery, load balancing, failure recovery, metrics, and monitoring, and often more complex operational requirements such as A/B testing, canary releases, rate limiting, access control, and end-to-end authentication.
No?
line 4: these three lines randomly cause an exception which will result in the span (associated with the rest endpoint invocation) being tagged as an error with associated log events identifying the error details.
these are: order manager’s controller:    @autowired    private io.opentracing.tracer tracer;     @requestmapping("/buy")    public string buy() throws interruptedexception {        thread.sleep(1 + (long)(math.random()*500));         tracer.activespan().setbaggageitem("transaction", "buy");         responseentity<string> response = resttemplate.getforentity(accountmgrurl + "/account", string.class);        return "buy + " + response.getbody();    }    @requestmapping("/sell")    public string sell() throws interruptedexception {        thread.sleep(1 + (long)(math.random()*500));         tracer.activespan().setbaggageitem("transaction", "sell");         responseentity<string> response = resttemplate.getforentity(accountmgrurl + "/account", string.class);        return "sell + " + response.getbody();    }    @requestmapping("/fail")    public string fail() throws interruptedexception {        thread.sleep(1 + (long)(math.random()*500));         responseentity<string> response = resttemplate.getforentity(accountmgrurl + "/missing", string.class);         return "fail + " + response.getbody();    }    line 2: the service injects the opentracing         tracer        to enable access to the active span.
lines 6, 14, and 22: all three methods introduce a random delay.
we will show you how a baggage item can be used to isolate the metrics relevant only for a particular business transaction.
line 23: invoking a non-existent endpoint on         accountmgr        will lead to an error being reported in the trace and metric data.
the first step is to expose an endpoint for collecting the prometheus metrics.
therefore in the second method above, we add an attribute that will inform the instrumentation to ignore the   /metrics  endpoint.
in this particular trace instance, the   accountmgr  invocation has reported an error, indicated by the   error=true  tag.
the standard labels included with the opentracing java-metrics project are:   operation  ,   span.kind  and   error  .
in our example prometheus queries, we have ignored most of the kubernetes added labels (except   service  ) so that the metrics are aggregated across the specific pods, namespaces, etc.
His product, "Redis To Go", mitigates the pain involved when adding a new Redis server to manage.
1.2 contains new protocols, sequence library functions, and revamped agent error handling.
How to Get Started on The Path to Kubernetes ExpertiseIntroductionNo doubt Kubernetes is hot right now.
These single-node installations are not recommended for production deployment.
Installation and removal are single one-liners and runs completely isolated.
To avoid any conflicts with existing installs of Kubernetes, Microk8s adds a prefix the kubectl command - microk8s.kubectl .
To check the config, run:kubectl config viewFor more vital kubectl commands, here is a cheat sheet.
If you have trouble accessing the dashboard, you may need to enable the Kubernetes proxy with the command  kubectl proxy .
If you have no credentials, you will need to pull them from the system secrets.
Here is a simple command for Linux and OS X users to pull the default secrets:kubectl -n kube-system describe secret default | grep -E '^token' | cut -f2 -d':' | tr -d '\t' | tr -d " "The results will print out the token you can use to login to your Kubernetes dashboard.
To inject that token into your kubeconfig, runkubectl config set-credentials docker-for-desktop --token="$(kubectl -n kube-system describe secret default | grep -E '^token' | cut -f2 -d':' | tr -d '\t' | tr -d ' ')"Note: set the context name if you are running multiple Kubernetes clusters.
It even provides additional commands to expose your new application.
In this article, I have put together the lessons I have learned the hard way from many integration projects as an integration consultant.
Today, many integration projects demand expertise beyond the integration middleware scope.
But if you depend on others too much, that will drag the development pace.
Then, your team will be self-sufficient to solve your own problems and move fast.
Time is critical for them — no matter how much it would cost them to build the project and support it.
On the flip side, there are organizations with constrained budgets and resources.
The operations team had nightmares when they tried to deploy and troubleshoot them as they lack integration tool-specific knowledge.
Imagine the pressure being placed on both developers and operations teams if there's a situation where you have to do multiple deployments in a day.
You can't assure the availability of source and target systems as they come and go without your control.
But you have total control over how middleware behaves on a rainy day.
Most importantly, if you fail in the middle, don't do it silently.
Do the necessary logging as much as possible and implement compensating transactions to guarantee consistency after a failure.
That could go wrong in many ways.
Perhaps the middleware failed to process the request, or the target system didn't respond.
That way, you can spot the places where messages are lost.
This helps the operations team to proactively attend to issues rather than waiting for a disaster.
Debugging Tools Are Friends of Your TeamYour team members shouldn't be playing out a distributed murder mystery game right after an incident happened in the system.
There should be a proper set of debugging tools to isolate failures in systems.
Having tools that mock the source and target systems are critical in troubleshooting integration middleware in isolation.
To quickly identify integration bottlenecks, your team members should also be familiar with skills like Java heap dump analysis and SQL query tracing.
The important thing is to start small, iterate faster, and learn from your mistakes.
Comparison of Open Source API Analytics and Monitoring ToolsFor any API-first company, implementing the right API analytics platform is critical for tracking the utilization of your APIs and to discover any performance or functional issues that may be impacting customers.
Not all tools support all use cases directly, and may require heavy investment in development and integration.
On the flip side, only the most recent data is needed to answer hair on fire engineering problems, so data can be retired after a short time period such as after 24 hours.
Ripping out and changing authentication and authorization design is not an easy task and could force a complete rewrite.
You don’t want to be caught off guard and be the engineer who failed at foreshadowing typical future enhancements.
The downside is that Kibana is only compatible with Elasticsearch.
This does leave everything else up to you including configuring your data source and processing your data into a time series metric that can be displayed by Grafana.
Due to it’s time-series based architecture, Grafana’s application for high-cardinality, high-dimension analytics on API calls is limited.
Keep in mind these alerts are limited to the same time series metrics you are already monitoring in Grafana.
This makes Jaeger quite a bit of a different tool than Grafana and Kibana in that each trace is created and viewed in isolation vs monitoring metrics or logs over time.
Since traces are created in isolation, the only view is a trace view as expected.
There is no way to create trends over time.
This is also known as user behavioral analytics which enables understanding complex user flows holistically across multiple user actions and API calls rather than looking at each time-series in isolation.
This enables you to see which endpoints are causing the most performance issues broken down by each customer email.
In the real world, your applications and developers often make bad assumptions or fail to implement best practices, so with this information, you can learn something about your own projects.
Right after deploying Coolstore, we'll stop the deployments using oc rollout cancel to give us a chance to do some hacking before things are up and running:oc process -f ${COOLSTORE_HOME}/openshift/coolstore-template.yaml | \ ${ISTIO_HOME}/bin/istioctl kube-inject -f - | \ oc apply -f -for i in $(oc get dc -o name) ; do  oc rollout cancel $i  oc rollout pause $idoneAt this point, your builds should be progressing (and CPU contributing to the heat death of the universe):% oc get buildsNAME             TYPE      FROM          STATUS    STARTED          DURATIONcart-1           Source    Git@f63f51d   Running   37 seconds agocatalog-1        Source    Git@f63f51d   Running   38 seconds agocoolstore-gw-1   Source    Git@f63f51d   Running   38 seconds agoinventory-1      Source    Git@f63f51d   Running   38 seconds agopricing-1        Source    Git@f63f51d   Running   37 seconds agorating-1         Source    Git@f63f51d   Running   37 seconds agoreview-1         Source    Git@f63f51d   Running   37 seconds agoweb-ui-1         Source    Git@f63f51d   Running   38 seconds agoYou can keep running oc get builds until the STATUS column shows Complete, but you don't have to wait for it in order to continue below.
Istio currently requires that for services to participate in the service mesh, their exposed TCP ports must be named, and they must be named starting with http or https.
So here's the magic hack to do all of that to our DeploymentConfigs:for i in $(oc get dc -o name) ; do oc label $i version=v1 DCNAME=$(echo $i | cut -d'/' -f 2) PATCH=$(mktemp) cat <<EOF > $PATCHspec:  strategy:    customParams:      command:      - /bin/sh      - '-c'      - 'sleep 5; echo slept for 5; /usr/bin/openshift-deploy'  template:    metadata:      labels:        version: v1    spec:      containers:      - name: $DCNAME        ports:        - containerPort: 8080          name: http          protocol: TCPEOF   oc patch $i -p "$(cat $PATCH)"   rm -f $PATCHdoneNext, since this demo is often used on low-powered laptops, by default, we disable (scale to 0 pods) some services.
Unfortunately for us, the bind address for JBoss EAP is fixed to always bind to a private IP address on an interface that Istio does not control or proxy, so we have to resort to another hack to work around this.
Unfortunately, some features of Camel (most notably the ones we are using to implement our AggregationStrategy) strip HTTP headers when making downstream calls to other services.
So in this case, our Coolstore gateway has failed to declare port 8081 (the port on which its health probes are exposed), so the health checks will fail.
You will access the application through the Ingress route installed in the istio-system project (you did run oc expose svc/istio-ingress -n istio-system, right?)
Containerized JBoss EAP has a limitation that we worked around.
JBoss Fuse has an unfortunate side effect for HTTP headers that we worked around.
It started with Jonah Kowall, a former Gartner APM analyst, and today VP of market development at AppDynamics, writing about Misunderstanding "Open Tracing" for the Enterprise.
His criticism is that OpenTracing is not a broadly adopted open standard and explains that Enterprises have highly heterogeneous environments that require automated, agent-based tracing.
Alois Reitbauer, Chief Technology Strategist at dynaTrace, reacts in his blog A CTO's strategy towards OpenTracing that OpenTracing is not a standard and agrees with Jonah about the need for standardization of TraceContext.
In microservices architectures, understanding dynamic dependencies using topology and graph analysis is more difficult than in traditional architecture.
Instana's AI leverages our unique Dynamic Graph which provides the context to pinpoint to the root cause of problems and ultimately arrive at accurate causation.
We predict that there will be no ultimate reason to drive to a single tracing standard.
in that case, i stop my vpn, invoke   minikube delete#  , delete the   .minikube  directory, restart my machine and start it again.
you only need the docker cli and to point it to minikube:$ eval $(minikube docker-env) to stop the cluster run this command:$ minikube stop  istio  to download istio, run this command:$ curl -l https://git.io/getlatestistio | sh - follow the instructions in the terminal to set the path.
Distributed Tracing With Zipkin and ELKWhile logs can tell us whether a specific request failed to execute or not and metrics can help us monitor how many times this request failed and how long the failed request took, traces help us debug the reason why the request failed or took so long to execute by breaking up the execution flow and dissecting it into smaller events.
In our case, I want to pass some environment variables and specify Elasticsearch as the storage type (this example assumes a locally running ELK Stack), and so I will use the following commands to download and run Zipkin: curl -sSL https://zipkin.io/quickstart.sh | bash -sjava -DSTORAGE_TYPE=elasticsearch -DES_HOSTS=http://127.0.0.1:9200 -jar zipkin.jarIf all goes as expected, you should see this output in your terminal: 2018-05-02 09:59:27.897  INFO [/] 12638 --- [  XNIO-2 task-1] o.s.web.servlet.DispatcherServlet        : FrameworkServlet 'dispatcherServlet': initialization completed in 108 msOpen Zipkin at http://localhost:9411:Of course, we have no traces to analyze in Zipkin yet, so our next step is to simulate some requests.
Azure FunctionService Deployment PlatformDocker Swarm + KubernetesElastic BeanstackCross Cutting MS ChasisSpring Boot and Spring CloudExternal ConfigurationsSpring Cloud Config ServerKubernetes Config ServerAPIAPI GatewaySpring Cloud API GatewayMule Soft API GatewayAWS GatewayI won't explain each and every component and software mentioned above.
Following things can be done to resolve these concerns  Prepare Test suite that test services in the isolation  Deploy all micro-services and perform end to end testing with all live servicesMock other micro-services during unit/integration testingWrite the Stubs Above mentioned solution has its own advantages and disadvantages and should be               considered while applying one of them.
“Distributed tracing” allows DevOps to automatically follow the path that any and every request takes across microservices; with ordinary transactions involving hundreds or thousands of distinct services in the span of a quarter second, there’s no other way to understand and explain the behavior of today’s intricate distributed systems.
Sigelman is now the co-founder of LightStep, which is still in stealth.
However, the solutions that engineering departments have depended on for the last decade – logging, metrics and conventional APM (and even how they deploy and do security) – depend on built-in assumptions about system architecture that no longer hold.
With the adoption of DevOps, distributed tracing needs to be automated so companies are able to explain any and every request, sort the signal from the noise, and know where the problem is that needs to be fixed.
1.2September, 2017Addition of Health Check 1.0,Metrics 1.0, Fault Tolerance 1.0,and JWT Propagation 1.0.Updating to Configuration 1.1.
1.4June 2018Updating to Open Tracing 1.1,REST Client 1.1, Fault Tolerance1.1, JWT Propagation 1.1, andConfiguration 1.3.
2.2February, 2019Updating to Open Tracing 1.3, Open API 1.1., REST client 1.2, and Fault Tolerance 2.0.
Fault Tolerance 2.0 was upgraded to CDI 2.0.
This is a lower level SPI style technology that will likely be incorporated into other parts of MicroProfile going forward.
The Open Tracing specification solves this problem by providing a standard for instrumenting microservices for distributed tracing in a technology agnostic manner.
Rest Client JAX-RS provides a powerful client API, but it can be hard to use and not really type safe.
Fault ToleranceMicroservices, especially running on the cloud, are inherently unreliable.
SECONDS)@Timeout(value=3, unit=ChronoUnit.SECONDS)@Bulkhead(2)public Membership getMembership(   @NotNull @PathParam(value = "id") int id) {In the code example, the @Timeout annotation causes the method call to fail unless it executes within three seconds.
Once a failure occurs, the method can be accessed again after 10 seconds.
The cluster can be used as a standalone instance for upper-level applications and can meet the business needs of low latency, high concurrency for massive-scale data.
Because Mishards is a stateless service, it does not save data or participate in complex computation.
Firstly, the memory size should be large enough to avoid too many disk IO operations.
Search has extremely high requirements on CPU and GPU configurations, while insertion or other operations have relatively low requirements.
In terms of service quality, when a node is performing search operations, the related hardware is running in full load and cannot ensure the service quality of other operations.
Only one writable node is allowedCurrently, Milvus does not support sharing data for multiple writable instances.
Because the data to process does not increase, the processing power for the same data shard also increases linearly.
Milvus nodes register their information when going online and log out when going offline.
Currently, Mishards provides a consistent hashing strategy based on the lowest segment level.
To pinpoint problems, you need to log to the corresponding servers to get the logs.
The services are exposed through HTTP.
The actual operations are mocked here, where the implementation does the interactions with the required services.
The simulator application execution takes in two runtime parameters, where the first parameter represents the delay between session executions in milliseconds and the second parameter represents the total number of transactions to execute.
Let’s look at a scenario of an error happening during a checkout and how this is shown in the trace.
Figure 5 - Checkout error execution trace inspectionIn this trace, we see that we have incurred an error at the “addItem” operation in the “ShoppingCart” service.
Here, we see the SQL query that has been executed at this time, and also, the log entry shows that we have tried to insert a duplicate entry, thus the database operation has failed.
Figure 9 - SQL Client Metrics DashboardHere, we can see the SQL execution counts that are happening at the moment, the error rates, response time percentiles, and the top SQL statements that are executed.
Inbound proxy architectureProxies and MicroservicesReverse proxies are a critical component of microservices applications.
Now it's time to get your hands dirty and learn by doing.
Speakers: Bernard Tison, Maciej Swiderski, Duncan DoyleDeveloping Applications on OpenShift as Mere MortalsMany developers face challenges and frustration when learning to develop applications on a platform that's new to them.
Thus, it not only provides better context switching between logs and metrics but also avoids full index logging.
Promtail exposes this custom metric through its /metrics endpoint.
Nginx is exposed via NodePort.
Shell     xxxxxxxxxx             1                         10                                                  1             └─ $ ▶ kubectl get pods              2             alertmanager-prometheus-prometheus-oper-alertmanager-0   2/2     Running   0          3m14s              3             loki-0                                                   1/1     Running   0          82s              4             loki-promtail-n494s                                      1/1     Running   0          82s              5             nginx-deployment-55bcb6c8f7-f8mhg                        1/1     Running   0          42s              6             prometheus-grafana-c4bcbbd46-8npgt                       2/2     Running   0          3m44s              7             prometheus-kube-state-metrics-6d6fc7946-ts5z4            1/1     Running   0          3m44s              8             prometheus-prometheus-node-exporter-jwz64                1/1     Running   0          3m44s              9             prometheus-prometheus-oper-operator-f8df9fcf9-kc5wc      2/2     Running   0          3m44s              10             prometheus-prometheus-prometheus-oper-prometheus-0       3/3     Running   1          3m2sLogging, Monitoring and AlertingTo simulate the condition for firing the alert we will run the following shell commands.
No changes are needed to the application itself.
No.
Since fluentd is shipping logs across the Kubernetes cluster, I’m using a search to narrow down on Istio logs only:kubernetes.namespace_name : "istio-system"Metric visualizationNo.
The technology itself is still relatively immature, so there is some risk involved.
As the technology matures, and costs and risks gradually go down, the tipping point for adopting service mesh is fast approaching.
Why Service Meshes Matter to DevelopersWhen developers deploy microservices to the cloud, they have to address nonfunctional microservices capabilities to avoid cascading failures, regardless of business functionalities.
Being able to remove a pod from load balancing based on poor performance (or non-performance) is a powerful feature of Istio, and this blog post demonstrated that point.
Again, without changing the source code, we saw how to direct traffic and handle network faults by YAML configuration files and some terminal commands.
Week six switched from monitoring and handling errors to creating errors: fault injection.
Being able to inject faults into your system without changing source code is an important part of testing.
Test undisturbed code means you can be assured that you didn't add any "testing code" that may have, itself, caused a problem.
Between zero cost, the Red Hat Developer OpenShift Container Platform, and our Istio tutorial, plus other assets available on our Service Mesh microsite, you have all the tools you need to start exploring OpenShift, Kubernetes, Linux containers, and Istio with zero risk.
JS engines such as Google's V8 and Opera's revamped Carakan are locked in an escalating competition to dominate the benchmarks.
In the increasingly fierce Browser Wars, TraceMonkey is Firefox's secret weapon, but in some situations it can't do very much.
The only catch is that when the JavaScript doesn't contain many loops or something causes the tracing to stop, the performance goes down the drain.
Complex code with recursion or a lot of nesting, for example, can cause tracing to stop.
These two compilers perform better than SpiderMonkey when tracing doesn't work.Mozilla developer David Anderson says this performance pitfall is unacceptable.
Anderson also notes that this performance was achieved without much optimization work.
Prometheus and CloudWatch are very different in the problem they solve, and a 1:1 comparison seems unfair, but as you start moving to cloud-native stack, Prometheus starts popping up in conversations and for many right reasons.
For example, currently, CloudWatch does not support Kubernetes metrics (Issue link here).
These metrics are then used to configure alarms and statistics.
Working of CloudWatchAn application or a service publishes metrics (a set of data points ordered by time) in a namespace (a construct to provide isolation of metrics, for example, or AWS/ApiGateway ) with dimensions (an identity key/value pair used to filter and lookup metrics).
Working of PrometheusAn application exposes metrics (or uses the exporter to do so) at a specific endpoint, which is scraped by the Prometheus server (this also acts as a sink for storing metric time-series data).
Prometheus works on a pull-based mechanism where it scrapes metrics exposed by applications at a specific endpoint.
When we say 'cost' here, it is not just the $ value.
Also, CloudWatch does charge for each additional dashboard, and in Prometheus setup, there is no reason not to create a dashboard if you need to.
We are looking at approximately 100 dashboards and different types of custom metrics used by 100 odd resources causing 1000s of alarms over a month by each major category.
Then we also add up 1,000 alarms for EC2 instances and 500 alarms each for other services.
Also, the cost of instances considered here is for on-demand and might vary for reserved or spot instances that have lower cost — and that is something we are not taking into account.
Isn't that cool?
How do you return error response and valid response from the REST API?
For returning error, will use the appropriate status code, such as, if the product is not found in the inventory, return 404 error.
If some unhandled exception occurs, return 500 error and so on.
Quickly though, using naive I/O for debugging becomes really tedious.
You have a hard time finding the information you need.
Things are just a mess.
Now let's add some log4cpp code:     C++     xxxxxxxxxx             1                                                  1             class printer {              2             private:              3                 log4cpp::Category& log;              4                 const std::string message;              5                           6             public:              7                 printer();              8                 void print() const;              9             };and     C++     xxxxxxxxxx             1                                                  1             class fancy_printer {              2             private:              3                 log4cpp::Category& log;              4                           5             public:              6                 fancy_printer();              7                 void print(const std::string message) const;              8             };This has forced a couple of changes on us.
Overall, kind of a naive approach.
We're going to use a simple properties file that copies our previous functionality and configures the loggers at different levels:     Properties files     xxxxxxxxxx             1                                                  1             log4cpp.rootCategory=WARN, rootAppender              2             log4cpp.category.printer=DEBUG              3                           4             log4cpp.appender.rootAppender=ConsoleAppender              5             log4cpp.appender.rootAppender.layout=BasicLayoutHere, we've configured the logging system as a whole to log at the WARN level, while the printer logger is configured to log at DEBUG.
If there was a failure, a user would pick up the phone to inform the help-desk that the app is broken.
Troubleshooting was all reactive and the only path to resolution was for someone to roll up their sleeves and go in and look at log files and manually fix errors by themselves.
Failure costs time and money to fix and ruins brand value.
Uptime, scalability and the ability to identify and fix errors are the factors that determine if you stay in business or lose to the competition.
You expose your application’s metrics using the Prometheus client libraries.
Errors: Number of HTTP 500s, non-zero return codes and any other indicators of a failure.
Saturation: An indicator for how "full" is the service — for example, 100% CPU, disk or network utilization or resource pressure of any kind that indicates an overload.
The dashboard should tell you what is wrong without you having to search inside logs.
Failures are not always on the application side; it can also be in the network connectivity and localized to certain geographical regions.
Such observations do not have to be restricted to “is it running” but can also be used to keep an eye on latency and quality of service.
Alerts are not just for infrastructure incidents like resource shortages and failures, they can also be used at the application level to notify application owners of exceptional events that the application cannot handle on its own.
Consider an e-commerce system that has taken payment for a product, but had an internal failure before the order was fully processed.
Distributed TracingOne of the challenges with moving to microservice-based architecture is that the call stack between microservices can grow tall and it can get difficult to know where the performance bottlenecks are, or to get a view of the dependencies between the various services.
The visual nature of these tools make it very useful in a crisis to understand where the bottlenecks are.
But what is new, is that unlike the old days, you do not want to start with the logs when there is an issue.
Ideally, all of the other observability features should tell you the issue, and you should scan through the logs only to confirm your theory of why the failure happened based on what you saw in other observability mechanisms.
SentrySentry is a more specialized tool with a very specific purpose — to surface those errors and exceptions in your application code.
The first one is that a cloud-native application should not attempt to create, manage, or rotate log files.
SynopsisThe purpose of making your applications and infrastructure more observable is not merely to ‘know’ what is going on, but to take that to the next level and prevent them from failing in the first place.
Failing which, to set up automated recovery mechanism.
Or at least be notified of the failure so you can act on that failure, in that order.
The worst failures are the ones which happen quietly without any indication of an issue having happened.
The best failures are the ones where the system detects a failure before it happens and takes remedial action to prevent the failure.
Most failures fall somewhere along the two ends of this spectrum.
Most issues just result in degraded performance.
A well-engineered observability framework will give you exactly that — a list of problems that you need to address in your application’s code and architecture to grow with your business.
The critical thing is event producers, consumers, and brokers are decoupled, which allows them to be scaled, updated, and deployed independently.
Pre-built connectors to produce and consume events from a variety of event sources — SourceDesigned To Handle Highly Scalable and Resilient DeploymentsEvent-driven applications demand on-demand scaling and high resiliency in the face of a failure.
Microsoft has identified the problems with proprietary Schema Registry APIs and submitted a vendor-neutral API specification to CNCF.
Let's launch the Employee service and submit a new leave application.
Employee service: Add a new leave application.
Let's now launch the Manager service to view the recorded leave application and action it as follows: Manager service: Reject leave application.
Finally, let's view the status of the leave application by launching a new instance of the ResultReader service as follows: Result reader service: View status of leave application.
However, we had to create Event Hub instances through the portal because Event Hubs do not support the Kafka Admin APIs.
Kubernetes defines a set of unique building blocks that collectively provide mechanisms to deploy, maintain, and scale applications.
Otherwise, the application code will not be well-fitting to the Kubernetes building blocks.
Dockerfile    xxxxxxxxxx            1                       20                                              1            # Auto Generated Dockerfile             2            FROM ballerina/jre8:v1             3                         4            LABEL maintainer="dev@ballerina.io"             5                         6            RUN addgroup troupe \             7                && adduser -S -s /bin/bash -g 'ballerina' -G troupe -D ballerina \             8                && apk add --update --no-cache bash \             9                && chown -R ballerina:troupe /usr/bin/java \             10                && rm -rf /var/cache/apk/*             11                         12            WORKDIR /home/ballerina             13                         14            COPY ordermgt.jar /home/ballerina             15                         16            EXPOSE  8081 9797             17            USER ballerina             18                         19            CMD java -jar ordermgt.jar --b7a.observability.enabled=true --b7a.observability.metrics.prometheus.port=9797Listing 2: Autogenerated OrderMgt DockerfileAs you can see, the Ballerina compiler read the source code and constructed the Dockerfile to build the image.
The Kubernetes service will be act as an internal load balancer with the default clusterIP type, where it’s only available to the internal network by blocking all external traffic to it.
Then it will generate the corresponding cloud load balancer config to expose admin services.
But one of the common mistakes seen in many projects is not considering the deployment aspect while developing the application.
We are believers that containers and Kubernetes will be the way companies build infrastructure in the future, and waiting to hop on that train will only be a competitive disadvantage.
Generally, we find that the answer to this question usually hinges on whether or not you are committed to cloud-native.
If you are using distributed tracing tools such as Jaeger or Zipkin, you no longer need to continue managing them separately as they are part of the Istio toolbox.
Leave a comment here or hit me up @zjory.
When you have more entry points, you have more places to worry about securing.
The more entry points to an application, the broader the attack surface is.
Once a request is inside the application layer, you don’t need to worry about security when one component talks to another.
The Broader the Attack Surface, the Higher the Risk of AttackIn a monolithic application, communication among internal components happens within a single process — in a Java application, for example, within the same Java Virtual Machine (JVM).
As the number of entry points to the system increases, the attack surface broadens too.
These repetitive, distributed security checks and remote connections could contribute heavily to latency and considerably degrade the performance of the system.
Jack Kleeman, a backend engineer at Monzo, explains in a blog (http://mng.bz/gyAx) how they built network isolation for 1,500 services to make Monzo more secure.
Also, you need to be able to revoke certificates (in case the corresponding private key gets compromised) and rotate certificates (change the certificates periodically to minimize any risks in losing the keys unknowingly).
A high number probably indicates that the system is under attack or the first-level defense is weak.
At any point, you can kill a running container and create a new one with the base configuration without worrying about runtime data.
Nothing is shared among microservices (or only a very limited set of resources), and the user context has to be passed explicitly from one microservice to another.
It also exposed a REST endpoint for search policies.
Each product had a code, name, image, description, cover-list, and question-list, which affected the price defined by the tariff.
On Windows, it’s a bit problematic because you have to do it through a bash (cygwin/GIT bash) or Linux subsystem for Windows.
Currently, we have two options for configuring MongoDB: non-blocking or blocking.
pom.xml        <dependency>            <groupId>io.micronaut.configuration</groupId>            <artifactId>mongo-reactive</artifactId>        </dependency>application.ymlmongodb:    uri: "mongodb://${MONGO_HOST:localhost}:${MONGO_PORT:27017}/products-demo"    cluster:      maxWaitQueueSize: 5    connectionPool:      maxSize: 20Then the non-blocking MongoClient will be available for injection and can be used in our repository:import com.mongodb.client.model.Filters;import com.mongodb.reactivestreams.client.MongoClient;import com.mongodb.reactivestreams.client.MongoCollection;import io.reactivex.Flowable;import io.reactivex.Maybe;import io.reactivex.Single;import lombok.RequiredArgsConstructor;import pl.altkom.asc.lab.micronaut.poc.product.service.domain.Product;import pl.altkom.asc.lab.micronaut.poc.product.service.domain.Products;import javax.inject.Singleton;import java.util.List;@Singleton@RequiredArgsConstructorpublic class ProductsRepository implements Products {    private final MongoClient mongoClient;    @Override    public Single add(Product product) {        return Single.fromPublisher(                getCollection().insertOne(product)        ).map(success -> product);    }    @Override    public Single<List> findAll() {        return Flowable.fromPublisher(                getCollection().find()        ).toList();    }    @Override    public Maybe findOne(String productCode) {        return Flowable.fromPublisher(                getCollection()                        .find(Filters.eq("code", productCode))                        .limit(1)        ).firstElement();    }    private MongoCollection getCollection() {        return mongoClient                .getDatabase("products-demo")                .getCollection("product", Product.class);    }}Exposing REST EndpointsREST endpoints are a basic way of communicating between the server application and the client application.
It is horizontally scalable, fault-tolerant, wicked fast, and runs in production in thousands of companies.
Unfortunately, this is impossible, but in the next version (RC1), this should already be possible.
For now, we have solved this problem by adding a method to an existing client and overwriting defined paths.
@Client(id = "policy-search-service", path = "/policies")@Retryable(attempts = "2", delay = "3s")public interface PolicySearchGatewayClient extends PolicySearchOperations {}This results in a retry two times with a delay of three seconds between each.
We should have an emergency plan called a fallback mechanism.
A fallback mechanism is a second way of doing things, in case the first way fails.
Instead, each microservice needs a dynamic port allocation to avoid collisions during replication.
You’ll find the answer in the Micronaut FAQ: The majority of Consul and Eureka clients that exist are blocking and include a mountain of external dependencies that inflate your JAR files...Micronaut’s DiscoveryClient uses Micronaut’s native HTTP client, thus greatly reducing the need for external dependencies and providing a reactive API onto both discovery servers.
Scheduling can be configured at a fixed rate (fixedRate), with a fixed delay (fixedDelay), or as a cron task (cron).
Sensitive data must be restricted to authenticated users.
ProsYou can access various data stores, both in blocking and non-blocking ways, connect your services via REST HTTP calls or asynchronously through a message broker, and secure your system with JWT.
Perhaps Grafana's most unique feature is that its data source neutral, meaning it doesn't matter where your data is stored, Grafana can unify it.
No matter the source, the data stays where it is, and you can visualize and analyze it at will.
Grafana was born, and its open-source nature soon made it a hit among people who had been looking for a solution to the specific data visualization problems they also had.
For example, Emergency Rooms in Tokyo use Grafana to monitor wait times across the city, relying on the visualizations to check where any potential problems may be.
Many of the services are old and too brittle to update, so we can't update the original service, plus it would take to long to update each service.
It exposes an index route which just returns a 'hello' message.
Let's fire up our deployment, and our service: $ kubectl create -f ./deployment.yml,./service.yml.
We’ve known that distributed systems are hard.
From a theoretical perspective, the difficulty mostly arises from two key areas: consensus and partial failure.
Nearly all distributed systems research attempts to grapple with this problem in some way.
Given that distributed consensus is hard, how does this problem manifest in the context of microservices?
This approach both does and doesn’t solve the consensus problem.
Partial FailureConsider an HTTP request serviced by a monolith.
If there is a problem, be it a software bug or hardware failure, the entire monolith crashes – every failure is a total failure.
Now suppose one of those microservices fails.
Partial failure has been described as an unqualified good thing.
Furthermore, they’re CPU and memory intensive, making them painfully slow on a laptop.
Note, check out Kelda, and specifically our whitepaper for a detailed description of this problem.
make the problem tractable, but it’s still hard.
Microservices Make Sense, SometimesFrom a technical perspective, microservices are strictly more difficult than monoliths.
This means that teams can move quickly without waiting for the lowest common denominator to get their code QA’d and ready for release.
Still, these systems are, themselves, extremely complicated, proving my point: distributed consensus is hard.
50+ Useful Kubernetes ToolsUpdated September 2019In the last few years, Kubernetes has laid waste to its fellow competitors in the battlefield of container orchestration.
Amazon EKS manages your Kubernetes infrastructure across multiple AWS Availability Zones, while automatically detecting and replacing unhealthy control plane nodes, and providing on-demand upgrades and patching.
Link: https://aws.amazon.com/eks/Cost: Pay for the resources usedMonitoring Tools10.
Searchlight periodically runs various checks on Kubernetes clusters and alerts you via email, SMS or chat if something goes wrong.
Also, it can enhance Prometheus monitoring with external black-box monitoring and serves as a fallback in case internal systems completely fail.
CAdvisor Kubelet exposes these metrics through Kubelet APIs (with a default of one-minute resolution).
The Metrics Server identifies all available nodes and calls Kubelet API to get containers and nodes resources usage before exposing the metrics through Kubernetes aggregation API.
Kube-monkeyKube-monkey is the Kubernetes’ version of Netflix's Chaos Monkey.
Kube-monkey is a tool that follows the principles of chaos engineering.
Kube-monkey is also configured by a TOML file where you can specify which app is to be killed and when to practice your recovery strategies.
Test-infra includes a few dashboards for displaying history, aggregating failures, and showing what is currently testing.
PowerfulSealPowerfulSeal is a tool similar to Kube-monkey and follows the Principles of Chaos Engineering.
PowerfulSeal can kill pods and remove/add VMs from or to your clusters.
With Sysdig Secure you can Implement service-aware policies, block attacks, analyze your history, and monitor cluster performance.
CabinNOTE: This project is currently not under active developmentCabin functions as a mobile dashboard for the remote management of Kubernetes clusters.
Deis WorkflowNOTE: This project is no longer maintainedDeis Workflow is an open source tool.
FaaS has Prometheus metrics baked-in, which means it can automatically scale your functions up and down for demand.
It's a common misunderstanding that when tracing with your service mesh, there aren't any code changes.
Analyzing the potentially lag inducing endpoint is as simple as clicking on the “Analyze Calls” button on the upper right-hand side, sorting by latency, and digging into the calls which matter - in this case, the slow ones (fig.
When Network Timing Is EverythingWhen observing distributed applications, networking and external service dependencies can become a major contributor to performance degradation.
The service is built to handle a specific domain and declares its boundaries accordingly; these boundaries are exposed as a contract, which technically can be implemented by API.
Loose Coupling, High CohesionChanging one microservice does not impact the other microservices, so altering our solution's behavior can be done only in one place while leaving the other parts intact.
In short, it means shortening the features’ time-to-market while reducing risks.
Therefore, the SDLC agility is affected; releasing a new feature incurs more testing time, prudent releases, and overall prolonged time-to-market.
Adding more services when the load rises is simple, and similarly, when the load is low, the number of running services can be reduced.
Tackling and troubleshooting can be isolated if the source of the problem is known.
After that, resolving the problem is an isolated chirurgical fix.
Technology-binding is limited to the service level.
This has its toll; lack of standardization costs more in testing, deploying, and maintaining the system.
Standardization should include the API the service exposes and the response structure, and the error handling.
It also enables the service consumers to be aware when the service has changed and avoided breaking the existing contract and the communication between them.
Naturally, this can be prioritized low in the tasks list of the developer.
However, if the environment is dynamic and frequent changes of services' paths are required, choosing DNS may not be the best solution since updating the DNS entries can be painful.
Using load-balancers as a buffer between the service and its consumers is one option to bypass this problem.
Instances are automatically assigned to the network location, so maintaining a central configuration is not efficient and almost not practical.
However, the disadvantages of the client-side discovery approach are:The clients are coupled to the service registry; without it, new clients may be blind to the existence of other services.
It hurts if the services differ by their tech-stack, which means this logic should be implemented across different technologies.
In this approach, the client is more naive, as it communicates only with a load-balancer.
However, this is also a disadvantage.
Only the registrar interacts with the service registry; the new microservice does not play actively in the registration process.
Therefore, this approach is not recommended.
In the monolithic application, transactions are ACID compliant: Atomic, Consistent, Isolated, and Durable.
One alternative is to implement a distributed transactions mechanism, but it is hard to develop and maintain.
There is no guarantee for immediate and isolated transactions in this model, but there is a goal to achieve consistency eventually.
We need to gauge the required hardware to supply a varying demand.
Firstly, even the cloud resources are limited in their compute and storage.
Planning for ResilienceThe microservices solution is composed of many isolated but communicative parts, so the nature of problems differs from the monolith world.
Failures happen in every system, all the more so in a distributes interconnected system; therefore, planning how to tackle these problems on the design stage is needed.
Some of these patterns are:TimeoutsThis is an effective way to abort calls that take too long or fail without returning to the caller promptly.
When a timeout happens, it needs to be logged as it might be caused by a network problem or some other issue besides the service itself.
RetriesFailing to receive a valid response can be due to an error or timeout.
It protects the service itself from being loaded and keeps the clients from sending futile requests.
It can reduce the latency by signaling; there is no point in executing retries.
Second, the service can disregard requests from clients it deems as nonessential or not-important in a given time; it prioritizes other services.
Lastly, to consume fewer resources and supply the demand, the service can continue providing service but with reduced quality; for example, lower resolution video streaming rather than cease the streaming totally.
Another benefit of using throttling is enhanced security; it limits a potential DDOS attack, prevents data harvesting, and blocks irresponsible requests.
BulkheadsThis pattern ensures that faults in one part of the system do not take the other parts down.
In a microservices architecture, when each service is isolated, naturally, it is limited when a service is down.
Besides partitioning the services, consumers can be partitioned,  meaning isolating critical consumers from standard consumers.
Similarly, a service can be flagged as down or unavailable after clients fail to interact with it.
Then, clients will avoid sending more requests to the “blown” service and offload the system's void calls.
In some of the patterns above, a client failed to receive a valid response from the server.
How to handle these faulty requests?
In the synchronous call, the solution can be logging the error and propagate it to the caller.
Nevertheless, the Choreography approach is more difficult to monitor since there is no central controller, and there is no mediator to intervene and prevent cascading failures.
Another drawback can happen when using REST response adversely when returning more information than needed or deserializing all the underlying objects as they are represented in the data layer, which means not using abstracting the data model.
It also keeps track of what message was seen to avoid multiple consumers for the same message.
Using message broker adheres to the concept of smart endpoint, dumb pipes.
Since communication is async, the overall solution should include advanced monitoring, correlation IDs, and tracing to tackle failures and problems (described in the Monitoring chapter, Part 4).
This problem is magnified when dealing with certificate rotation.
Understanding the complete flow of the data is arduous, not to mention understanding where errors stem from.
To overcome this intricacy, there’s a need to reconstruct the chain of calls to reproduce a problem and remediate it thereafter.
Using a logging system, combined with monitoring and alerting solutions, is essential to understand the chain of events and to identify problems and rectify them.
Distributed TracingIt is important to standardize the log message format in any log aggregation solution and define all log levels (debug, information, warning, errors, fatal).
A service’s failure might be hidden or seem negligible, but it can greatly impact the system’s functionality.
One way to mitigate this problem by re-playing messages in a testing environment, but sometimes physical or regulatory limitations do not allow it.
These metrics can reveal whether a service is working as per normal or some problems or latency.
For example, if we know the throughput between 1 pm to 2 pm every day is 20K messages, there may be a problem if the system deviates from this extent.
The data is presented in dashboards, which reflect and reveal problems and anomalies.
The results are collected by the monitoring tools and trigger alerts in case of errors or problems.
It is easy to fall into the trap that many errors raise alerts, and by that, the alerts mechanism is being abused and becomes irrelevant.
Beware of alerts fatigue, as it leads to overlooking critical alerts.
This development means Kubernetes is becoming even more critical and an even more viable option for enterprise applications, including legacy ones, many of which require RDBMSs.
Kubernetes is still difficult to run and operate at scale, particularly for multicloud/hybrid environments- spanning on-premises data centers and public cloud infrastructure.
Despite their popularity, they still may require you to use other products-or even pair with Istio itself to match Istio's feature set.
These processes are themselves prone to error, in addition to being potentially costly and time-consuming.
However, they will likely incur costs and experience delays in the process, and they are more likely to make mistakes in the implementation of either functionality or compliance.
These gateway abstractions can be configured to allow you to define policies for retries and timeouts, to inject faults into the system at will to test its resilience, to direct traffic to legacy services, or even to add services in another service mesh through a multicluster configuration.
Observability: A Deeper Dive Service meshes offer centralized, platform-level solutions to the general problems surrounding the observability of applications.
These benefits do not come for free, however, and service meshes such as Istio are famous for their management complexities.
In this tutorial, I’m going to show you how to bootstrap a Spring Boot web application, containerize this using the latest Spring buildpacks, and deploy this onto an application-ready Kubernetes cluster with the app exposed to users via the Ambassador API gateway.
If you run the application in your IDE the Spring skeleton provided by the Initializr should spin up an embedded Tomcat server and expose your application via port 8080 on localhost.
As of Spring Boot 2.3.X this is no longer the case, as you can now take advantage of inbuilt container buildpacks.
If you are using something other than Docker for Mac you may have to figure out the default IP address that Docker is exposing apps on, rather than using localhost.
Initializing Kubernetes With the K8s InitializerNow you need to initialize your empty new Kubernetes cluster so that you can expose your application to user traffic.
You’ve built a simple Spring Boot app, containerized it, deployed this to Kubernetes, and exposed it to end users via an API gateway!
If you mess something up badly, you can simply start again.
Although the combination of those things might seem contradictory at first, this series of articles tries to reveal the reasons why the collection of practices that today we know as DevOps and SRE (Site Reliability Engineering) are becoming the norm for modern systems.
— John Shook          John Shook model for culture transformation      The traditional western approach to change culture has always been an attempt to change the way people think and how they behave and with that, change the way people act.
Changing culture doesn't have to be something abstract and hard to describe.
As a matter of fact, it is very hard to really determine if processes drive the use of technology or vice-versa.
Please...Just Stop.
Both on-premises and public cloud infrastructure have their own difficulties, and it's important to take the Kubernetes architecture into account.
Big clusters put a higher burden on the master nodes, and they need to be sized appropriately.
It's recommended to run at least three nodes for etcd, which allows a single node failure.
Adding more nodes will protect against multiple node failures simultaneously (5 nodes/2 failures and 7 nodes/4 failures), but each node added can decrease Kubernetes' performance.
For master nodes, run two protects against failure of any one node.
For both the etcd cluster and Kubernetes master nodes, designing for availability across multiple physical locations (such as Availability Zones in AWS) protects the Kubernetes environment against physical and geographical failure scenarios.
While some instance types are explicitly a bad idea (for example, VMs with partial physical CPUs assigned or with CPU oversubscription), others might be too expensive.
Kubernetes allocates an IP block for pods, as well as a block for services.
Depending on the pod network type — overlay or routed — additional steps have to be taken to advertise these IP blocks to the network or publish services to the network.
Introduction to Monitoring Serverless ApplicationsServerless computing as an architectural pattern is now widely-adopted, and has quite rightly challenged traditional approaches when it comes to design.
"Properly" can be defined by multiple parameters:Errors: every request or event that yielded an error resultLatency: the amount of time it takes for a request to be processedTraffic: the total number of requests that the resource is handlingCompounded together, monitoring allows us to detect highly errored services, performance degradation across our resources, and even scaling issues when we hit higher traffic rates.
Error counts and availability (derived from errors/invocations ratio).
They often include very verbose information — that's why they are a necessity for debugging a problem.
With this in place, when a function fails you, you can simply query and find the corresponding log for the specific function invocation.
The main problem that is often associated and attributed to logs, is that they have minimal or no context.
When being applied correctly we can utilize it to find the root cause of our errors with minimal effort.
Let's imagine for example that we've built a blog site that has the following public endpointsView an existing post /postPost a new blog post /new_postAnd it consists of these resources and services:Now, by using the monitoring and logs methods from before we've noticed that there's an error in our Post Analysis lambda.
A transaction is basically a story of how data has been transmitted from one end to the other within the systemWith an engine such as Jaeger we can view the traces organized in a timeline manner:This way we can try and find the exact time this error happened and therefore find the originating event that caused the error.
By utilizing Epsagon, a purpose-built distributed tracing application which we earlier introduced, we can see that the errored lambda in question has received its input from a malformed lambda (Request Processor) two hops earlier, and that it handled an authentication error and propagated a false input to the Post Analysis lambda via the SNS message broker.
It's important to remember that when going serverless we have broken each of our microservices into nano-services.
Each of them will have an impact on the other, and attempting to figure out the root cause can be very frustrating.
For Lambda functions, basic alerts can be configured in CloudWatch Alarms:In this example, we want to get notified when we breach the threshold of 10 or more errors within 2 consecutive 5-minute windows.
Alert regarding the performance degradation of a resource (for example Redis).
How to Photograph a Black Hole — Observing Microservices With OpenTelemetryMicroservices architecture is a software development architectural style, whereby a complex business problem is solved by a suite of small services.
Event Horizon Telescope releases first-ever black hole imageYou don’t have to be a unicorn, like Amazon or Netflix, in order for your microservices architecture to turn into the death star or a murder mystery for every outage.
Metcalfe’s Law is related to the fact that the number of unique possible connections in a network of n nodes can be expressed mathematically as the n(n-1)/2:Metcalfe’s LawAs the chart above depicts, as the number of nodes increases, the explosion of interconnections occurs quickly and any service could fail or behave unexpectedly.
You must operate with the mantra that all systems eventually fail.
A single microservice may fail, leading one to the false conclusion that this microservice is the culprit for an outage; but in fact, it is likely due to one of its dependencies which could be another microservice(s), file system, database, cloud service, etc.
Every interaction between a service and its dependencies is a potential cause for failure, compounding the points of failure in the architecture even further beyond the graph of microservice nodes.
OpenTelemetryGiven all the potential failure locations how do you separate the signal amid the noise so that you can do preventive/proactive maintenance of the system?
Distributed tracing helps pinpoint where failures occur and what causes poor performance.
It took half a ton of hard drives to store the Black Hole image which adds up to five petabytes.
Ops would say that a microservice architecture without distributed tracing is “embellished dark source” as they’ve to cope with the death star.
An application connector module which connects any application either on-premise and/or cloud-native with Kyma and registers its APIs/events through the Kubernetes Service CatalogA serverless module that exposes Lambda functions as FaaS.
We want to validate the user credentials (via email) with an external service for fraud check.
Kyma — via its Service Catalog Component — knows the web services that are exposed by Hybris.
Lambda invokes an external email microservice to validate the email ID for fraud check.
ConclusionThe biggest benefit of Kyma is that you no longer have to add custom code to those legacy enterprise applications, as custom code typically introduces performance and upgrade risks.
Find out more:Providing metrics from a microserviceHandle Unexpected Failures in Your Microservices (Fault Tolerance 1.0)The Fault Tolerance component provides an API and annotations for building robust behavior to cope with unexpected failures in your microservice.
Find out more:Building fault-tolerant microservicesPreventing repeated failed calls to microservices (interactive guide)Determine a Microservice’s Availability (Health Check 1.0)The Health Check component provides a common REST endpoint format to determine microservice availability.
This will encrypt Secret resources in etcd, preventing access to your etcd backups from viewing the content of those secrets.
Encryption offers an additional level of defense when backups are not encrypted or an attacker gains read access to etcd.
A recommended practice is to have a short lifetime for a secret or credential to make it harder for an attacker to use them.
By default, there are no restrictions on which nodes may run a pod.
Kubernetes offers multiple ways to control pod assignment to nodes such as policies for controlling placement of pods onto nodes and taint based pod placement and eviction.
You can package these classes as a JAR (Java Archive), WAR (Web Archive), and EAR (Enterprise Archive) that contains the front end, back end, and libraries embedded.
With Kubernetes, there's no need to use an external server or framework.
Basic InvocationApplications running inside containers can be accessed through Ingress access — in other words, routes from the outside world to the service you are exposing.
The EFK stack is composed of:Elasticsearch (ES), an object store where all logs are storedFluentd, which gathers logs from nodes and feeds them to ElasticsearchKibana, a web UI for ElasticsearchMonitoringAlthough logging and monitoring seem to solve the same problem, they are different from each other.
No software should be deployed into production without a CI/CD pipeline.
Kubernetes' ReplicationControllers/deployments ensure that the specified numbers of pod replicas are consistently deployed across the cluster, which automatically handles any possible node failure.
Fault tolerance can also be provided to an application that is running on Kubernetes through Istio by its retries rules, circuit breaker, and pool ejection.
Are Application Servers Dead?
Does that mean application servers are dead?
Observability vs. MonitoringObservability is a hot topic at the moment, stirring a lot of debate principally around the argument that observability is different than monitoring.
It does not make any sense to continually push out changes without knowing if they make things better or worse.
Without meaningful analysis, you've fallen short of the whole purpose of creating observability and performing monitoring in the first place.
The most difficult form of observability is distributed tracing within and between application services.
Modern application delivery has shifted to CI/CD, containerization, microservices, and polyglot environments creating a new problem for APM vendors and for observability in general.
New software is deployed so quickly, in so many small components, that the production profilers of the SOA generation have trouble keeping pace.
They have trouble identifying and connecting dependencies between microservices, especially at the individual request level.
This strategy MIGHT be acceptable for SOA applications, but is completely unacceptable in the microservices world.
The problem is so pervasive that the Cloud Native Computing Foundation (CNCF) has multiple open source observability projects in either the Incubation or Graduated phase.
Before big data analytics, it was what I call ‘ difficult data’ analytics.’ There was a lot of manual aggregation and crunching of data from largely on-premise systems.
However, Dynes says:  “Data scientists are providing solutions to intricate and complex problems confronted by various sectors today.
It is evident in the e-commerce sector, and has vast applications in the mobile banking and finance.”When I asked him about his opinion regarding the transformation in the demand of machine learning processes and platforms, he added that, “The demand has always been there.
The data plumbing and data pipelines were critical to enable data insights to become continuous rather than one-off”The Reasons for Rapid Digitalization Dynes says, “We are experiencing rapid digitalization because of two major reasons.
The second challenge is to combat the lack of human skills in data engineering, advanced analytics, and machine learning.
this functionality works generically for microservices, no matter in which language they have been implemented and independent from the application logic.
unfortunately, that image doesn't seem to support microprofile 2.2 yet (at least i haven't found it).
additionally, i have created another variation of the   docker image  so that my sample application can be installed even by people who don't have java and maven installed locally (or who have wrong java/maven versions).
What Is Bookinfo ApplicationThe Bookinfo application is broken into four separate microservices (There are 3 versions of the reviews microservice):Productpage —The productpage microservice calls the details and reviews microservices to populate the Details— The details microservice contains book information.
Drag the slider to adjust v2 takes up 30% traffic, and v2 takes up 70%.
Note: If anything goes wrong along the way, we can abort and rollback to the previous version (v1) in time.
Click on the Job Offline button to take down the old version,Notice: If take down a specific version of the component, the associated workloads and istio related configuration resources will be removed simultaneously, it turns out that v1 is being replaced by v2.
The dashboard also provides information on the state of Kubernetes resources in your cluster, and any errors that may have occurred.
Because the performance of this infrastructure dictates your application performance, it's a critical area.
There's another important reason to study these metrics: they define the behavior of the infrastructure on which the applications run, and they can serve as an early warning sign of potential issues.
* BTW - If you don't want to go through the trouble setting up and managing Prometheus on your own, check out our Managed Prometheus solution for multi-tenant, out-of-the-box Prometheus monitoring with 99.9% SLA on any environment.
Kubernetes simplifies this by separating supply and demand.
What happens if the application detects a system fault, and how does the application relay this to the orchestration layer?
What happens if I introduce a network fault between my services, and how do I test this scenario?
We do not recommend using this version of Istio for production deployments due to some key features still in the alpha/beta state.
From the UI exposed by the route, when you click on the Stop Service button, you should see in OpenShift an indication that OpenShift has detected that the liveness probe has failed and is attempting to restart the container.
It is also worth clicking the Stop Service button to test how Istio handles the service being unavailable.
These tools make it a lot easier for developers to control the deployment of their application to OpenShift, interact with the service orchestration framework, monitor the performance of their application, understand how the application relates to other applications (service mesh), and also introduce and test system faults.
Kubectl Should Not Be Used DirectlyAs a general rule, it's not a good idea to push directly from CI to production.
Observability - monitoring, latency QPS error rates of all of the traffic.
Performance - including failure scenarios retries timeouts, fault injection, and circuit breaking.
When something goes wrong, the older version can be rolled out, and you can iterate on the canary deployment branch and keep rolling that out until it meets expectations.
When something does go wrong, or not according to plan, tracking down the cause is trickier than with "traditional" applications.
It's a common problem in any architecture, but compounded with distributed systems with multiple components, instances, network speeds and competition for system resources.
What if elements of your cluster experience downtime, and data loss in transmission?
None of these log sources provide any method of reading or storing output beyond writing to standard output and error streams.
TracingTaking logging a step further, tracing allows you to follow the execution of an application component, helping you drill down into what went wrong and where.
Then if an application state is ever in doubt, and you need to debug what happened, you replay the events leading up to it to ascertain at what state the application should be.
Having invocation counts, error rates, logs, and even stack traces at your fingertips is very compelling.
When performance data is segmented by actor, piecing together a cohesive story is an exercise in frustration.
var express = require('express');var router = express.Router();var createError = require('http-errors');var request = require('request-promise');var opentracing = require('opentracing');router.get('/ot-gen', function(req, res, next) {  // create our parent span, using the operation name "router"  let parentSpan = opentracing.globalTracer().startSpan('router');  // assign the count passed in our request query to a variable which will be used in our for loop below  count = parseInt(req.query.count);  promises = [];  if (count) {    for (let c = 0; c < count; c++) {      promises.push(new Promise(() => {    // create our child spans for each request that will be made to Lambda        let childSpan = opentracing.globalTracer().startSpan('service-egress', { childOf : parentSpan });    // create an empty carrier object and inject the child span's SpanContext into it        var headerCarrier = {          'Content-Type': 'application/json'        };        opentracing.globalTracer().inject(childSpan.context(), opentracing.FORMAT_HTTP_HEADERS, headerCarrier);    // make our outbound POST request to our Lambda function, and inject the SpanContext into the request headers        request.post(process.env.lambda-url, { headers: headerCarrier, body: { 'example': 'request'}, json: true }).then((response) => {      // append some contextual information into span for use later in LightStep          childSpan.logEvent('remote request ok', { response });      childSpan.setTag('Destination', 'xxx.us-east1.amazonaws.com/production/');        })        .catch((err) => { // if there is an error in the Lambda response, then we attach an error boolean tag, and the error message received to the span, which we can then access later in LightStep             childSpan.setTag('error', 'true');            childSpan.logEvent('remote request failed', {                error   : err            });        })        .then(() => {// finish our child span            childSpan.finish();        });      }));    }  } else {    // basic route functionality    next(createError(400, 'Count not provided'));  }  // end the response so it doesn't stay open while we wait for Lambda to return our responses  res.end();  // after all requests have been made, end the parent span  Promise.all(promises).then(parentSpan.finish());});module.exports = router;As you can see, I’ve made a route here which allows us to instruct Express to send a variable number of requests (passed as a query parameter) to our Lambda function.
If there is an error in the request, we attach the error along with a boolean KV pair which tells our backend that there is an error present on the active span.
We’re ending our parent span here after the Lambda requests leave our app, but we aren’t waiting for the responses.
There’s one extra step when tracing HTTP requests on Lambda: exposing the HTTP headers in the event.
var opentracing = require('opentracing');var lightstep   = require('lightstep-tracer');opentracing.initGlobalTracer(new lightstep.Tracer({    access_token: process.env.lightstep-token,    component_name : 'lambda_function',    // along with other considerations, we remove any initial reporting delay in the library initialization to remove unnecessary overhead.
In our example, we are ingesting data within the same region as our function to reduce any delay to a negligible level.
To make this automatic, many users add the OpenTracing logging calls to their existing error handling or logging libraries, which adds exception messages directly to the individual operation.
This view, enabled through the lightweight data model of OpenTracing and the low overhead of the LightStep client library, is the fastest and most context-rich way to understand a distributed performance story.
Using [x]PM alongside your Istio deployment can be an invaluable tool for figuring out what went wrong when the inevitable happens.
It's a common misunderstanding that when tracing with your service mesh, there aren't any code changes.
Anything that can’t be covered with these libraries can be instrumented directly with the OpenTracing API, so there are no gaps in tracing and instrumentation.
Modern distributed systems have many moving parts, and it’s often difficult to determine what’s a transaction or what’s a request.
We’re interested in how much time was spent on the bicycle, buying bread, or on a particular block.
We’re not interested in granular operations such as how much time a single pedal turn took because it would create a million spans that are extraneous.
The OpenTracing project defines standard tags, such as error to indicate if the operation has failed and component to identify the software package.
Many teams and services can use them, and it’s a way to get broad coverage with relatively low effort.
Pitfalls to AvoidThe biggest risk in the process of getting started with distributed tracing is only doing partial instrumentation.
These modular bundles of code allow developers to work in a far more agile fashion, thus increasing the speed with which organizations can deploy functionalities and fix errors in their applications.
But, while there's plenty of good that comes with microservices, there's also plenty of bad.
As more and more services get added to an application codebase, it becomes increasingly hard to test your services.
This system has devs pulling out their hair, stressing over code and business logic that is completely out of their control.
Sometimes telemetry is added after the fact, but this will only hurt your efforts because you won't be able to catch errors before they occur.
Seriously, that’s it.
:)Hello, WorldOnce again, it is time to say Hello to this cruel World.
Install OpenTelemetryOk so if you know python that was all very boring.
Just to unpack it a bit, there are three critical packages, beyond the launcher itself, which are worth understanding as they explain how OpenTelemetry is structured.
This package only contains interfaces, no implementation.
No Code RequiredThe biggest, most important note is that we added OpenTelemetry to our service, but didn’t write any code.
The most important details to add are application-level attributes critical to segmenting your data.
For example, a projectID allows you to differentiate between errors that are affecting everyone connecting to a service, vs errors that are localized to a handful of accounts.
Generally, this means you shouldn't be creating spans in your application code, they should be managed as part of the framework or library you are using.
10               # WARNING: failing to end a span will create a leak.
Recording ErrorsOk, one final bit.
Python    xxxxxxxxxx            1                       16                                              1            from opentelemetry import trace             2            from opentelemetry.trace.status import StatusCode             3                         4            @app.route("/hello")             5            def hello():             6              try:             7                  1 / 0             8                         9              except ZeroDivisionError as error:             10                  span = trace.get_current_span()             11                  # record an exception             12                  span.record_exception(error)             13                  # fail the operation             14                  span.set_status(StatusCode.ERROR)             15                         16               return "Hello World\n"Uncaught exceptions are automatically recorded as errors.
No?
The Span is the primary building block of distributed tracing.
):Img.5.
As the tracing provides you the ability to get a view into the application communication layer and recognize potential issues, when the JFR is attached to the microservice JVMs, you can directly analyze the potentially suspicious code.
):Img.6.
Google's June 2nd Outage: Their Status Page ≠ RealityPreviously we've written about having hard conversations with cloud providers.
At the bottom is the error rate percentage, meaning the number of errors divided by the total number of requests.
The recovery, in this case, is less clear and there appears to be a further, though less severe (affecting only p99 and p99.9), disruption at the end of the displayed time window.
Though the GCP incident states that the disruption was in the east, the central region internally was impacted through most of the outage window.
As expected, impact from us-central1 to us-east1 is more severe in terms of peak latencies.
Status page updates will often be delayed by tens of minutes and will not include enough detail to be actionable.
Distributed traces capture and propagate critical details about transactions as they cross microservices boundaries.
Which customer is sending bad requests?
The goal of Distributed Tracing is to pinpoint failures and causes of poor performance across various system boundaries.
To solve this problem, OpenTracing and OpenCensus projects were started.
Concepts and TerminologySpan — Span is the building block of a trace.
If no         context is found, then a new span is started.
Acmeshop ArchitectureLet's say the end-user experiences poor performance.
This can then be used to go to those services and start looking at their logs and other stats to figure out the actual problem with the application.
This is only for local testing    var options = {      uri: endpoints.usersUrl + "/users/" + req.params.id,      method: 'GET',      json: true,      headers: req.headers    };    // Leverages request library    request(options, function(error, response, body) {        if (error) {          return next(error);        }        if (response.statusCode == 200) {            console.log('printing from within request')            res.writeHead(200)            res.write(JSON.stringify(body))     // Section 3            userSpan.setTag(opentracing.Tags.HTTP_STATUS_CODE, response.statusCode)            userSpan.log({              'event': 'request_end'            });            userSpan.finish();            res.end();        }        else {            res.status(response.statusCode);            res.write(JSON.stringify(response.statusMessage))  // Section 4            userSpan.setTag(opentracing.Tags.HTTP_STATUS_CODE, response.statusCode)            userSpan.setTag(opentracing.Tags.ERROR, true);            userSpan.log({              'event': 'error',              'message': response.statusMessage.toString()            });            userSpan.log({              'event': 'request_end'            })            userSpan.finish();            res.end();        }    }); // end of request}); // end of methodIn the first code block, you can notice that we start a span.
In the second code block, we are making an HTTP request to the user service.
In the third and fourth code blocks, there were tags and log fields added.
You may also add tags to indicate an ERROR by using userSpan.setTag(opentracing.Tags.ERROR, true);Messages can also be logged along with the span by using    userSpan.log({      'event': 'error',      'message': response.statusMessage.toString()    });Augmenting additional data along with the span can increase the network and storage costs.
func initJaeger(service string) (opentracing.Tracer, io.Closer) {// Uncomment the lines below only if sending traces directly to the collector// tracerIP := GetEnv("TRACER_HOST", "localhost")// tracerPort := GetEnv("TRACER_PORT", "14268")agentIP := GetEnv("JAEGER_AGENT_HOST", "localhost")agentPort := GetEnv("JAEGER_AGENT_PORT", "6831")logger.Infof("Sending Traces to %s %s", agentIP, agentPort)cfg := &jaegercfg.Configuration{Sampler: &jaegercfg.SamplerConfig{Type:  "const",Param: 1,},Reporter: &jaegercfg.ReporterConfig{LogSpans:          true,LocalAgentHostPort: agentIP + ":" + agentPort,// Uncomment the lines below only if sending traces directly to the collector//CollectorEndpoint: "http://" + tracerIP + ":" + tracerPort + "/api/traces",},}tracer, closer, err := cfg.New(service, config.Logger(jaeger.StdLogger))if err != nil {panic(fmt.Sprintf("ERROR: cannot init Jaeger: %v\n", err))}return tracer, closer}// Set tracer to Global tracerfunc main() {  // Start Tracer  tracer, closer := initJaeger("user")stdopentracing.SetGlobalTracer(tracer)  // Start the serverhandleRequest()  // Stop Tracerdefer closer.Close()}Extract the Context and Start Span in User Service (Go)Once the request is received by the service, it is re-directed to GetUser Function, as shown below.
func GetUser(c *gin.Context) {var user UserResponsetracer := stdopentracing.GlobalTracer()userSpanCtx, _ := tracer.Extract(stdopentracing.HTTPHeaders, stdopentracing.HTTPHeadersCarrier(c.Request.Header))userSpan := tracer.StartSpan("db_get_user", stdopentracing.ChildOf(userSpanCtx))defer userSpan.Finish()userID := c.Param("id")userSpan.LogFields(tracelog.String("event", "string-format"),tracelog.String("user.id", userID),)if bson.IsObjectIdHex(userID) {error := collection.FindId(bson.ObjectIdHex(userID)).One(&user)if error != nil {message := "User " + error.Error()userSpan.LogFields(tracelog.String("event", "error"),tracelog.String("message", error.Error()),)userSpan.SetTag("http.status_code", http.StatusNotFound)c.JSON(http.StatusNotFound, gin.H{"status": http.StatusNotFound, "message": message})return}} else {message := "Incorrect Format for UserID"userSpan.LogFields(tracelog.String("event", "error"),tracelog.String("message", message),)userSpan.SetTag("http.status_code", http.StatusBadRequest)c.JSON(http.StatusBadRequest, gin.H{"status": http.StatusBadRequest, "message": message})return}userSpan.SetTag("http.status_code", http.StatusOK)c.JSON(http.StatusOK, gin.H{"status": http.StatusOK, "data": user})}Here, the request is extracted by usinguserSpanCtx, _ := tracer.Extract(stdopentracing.HTTPHeaders, stdopentracing.HTTPHeadersCarrier(c.Request.Header))This provides the traceID from the parent.
NOTE: In the end, if you use Opentracing with Jaeger agents (as we have described in the article above), you can pick essentially ANY tracing UI you need (open source or commercial)Tracing Is DifficultIt's difficult to get tracing correct, especially when adding it to existing applications.
Similarly, when ants are in an alarmed or panicked state, they emit chemicals that alert their peers to the threat and protect the group.
…and Back to MicroservicesThere’s no question that hippies are more independent than ants.
And I suppose I should acknowledge that ants would make lousy engineering managers (they can’t even drink coffee).
No?
The story goes like this:If you’re using microservices, you already know that they’re nearly impossible to understand using a conventional monitoring toolchain: since microservices were literally designed to prevent “two-pizza” DevOps teams from knowing about each other, it turns out that it’s incredibly difficult for any individual to understand the whole or even their nearest neighbor in the service graph.
Fatal FlawsPerhaps the above is hyperbolic.
Still, for those who deployed “the three pillars” as bare technologies, the initial excitement dissipated quickly as fatal flaws emerged.
Unfortunately, many real-world tags have thousands or millions of values (e.g., user-id, container-id, and so forth), so metrics often prove to be a dead end from an investigative standpoint.
Logging Volumes With MicroservicesFor Logs, the problem is simpler to describe: they just become too expensive, period.
Tracing and ForeknowledgeWhich brings us to “Distributed Tracing,” a technology specifically developed to address the above problem with logging systems.
It certainly had its uses, especially for steady-state latency analysis, but we dealt with the data volume problem by applying braindead, entirely random, and very aggressive sampling.
This has long been the elephant in the room for distributed tracing, and it’s the reason why Dapper was awkward to apply in on-call scenarios.
The obvious answer would be to avoid sampling altogether.
If we’re restricting our analysis to individual traces, we typically focus on “the slow ones” or those that result in an error; however, performance and reliability problems in production software are typically a byproduct of interference between transactions, and understanding that interference involves much more sophisticated sampling strategies that aggregate across related traces that contend for the same resources.
Google’s microservices generate about 5 billion RPCs per second; building observability tools that scale to 5B RPCs/sec, therefore, boils down to building observability tools that are profoundly feature poor.
Bits vs. BenefitsSo each “pillar” has a fatal flaw (or three), and that’s a problem.
The other problem is even more fundamental: Metrics, Logs, and Distributed Traces are just bits.
None of the above directly addresses a particular pain point, use case, or business need.
That is, with the “three pillars” orthodoxy, we implicitly delegate the extraordinarily complex task of actually analyzing the metric, log, and trace data as “an exercise to the reader.” And, given the fatal flaws above and the subtle interactions and co-dependencies between these three types of data, our observability suffers greatly as a result.
“Distributed tracing” allows DevOps to automatically follow the path that any and every request takes across microservices; with ordinary transactions involving hundreds or thousands of distinct services in the span of a quarter second, there’s no other way to understand and explain the behavior of today’s intricate distributed systems.
Sigelman is now the co-founder of LightStep, which is still in stealth.
However, the solutions that engineering departments have depended on for the last decade – logging, metrics and conventional APM (and even how they deploy and do security) – depend on built-in assumptions about system architecture that no longer hold.
With the adoption of DevOps, distributed tracing needs to be automated so companies are able to explain any and every request, sort the signal from the noise, and know where the problem is that needs to be fixed.
The Myth of the Server’s Terrible, Horrible, No Good, Very Bad DayHave you ever been on call when something had a temporary latency spike that was possibly just a network blip?
Or maybe the service was unhappy and went into a GC spiral?
Programs don’t get tired, so why does restarting do anything at all, let alone magically fix it?
That’s definitely a strategy, but it tends to bog down development and cause all sorts of unhappiness in the humans, so I don’t recommend it.
An Unhappy Server StoryA couple of weeks ago, we had an issue when one of our services failed to load the latest configuration consecutively for 10 minutes, at which point it exited to avoid running on a stale configuration.
The backend has a local cache to avoid concurrently and repeatedly loading the config from the database because it’s totally fine to serve data that is a few seconds stale in this case.
The tail end of the latency was high enough to cause repeated timeouts, so it was possible for an instance to get unlucky several tries in a row and then fail.
You can see the current data (blue bars) has many more operations falling into the slower buckets compared to the previous week (yellow line).
Latency dramatically increases with a corresponding increase in errors  This graph above shows the latency dramatically increasing over the course of the week.
There is a corresponding increase in errors (red line at the bottom).
By looking at the critical path in LightStep [x]PM, we saw that the latency was attributed to our database service.
After we spun up a few more instances, the latency didn’t return to normal unfortunately even after the load per instance returned to levels prior to the start of the problem.
The invalid requests were causing errors and our local-cache wasn’t performing negative caching, so it ended up hammering the database with these requests.
By adding support for negative caching, the latencies dropped dramatically and the error rate for the polling endpoint recovered.
By adding negative caching, operation and error rate drop dramatically and latency recovers  In this case, by digging deep into why this particular polling endpoint had gotten “unlucky” with consecutive timeouts, and then subsequently digging into the traffic changes on our database service, we found a long-standing bug that had been triggered by a customer change around the same time we made some changes ourselves.
Sharing What We LearnedOver time, we’ve learned that we can leverage data about what our system is actually doing to can find logical explanations for most problems we observe.
At LightStep, we’re pragmatic about it — we don’t beat ourselves up if we observe a problem in a poorly instrumented part of the system or fail to find a root cause immediately.
Don’t get discouraged by complexity.
When assessing "right now," our industry relies almost entirely on averages and percentile estimates: these are not enough to efficiently diagnose performance problems in modern systems.
Performance is a shape — not a number — and effective tools and workflows should present and explore that shape, as we illustrate below.
They are less appropriate when there's high variance, and they are particularly bad when the sample values are not normally distributed.
Unfortunately, latency distributions today are rarely normally distributed, can have high variance, and are often multimodal to boot (more on that later).
Unfortunately, it's not so simple: average latency is a poor leading indicator of reliability woes, especially for scaled-out Internet applications.
As our applications transitioned to the cloud, we learned that high-percentile latency was an important leading indicator of systemic performance problems.
With a p99.9 like that, we suspect that the shape of our latency distribution is not a nice, clean bell curve, after all...
Phase 3: Microservices and Detailed Latency Histograms (2018)Given 100 percent of the (unsampled) data, we can isolate and zoom in on any feature, no matter how small.
Here the user restricts the analysis to project_id 22, then project_id 36 (which have completely different performance characteristics).
Here, we filter to outliers for project_id 36, choose a trace from a few seconds ago, and realize it took 109ms to acquire a mutex lock: our smoking gun.
Other times, we are more disciplined, but our tools only expose bare statistics without context or relevant example transactions.
Since [x]PM has access to all (unsampled) trace data, we can isolate and zoom in on any feature regardless of its size.
We filter interactively to hone in on an explanation: first by restricting to a narrow latency band, and then further by adding key:value tag restrictions.
We restrict our view to project_id=36, filter to examine the latency outliers, and open a trace.
Since [x]PM can assemble these traces retroactively, we always find an example, even for rare behavior:The (rare) trace we isolated shows us the smoking gun: that contention around mutex acquisition dominates the critical path (and explains why this particular project - with its own highly-contended mutex - has inferior performance relative to others).
Not Budgeting for MaintenanceThe first big failure mode with Kubernetes is not budgeting for maintenance.
Just because K8s hides a lot of details from application developers doesn't mean that those details aren't there.
Moving Too FastThe second big failure mode is that teams move so quickly they forget that adopting a new paradigm for orchestrating services creates new challenges around observability.
Not Thinking Through "Matters of State"Of course, how you divide your application into smaller services is a critical decision to get right.
It's all too easy to get burned by a corrupted shard in a database or other storage system, and recovering from these sorts of failures is by definition more complex when running on K8s.
Needless to say, make sure you are testing disaster recovery for stateful services as they are deployed on your cluster  (and not just trusting that it will just work like it did before you moved to K8s).
That's where LightStep comes in: , we enable teams to understand how a bad deployment in one service is affecting users 5, 10, or 100 services away.
The team needs to know what’s going on across the landscape as well as understand what’s required for day two operations – upgrades, patches, disaster recovery, and scale.
K8s enables teams to scale production workloads and fault tolerance not previously possible.
Containers have far less exposure and fewer attack surfaces.
By automating concepts and constructs of where things go using rules and a stabilized environment, you eliminate a lot of human errors that occur in a manual configuration process.
K8s standardizes container deployment — set it once and forget it.
The most common failures are around lack of skills/knowledge, complexity, security, and day two operations.
There is a lack of understanding of how K8s functions.
People give up on implementations because it’s too hard.
People underestimate the complexity of installing and operating K8s.
We see failures around security with new vulnerabilities weekly that require patches.
There’s an ongoing need to deploy persistent storage, monitor and alert on failure events, and how to deploy applications across multiple K8s clusters.
You cannot assume that managed K8s offerings are somehow inherently secure, or that by clamping down on access to CI/CD to just a few DevOps people, that the risk has been avoided.
People who try to implement K8s on their own have trouble maintaining their own platform.
People assume its self-healing and ignore the details.
If you experience a problem, reach out to the community.
Leave large scale to ops teams or an external company.
And of course, people are a little more stressed out with all that’s going on too.
Lisa:No.
We’ve had our dashboards, we write log data to a log file so that when something goes wrong, we can debug it.
The thing is that we tend to instrument our code for the things that we know might go wrong or that we expect could go wrong.
We set up alerts when we exceed certain performance thresholds or error thresholds.
If there’s a crash, we record the crash.
It’s a bad problem in production.
Now we have to add some logging to our code and re-deploy.”And depending on how easy it is to deploy in your environment, that can be really painful under a previous issue.
Meanwhile, your customers are feeling pain.
So observability is instrumenting more things in our code, more events in our system, so that if something goes wrong that we totally could not have imagined that (all of the risk assessment and things that we did in advance) we just didn’t think of it.
Observability lets us delve in and see, “Oh, what went wrong?
“Observability allows us to trace what users did across all the different services and APIs to see exactly all the steps they took so we can reproduce the problem.
But what I understood is that basically, we have to get ready for possible problems that we don’t have any idea that occur, right?
I’m struggling with that myself.
They tend to know the most about it because they’re the ones struggling with debugging the problems.
Federico:I don’t think collaboration is a bad thing.
You have to understand the data, you have to understand the types of things people are doing so that you can know how you want to investigate the problems.
Where do they give up and abandon the product because they couldn’t figure it out?
We still want to do all the testing we can before we release, but we have to take advantage of all these extra tools like observability, testing in production and using feature toggles, chaos engineering.
Federico:I haven’t thought about that… Chaos engineering is also related to observability because it’s maybe a way of trying to improve the observability of the system by injecting errors, right?
Let’s drop a database table, see what happens.” And you don’t have to do it in production, you can do it in the staging environment.
But a couple of years ago at European testing conference, Sarah Wells did a keynote on what they do at Financial Times and she called chaos engineering, “tool-assisted exploratory testing in production.” And I thought that was so apt since these are all forms of exploratory testing and learning about our production system.
Not to solve the problem for you, because people talk about AIOPs and that Artificial Intelligence could do all this for you.
These tools can be scary because they’re not intuitive, you can’t just jump in and use them because like I said, you have to understand your data and your application and what your customers are doing.
And not only that, because from what you mentioned, I understand that you need an understanding of the architecture and the technology behind the system in order to understand where to pay attention or what to look for, right?
It’s something we didn’t have before and if it can reduce our pain in the investigating of the cause of our problems or preventing customer problems… because now that we have things like dark launches, released feature toggles, progressive rollouts, we can put something in production and not only monitor it, but use the observability tools to spot unusual patterns.
So, maybe it’s kind of boring, but the Accelerate book by Dr. Nicole Forsgren and Jez Humble and Gene Kim.
And it’s all part of a piece of getting to that, being able to frequently deliver small changes that are valuable to customers at a sustainable pace and lower risk because we are making really small changes that we can revert if we need to or turn that feature flight off if we need to.
How to keep patched, back up, disaster recovery, scale.
If something fails it, it will automatically recreate it.
The downside of that is if something goes wrong with the system, now you have to search through multiple levels of abstraction to figure out where the problem is.
If it fails, you need to know the nitty-gritty details of all of the services that are running.
How you divide your application into smaller services is a critical decision to get right.
The overall strategy of automating testing is critical.
When environments cannot take the scale and you fail in your expectations of what’s possible.
K8s alone won’t solve your problem.
Moving from monolithic apps to microservices running on Kubernetes has many benefits, but trying to solve every problem at the same time is a recipe for delayed migration, and frustration.
compute, storage, network), you can impose cross-silo controls.
Despite Kubernetes’ growing popularity, it is still often challenging for developers to manage manually.
It isolates you from the infrastructure requirements and needs.
We are not looking at security and high availability concerns.
It solves the basic problem that companies are running into right now.
When people develop applications in the cloud, they will do it on top of the OS with abstractions with no vendor lock-in.
K8s might disappear lower in the stack as people build more abstractions above it.
A standard set of APIs to deploy and build defining new resources and letting K8s do the hard work.
Global-scale businesses are in dire need of solutions to complex deployment requirements, such as multi-cloud and hybrid cloud.
Stability/EcosystemExternalization of the platform will happen to ease the burden of the customer so they can focus on revenue-generating applications.
In addition, users will pay based on a usage model, only paying for the compute time consumed — and no charge when their applications are not running.
The biggest problem is multi-cluster and multi-cloud.
It comes down to complexity, rather than try to add significant layers of abstraction around hard problems to build on top they introduce minimal levels of abstraction and tools for the community to abstract the right answer will change as to how we build applications change.
For example, in December of 2018, a major vulnerability in K8s was made public by its maintainers and patch releases were quickly released.
Specifically, it can be an extremely difficult task to pick the right set of tech and tools to be included.
Our main concern, based on what we have seen so far, is an assumption that managed K8s offerings are somehow inherently secure, or that by clamping down access to CI/CD to just a few DevOps people, that risk has been avoided.
People will try on their own and have trouble maintaining their own platform.
A lot of people are flying blind.
People assume its self-healing and ignore the details.
Testing is the weakest link in the CI/CD pipeline.
This has been a challenge for a long time, even predating K8s, but with increased horizontal scale there is more reliance on network performance as the weak link in the performance picture.
Our move to K8s was really about cleaning up technical debt.
In these environments, we’re able to see if there is a failure and what caused it.
The toolset lets you look at the big picture and see where the failure is.
IoT, connected cars, fraud detection, customer analytics.
It’s hard to do these traditional application operations in the traditional world.
We had a customer with application servers that were losing connectivity and restarting.
When something does go wrong, or not according to plan, tracking down the cause is trickier than with "traditional" applications.
It's a common problem in any architecture, but compounded with distributed systems with multiple components, instances, network speeds and competition for system resources.
What if elements of your cluster experience downtime, and data loss in transmission?
None of these log sources provide any method of reading or storing output beyond writing to standard output and error streams.
TracingTaking logging a step further, tracing allows you to follow the execution of an application component, helping you drill down into what went wrong and where.
Then if an application state is ever in doubt, and you need to debug what happened, you replay the events leading up to it to ascertain at what state the application should be.
We asked, "What are the most common failures you see with K8s?"
Problems happen when you scale, and you cannot keep up with all of the necessary work.
Even if you can find them, it’s hard to retain them.
Failure when attempting an upgrade.
The most common failures I see are due to a lack of understanding of how K8s functions.
2) Not updating your kubectl tool along with your configs when K8s versions updates will stop you from being able to manage your containers.
We see failures around security with new vulnerabilities weekly that require patches.
You have to dig into the code for functionality errors.
Check out these K8s failure stories.
Similarly, enterprises can find it difficult to recruit experts with the depth of K8s and DevOps knowledge required to create proper tools and implement application workflows in containerized environments.
There is a lot of thought given to which images can be deployed (and to scanning for vulnerabilities before and during runtime) but many companies have a blind spot related to just how easy it is to accidentally steal traffic from one workload and send it to another.
The best-case scenario in this common failure is downtime — the worst case is that sensitive data gets exposed to the Internet, and no one notices, because there is no alert and no notification.
2) Similarly, we see failures come from human error or accidental oversight when it comes to naming and labeling policies.
3) We also see surprising failures resulting from mundane issues.
Day-Two Operations1) Humans that have rolled out a change in code and cause cascading failures.
A common K8s failure scenario is when the K8s cluster infrastructure hardware fails to satisfy the container startup policies.
Since K8s provides a declarative way to deploy applications and those policies are strictly enforced, it's critical the declared and desired container states can be met by the infrastructure allocated, otherwise a container will fail start!
Other areas where failures can occur, or cause concern, is when deploying persistent storage, properly monitoring and alerting on failure events, and deploying applications across multiple K8s clusters.
The first big failure mode is not budgeting for maintenance.
Just because K8s hides a lot of details from application developers doesn’t mean that those details aren’t there.
The second big failure mode is that teams move so fast that they forget that adopting a new paradigm for orchestrating services means they need to rethink observability for those services as well.
OtherA fundamental issue is moving a container deployment from an experimental, pilot, or playground environment into an enterprise-ready, stable, operationalized environment where you measure availability, reliability, you have to be concerned about compliance, backups, and disaster recovery.
People are struggling just to deploy K8s and make it work reliably.
People are aware of the benefits of K8s, but they have trouble getting it to work.
People give up because it’s too hard for them.
2) It's problematic when K8s and containers are employed to the organization without first making sure the software can be modularized enough to support a K8s workflow.
An executive decision to use K8s with the dev team not on board is sure to fail.
See container restarts all over the place without telling you it’s broken.
Persistent volumes failed jobs without being notified it’s a problem and K8s will not fix.
When migrating to K8s, the most serious consequence is typically from a failed strategy, rather than a technical one.
Of course, there will be technical issues which might cause application downtime with pod crashing or severe performance degradation due to an unoptimized pod placement strategy or incorrect load balancing.
But the goal should be to ensure minimal disruption to the business and your customers which requires proper risk assessment on the migration plan.
Migrating your entire stack to K8s in one go will be an eventual failure.
You must also avoid cloud vendor lock-in at all costs when adopting a multi-cloud strategy.
Since the beginning, containers have been perceived as a potential security threat because you are running these entities on the same machine with low isolation.
There's a perceived risk of data leakage, moving from one container to another.
Containers have far less exposure and attack surface.
You can keep containers clean and small for minimal attack surface.
You offer limited access to the container running the application.
By automating concepts and constructs of where things go using rules and stabilizing the environment, it eliminates a lot of human errors that occur in a manual configuration process.
This mitigates problems escaping from containers and helps you stay isolated.
K8s-orchestrated containerized environments and microservices present a large attack surface.
The highly-dynamic container-to-container communications internal to these environments offer an opportune space for attacks to grow and escalate if they aren’t detected and thwarted.
Enterprise environments must be protected along each of the many vectors through which attackers may attempt entry.
Here's what they told us:DataIf you don’t slice and dice responsibilities correctly you run into problems.
Think about the potential problems you will encounter.
Managing data and managing states is where a lot of people have problems managing stateful and stateless data.
If crash and burn, you can just replace if it's stateless.
You cannot stop supporting the monolith.
This was a hassle developers didn’t worry about in past.
Microservices are more difficult to debug and troubleshoot.
No tools currently exist to inspect the entire application.
How do we instrument our app and get data back to interrogate the platform and the app, and when going well to compare when they start to degrade?
Visibility and exposing to the people who have responsibility for the applications – developers.
Lack of visibility into end-to-end processes that span multiple microservices: 59% of respondents.
Ambiguous error handling, leading to unaddressed errors at the boundaries between microservices: 50% of respondents.
Difficulty hiring developers who have the right skill set: 35%.
If you don’t have a platform and your culture is not aligned properly it’s difficult to get microservices into the product.
1) Could become the wild west without proper governance leveraging a service that exposes data that should not be exposed.
It's not easy to find the answer people are looking for.
Not sure what question to ask to solve the problem.
Without automation, if you are having a hard time deploying large things deploying hundreds or thousands of smaller things can’t be done without automation.
Organizations will misapply, misunderstand, and fail to understand the complexity of microservices.
Be realistic and realize microservices are not a magic solution to every problem.
Without monitoring, you are not able to see where the problem is.
It’s really important to have visibility into these business processes—to know the current state of the business, to see when something has gone wrong, so problems can be quickly addressed—but most microservice monitoring tools were built with single services in mind or concentrate on relatively short call stacks, rather than long-running, cross-microservice flows.
We’ve observed at a couple of customers that choreographed microservices get a lot of hype, meaning that the microservices use peer-to-peer or event-driven communication and often neglect orchestration completely.
These customers quickly lose sight of their end-to-end flows, making it at least hard to change anything in the flow.
We have glitches and errors we didn’t see in the past with traditional virtual machines.
Problems are not easy to pinpoint and identifying the root cause can be hard.
Small shops adopting microservices because of their desire to be on the cutting edge is not beneficial.
Make technology choices in a vendor-neutral way so vendors can fight over you.
Lack of standards body has resulted in confusion.
Focus on solving business or technical problems rather than semantics.
There’s no one right answer for all things.
There are not enough best practices developed.
It's early but we see people going in and struggling.
The hype has gone up, people are confronted with difficult decisions about tradeoffs and they need to evaluate if microservices is the right approach.
Building applications for cloud platforms with the assumption things will fail is an area where as many architectures as possible should go toward technology.
Once you push past 500 to 1000 topics you start stressing a Kafka cluster out.
To be pervasive those type of problems need to be solved.
Kafka helps fulfill a need, but you’ll run into problems.
Microservices almost gives you carte blanch because they are disposable, people think about redoing tomorrow and forget about it.
Pushing services out is fun and easy but if they are not run properly with the correct tooling, they fall over, and the impact is exponentially worse with each microservice you add.
If you encounter a problem, you can get up to speed and contribute code.
Agile DevOps would not be successful without microservices.
Autonomy – a team bigger than ten people is not efficient.
Instead of focusing on the rigidity and stability of monolithic applications, modern architectures need to be built to change.
This enables adopters to address disruptive trends in their industries before they can have a negative effect on their business.
You can finish things and leave them alone.
Once you finish something you leave it and move on, no maintenance overhead.
Things happen faster once you’ve broken things down – that’s the loose coupling.
Reduces risk with canary deployment patterns.
Make CI/CD more pervasive but people still struggle with contract definition interface management.
Remember microservices principles like data isolation.
From a business point of view, we’re in the risk mitigation space when someone is looking for a loan, they have a problem, they need money.
When writing my application, I’m leveraging a relational database which abstracts away a lot of nasty problems.
Infrastructures can and do fail and the effect of breaking into smaller services we build for reliability expecting things to fail.
It can become more difficult to detect or troubleshoot issues inside a system that’s made up of many microservices.
The downside is that you end up with more applications, a lot of different apps to deploy and manage.
This is why application platforms exist – they mitigate the downside.
It can be difficult to get the paradigm shift of working with constituent parts.
While it’s critical that an individual microservice is monitored and deployed in a responsible way, it’s also critical to keep an eye on the cross-microservice flows that make up most companies’ revenue-generating core business processes.
Not more efficient message brokers.
As such, they should address a domain broadly enough to avoid challenges around “nanoservice” orchestration but bounded enough to avoid unnecessary dependencies.
What are the problems Netflix and Amazon tried to solve and are you trying to solve the same problem?
The architecture of the application became less defined and started to step on itself and became difficult to reason with and change.
Caution teams to question that doing microservices is optimization for a particular problem that comes with a lot of complexity.
Microservices code has 5X the vulnerabilities of monolithic code.
Microservices are interface based, there is no governance behind the scenes.
When writing microservices many assumptions are made around how they are used and consumed that does not hold true.
Isolation and API guarantees are the most important elements of microservices.
Isolation allows developers to iterate quickly and independently.
However, developers must maintain API compatibility so other microservices are not impacted (and create dependencies that slow down iterations).
People have difficulty understanding where ingress and egress points give way to microservices.
Avoid premature decomposition.
The hard part is breaking the data dependencies of state apart.
Splitting the data becomes the hard part.
Caching can become more difficult.
A common mistake is that people don’t understand the boundaries of what a service should be (e.g., service for data access layer).
Lack of understanding around microservices and properly defining boundaries.
Solving a real problem around engineering philosophy and scaling.
It's the manifestation of communication problems.
Further, operations become a critical piece of the equation as we introduce automation of the software supply chain with these services.
How you can group a set of microservices and expose via a gateway.
This does not optimize the process.
We asked respondents, "What are some real-world problems being solved with microservices?"
Here's what they told us:FraudThe biggest and most relevant use case is fraud detection.
Fault detection apps need to access the server.
It looks a lot like ETL but microservices to detect fraud.
With microservices, as long as don’t violate the contract, four variables in one answer out, you can make changes in hours.
Take one service, expose it, you’re able to make changes in hours versus months.
T-Mobile took their payment system, the heart of their business and broke apart the microservices they wanted and started iterating on it.
We store data people are uncomfortable putting in the cloud, clustering horizontally without configuration.
As reliability degrades it needs to be refreshed.
We enable developers to use statistical data to determine where problems lie.
Stop doing boring repetitive things that don’t add value – automate.
Proxy metrics are 1) number of deployments per day, 2) how quickly you can get a code change from Git commits to production, 3) MTTR, 4) how often are you experiencing failures.
This means that when something goes wrong it becomes more difficult to pinpoint the issue.
As we break pieces of the monolith, the mean-time-to-detection of issues starts to drop as it’s easier to detect performance problems, memory leaks, and other issues in smaller units, especially since each release in a microservice has fewer changes in it.
Both perspectives are critical for the advancement of science, and similarly, software development continuously iterates between theory-building via source code and theory-testing via software utilization.
In scientific research, elegant theories are not valuable if they do not correlate with reality and mountains of data are equally useless if they do not contribute to meaningful theories.
Source code is not valuable if it is not computable, and software is equally useless if it does not work.
This is problematic given an ongoing trend in software development is the blurring boundaries between building and operating software.
It requires little to no code changes to implement and is lightweight enough to be used in production.
The Span is the primary building block of distributed tracing.
):Img.5.
As the tracing provides you the ability to get a view into the application communication layer and recognize potential issues, when the JFR is attached to the microservice JVMs, you can directly analyze the potentially suspicious code.
):Img.6.
Squashed Bugs, Served Hot and Fresh With Failure Rate HeatmapsLearn how to squash bugs!
Debugging is not just about the fix — sometimes it's the drudgery of trying to locate the root cause that kills passion, especially when you have thousands or even millions of lines of code.
A former Oracle database developer, who helped maintain Oracle v12.2 and its 25 million lines of C code, described his struggling life with bugs this way: Spend weeks trying to understand the 20 different flags that interact in mysterious ways to cause a bug.
Find out that his fix causes 100~1000 failed tests.
Finally, after a few weeks or months, get the combination of flags right and succeed with zero failing tests.
He was frustrated with the situation, and this is something we want to avoid in our projects!
Below, you can see how our bot highlights potentially faulty areas in the code.
The higher the failure rate, the darker the color; the higher the failure count, the brighter the block.
Statistical debugging to analyze the TiDB source code's participation in the passed and failed test cases.
Visualization to graphically map the potentially faulty lines of code in the source files.
The source files that contain potentially faulty code.
So we found another paper, Visualization of Test Information to Assist Fault Localization.
This paper presents a technique that uses varying colors and brightness to visually map the participation of each code block in the passed and failed test cases.
Before that, let's see how we hacked our way to build this bug-hunting bot.
The “Hack” RoadEvery time we hack on a project, our team first breaks down our goal into several actionable items, and then focuses our efforts on answering the most difficult questions.
How do we visually display the potentially faulty basic blocks?
Also, as shown below, we implemented a SQL fuzzing demonstration page that shows which SQL BNF grammar is most likely to generate a failed SQL query.
Each BNF grammar is assigned a color value based on the ratio of the passed and failed test cases (SQL queries) that the grammar appears in.
But we were not sure whether this tool worked for a Golang database and how long it would take, so we had this workaround idea — why not directly instrument the source code before compiling it?
Shell     x             23                                                  1             $ curl http://localhost:43222/trace/df6bfbff | jq              2             {              3               "sql": "show databases",              4               "trace": [              5                 {              6                   "file": "executor/batch_checker.go",              7                   "line": null              8                 },              9                 {              10                   "file": "infoschema/infoschema.go",              11                   "line": [              12                     [              13                       113,              14                       113              15                     ],              16                     [              17                       261,              18                       261              19                     ],              20                    //....              21                 }              22                ],              23             }In the returned JSON list, each array of the “line” field stores the line numbers at which a basic block starts and ends.
A basic block is a code block that cannot be split into smaller branches.
But how can we identify all the basic blocks in a Golang source file?
Question #3: How Do We Visually Display the Potentially Faulty Basic Blocks?
Now we have all the basic blocks that each test case (SQL query) executes.
Next, let's build a visualization model that illustrates the chance that each basic block leads to a failed test case.
For each basic block, the color score is determined by dividing the number of failed test cases that have executed the block by the total number of test cases that have executed the block.
For each basic block, the brightness score is determined by dividing the number of failed test cases that have executed the block by the total number of test cases that have ever failed.
The higher the score, the more failed test cases that the block causes, and the brighter the block.
The BlockBrightness metric is introduced to correct the bias of the BlockColor metric.
For example, if only one failed test case executes a basic block, the block will get the highest score, 1on BlockColor.
But most of the time, this block is not the real cause of the bug because only one out of many failed test cases executes the block.Thus, BlockBrightness is significant for reducing this kind of noise.
For example, the following basic block might contain the fault—the BlockColor score is 0.82 (the value of Failure rate) and the value of FailedTests with the block subscript is 292 (the value of Failure count).
For each source file, the file rank score is determined by dividing the total number of failed test cases that execute each basic block by the total number of basic blocks.
The higher the score, the more test cases that a file fails, and the higher the file ranks.To illustrate, assume there are two basic blocks in file_1.
Five failed test cases execute the first block block_1, and three failed test cases execute the second block block_2.
The FileRank score for file_1 will be 4 ((5+3)/2).In the figure below, the files that rank higher are more likely to contain the fault.
LimitationsOur limited time in the Hackathon only allowed us to present a prototype for this dream: When we finish the hard work of the day, we don't have to wake up in the middle of the night to find bugs and correct code.
When we get up in the morning, the bot automatically reports to us where the program's fault is most likely to occur.
Thus, although our bot's diagnosis for faulty files is relatively accurate, its diagnosis for basic blocks needs improvement.
The bot does not yet support concurrently executing SQL queries to the TiDB database.
Here are some of our thoughts: Chaos Engineering.
Chaos testing platforms such as Jepsen are widely used to validate the robustness of a distributed system.
One limitation of chaos testing is that it's pretty hard to reproduce a problem when it's reported.
If we record the execution traces when the program is running, we can narrow down the search using logs and monitoring, and quickly find the fault based on the execution trace analysis.
If we pass trace_id and span_id as annotations to code blocks for instrumentation, is it possible that we can directly drill into the code and see exactly which lines of code are executed on the interface of the tracing system?
it offers an in-browser widget with no backend required that is automatically injected to the monitored web page.
meaning that it aims to explain how every transaction gets executed, trace the flows between the components and (bad joke ahead) pinpoints problematic areas and potential bottlenecks.
pinpoint also lets you see the request count and response patterns so you’ll be able to identify potential problems.
you can view critical details that include cpu usage, memory/garbage collection, and jvm arguments.
to get started, all you need to do is drop the .jar file into the web-inf/lib folder or by including a small new section in the web.xml file.
to get started with glowroot, you need to download and unzip the main installation file and add -javaagent:path/to/glowroot.jar to your application’s jvm arguments.
glowroot’s dashboard   bottom line:  if clean and simple is what you’re looking for, no doubt you’d want to check out glowroot over the other tools here.
kamon is distributed as a core module with all the metric recording and trace manipulation apis and optional modules that provide bytecode instrumentation and/or reporting capabilities to your application.
the only problem is that once you get that haystack in which the problem was found, you have to start digging around looking for the actual needle that caused it.
Generally, Harbor will be configured to prevent vulnerable images from running with the severity of high or above.
Shell    xxxxxxxxxx            1                                              1            $ helm install -f values.yaml yugabytedb/yugabyte --namespace yb-demo --name yugabyte-demo-cluster --waitCheck YugabyteDB Cluster StatusEvery YugabyteDB cluster exposes a YugabyteDB Master UI, which provides the details of the cluster, sharding information of the tables, and also the cluster metrics.
See how we use VMware Tanzu Observability (Wavefront) and Sentry to proactively monitor and fix issues before they become production problems.
Every engineering leader has experienced the anxiety and stress of taking an app to production.
It's a mix of excitement and trepidation — your creation will be used in real life, but what if something goes wrong?
While this minimizes the probability of something obvious being wrong, the ultimate responsibility of meeting the app's SLA rests with the engineering team.
If that length of time is higher than your depth of log persistence it will become hard to debug the problem, resulting in lowered customer satisfaction.
Increasing the depth of log persistence becomes expensive so we need a lightweight separate system to log customer exceptions and errors.
Logging is not exceptions tracing — Traditionally, logs are multiple-level informational, warning, debug or error logs.
One can configure the logger to log exceptions — which can be a failure or a bug, but the single line logging entries are not enough to debug an issue across disjointed microservices.
In addition, logging is ineffective when it comes to the deduplication of exceptions.
Sentry Slashes Our Time to ResolutionGiven these challenges, we realized the need for a tool that meets the same requirements we had for Wavefront.
Recently, Sentry notified our engineering team via Slack that Firefox version 68 threw a certificate error for API gateway to a customer who was trying to access Cloud Marketplace.
Because Sentry sent us all the context about the issue including the stack trace, we were able to get an intermediate certificate from IT and push fixes to production in a matter of hours — before it became a widespread problem.
Backup the namespace:    Shell    xxxxxxxxxx            1                       50                                              1            $ velero backup create wp-backup --include-namespaces wp --storage-location azure --wait             2            $ velero get backup             3            NAME  STATUS  ERRORS  WARNINGS  CREATED  EXPIRES  STORAGE LOCATION  SELECTOR             4            wp-backup1  Completed  0  0  2021-03-19 20:07:22 +0530 IST   29d  azure          <none>             5                         6                         7            $ velero backup describe wp-backup             8            Name:         wp-backup             9            Namespace:    velero             10            Labels:       velero.io/storage-location=azure             11            Annotations:  velero.io/source-cluster-k8s-gitversion=v1.19.3             12                          velero.io/source-cluster-k8s-major-version=1             13                          velero.io/source-cluster-k8s-minor-version=19             14                         15            Phase:  Completed             16                         17            Errors:    0             18            Warnings:  0             19                         20            Namespaces:             21              Included:  wp             22              Excluded:  <none>             23                         24            Resources:             25              Included:        *             26              Excluded:        <none>             27              Cluster-scoped:  auto             28                         29            Label selector:  <none>             30                         31            Storage Location:  azure             32                         33            Velero-Native Snapshot PVs:  auto             34                         35            TTL:  720h0m0s             36                         37            Hooks:  <none>             38                         39            Backup Format Version:  1.1.0             40                         41            Started:    2021-03-19 20:07:22 +0530 IST             42            Completed:  2021-03-19 20:07:26 +0530 IST             43                         44            Expiration:  2021-04-18 20:07:22 +0530 IST             45                         46            Total items to be backed up:  50             47            Items backed up:              50             48                         49            Velero-Native Snapshots: 2             50                         51            Kubernetes objects will be saved under storage account in Azure storage account:Delete the namespace:    Shell    xxxxxxxxxx            1                                              1            $ kubectl delete ns wp             2            $ kubectl get pods -n wp             3            No resources found in wp namespace.
The usual way to cope with this limitation is to record all interactions with the application running on a JVM via a Java agent provided by Graal VM.
It can be automated, but there's always a risk that a specific release forgets to test a particular use-case and crashes when deployed.
Blockhound allows verifying that no blocking code runs in unwanted places.
Since the project is in Kotlin anyway, it has no other impact.
You can run the image with:    Shell    xxxxxxxxxx            1                                              1            docker run -it --rm -p8080:8080 docker.io/library/imperative-to-reactive:1.0-SNAPSHOT     #1I use --rm so it removes the container after it has run and doesn't waste disk spaceUnfortunately, this fails with the following exception:Caused by: java.lang.ClassNotFoundException: org.springframework.boot.autoconfigure.r2dbc.ConnectionFactoryConfigurations$PooledConnectionFactoryCondition    at com.oracle.svm.core.hub.ClassForNameSupport.forName(ClassForNameSupport.java:60) ~[na:na]    at java.lang.Class.forName(DynamicHub.java:1260) ~[na:na]    at org.springframework.util.ClassUtils.forName(ClassUtils.java:284) ~[na:na]    at org.springframework.util.ClassUtils.resolveClassName(ClassUtils.java:324) ~[na:na]    ... 28 common frames omittedIt seems that Spring Native missed this one.
JSON    xxxxxxxxxx            1                                              1            [             2            {             3              "name":"org.springframework.boot.autoconfigure.r2dbc.ConnectionFactoryConfigurations$PooledConnectionFactoryCondition",             4              "methods":[{"name":"<init>","parameterTypes":[] }]             5            }             6            ]Building and running again yields the following:Caused by: java.lang.NoSuchFieldException: VERSION    at java.lang.Class.getField(DynamicHub.java:1078) ~[na:na]    at com.hazelcast.instance.BuildInfoProvider.readStaticStringField(BuildInfoProvider.java:139) ~[na:na]    ... 79 common frames omittedThis time, a Hazelcast-related static field is missing.
We need to configure the missing field, re-build and re-run again.
It still fails.
At some point, we also need to keep a resource bundle in the native image:    JSON    xxxxxxxxxx            1                                              1            {             2            "bundles":[             3              {"name":"com.sun.org.apache.xml.internal.serializer.XMLEntities"}             4            ]             5            }Unfortunately, the build continues to fail.
Caused by: java.lang.RuntimeException: internal error    at com.sun.org.apache.xerces.internal.impl.dv.xs.XSSimpleTypeDecl.applyFacets1(XSSimpleTypeDecl.java:754) ~[na:na]    at com.sun.org.apache.xerces.internal.impl.dv.xs.BaseSchemaDVFactory.createBuiltInTypes(BaseSchemaDVFactory.java:207) ~[na:na]    at com.sun.org.apache.xerces.internal.impl.dv.xs.SchemaDVFactoryImpl.createBuiltInTypes(SchemaDVFactoryImpl.java:47) ~[org.hazelcast.cache.ImperativeToReactiveApplicationKt:na]    at com.sun.org.apache.xerces.internal.impl.dv.xs.SchemaDVFactoryImpl.<clinit>(SchemaDVFactoryImpl.java:42) ~[org.hazelcast.cache.ImperativeToReactiveApplicationKt:na]    at com.oracle.svm.core.classinitialization.ClassInitializationInfo.invokeClassInitializer(ClassInitializationInfo.java:375) ~[na:na]    at com.oracle.svm.core.classinitialization.ClassInitializationInfo.initialize(ClassInitializationInfo.java:295) ~[na:na]    ... 82 common frames omittedSwitching to YAMLXML is a huge beast, and I'm not expert enough to understand the exact reason behind the above exception.
It's simple anyway:    YAML    xxxxxxxxxx            1                                              1            hazelcast:             2              instance-name: hazelcastInstanceWe shouldn't forget to add the above resource into the resource configuration file:    JSON    xxxxxxxxxx            1                                              1            {             2            "resources":{             3              "includes":[             4                {"pattern":"hazelcast.yaml"}             5              ]}             6            }Because of missing charsets at runtime, we also need to initialize the YAML reader at build time:    Properties files    xxxxxxxxxx            1                                              1            Args = --initialize-at-build-time=com.hazelcast.org.snakeyaml.engine.v2.api.YamlUnicodeReaderWe need to continue adding a couple of reflectively-accesses classes, all related to Hazelcast.
Missing ProxiesAt this point, we hit a brand new exception at runtime!
Generating proxy classes at runtime is not supported.
____          _            __ _ _ /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \ \\/  ___)| |_)| | | | | || (_| |  ) ) ) )  '  |____| .__|_| |_|_| |_\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot ::                (v2.4.3)...blah blah blah...2021-03-18 20:22:30.654  INFO 1 --- [           main] o.s.b.web.embedded.netty.NettyWebServer  : Netty started on port 80802021-03-18 20:22:30.655  INFO 1 --- [           main] o.s.boot.SpringApplication               : Started application in 2.355 seconds (JVM running for 2.358)If we access the endpoint at this point, the app throws a runtime exception:java.lang.IllegalStateException: Required identifier property not found for class org.hazelcast.cache.Person!
Of course, not everything is perfect: the build displays some exceptions, some logs are duplicated at runtime, and it seems the Hazelcast node cannot join a cluster.
In the meanwhile, I'll probably investigate a bit more closely the remaining warnings.
To go further:Spring Native documentationPackaging OCI ImagesOriginally published at A Java Geek on March 22nd 2021
It’s insanely complicated, it’s quite dangerous.
We still have these problems with kids.
One of the problems we discovered in thinking about TV and the way people interact with TV sets, we kept discovering that the bottom of your TV set has this mucky stripe on it, where the kids are trying to get the thing to go to next page.
I have one friend who said they can’t watch TV with their kid in the room, but it gets incredibly upset that this stupid thing is not behaving properly.
It looks kind of like this, it’s just a big mess of apps and stuff.
That’s ridiculous all on it’s own.
Then Sheryl Sandberg of Facebook said that this may be one of the most important documents to ever come out of the valley and got Reed Hastings to be on Facebook’s board, to try to learn what the hell was going on there.Freedom and Responsibility CultureThat’s interesting.
When I was interviewing at Netflix, they were trying to explain this culture to me, and half the interview was them trying to explain this strange set of ideas.
[8:10] What we were doing was trying to preset the people we were hiring, so that they wouldn’t come in and be baffled and confused.
If you were horrified by what it said in the document, you wouldn’t even bother applying, and we wouldn’t have to deal with you.
Don’t wait until the company gets big, and you have accidental culture of whoever will happen to be there.
There’s a lot of tricks to how to actually get that done.
That is the essential reason why we were able to do, with a relatively small team, build an awful lot of things.
This is the fore thing for Conway’s Law, the basic thesis of this article is that organization of the design systems are constrained to produce designs that are copies of the communication structures of the organizations.
[13:24] If you’re trying to build a microservice architecture, and you haven’t got that organization, you’re going to have a lot of trouble.
[14:18] It’s really no different to programming on the internet.
The more rules you put in, the more complex it gets, and the more rigid it gets.
That’s an old, rigid based way of thinking about it.
There’s no actual version of Netflix.
There’s no version of the system there.
One systems thinking sort of trick to try.
They don’t push stuff to production on Friday afternoons, because they want to go home for the weekend and not have to deal with broken things.
You’re sending the pain back to where it can do the most good.
If you put an ops team in charge of production, and sort of throw it over the wall and it’s their problem, then you’re giving up on that feedback loop.
To everyone that was delivering a JAR, now delivered a service that incorporated that JAR with an API, and you could manage, you were on call if that service broke.
Unfortunately if your title is Project Manager, that’s not a good time.
I’m sorry, they’re really aren’t any Project Managers in this new model.
That’s kind of the bad news, I feel like.From Project to Product[18:46] The thing about a project is you form a team and go do something like upgrade SAP, or here’s sort of my horrific thing, you spend 9 months desperately trying to get this thing upgraded.
Then everybody runs away from the project in horror and goes to work on something else.
Operations is left holding this thing which is no longer able to change.
It’s very hard to get locked in.
It’s the ops people that get stuck with the thing that hate lock-in.This was actually last summer, I basically pointed out that it was kind of like the developers were dating and getting married.
Divorces are not nice things, right?
If that’s your general experience of technology, is you’re trying to unlock yourself from it, it’s a very unhappy thing.
If that piece of the system stops evolving very rapidly, you just put it in some maintenance mode.
If you write a piece of code, or you build a thing, and it’s not reached the customer yet, it’s not added value.
If you live in Zimbabwe, and all of a sudden there’s this thing called Netflix you’ve never heard of, and somebody’s trying to explain it to you, the more complex and the more features there are in it, the more confusing it will be.
You end up with Word or something, millions of features that no one knows how to use.
It’s missing all these features.
If anyone ever says microservices are too complicated, no they’re not.
Fight back, argue back on that one.
You’re isolating things, you’re preventing methods from creeping in and gathering data where they shouldn’t be seeing it.
You’re preventing end runs around methods to go into the data store.
It’s simpler to learn, it’s simpler to get people onboard, and it’s simpler to manage.The problem back in 2009, 2010 is the tooling, the monitoring systems couldn’t deal with it.
We used to have problems in our data center where we didn’t even have the same firmware revision on all the machines we were running, and that would cause problems.
You can get in some very low level problems.The final thing is just systems thinking.
“We see the world as increasingly more complex and chaotic, because we’re using inadequate concepts to explain it.” Once you have the concepts, once you’ve built your mental models, all of a sudden it’s no longer chaotic or complex.Think about driving a car, there’s a mental model around driving and how cities work, and how traffic works.
Once you’ve got that model, it’s no longer chaotic and complex.
You can tell it’s an innovation because people are arguing about it, and their words aren’t very well-defined.
When they first came out, it was controversial.
Then we got to public cloud, I’m using say AWS or Google, I have no idea.
I have no idea.
People are still arguing about Serverless, and what it means, and all these things.
Forget these tiny little 10 lines of node, I’m trying to figure out how to run a 2-terabyte graph database in memory or something.
The terminology is no longer controversial.
It’s just ridiculous.
You can do it with Cloud Foundry, you can do it with Kubernetes, and Swarm, and Nomad, and Mesos, and I’ve probably forgotten two or three of them.
You can get it all jammed into one thing, and then it starts to become a commodity.Commodity StageAt the commodity stage, you just stop worrying about it.
It’s not that the platform stops changing, it’s continuing to evolve.
An Introduction to Big Data and NoSQL SolutionsIn this presentation we will look into the "big data" problem and present NoSQL solutions for dealing with these types of systems.
The key drivers are scalability, strategic IT initiative, and cost savings and the top use cases are data center extension, disaster recovery, and cloud migration.
Roshan Kumar, Senior Product Marketing Manager, Redis Labs — developers tend to be the first adopters of Redis for the cloud or Docker since the database can hold data in specific structures with no objects or relational standing.
It addresses DevOps and Ops concerns for backup and disaster recovery with high availability, reliability, and scalability.
While AWS has had more than 2,000 node failures, no data has been lost on Redis due to their primary and backup servers.
Tintri enables developers to manage their virtual machines through automation, isolate traffic between virtual machines, and use self-service automation in the development phase.
Danny Allen, V.P., Product Strategy, Veeam — moving to a subscription model to provide cloud data availability and backup which is critical for applications and containers.
Caching can be used and customized with an acceptable eviction policy based on business requirements.
It’s difficult to identify issues between microservices when services are dependent on each other and they have a cyclic dependency.
Testing — This issue can be addressed with unit and integration testing by mocking microservices individually or integrated/dependent APIs which are not available for testing using WireMock, BDD, Cucumber, integration testing.
It will trace all microservices communication and show request/response, errors on its dashboard.
Instead, it initializes the attribute that references the lazy relationship with a proxy.
If the Session is closed - or the object has been detached, Hibernate cannot connect to the database and throws the dreaded LazyInitializationException!
Possible SolutionsNow that we have framed the problem and what causes it, we can look at possible solutions.
Setting your associations to be eager is a terrible idea!
Collections have no limits on their size.
Putting those together, we end up with an object graph with:A nesting level dictated by the modelAnd an unbounded number of objects at each level shaped by the data in the databaseWorse, since the development database probably has a handful of data lines, you'll likely experience performance issues in staging or even, worse, in production.
The bad thing is that each lazily-loaded attribute in the graph executes a new SQL query and requires a new roundtrip to the database.
It's known as the N+1 problem: one query to fetch the root entity and N queries to fetch each child entity.
If a problem happens, there's no way to handle the exception properly.
Even Spring Boot registers an OSIV filter by default though it logs a warning message about it.
To avoid nasty surprises in production, I'd advise you to set spring.jpa.open-in-view=false as the first step in every Spring Boot project that uses both Hibernate and Web MVC.
Copy-pasting each attribute manually is a bore and error-prone.
However, it doesn't solve the N+1 problem.
A cheap trick is to avoid DTO, keep the entity and explicitly call its getters before sending it to the view.
It has the same pro and con as DTOs, respectively error-handling in the right place and N+1 queries problem.
For example, when an uninitialized proxy or a collection is accessed after the session was closed, or after the object has been detached from the session.
To solve this problem, you may prefetch all properties you need before the session is closed or use the OpenSessionInView pattern.
The other alternatives do have the N+1 queries problem.
By loading eagerly, we stuff the memory because we load everything; by loading lazily, we overload the network with additional requests when lazy attributes are requested.
But I believe it's the wrong way to look at it.
Still, they suffer from two downsides:Using fetch joins is error-prone, especially in JPQL.
Barring that, you'll at least be aware of what problems you'll encounter.
I admit I was not sure of the answer, so I did a bit of research.
Back pressure (or backpressure) is a resistance or force opposing the desired flow of fluid through pipes, leading to friction loss and pressure drop.
The term back pressure is a misnomer, as pressure is a scalar quantity, so it has a magnitude but no direction.
-- WikipediaIn software, backpressure has a slightly related but still different meaning: considering a fast data producer and a slow data consumer, backpressure is the mechanism that 'pushes back' on the producer not to be overwhelmed by data.
Whether based on reactivestreams.org or Java's java.util.concurrent.Flow, Reactive Streams provides four building blocks:A Publisher that emits elements.
It doesn't support backpressure.
SingleA flow of exactly:     1 item,or an error.
MaybeA flow with either:     no items,exactly one item,or an error.
CompletableA flow with no item but either:     a completion,or an error signal.
As RxJava's wiki states:Backpressure doesn’t make the problem of an overproducing Observable or an underconsuming Subscriber go away.
To cope with that, RxJava offers two main strategies to handle 'overproduced' items:Store items in a buffer:Note that if you set no upper bound to the buffer, it might cause OutOfMemoryError.
Drop items:The following diagram summarizes the different methods that implement those strategies:Note that the onBackPressureLatest operator is similar to using onBackpressureBuffer(1):Note that I took the above Marble diagrams from RxJava's wiki.
All cope with a producer that is faster than its subscriber by offering two strategies: either buffer items or drop them.
To Go Further:Reactive Streams JVM specificationsHow (not) to use Reactive Streams in Java 9+RxJava BackpressureOriginally published at A Java Geek on March 14th 2021.
Distributed Tracing With Zipkin and ELKWhile logs can tell us whether a specific request failed to execute or not and metrics can help us monitor how many times this request failed and how long the failed request took, traces help us debug the reason why the request failed or took so long to execute by breaking up the execution flow and dissecting it into smaller events.
In our case, I want to pass some environment variables and specify Elasticsearch as the storage type (this example assumes a locally running ELK Stack), and so I will use the following commands to download and run Zipkin: curl -sSL https://zipkin.io/quickstart.sh | bash -sjava -DSTORAGE_TYPE=elasticsearch -DES_HOSTS=http://127.0.0.1:9200 -jar zipkin.jarIf all goes as expected, you should see this output in your terminal: 2018-05-02 09:59:27.897  INFO [/] 12638 --- [  XNIO-2 task-1] o.s.web.servlet.DispatcherServlet        : FrameworkServlet 'dispatcherServlet': initialization completed in 108 msOpen Zipkin at http://localhost:9411:Of course, we have no traces to analyze in Zipkin yet, so our next step is to simulate some requests.
A trace tells you when one of your flows is broken or slow along with the latency of each step.
However, traces don't explain latency or errors.
Metrics allow deeper analysis into system faults.
Traces are also specific to a single operation, they are not aggregated like logs or metrics.
The situation is likely worse for unofficial clients.
The official clients are better across the board, but the unofficial libraries are worse off.
However, you don't want to have to instrument everything.
Zipkin is not part of a wider ecosystem, it's an isolated project part of a pre-container world.
If Jaeger doesn't fit, then go with Zipkin.
i’m using an external pre-baked container image for zipkin that exposes port 9411 for the admin gui.
google themselves says the following in the official docs:"at google, we require that go programmers pass a context parameter as the first argument to every function on the call path between incoming and outgoing requests."
this is a somewhat controversial thing within the go community.
oh well - enough of this   “i dislike it but i use it anyway”  stuff.
there’s a few typical use cases where we need to concern ourselves with tracing info:  incoming http requests: we look for opentracing correlation id’s in http headers and if found - starts a trace as well as dumping the required data structure into a go context.
error: " + err.error())        }        tracer, err = zipkin.newtracer(                zipkin.newrecorder(collector, false, "127.0.0.1:0", servicename))        if err != nil {                panic("error starting new zipkin tracer.
error: " + err.error())        }} note the initialization of the package-scoped   tracer opentracing.tracer  variable, which is the object we’ll be doing all our tracing work with.
probably not the most efficient protocol for this purpose.
as argument to   loadtracing()  , we’re passning the func defined in the   route  .
note the ugly use of   “opentracing-span”  as key.
func performhttprequestcircuitbreaker(ctx context.context, breakername string, req *http.request) ([]byte, error) {        output := make(chan []byte, 1)                          // hystrix stuff...        errors := hystrix.go(breakername, func() error {        // hystrix stuff...                tracing.addtracingtoreqfromcontext(ctx, req)    // here!!!
(opentracing.span).context(), // note ugly typecast here and use of the hard-coded key...                 opentracing.httpheaders,                 carrier)         if err != nil {                 panic("unable to inject tracing context: " + err.error())   // here be dragons.         }
5.2.4 internal tracing we can of course add child traces without dealing with http headers - we could even pass   opentracing.span  structs around as parameters instead of using that ugly   context.context  .
span := tracing.startchildspanfromcontext(ctx, "queryaccount")    // start a child span of the current one,                                                                           // named queryaccount        defer span.finish()      // note use of defer, e.g.
account := model.account{}        err := bc.boltdb.view(func(tx *bolt.tx) error {            ......... more code .........        } see comments for details.
given that we’ve got a working go environment (remember gopath) and docker running (don’t forget to   eval “$(docker-machine env swarm-manager-0)”  etc.
run a few calls using url:curl -k https://192.168.99.100:8765/api/accounts/10000  (the -k flag is to ignore ssl warnings, i’m running zuul with a self-signed cert)  open the zipkin gui at   http://192.168.99.100:9411  and click the “find traces” button: cool!
vipservice#onmessage uses about 11ms, but remember that we’re just sending an asynchronous message so that execution isn’t blocking anything else.
since the   getquote  span makes up 30 ms of the total 32 ms of the   getaccount  span, we can probably say for certain that the quotes-service is guilty.
(in this case, we shouldn’t blame java.
It is easy to identify which microservice is failed or having performance issues whenever there are multiple service calls within a single request.
Currently, you will not see any traces as there is no client application registered with Zipkin Server.
When an application experiences a slowdown and its "data flow" goes through several different microservices, pinpointing the exact location of a slowdown may be difficult for a developer.
This is all well and good, but when a problem occurs a developer must go through a lot of logs (produced by the selected logging framework, such as Fluentd or Logstash – in KumuluzEE you can use the KumuluzEE Logs extension to simplify logging) to find the problem.
If a problem is severe (such as a complete crash), a solution is usually discovered quickly or even handled automatically by some container orchestration tool (such as Kubernetes).
Tougher problems are the slowdowns in which the application still works but in a limited capacity.
Right now, we have no data.
Simulated lag is added to this request (random delay).
The same does not apply to outgoing requests.
Locate the Resource.java file (src/main/java/com/kumuluz/ee/samples/opentracing/tutorial/delta) and add the following line:@GETpublic Response get() {    throw new RuntimeException("Something went wrong here.
Adding Custom Data to SpansIf we look back to our project structure, we added some simulated lag to our application in the beta microservice.
Basically, we added a random delay from 1 to 1000 milliseconds to the request.
After that, we can access the current span with tracer.activeSpan and add our delay to it.
For example, we did not include integration with a MicroProfile Config extension and KumuluzEE config frameworks, which would allow additional tracing configuration such as ignoring tracing on JAX-RS endpoints and changing the way that spans are named.
Suppose, an exception has occurred or the data returned is invalid and you want to investigate what is wrong by looking at logs.
But as of now, there is no way to correlate the logs of that particular user across multiple services.
If there is no CORRELATION_ID in the header then create a new one and set it in MDC.
As of now, Zipkin doesn't support Spring Boot 2.
version: '2'services:  # The zipkin process services the UI, and also exposes a POST endpoint that  # instrumentation can send trace data to.
Setup Zipkin, Zipkin-dependencies Services With Docker ImageDocker Compose file includes Zipkin and Zipkin-dependencies services;  Zipkin_RabbitMQCollector”Zipkin-dependencies” is a process service which combines Zipkin indices: “Zipkin-span-xxx” into “Zipkin-dependency-xx”; in the following sample docker-compose.xml, it is set to run every hour:    XML    xxxxxxxxxx            1                       58                                              1            version: '3.7'             2            services:             3              zipkin:             4                image: openzipkin/zipkin             5                deploy:             6                  replicas: 1             7                  update_config:             8                    parallelism: 1             9                    delay: 1m30s             10                    failure_action: rollback             11                  rollback_config:             12                    parallelism: 1             13                    delay: 1m30s             14                  restart_policy:             15                    condition: on-failure             16                    delay: 5s             17                    max_attempts: 3             18                  resources:             19                    limits:             20                      memory: 500M             21                    reservations:             22                      memory: 100M             23                ports:             24                  - 9411:9411             25                environment:             26                  RABBIT_CONCURRENCY: 1             27                  RABBIT_CONCURRENCY: 1             28                  RABBIT_CONNECTION_TIMEOUT: 60000             29                  RABBIT_QUEUE: zipkin             30                  RABBIT_ADDRESSES: {rabbitmq_host}:5672             31                  RABBIT_PASSWORD: password             32                  RABBIT_USER: user             33                  RABBIT_VIRTUAL_HOST: app_vhost             34                  RABBIT_USE_SSL: "false"             35                  STORAGE_TYPE: elasticsearch             36                  ES_HOSTS: http://{elasticsearch_host}:9200             37                               38                healthcheck:             39                  test: wget --no-verbose --tries=1 --spider http://localhost:9411/health  || exit 1             40                  interval: 10s             41                  start_period: 15s             42                  retries: 3             43                         44              zipkin-dependencies:             45                image: openzipkin/zipkin-dependencies             46                deploy:             47                  replicas: 1             48                  restart_policy:             49                    delay: 1h             50                  resources:             51                    limits:             52                      memory: 550M             53                    reservations:             54                      memory: 100M             55                environment:             56                  STORAGE_TYPE: elasticsearch             57                  ES_HOSTS: http://{elasticsearch_host}:9200It fetches Sleuth messages from RabbitMQ  and stored the data in ElasticSearch, check the ES indices: http://{elasticsearch_host}:9200/_cat/indices/zipkin*?v&s=indexDeploy the services in Docker Host, access it with URL: http://{docker_server_IP}:9411/zipkinTrace logs:Service Dependencies:Setup Zipkin Data Clean up Policy in ElasticSearchTo archive, the old data, set up the ElasticSearch index policy to remove the data created 180 days before.
An Introduction to Distributed Tracing and Zipkin [Video]Latency analysis is the act of blaming components for causing user-perceptible delay.
This session will overview how to debug latency problems using call graphs created by Zipkin.
When you leave, you’ll at least know something about distributed tracing, and hopefully be on your way to blaming things for causing latency!
Cloud computing enables us to automate away the pain (from days or weeks (gasp!)
The proliferation of new services complicates reasoning about system-wide and request-specific performance characteristics.When all of an application’s functionality lives in a monolith - what we call applications written as one, large, unbroken deployable like a .war or .ear - it’s much easier to reason about where things have gone wrong.
Is there a memory leak?
You don’t want to overwhelm your logging and analysis infrastructure, though.
The Span given as an argument represents the span for the current in-flight request in the larger trace.
However, tracing, by its very nature, is a cross-cutting concern for all services no matter which technology stack they’re implemented in.
Finally, look no further than the Spring Initializr and add the Spring Cloud Sleuth Stream and Zipkin Stream Server to your Maven builds to get started.
Backend storage and processing,fully managed by Instana — no need to waste time deploying, configuring, and managing the backend monitoring systemWe expect as the OpenTracing community evolves and matures there will be a number of applications, libraries, and plugins developed which interact natively with the Jaeger and Zipkin compatible clients.
This will enable our customers to leverage these predefined traces and spans with no extra effort required.
Each microservice should be run in an environment isolated from the other microservices, so they do not share any resources like data sources or log files between them.
Grepping the logs is not the right solution for that problem.
Each microservice should be run in an environment isolated from the other microservices, so they do not share any resources like data sources or log files with them.
Grepping the logs is not the right solution for that problem.
Architecture and exposed services are the same as in the previous sample.
There is no an upper limit for the number of microservices in your system.
ZipKin, by default, does not persist traces, which are stored in-memory and thus lost on server startup.
The downside is it also decreases error and bottleneck detection capabilities, making it much harder to detect why and where a request failed.
The collected data then provides information on which service a request failed or slowed down.
The downside is that it doesn’t offer scalability, nor is it reliable.
It, however, does not scale as well as Jaeger since all services are built in the same artifact.
However, it does not support multiple Zipkin instances, so scalability is limited.
There are also no agents/sidecars.
Grafana TempoAnnounced in 2020, Grafana Tempo is a new contender designed to address specific issues with other distributed tracing tools, particularly the use of large servers and difficulties with data sampling.
Despite being a new project, it offers nothing special for Kubernetes deployment.
This way, you can avoid putting large configuration properties inside the application at runtime and offloads all data ingestion to another service.
As for Zipkin and Grafana Tempo, neither provides a standardized solution for Kubernetes.
This can make it difficult to switch to another tracing tool (though most are cross-compatible).
To make an example a bit more illustrative, we will pretend that server uses some kind of database to retrieve the data.
We will start with JAX-RS 2.0 client part as it's very straightforward (ClientStarter.java):package com.example.client;import javax.ws.rs.client.Client;import javax.ws.rs.client.ClientBuilder;import javax.ws.rs.core.MediaType;import javax.ws.rs.core.Response;import com.example.zipkin.Zipkin;import com.example.zipkin.client.ZipkinRequestFilter;import com.example.zipkin.client.ZipkinResponseFilter;public class ClientStarter {  public static void main( final String[] args ) throws Exception {     final Client client = ClientBuilder      .newClient()      .register( new ZipkinRequestFilter( "People", Zipkin.tracer() ), 1 )      .register( new ZipkinResponseFilter( "People", Zipkin.tracer() ), 1 );            final Response response = client      .target( "http://localhost:8080/rest/api/people" )      .request( MediaType.APPLICATION_JSON )      .get();    if( response.getStatus() == 200 ) {      System.out.println( response.readEntity( String.class ) );    }    response.close();    client.close();    // Small delay to allow tracer to send the trace over the wire    Thread.sleep( 1000 );  }}Except a couple of imports and classes with Zipkin in it, everything should look simple.
Our JAX-RS 2.0 server will expose the single endpoint (PeopleRestService.java):package com.example.server.rs;import java.util.Arrays;import java.util.Collection;import java.util.concurrent.Callable;import javax.ws.rs.GET;import javax.ws.rs.Path;import javax.ws.rs.Produces;import com.example.model.Person;import com.example.zipkin.Zipkin;@Path( "/people" ) public class PeopleRestService {  @Produces( { "application/json" } )  @GET  public Collection< Person > getPeople() {    return Zipkin.invoke( "DB", "FIND ALL", new Callable< Collection< Person > >() {      @Override      public Collection<person> call() throws Exception {        return Arrays.asList( new Person( "Tom", "Bombdil" ) );      }       } );     }}As we mentioned before, we will simulate the access to database and generate a child trace by using Zipkin.invoke wrapper (which looks very simple, Zipkin.scala):package com.example.zipkinimport java.util.concurrent.Callableimport com.twitter.finagle.stats.DefaultStatsReceiverimport com.twitter.finagle.tracing.Traceimport com.twitter.finagle.zipkin.thrift.ZipkinTracerimport com.twitter.finagle.tracing.Annotationobject Zipkin {  lazy val tracer = ZipkinTracer.mk( host = "localhost", port = 9410, DefaultStatsReceiver, 1 )  def invoke[ R ]( service: String, method: String, callable: Callable[ R ] ): R = Trace.unwind {    Trace.pushTracerAndSetNextId( tracer, false )          Trace.recordRpcname( service, method );    Trace.record( new Annotation.ClientSend() );    try {      callable.call()    } finally {      Trace.record( new Annotation.ClientRecv() );    }  }   }As we can see, in this case the server itself becomes a client for some other service (database).The last and most important part of the server is to intercept all HTTP requests, extract the Trace Id from them so it will be possible to associate more data with the trace (annotate the trace).
In Apache CXF it's very easy to do by providing own invoker (ZipkinTracingInvoker.scala):package com.example.zipkin.serverimport org.apache.cxf.jaxrs.JAXRSInvokerimport com.twitter.finagle.tracing.TraceIdimport org.apache.cxf.message.Exchangeimport com.twitter.finagle.tracing.Traceimport com.twitter.finagle.tracing.Annotationimport org.apache.cxf.jaxrs.model.OperationResourceInfoimport org.apache.cxf.jaxrs.ext.MessageContextImplimport com.twitter.finagle.tracing.SpanIdimport com.twitter.finagle.http.HttpTracingimport com.twitter.finagle.tracing.Flagsimport scala.collection.JavaConversions._import com.twitter.finagle.tracing.Tracerimport javax.inject.Injectclass ZipkinTracingInvoker extends JAXRSInvoker {  @Inject val tracer: Tracer = null  def trace[ R ]( exchange: Exchange )( block: => R ): R = {    val context = new MessageContextImpl( exchange.getInMessage() )    Trace.pushTracer( tracer )    val id = Option( exchange.get( classOf[ OperationResourceInfo ] ) ) map { ori =>      context.getHttpHeaders().getRequestHeader( HttpTracing.Header.SpanId ).toList match {        case x :: xs => SpanId.fromString( x ) map { sid =>           val traceId = context.getHttpHeaders().getRequestHeader( HttpTracing.Header.TraceId ).toList match {            case x :: xs => SpanId.fromString( x )            case _ => None          }          val parentSpanId = context.getHttpHeaders().getRequestHeader( HttpTracing.Header.ParentSpanId ).toList match {            case x :: xs => SpanId.fromString( x )            case _ => None          }          val sampled = context.getHttpHeaders().getRequestHeader( HttpTracing.Header.Sampled ).toList match {             case x :: xs =>  x.toBoolean            case _ => true          }          val flags = context.getHttpHeaders().getRequestHeader( HttpTracing.Header.Flags ).toList match {            case x :: xs =>  Flags( x.toLong )            case _ => Flags()          }          val id = TraceId( traceId, parentSpanId, sid, Option( sampled ), flags )                               Trace.setId( id )          if( Trace.isActivelyTracing ) {            Trace.recordRpcname( context.getHttpServletRequest().getProtocol(), ori.getHttpMethod() )            Trace.record( Annotation.ServerRecv() )          }          id        }                   case _ => None      }    }    val result = block    if( Trace.isActivelyTracing ) {      id map { id => Trace.record( new Annotation.ServerSend() ) }    }    result  }  @Override  override def invoke( exchange: Exchange, parametersList: AnyRef ): AnyRef = {    trace( exchange )( super.invoke( exchange, parametersList ) )       }}Basically, the only thing this code does is extracting Trace Id from request and associating it with the current thread.
In this example, we’re tracing all requests, but in a production setting you may want to set the rate to be much lower (the default is 0.001, or 0.1% of all requests).
If you’re sending a barrage of requests in a loadtest that you don’t want flooding your tracing system, consider setting it to something much lower than the steady-state sample rate defined in your Linkerd config.
Each microservice should be run in an environment isolated from the other microservices so it does not share resources such as databases or log files with them.
Grepping the logs is not the right solution for that problem.
Architecture and exposed services are the same as in the previous sample.
Tracing in Microservices With Spring Cloud SleuthOne of the problems developers encounter as their microservice apps grow is tracing requests that propagate from one microservice to the next.
Remember the real problem here is not identifying logs within a single microservice but instead tracing a chain of requests across multiple microservices.
You could certainly calculate how long a request took from one microservice to the next but that is quite a pain to do yourself.
This is because log collection, transformation, and storage are costly.
It thus increases complexity and takes more time while troubleshooting problems.
Distributed tracing helps to get insight into the individual operation and pinpoint the areas of failure caused by poor performance.
Span: It is a primary building block of a distributed trace.
As you have seen earlier, there was no trace id label on the Loki log.
Incorrect value or missing value may lead to the following error:Re-apply the hotrod-deployment manifest to incorporate the changes made.
Thus, if your stack is not Java only, I guess it’s hard to consider inspectIT as the option as some parts of your system would be untraceable, thus losing the complete end-to-end picture that we aim for.
More DataHowever, soon you will discover that with no additional hacking, the data provided by both tools is not enough for meaningful performance diagnosis.
inspectIT can solve this problem in no time, because it offers a UI-based configuration interface that allows you to quickly bound measurement points to any Java method, thus you will get the duration of the method executions together with your tracing data.
The sampling rate approach is based on the Google Dapper paper that concludes that if a problem exists in a system with high-throughput, then the same problem will surface multiple times and would be part of one of the captured traces: "New Dapper users often wonder if low sampling probabilities – often as low as 0.01% for high-traffic services – will interfere with their analyses.
Our experience at Google leads us to believe that, for high-throughput services, aggressive sampling does not hinder most important analyses.
inspectIT, at the moment, does not provide the sampling rate feature; it collects all the traces and is thus more suitable for services with lower volume.
Anyhow, both tools have nothing less than a sunny future.
The goal of Distributed Tracing is to pinpoint failures and causes of poor performance across various system boundaries.
To solve this problem, OpenTracing and OpenCensus projects were started.
Concepts and TerminologySpan — Span is the building block of a trace.
If no         context is found, then a new span is started.
Acmeshop ArchitectureLet's say the end-user experiences poor performance.
This can then be used to go to those services and start looking at their logs and other stats to figure out the actual problem with the application.
This is only for local testing    var options = {      uri: endpoints.usersUrl + "/users/" + req.params.id,      method: 'GET',      json: true,      headers: req.headers    };    // Leverages request library    request(options, function(error, response, body) {        if (error) {          return next(error);        }        if (response.statusCode == 200) {            console.log('printing from within request')            res.writeHead(200)            res.write(JSON.stringify(body))     // Section 3            userSpan.setTag(opentracing.Tags.HTTP_STATUS_CODE, response.statusCode)            userSpan.log({              'event': 'request_end'            });            userSpan.finish();            res.end();        }        else {            res.status(response.statusCode);            res.write(JSON.stringify(response.statusMessage))  // Section 4            userSpan.setTag(opentracing.Tags.HTTP_STATUS_CODE, response.statusCode)            userSpan.setTag(opentracing.Tags.ERROR, true);            userSpan.log({              'event': 'error',              'message': response.statusMessage.toString()            });            userSpan.log({              'event': 'request_end'            })            userSpan.finish();            res.end();        }    }); // end of request}); // end of methodIn the first code block, you can notice that we start a span.
In the second code block, we are making an HTTP request to the user service.
In the third and fourth code blocks, there were tags and log fields added.
You may also add tags to indicate an ERROR by using userSpan.setTag(opentracing.Tags.ERROR, true);Messages can also be logged along with the span by using    userSpan.log({      'event': 'error',      'message': response.statusMessage.toString()    });Augmenting additional data along with the span can increase the network and storage costs.
func initJaeger(service string) (opentracing.Tracer, io.Closer) {// Uncomment the lines below only if sending traces directly to the collector// tracerIP := GetEnv("TRACER_HOST", "localhost")// tracerPort := GetEnv("TRACER_PORT", "14268")agentIP := GetEnv("JAEGER_AGENT_HOST", "localhost")agentPort := GetEnv("JAEGER_AGENT_PORT", "6831")logger.Infof("Sending Traces to %s %s", agentIP, agentPort)cfg := &jaegercfg.Configuration{Sampler: &jaegercfg.SamplerConfig{Type:  "const",Param: 1,},Reporter: &jaegercfg.ReporterConfig{LogSpans:          true,LocalAgentHostPort: agentIP + ":" + agentPort,// Uncomment the lines below only if sending traces directly to the collector//CollectorEndpoint: "http://" + tracerIP + ":" + tracerPort + "/api/traces",},}tracer, closer, err := cfg.New(service, config.Logger(jaeger.StdLogger))if err != nil {panic(fmt.Sprintf("ERROR: cannot init Jaeger: %v\n", err))}return tracer, closer}// Set tracer to Global tracerfunc main() {  // Start Tracer  tracer, closer := initJaeger("user")stdopentracing.SetGlobalTracer(tracer)  // Start the serverhandleRequest()  // Stop Tracerdefer closer.Close()}Extract the Context and Start Span in User Service (Go)Once the request is received by the service, it is re-directed to GetUser Function, as shown below.
func GetUser(c *gin.Context) {var user UserResponsetracer := stdopentracing.GlobalTracer()userSpanCtx, _ := tracer.Extract(stdopentracing.HTTPHeaders, stdopentracing.HTTPHeadersCarrier(c.Request.Header))userSpan := tracer.StartSpan("db_get_user", stdopentracing.ChildOf(userSpanCtx))defer userSpan.Finish()userID := c.Param("id")userSpan.LogFields(tracelog.String("event", "string-format"),tracelog.String("user.id", userID),)if bson.IsObjectIdHex(userID) {error := collection.FindId(bson.ObjectIdHex(userID)).One(&user)if error != nil {message := "User " + error.Error()userSpan.LogFields(tracelog.String("event", "error"),tracelog.String("message", error.Error()),)userSpan.SetTag("http.status_code", http.StatusNotFound)c.JSON(http.StatusNotFound, gin.H{"status": http.StatusNotFound, "message": message})return}} else {message := "Incorrect Format for UserID"userSpan.LogFields(tracelog.String("event", "error"),tracelog.String("message", message),)userSpan.SetTag("http.status_code", http.StatusBadRequest)c.JSON(http.StatusBadRequest, gin.H{"status": http.StatusBadRequest, "message": message})return}userSpan.SetTag("http.status_code", http.StatusOK)c.JSON(http.StatusOK, gin.H{"status": http.StatusOK, "data": user})}Here, the request is extracted by usinguserSpanCtx, _ := tracer.Extract(stdopentracing.HTTPHeaders, stdopentracing.HTTPHeadersCarrier(c.Request.Header))This provides the traceID from the parent.
NOTE: In the end, if you use Opentracing with Jaeger agents (as we have described in the article above), you can pick essentially ANY tracing UI you need (open source or commercial)Tracing Is DifficultIt's difficult to get tracing correct, especially when adding it to existing applications.
2017-08-21 06:36:13.698  INFO ["Spring Boot 1",f21c03b4f86f31a8,f21c03b4f86f31a8,false] 25841 --- [http-nio-8080-exec-4] c.b.l.controller.LoggingController       : Quote{type='success', value=Value{id=4, quote='Previous to Spring Boot, I remember XML hell, confusing set up, and many hours of frustration.'}}
2017-08-21 06:36:13.693  INFO ["Spring Boot 2",f21c03b4f86f31a8,cbeb473567ccd74a,false] 25867 --- [http-nio-8082-exec-6] c.b.l.controller.LoggingController       : Quote{type='success', value=Value{id=4, quote='Previous to Spring Boot, I remember XML hell, confusing set up, and many hours of frustration.'}}
As per the monitoring, logging, and tracing installation documentation of Knative:  Knative Serving offers two different monitoring setups:  Elasticsearch, Kibana, Prometheus and Grafana or  Stackdriver, Prometheus and Grafana You can install only one of these two setups and side-by-side installation of these two are not supported.
Scope is deployed onto a Kubernetes cluster with the commandkubectl apply -f “https://cloud.weave.works/k8s/scope.yaml?k8s-version=$(kubectl version | base64 | tr -d ‘\n’)”To open Weavescope, run the command and open http://localhost:4040/kubectl port-forward -n weave “$(kubectl get -n weave pod — selector=weave-scope-component=app -o jsonpath=’{.items..metadata.name}’)” 4040Kibana + ElasticSearchI tried to visualize the logs using Kibana UI (the visualization tool for Elasticsearch), but struck with the following error while configuring an index pattern — “Unable to fetch mapping.
This article gives an overview of the traceability problem in distributed systems and provides some tips on how to implement tracing with Spring Cloud Sleuth.
Traceability in Distributed SystemsEven in a monolithic system, tracing a bug can be hard enough.
To find the root cause of an error you search through the log files of the application servers around the point in time the error occurred and hope that you find a stacktrace that explains the error.
Ideally, the error message contained a correlation ID that uniquely identifies the error, so that you can just search for that correlation ID in the log files.
If an error occurs in one of those upstream services, the upstream service will probably log the error.
Since the downstream services receives an error response, it will probably also log an error.
For a straightforward error analysis, both log entries should be connected by a shared correlation ID.
Sadly, exceptions that bubble up in Spring Boot cannot access the trace ID since they are not handled by Spring Boot but by the application server.
Since the trace ID is accessible via MDC, it can be read from the MDC at any time and may be passed on to the client in case of an error.
This class implements a simple interface designed for this purpose called KalahaGameSowApi:public interface KalahaGameSowApi {    KalahaGame sow (KalahaGame game, int pitIndex);}The implementation for the above interface provided as below:@Servicepublic class MancalaSowingService implements KalahaGameSowApi {    // This method perform sowing the game on specific pit index    @Override    public KalahaGame sow(KalahaGame game, int requestedPitId) {        // No movement on House pits        if (requestedPitId == KalahaConstants.rightPitHouseId || requestedPitId == KalahaConstants.leftPitHouseId)            return game;        // we set the player turn for the first move of the game based on the pit id        if (game.getPlayerTurn() == null) {            if (requestedPitId < KalahaConstants.rightPitHouseId)                game.setPlayerTurn(PlayerTurns.PlayerA);            else                game.setPlayerTurn(PlayerTurns.PlayerB);        }        // we need to check if request comes from the right player otherwise we do not sow the game.
In other words,        // we keep the turn for the correct player        if (game.getPlayerTurn() == PlayerTurns.PlayerA && requestedPitId > KalahaConstants.rightPitHouseId ||                game.getPlayerTurn() == PlayerTurns.PlayerB && requestedPitId < KalahaConstants.rightPitHouseId)            return game;        KalahaPit selectedPit = game.getPit(requestedPitId);        int stones = selectedPit.getStones();        // No movement for empty Pits        if (stones == KalahaConstants.emptyStone)            return game;        selectedPit.setStones(KalahaConstants.emptyStone);        // keep the pit index, used for sowing the stones in right pits        game.setCurrentPitIndex(requestedPitId);        // simply sow all stones except the last one        for (int i = 0; i < stones - 1; i++) {            sowRight(game,false);        }        // simply the last stone        sowRight(game,true);        int currentPitIndex = game.getCurrentPitIndex();        // we switch the turn if the last sow was not on any of pit houses (left or right)        if (currentPitIndex !=KalahaConstants.
rightPitHouseId && currentPitIndex != KalahaConstants.leftPitHouseId)            game.setPlayerTurn(nextTurn(game.getPlayerTurn()));        return game;    }    // sow the game one pit to the right    private void sowRight(KalahaGame game, Boolean lastStone) {        int currentPitIndex = game.getCurrentPitIndex() % KalahaConstants.totalPits + 1;        PlayerTurns playerTurn = game.getPlayerTurn();        if ((currentPitIndex == KalahaConstants.rightPitHouseId && playerTurn == PlayerTurns.PlayerB) ||                (currentPitIndex == KalahaConstants.leftPitHouseId && playerTurn == PlayerTurns.PlayerA))            currentPitIndex = currentPitIndex % KalahaConstants.totalPits + 1;        game.setCurrentPitIndex(currentPitIndex);        KalahaPit targetPit = game.getPit(currentPitIndex);        if (!lastStone || currentPitIndex == KalahaConstants.rightPitHouseId || currentPitIndex == KalahaConstants.leftPitHouseId) {            targetPit.sow();            return;        }        // It's the last stone and we need to check the opposite player's pit status        KalahaPit oppositePit = game.getPit(KalahaConstants.totalPits - currentPitIndex);        // we are sowing the last stone and the current player's pit is empty but the opposite pit is not empty, therefore,        // we collect the opposite's Pit stones plus the last stone and add them to the House Pit of current player and        // make the opposite Pit empty        if (targetPit.isEmpty() && !oppositePit.isEmpty()) {            Integer oppositeStones = oppositePit.getStones();            oppositePit.clear();            Integer pitHouseIndex = currentPitIndex < KalahaConstants.rightPitHouseId ?
To enable Spring Boot Actuator, you will need to add below dependency into your pom.xml file:<!-- Spring boot actuator to expose metrics endpoint --><dependency>  <groupId>org.springframework.boot</groupId>  <artifactId>spring-boot-starter-actuator</artifactId></dependency>You can observe those metrics by navigating to the below address if you are running your Spring boot application for instance in port 8080: http://localhost:8080/actuator.
But in our implementation, since we are using Consul and Httpd proxy, the URL would be as below:http://localhost/mancala-api/actuator{"_links":{"self":{"href":"http://localhost/actuator","templated":false},           "archaius":{"href":"http://localhost/actuator/archaius","templated":false},           "auditevents":{"href":"http://localhost/actuator/auditevents","templated":false},           "beans":{"href":"http://localhost/actuator/beans","templated":false},           "caches":{"href":"http://localhost/actuator/caches","templated":false},           "caches-cache":{"href":"http://localhost/actuator/caches/{cache}","templated":true},           "health":{"href":"http://localhost/actuator/health","templated":false},           "health-component":{"href":"http://localhost/actuator/health/{component}","templated":true},           "health-component-instance":{"href":"http://localhost/actuator/health/{component}/{instance}","templated":true},           "conditions":{"href":"http://localhost/actuator/conditions","templated":false},           "configprops":{"href":"http://localhost/actuator/configprops","templated":false},           "env-toMatch":{"href":"http://localhost/actuator/env/{toMatch}","templated":true},           "env":{"href":"http://localhost/actuator/env","templated":false},           "info":{"href":"http://localhost/actuator/info","templated":false},           "logfile":{"href":"http://localhost/actuator/logfile","templated":false},           "loggers-name":{"href":"http://localhost/actuator/loggers/{name}","templated":true},           "loggers":{"href":"http://localhost/actuator/loggers","templated":false},           "heapdump":{"href":"http://localhost/actuator/heapdump","templated":false},           "threaddump":{"href":"http://localhost/actuator/threaddump","templated":false},           "prometheus":{"href":"http://localhost/actuator/prometheus","templated":false},           "metrics-requiredMetricName":{"href":"http://localhost/actuator/metrics/{requiredMetricName}","templated":true},           "metrics":{"href":"http://localhost/actuator/metrics","templated":false},           "scheduledtasks":{"href":"http://localhost/actuator/scheduledtasks","templated":false},           "httptrace":{"href":"http://localhost/actuator/httptrace","templated":false},           "mappings":{"href":"http://localhost/actuator/mappings","templated":false},           "refresh":{"href":"http://localhost/actuator/refresh","templated":false},           "features":{"href":"http://localhost/actuator/features","templated":false},           "service-registry":{"href":"http://localhost/actuator/service-registry","templated":false},           "consul":{"href":"http://localhost/actuator/consul","templated":false}}}You can define which endpoints to be exposed in your application.properties file:management.endpoints.jmx.exposure.include=*management.endpoints.jmx.exposure.exclude=To see the details information about any of above endpoints, you will need to add the name of that metric to the end of the above URL.
With the rise of containers and ephemeral infrastructure, it is even more critical to have all the data available for the applications.
The monitoring as of today is usually done across three axes: metric monitoring, logs, and distributed tracingStop reverse engineering applications and start monitoring from the inside.
Tracing in CassandraApache Cassandra is fault tolerant and highly scalable database system (source).
In case of an error, say if the reply does not appear in time, a stock response is provided back to the user.
headers['x-death'][0].count < 3: true", f -> f.discardChannel("nullChannel"))                .handle("workHandler", "process")                .transform(Transformers.toJson())                .get();    }}The only wrinkle in this flow is the retry logic which discards the message after three retries.
It has been designed specifically for building serverless functions, Android applications, and low memory-footprint microservices.
However, today I don't want to focus on those characteristics of Micronaut.
The applications organization-service and department-service call endpoints exposed by other microservices using the Micronaut declarative HTTP client.
For example, micronaut-management needs to be included in case we want to expose some built-in management and monitoring endpoints.
When building a new application, it is also worth exposing basic information about it under the /info endpoint.
Exposing HTTP EndpointsMicronaut provides their own annotations for pointing out HTTP endpoints and methods.
We can expose it outside the application using an HTTP endpoint.
In comparison to Spring Boot, we don't have projects like Swagger SpringFox for Micronaut, so we need to copy the content to an online editor in order to see the graphical representation of our Swagger YAML.
The following Docker command will start single-node Consul instance and expose it on port 8500.
However, I have observed that using the Eureka discovery client together with the Consul config client causes some errors on startup.
During the tests of my application, I have observed that using distributed configurations together with Zipkin tracing results in problems in communication between microservices and Zipkin.
After making some test calls to GET methods exposed by organization-service and department-service, we may take a look on Zipkin web console, available under the address http://192.168.99.100:9411.
The following picture illustrates the timeline for HTTP method exposed by organization-serviceGET /organizations/{id}/with-departments-and-employees.
This method finds the organization in the in-memory repository, and then calls the HTTP method exposed by department-serviceGET /departments/organization/{organizationId}/with-employees.
The answer to the perplexing problem is Distributed Tracing.
The specification allows the vendors such as Jaeger and Zipkin to implement their unique distributed tracing functionality and enables the users to avoid vendor lock-in from a particular tracer implementation.
You can use it also to customize other features such as ignoring specific requests, adding tags to spans, and modifying the spans created when an error occurs.
Because tracing instrumentation has been broken until now.
In particular, tracing context must be passed through:Self-contained OSS services (e.g., NGINX, Cassandra, Redis, etc)OSS packages linked into custom services (e.g., grpc, ORMs, etc)Arbitrary application glue and business logic built around the aboveAnd there’s the rub:It is not reasonable to ask all OSS services and all OSS packages and all application-specific code to use a single tracing vendor; yet, if they don’t share a mechanism for trace description and propagation, the causal chain is broken and the traces are truncated, often severely.We need a single, standard mechanism to describe the behavior of our systems.
Every component of a distributed system can be instrumented in isolation, and the distributed application maintainer can choose (or switch, or multiplex) a downstream tracing technology with a configuration change.
It is difficult to monitor the microservices using Spring Boot Actuator Endpoint.
In such scenarios, it is difficult to manage and monitoring the microservices.
management.endpoints.web.exposure.include: determines which actuator endpoints needs to be exposed and * represents all actuator enpoints will be exposed.
Problem StatementIn the Distributed Application world, their many application participates for a given customer business process.
There are difficulties to trace the log details when any problem occurs for these distributed applications.
In the real production or Integration Environment, each of the module developers struggles to debug the problem since there is no Unique TraceId across the multiple Applications.
Also, developers might struggle to identify the Functional Context information since which is not shared across multiple Applications.
Let's start with our solutionPrerequisitesJava 1.8 or 11 or 13Any Java IDE (IntelliJ / Eclipse / ..)Install LogStash: https://www.elastic.co/guide/en/logstash/current/installing-logstash.htmlInstall Elastic Search: https://www.elastic.co/guide/en/elasticsearch/reference/current/install-elasticsearch.htmlInstall Kibana: https://www.elastic.co/guide/en/kibana/current/install.htmlCreate 3 Springboot Projects using https://start.spring.ioProject 1: CoreAppOneService     Java     x             11                         9                                                  1             src/main/resources/application.properties              2             server.port=8080              3             src/main/resources/bootstrap.yml              4             spring:              5             application:              6             name: coreapponeSrc/main/resources/logback-spring.xml     Java     xxxxxxxxxx             1                         80                                                  1             <?xml version="1.0" encoding="UTF-8"?>              2             <configuration>              3               <include resource="org/springframework/boot/logging/logback/defaults.xml"/>              4                             5               <springProperty scope="context" name="springAppName" source="spring.application.name"/>              6               <!-- Example for logging into the build folder of your project -->              7               <p>              8                           9               <!-- You can override this to have a custom pattern -->              10               <property name="CONSOLE_LOG_PATTERN"              11                 value="%clr(%d{yyyy-MM-dd HH:mm:ss.SSS}){faint} %clr(${LOG_LEVEL_PATTERN:-%5p}) %X{context.userId} %X{context.moduleId} %X{context.caseId} %clr(${PID:- }){magenta} %clr(---){faint} %clr([%15.15t]){faint} %clr(%-40.40logger{39}){cyan} %clr(:){faint} %m%n${LOG_EXCEPTION_CONVERSION_WORD:-%wEx}"/>              12                           13               <!-- Appender to log to console -->              14               <appender name="console" class="ch.qos.logback.core.ConsoleAppender">              15                 <filter class="ch.qos.logback.classic.filter.ThresholdFilter">              16                   <!-- Minimum logging level to be presented in the console logs-->              17                   <level>DEBUG</level>              18                 </filter>              19                 <encoder>              20                   <p>${CONSOLE_LOG_PATTERN}</pattern>              21                   <charset>utf8</charset>              22                 </encoder>              23               </appender>              24                           25               <!-- Appender to log to file -->              26               <appender name="flatfile" class="ch.qos.logback.core.rolling.RollingFileAppender">              27                 <file>${LOG_FILE}</file>              28                 <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">              29                   <fileNamePattern>${LOG_FILE}.%d{yyyy-MM-dd}.gz</fileNamePattern>              30                   <maxHistory>7</maxHistory>              31                 </rollingPolicy>              32                 <encoder>              33                   <p>${CONSOLE_LOG_PATTERN}</pattern>              34                   <charset>utf8</charset>              35                 </encoder>              36               </appender>              37                             38               <!-- Appender to log to file in a JSON format -->              39               <appender name="logstash" class="ch.qos.logback.core.rolling.RollingFileAppender">              40                 <file>${LOG_FILE}.json</file>              41                 <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">              42                   <fileNamePattern>${LOG_FILE}.json.%d{yyyy-MM-dd}.gz</fileNamePattern>              43                   <maxHistory>7</maxHistory>              44                 </rollingPolicy>              45                 <encoder class="net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder">              46                   <p>              47                     <timestamp>              48                       <timeZone>UTC</timeZone>              49                     </timestamp>              50                     <p>              51                       <p>              52                         {              53                         "severity": "%level",              54                         "service": "${springAppName:-}",              55                         "trace": "%X{X-B3-TraceId:-}",              56                         "span": "%X{X-B3-SpanId:-}",              57                         "parent": "%X{X-B3-ParentSpanId:-}",              58                         "exportable": "%X{X-Span-Export:-}",              59                         "pid": "${PID:-}",              60                         "thread": "%thread",              61                         "pvai.userId": "%X{context.userId:-}",              62                         "pvai.moduleId": "%X{context.moduleId:-}",              63                         "pvai.caseId": "%X{context.caseId:-}",              64                         "class": "%logger{40}",              65                         "rest": "%message"              66                         }              67                       </pattern>              68                     </pattern>              69                   </providers>              70                 </encoder>              71               </appender>              72                             73               <root level="INFO">              74                 <appender-ref ref="console"/>              75                 <!-- uncomment this to have also JSON logs -->              76                 <appender-ref ref="logstash"/>              77                 <appender-ref ref="flatfile"/>              78               </root>              79             </configuration>              80             src/main/java/com.dzone.sample.logging     Java     xxxxxxxxxx             1                         39                                                  1             package com.dzone.sample.logging;              2                           3             import java.io.IOException;              4             import javax.servlet.FilterChain;              5             import javax.servlet.ServletException;              6             import javax.servlet.ServletRequest;              7             import javax.servlet.ServletResponse;              8             import org.slf4j.Logger;              9             import org.slf4j.LoggerFactory;              10             import org.slf4j.MDC;              11             import org.springframework.stereotype.Component;              12             import org.springframework.web.filter.GenericFilterBean;              13                           14             @Component              15             public class CoreApp1MDCFilter extends GenericFilterBean {              16                           17               private static final Logger logger = LoggerFactory.getLogger(CoreApp1MDCFilter.class);              18                           19               @Override              20               public void destroy() {              21               }              22                           23               @Override              24               public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse,              25                   FilterChain filterChain) throws IOException, ServletException {              26                           27                 MDC.put("context.userId", "amarutha");              28                 MDC.put("context.moduleId", "CoreApp1");              29                 MDC.put("context.caseId", "Case001");              30                           31                 logger.trace("A TRACE Message");              32                 logger.debug("A DEBUG Message");              33                 logger.info("An INFO Message");              34                 logger.warn("A WARN Message");              35                 logger.error("An ERROR Message");              36                 filterChain.doFilter(servletRequest, servletResponse);              37               }              38             }              39             src/main/java/com.dzone.sample.logging     Java     xxxxxxxxxx             1                         57                                                  1             package com.dzone.sample.logging;              2                           3             import org.slf4j.Logger;              4             import org.slf4j.LoggerFactory;              5             import org.slf4j.MDC;              6             import org.springframework.boot.SpringApplication;              7             import org.springframework.boot.autoconfigure.SpringBootApplication;              8             import org.springframework.boot.devtools.remote.client.HttpHeaderInterceptor;              9             import org.springframework.context.annotation.Bean;              10             import org.springframework.http.MediaType;              11             import org.springframework.http.client.ClientHttpRequestInterceptor;              12             import org.springframework.util.CollectionUtils;              13             import org.springframework.web.bind.annotation.RequestMapping;              14             import org.springframework.web.bind.annotation.RestController;              15             import org.springframework.web.client.RestTemplate;              16                           17             import java.util.ArrayList;              18             import java.util.List;              19                           20                           21             @RestController              22             @SpringBootApplication              23             public class LoggingCoreApplicationOne {              24                           25               private static final Logger logger = LoggerFactory.getLogger(LoggingCoreApplicationOne.class);              26                           27               public static void main(String[] args) {              28                 SpringApplication.run(LoggingCoreApplicationOne.class, args);              29               }              30                           31               @Bean              32               public RestTemplate getRestTemplate() {              33                 return new RestTemplate();              34               }              35                           36               @RequestMapping("/coreapp1")              37               public String callCoreApp2Service() throws Exception {              38                 logger.trace("A TRACE Message");              39                 logger.debug("A DEBUG Message");              40                 logger.info("An INFO Message");              41                 logger.warn("A WARN Message");              42                 logger.error("An ERROR Message");              43                           44                 final RestTemplate restTemplate = getRestTemplate();              45                 List<ClientHttpRequestInterceptor> interceptors = restTemplate.getInterceptors();              46                 if (CollectionUtils.isEmpty(interceptors)) {              47                   interceptors = new ArrayList<>();              48                 }              49                           50                 interceptors.add(new HttpHeaderInterceptor("pvai.userId", MDC.get("pvai.userId")));              51                 interceptors.add(new HttpHeaderInterceptor("pvai.caseId", MDC.get("pvai.caseId")));              52                           53                 restTemplate.setInterceptors(interceptors);              54                 return "This is CoreAppOne Service " + restTemplate              55                     .getForObject("http://localhost:8081/coreapp2", String.class);              56               }              57             }pom.xml     Java     xxxxxxxxxx             1                         62                                                  1             <?xml version="1.0" encoding="UTF-8"?>              2             <project xmlns="http://maven.apache.org/POM/4.0.0"              3               xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"              4               xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd">              5               <modelVersion>4.0.0</modelVersion>              6               <p>              7                 <groupId>org.springframework.boot</groupId>              8                 <artifactId>spring-boot-starter-parent</artifactId>              9                 <version>2.2.3.RELEASE</version>              10                 <relativePath/> <!-- lookup parent from repository -->              11               </parent>              12               <groupId>com.dzone.sample.logging</groupId>              13               <artifactId>Logging_CoreAppOne</artifactId>              14               <version>0.0.1-SNAPSHOT</version>              15               <name>Logging_CoreAppOne</name>              16               <description>Demo project for Spring Boot</description>              17                           18               <p>              19                 <java.version>1.8</java.version>              20               </properties>              21                           22                           23               <dependencies>              24                 <dependency>              25                   <groupId>org.springframework.cloud</groupId>              26                   <artifactId>spring-cloud-starter-sleuth</artifactId>              27                   <version>2.2.1.RELEASE</version>              28                 </dependency>              29                 <dependency>              30                   <groupId>org.springframework.boot</groupId>              31                   <artifactId>spring-boot-starter-web</artifactId>              32                 </dependency>              33                 <dependency>              34                   <groupId>org.springframework.boot</groupId>              35                   <artifactId>spring-boot-starter-test</artifactId>              36                   <scope>test</scope>              37                 </dependency>              38                 <dependency>              39                   <groupId>net.logstash.logback</groupId>              40                   <artifactId>logstash-logback-encoder</artifactId>              41                   <version>6.3</version>              42                 </dependency>              43                 <dependency>              44                   <groupId>org.slf4j</groupId>              45                   <artifactId>log4j-over-slf4j</artifactId>              46                 </dependency>              47                 <dependency>              48                   <groupId>org.springframework.boot</groupId>              49                   <artifactId>spring-boot-devtools</artifactId>              50                 </dependency>              51               </dependencies>              52                           53               <build>              54                 <p>              55                   <p>              56                     <groupId>org.springframework.boot</groupId>              57                     <artifactId>spring-boot-maven-plugin</artifactId>              58                   </plugin>              59                 </plugins>              60               </build>              61                           62             </project>Project 2: CoreAppTwoService     Java     xxxxxxxxxx             1                         11                         9                                                  1             src/main/resources/application.properties              2             server.port=8081              3             src/main/resources/bootstrap.yml              4             spring:              5             application:              6             name: coreapptwosrc/main/resources/logback-spring.xml     Java     xxxxxxxxxx             1                         79                                                  1             <?xml version="1.0" encoding="UTF-8"?>              2             <configuration>              3               <include resource="org/springframework/boot/logging/logback/defaults.xml"/>              4                             5               <springProperty scope="context" name="springAppName" source="spring.application.name"/>              6               <!-- Example for logging into the build folder of your project -->              7               <p>              8                           9               <!-- You can override this to have a custom pattern -->              10               <property name="CONSOLE_LOG_PATTERN"              11                 value="%clr(%d{yyyy-MM-dd HH:mm:ss.SSS}){faint} %clr(${LOG_LEVEL_PATTERN:-%5p}) %X{context.userId} %X{contet.moduleId} %X{context.caseId} %clr(${PID:- }){magenta} %clr(---){faint} %clr([%15.15t]){faint} %clr(%-40.40logger{39}){cyan} %clr(:){faint} %m%n${LOG_EXCEPTION_CONVERSION_WORD:-%wEx}"/>              12                           13               <!-- Appender to log to console -->              14               <appender name="console" class="ch.qos.logback.core.ConsoleAppender">              15                 <filter class="ch.qos.logback.classic.filter.ThresholdFilter">              16                   <!-- Minimum logging level to be presented in the console logs-->              17                   <level>DEBUG</level>              18                 </filter>              19                 <encoder>              20                   <p>${CONSOLE_LOG_PATTERN}</pattern>              21                   <charset>utf8</charset>              22                 </encoder>              23               </appender>              24                           25               <!-- Appender to log to file -->              26               <appender name="flatfile" class="ch.qos.logback.core.rolling.RollingFileAppender">              27                 <file>${LOG_FILE}</file>              28                 <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">              29                   <fileNamePattern>${LOG_FILE}.%d{yyyy-MM-dd}.gz</fileNamePattern>              30                   <maxHistory>7</maxHistory>              31                 </rollingPolicy>              32                 <encoder>              33                   <p>${CONSOLE_LOG_PATTERN}</pattern>              34                   <charset>utf8</charset>              35                 </encoder>              36               </appender>              37                             38               <!-- Appender to log to file in a JSON format -->              39               <appender name="logstash" class="ch.qos.logback.core.rolling.RollingFileAppender">              40                 <file>${LOG_FILE}.json</file>              41                 <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">              42                   <fileNamePattern>${LOG_FILE}.json.%d{yyyy-MM-dd}.gz</fileNamePattern>              43                   <maxHistory>7</maxHistory>              44                 </rollingPolicy>              45                 <encoder class="net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder">              46                   <p>              47                     <timestamp>              48                       <timeZone>UTC</timeZone>              49                     </timestamp>              50                     <p>              51                       <p>              52                         {              53                         "severity": "%level",              54                         "service": "${springAppName:-}",              55                         "trace": "%X{X-B3-TraceId:-}",              56                         "span": "%X{X-B3-SpanId:-}",              57                         "parent": "%X{X-B3-ParentSpanId:-}",              58                         "exportable": "%X{X-Span-Export:-}",              59                         "pid": "${PID:-}",              60                         "thread": "%thread",              61                         "pvai.userId": "%X{context.userId:-}",              62                         "pvai.moduleId": "%X{context.moduleId:-}",              63                         "pvai.caseId": "%X{context.caseId:-}",              64                         "class": "%logger{40}",              65                         "rest": "%message"              66                         }              67                       </pattern>              68                     </pattern>              69                   </providers>              70                 </encoder>              71               </appender>              72                             73               <root level="INFO">              74                 <appender-ref ref="console"/>              75                 <!-- uncomment this to have also JSON logs -->              76                 <appender-ref ref="logstash"/>              77                 <appender-ref ref="flatfile"/>              78               </root>              79             </configuration>src/main/java/com.dzone.sample.logging     Java     xxxxxxxxxx             1                         40                                                  1             package com.dzone.sample.logging;              2                           3             import java.io.IOException;              4             import javax.servlet.FilterChain;              5             import javax.servlet.ServletException;              6             import javax.servlet.ServletRequest;              7             import javax.servlet.ServletResponse;              8             import javax.servlet.http.HttpServletRequest;              9             import org.slf4j.Logger;              10             import org.slf4j.LoggerFactory;              11             import org.slf4j.MDC;              12             import org.springframework.stereotype.Component;              13             import org.springframework.web.filter.GenericFilterBean;              14                           15             @Component              16             public class CoreAppTwoMDCFilter extends GenericFilterBean {              17                           18               private static final Logger logger = LoggerFactory.getLogger(CoreAppTwoMDCFilter.class);              19                           20               @Override              21               public void destroy() {              22               }              23                           24               @Override              25               public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse,              26                   FilterChain filterChain) throws IOException, ServletException {              27                           28                 MDC.put("context.userId", ((HttpServletRequest) servletRequest).getHeader("context.userId"));              29                 MDC.put("context.moduleId", "CoreAppTwoService");              30                 MDC.put("context.caseId", ((HttpServletRequest) servletRequest).getHeader("context.caseId"));              31                           32                 logger.trace("A TRACE Message");              33                 logger.debug("A DEBUG Message");              34                 logger.info("An INFO Message" );              35                 logger.warn("A WARN Message" );              36                 logger.error("An ERROR Message");              37                 filterChain.doFilter(servletRequest, servletResponse);              38               }              39             }              40             src/main/java/com.dzone.sample.logging     Java     xxxxxxxxxx             1                         57                                                  1             package com.dzone.sample.logging;              2                           3             import org.slf4j.Logger;              4             import org.slf4j.LoggerFactory;              5             import org.slf4j.MDC;              6             import org.springframework.boot.SpringApplication;              7             import org.springframework.boot.autoconfigure.SpringBootApplication;              8             import org.springframework.boot.devtools.remote.client.HttpHeaderInterceptor;              9             import org.springframework.context.annotation.Bean;              10             import org.springframework.http.MediaType;              11             import org.springframework.http.client.ClientHttpRequestInterceptor;              12             import org.springframework.util.CollectionUtils;              13             import org.springframework.web.bind.annotation.RequestMapping;              14             import org.springframework.web.bind.annotation.RestController;              15             import org.springframework.web.client.RestTemplate;              16                           17             import java.util.ArrayList;              18             import java.util.List;              19                           20             @RestController              21             @SpringBootApplication              22             public class LoggingCoreApplicationTwo {              23                           24               private static final Logger logger = LoggerFactory.getLogger(LoggingCoreApplicationTwo.class);              25                           26               public static void main(String[] args) {              27                 SpringApplication.run(LoggingApplicationTwo.class, args);              28               }              29                           30               @Bean              31               public RestTemplate getRestTemplate() {              32                 return new RestTemplate();              33               }              34                           35               @RequestMapping("/coreapp3")              36               public String callCoreAppThreeService() throws Exception {              37                 logger.trace("A TRACE Message");              38                 logger.debug("A DEBUG Message");              39                 logger.info("An INFO Message");              40                 logger.warn("A WARN Message");              41                 logger.error("An ERROR Message");              42                           43                 final RestTemplate restTemplate = getRestTemplate();              44                 List<ClientHttpRequestInterceptor> interceptors = restTemplate.getInterceptors();              45                 if (CollectionUtils.isEmpty(interceptors)) {              46                   interceptors = new ArrayList<>();              47                 }              48                           49                 interceptors.add(new HttpHeaderInterceptor("context.userId", MDC.get("context.userId")));              50                 interceptors.add(new HttpHeaderInterceptor("context.caseId", MDC.get("context.caseId")));              51                           52                 restTemplate.setInterceptors(interceptors);              53                 return "This is CoreAppTwo Service " + restTemplate              54                     .getForObject("http://localhost:8082/coreapp3", String.class);              55               }              56             }              57             Project 3: CoreAppThreeService     Java     xxxxxxxxxx             1                         11                         9                                                  1             src/main/resources/application.properties              2             server.port=8082              3             src/main/resources/bootstrap.yml              4             spring:              5             application:              6             name: coreappthreesrc/main/resources/logback-spring.xml     Java     x             80                                                  1             <?xml version="1.0" encoding="UTF-8"?>              2             <configuration>              3               <include resource="org/springframework/boot/logging/logback/defaults.xml"/>              4                             5               <springProperty scope="context" name="springAppName" source="spring.application.name"/>              6               <!-- Example for logging into the build folder of your project -->              7               <p>              8                           9               <!-- You can override this to have a custom pattern -->              10               <property name="CONSOLE_LOG_PATTERN"              11                 value="%clr(%d{yyyy-MM-dd HH:mm:ss.SSS}){faint} %clr(${LOG_LEVEL_PATTERN:-%5p}) %X{context.userId} %X{context.moduleId} %X{context.caseId} %clr(${PID:- }){magenta} %clr(---){faint} %clr([%15.15t]){faint} %clr(%-40.40logger{39}){cyan} %clr(:){faint} %m%n${LOG_EXCEPTION_CONVERSION_WORD:-%wEx}"/>              12                           13               <!-- Appender to log to console -->              14               <appender name="console" class="ch.qos.logback.core.ConsoleAppender">              15                 <filter class="ch.qos.logback.classic.filter.ThresholdFilter">              16                   <!-- Minimum logging level to be presented in the console logs-->              17                   <level>DEBUG</level>              18                 </filter>              19                 <encoder>              20                   <p>${CONSOLE_LOG_PATTERN}</pattern>              21                   <charset>utf8</charset>              22                 </encoder>              23               </appender>              24                           25               <!-- Appender to log to file -->              26               <appender name="flatfile" class="ch.qos.logback.core.rolling.RollingFileAppender">              27                 <file>${LOG_FILE}</file>              28                 <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">              29                   <fileNamePattern>${LOG_FILE}.%d{yyyy-MM-dd}.gz</fileNamePattern>              30                   <maxHistory>7</maxHistory>              31                 </rollingPolicy>              32                 <encoder>              33                   <p>${CONSOLE_LOG_PATTERN}</pattern>              34                   <charset>utf8</charset>              35                 </encoder>              36               </appender>              37                             38               <!-- Appender to log to file in a JSON format -->              39               <appender name="logstash" class="ch.qos.logback.core.rolling.RollingFileAppender">              40                 <file>${LOG_FILE}.json</file>              41                 <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">              42                   <fileNamePattern>${LOG_FILE}.json.%d{yyyy-MM-dd}.gz</fileNamePattern>              43                   <maxHistory>7</maxHistory>              44                 </rollingPolicy>              45                 <encoder class="net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder">              46                   <p>              47                     <timestamp>              48                       <timeZone>UTC</timeZone>              49                     </timestamp>              50                     <p>              51                       <p>              52                         {              53                         "severity": "%level",              54                         "service": "${springAppName:-}",              55                         "trace": "%X{X-B3-TraceId:-}",              56                         "span": "%X{X-B3-SpanId:-}",              57                         "parent": "%X{X-B3-ParentSpanId:-}",              58                         "exportable": "%X{X-Span-Export:-}",              59                         "pid": "${PID:-}",              60                         "thread": "%thread",              61                         "pvai.userId": "%X{context.userId:-}",              62                         "pvai.moduleId": "%X{context.moduleId:-}",              63                         "pvai.caseId": "%X{context.caseId:-}",              64                         "class": "%logger{40}",              65                         "rest": "%message"              66                         }              67                       </pattern>              68                     </pattern>              69                   </providers>              70                 </encoder>              71               </appender>              72                             73               <root level="INFO">              74                 <appender-ref ref="console"/>              75                 <!-- uncomment this to have also JSON logs -->              76                 <appender-ref ref="logstash"/>              77                 <appender-ref ref="flatfile"/>              78               </root>              79             </configuration>              80             src/main/java/com.dzone.sample.logging     Java     xxxxxxxxxx             1                         39                                                  1             package com.dzone.sample.logging;              2                           3             import java.io.IOException;              4             import javax.servlet.FilterChain;              5             import javax.servlet.ServletException;              6             import javax.servlet.ServletRequest;              7             import javax.servlet.ServletResponse;              8             import javax.servlet.http.HttpServletRequest;              9             import org.slf4j.Logger;              10             import org.slf4j.LoggerFactory;              11             import org.slf4j.MDC;              12             import org.springframework.stereotype.Component;              13             import org.springframework.web.filter.GenericFilterBean;              14                           15             @Component              16             public class CoreAppThreeMDCFilter extends GenericFilterBean {              17                           18               private static final Logger logger = LoggerFactory.getLogger(CoreAppThreeMDCFilter.class);              19                           20               @Override              21               public void destroy() {              22               }              23                           24               @Override              25               public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse,              26                   FilterChain filterChain) throws IOException, ServletException {              27                           28                 MDC.put("context.userId", ((HttpServletRequest) servletRequest).getHeader("context.userId"));              29                 MDC.put("context.moduleId", "CoreAppThreeService");              30                 MDC.put("context.caseId", ((HttpServletRequest) servletRequest).getHeader("context.caseId"));              31                           32                 logger.trace("A TRACE Message");              33                 logger.debug("A DEBUG Message");              34                 logger.info("An INFO Message");              35                 logger.warn("A WARN Message");              36                 logger.error("An ERROR Message");              37                 filterChain.doFilter(servletRequest, servletResponse);              38               }              39             }src/main/java/com.dzone.sample.logging     Java     xxxxxxxxxx             1                         36                                                  1             package com.dzonea.sample.logging;              2                           3             import org.slf4j.Logger;              4             import org.slf4j.LoggerFactory;              5             import org.springframework.boot.SpringApplication;              6             import org.springframework.boot.autoconfigure.SpringBootApplication;              7             import org.springframework.context.annotation.Bean;              8             import org.springframework.web.bind.annotation.RequestMapping;              9             import org.springframework.web.bind.annotation.RestController;              10             import org.springframework.web.client.RestTemplate;              11                           12             @RestController              13             @SpringBootApplication              14             public class LoggingCoreApplicationThree {              15                           16               private static final Logger logger = LoggerFactory.getLogger(LoggingCoreApplicationThree.class);              17                           18               public static void main(String[] args) {              19                 SpringApplication.run(LoggingCoreApplicationThree.class, args);              20               }              21                           22               @Bean              23               public RestTemplate getRestTemplate() {              24                 return new RestTemplate();              25               }              26                           27               @RequestMapping("/coreapp3")              28               public String rageService() throws Exception {              29                 logger.trace("A TRACE Message");              30                 logger.debug("A DEBUG Message");              31                 logger.info("An INFO Message");              32                 logger.warn("A WARN Message");              33                 logger.error("An ERROR Message");              34                 return "This is CoreAppThree Service ";              35               }              36             }Configure LogStash PipeLine/usr/local/logstash-7.5.1/config/logstash.conf     Java     xxxxxxxxxx             1                         28                                                  1             input {              2               file {              3                 path =>  [ "/Users/703258481mac/Marutha/Genpact/PVAI/IntelliJWorkspace/build/*.json" ]              4                 codec =>   json {              5                   charset => "UTF-8"              6                 }              7               }              8             }              9                           10             filter {              11               if [message] =~ "timestamp" {              12                 grok {              13                   match => ["message", "^(timestamp)"]              14                   add_tag => ["stacktrace"]              15                 }              16               }              17             }               18                           19             output {              20               stdout {              21                 codec => rubydebug              22               }              23                            24               elasticsearch {              25                 hosts => ["localhost:9200"]              26               }              27             }              28             Start LogStash:/usr/local/logstash-7.5.1/bin./logstash -f ../config/logstash.confSample Logstash Pipeline execution logStart Kibana/usr/local/var/homebrew/linked/kibana-full/bin./kibanaStart ElasticSearch/usr/local/var/homebrew/linked/elasticsearch-full/bin./elasticsearchOnce all the Servers are started, then hit the CoreAppOne Service in Browser.
For that functionality, there is no need to search for any external tool — it can be provided using Camel Rest DSL.
Service registration with Camel is not as easy, as we could expect.
There are no mechanisms out-of-the-box for service registration; there is only a component for a searching service.
Fortunately, Consul exposes REST APIs that meet those needs.
Each service is run with the -Dport VM argument, which is injected into the Spring Boot application.
When you are launching account and customer service, you need to set the -Dport VM argument.
Of course, I'm also able to find some disadvantages — like inaccuracies or errors in documentation, only short description of some new components in the developer guide, and no registration process in the discovery server like Consul.
In microservices architecture, there are a lot of small services and each service gas its own configuration, making it very difficult to manage the configuration for each service.
Spring Cloud Config Server solves the problem of managing the configuration by storing it on a centralized repository and that repository can be Git or SVN.
package com.example.config.limitsservice;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.cloud.context.config.annotation.RefreshScope;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RestController;import com.in28minutes.microservices.limitsservice.bean.LimitConfiguration;@RestController@RefreshScopepublic class LimitsConfigurationController {@Autowiredprivate Configuration configuration;@GetMapping(value="/limits")public LimitConfiguration retrieveLimitsFromConfiguration(){return new LimitConfiguration(configuration.getMaximum(),configuration.getMinimum());}} @RefreshScope  will expose one endpoint  /refresh .
Zookeeper is available under the name zookeeper and is exposed on port 2181.
Building Microservice Order-ServiceThe application order-service as the only one that starts an embedded HTTP server and exposes REST API.
If no it cancels the trip, otherwise, it does not do anything.
There is no sense to include it to Driver class just to assign it to the right trip on the listener side.
Indexing unstructured log messages is not very useful.
Unfortunately, the hard part comes in writing the matching pattern itself, and those hints don’t help.
But it seems to suffer from performance issues, especially if the pattern doesn’t match.
The Span is the primary building block of distributed tracing.
):Img.5.
As the tracing provides you the ability to get a view into the application communication layer and recognize potential issues, when the JFR is attached to the microservice JVMs, you can directly analyze the potentially suspicious code.
):Img.6.
Spring Cloud and Kubernetes may be threatened as competitive solutions when you build a microservices environment.
You should start it and expose the embedded Docker client provided by both of them.
In fact, it does not matter if you choose Kubernetes or Openshift — the next part of this tutorial will be applicable for both of them.
FROM openjdk:8-jre-alpineENV APP_FILE employee-service-1.0-SNAPSHOT.jarENV APP_HOME /usr/appsEXPOSE 8080COPY target/$APP_FILE $APP_HOME/WORKDIR $APP_HOMEENTRYPOINT ["sh", "-c"]CMD ["exec java -jar $APP_FILE"]Let's build Docker images for all three sample microservices.
Let's take a look at other microservices that call the API exposed by employee-service and communicate between each other ( organization-service calls department-service API).
@SpringBootApplication@EnableDiscoveryClient@EnableFeignClients@EnableMongoRepositories@EnableSwagger2public class DepartmentApplication { public static void main(String[] args) {  SpringApplication.run(DepartmentApplication.class, args); } // ...}Here's the implementation of Feign client for calling the method exposed by employee-service.
Well, here things are getting complicated... We can run a container with Swagger UI, and map all paths exposed by the ingress manually, but it is not a good solution...
<dependency><groupId>org.springframework.cloud</groupId><artifactId>spring-cloud-starter-netflix-zuul</artifactId></dependency><dependency><groupId>org.springframework.cloud</groupId><artifactId>spring-cloud-starter-kubernetes</artifactId><version>0.3.0.BUILD-SNAPSHOT</version></dependency><dependency><groupId>org.springframework.cloud</groupId><artifactId>spring-cloud-starter-netflix-ribbon</artifactId></dependency><dependency><groupId>org.springframework.cloud</groupId><artifactId>spring-cloud-starter-kubernetes-ribbon</artifactId><version>0.3.0.BUILD-SNAPSHOT</version></dependency><dependency><groupId>io.springfox</groupId><artifactId>springfox-swagger-ui</artifactId><version>2.9.2</version></dependency><dependency><groupId>io.springfox</groupId><artifactId>springfox-swagger2</artifactId><version>2.9.2</version></dependency>Kubernetes discovery client will detect all services exposed on the cluster.
Kubernetes's popularity as a platform has been rapidly growing over the last few months, but it still has some weaknesses.
Istio provides mechanisms for traffic management like request routing, discovery, load balancing, handling failures, and fault injection.
Both of them expose an endpoint ping which prints the application's name and version.
This service will be exposed inside the Minikube node under port 8091.
FROM openjdk:8-jre-alpineENV APP_FILE caller-service-1.0.0-SNAPSHOT.jarENV APP_HOME /usr/appEXPOSE 8090COPY target/$APP_FILE $APP_HOME/WORKDIR $APP_HOMEENTRYPOINT ["sh", "-c"]CMD ["exec java -jar $APP_FILE"]A similar Dockerfile is available for callme-service.
Also, because Istio Ingress is not supported on Minikube, we will just use Kubernetes Service.
If we need to expose it outside the Minikube cluster, we should set the type to NodePort.
I'm going to show you the following rules:Split traffic between different service versionsInjecting the delay in the request pathInjecting HTTP error as a response from the serviceHere's sample route rule definition for callme-service.
It also adds a 3-second delay in 10% of the requests,= and returns an HTTP 500 error code for 10% of the requests.
Pragmatic Tracing for Distributed SystemsIntroduction  A distributed system is one in which the failure of a computer you didn't even know existed can render your own computer unusable.
Just a single neuron acting as a proto-eye doesn’t cut it, but a full-fledged eye does.
Therefore, your new systems cannot be monitored as they were during a lower complexity era.
Usually, log event levels are modeled after syslog levels, DEBUG, INFO, WARNING, ERROR, and CRITICAL being used the most in software.
only ERROR level or higher.
In practical terms, this means you’re writing log level INFO or greater to stderr, but to the file, you’re dumping all logs.
Usually, alarms are tied to some metric; that is, whenever the specified metric is outside given bound perform an action, it will auto-heal or notify the operator.
Another human metric would be pain levels in your left leg.
Usually, it’s near 0, but over a certain threshold, you’re vividly aware of it – that is, an alarm is raised.
For computer systems, the usual metrics are related to throughput, error rate, latency, and resource (CPU/GPU/network) utilization.
X times per second, it stops your program and checks which line is being executed on the machine.
Despite them being parallel (though they’re not necessarily), you shall hit a tail latency problem.
The biggest downside is working with the visualizer.
As a newcomer, I had a hard time finding how can I filter datapoint in categories, by name, and overall advance use cases beyond the basic scroll and see.
Distributed TracingAll’s fine and dandy on a single node, but the trouble starts with distributed systems.
The problem is how to connect/correlate traces coming from multiple nodes.
Here’s a screenshot from Jaeger frontend for searching and looking at your traces: Since writing against a specific backend could be a hard vendor lock-in, there have emerged two client vendor-neutral APIs – OpenTracing and OpenCensus.
OpenCensus ExampleThis subsection will describe basic OpenCensus building blocks.
error, http.statuscode, …).
func HandleXXX (w http.ResponseWriter, req *http.Request) {    ctx := req.Context() // ...}This chapter could be summarized as an OpenCensus crash course/cheat sheet/in 5 minutes.
See this post on a Linux (Xenial HWE) vulnerability that affects your Cloud Foundry platform installations.
@Scheduled is no longer used for Consul watches.
If you run into any issues with docker, you can start fresh with: docker container ls -a | cut -c1-12 | xargs docker container rm --force docker images | cut -c69-80 | xargs docker rmi docker system prune -aNOTE: This will destroy all docker containers, images, and networks, so use at your own risk.
If the database is clustered: no.
When the generator has almost finished, a warning shows in the output:WARNING!
You can apply MicroProfile Fault Tolerance annotations to your methods then collect metrics from the MicroProfile Metrics /metrics endpoint to monitor exactly what happens if your application fails.
Monitor Faults in Your Microservices (MicroProfile Fault Tolerance 1.1)All applications need to deal with unexpected problems, whether it’s an important service being unavailable or a sudden influx of requests that could overload your application.
Here’s a quick overview:If you use @Retry, metrics are added for how often your method is failing and being retriedIf you use @Timeout, metrics are added for how long calls to your method are taking and how often they’re timing outIf you use @CircuitBreaker, metrics are added to track what state the CircuitBreaker is in, how often your method fails and how often an open circuit breaker causes your method to fail without being runIf you use @Bulkhead, metrics are added for how many concurrent calls to your method are currently executing, how often calls are rejected, how long calls take to return and how long they spend queued (if you’re also using @Asynchronous)If you use @Fallback, metrics are added for how often your method falls back to its fallback handler or fallback methodIf you use any of the annotations listed in the previous points, we add metrics for how often your method ultimately returns an exception to the caller, after all of the fault tolerance processing has finished.
In general, the idea is that if you’re worried about your method failing in a certain way and are using fault tolerance to try to mitigate that, metrics are added so that you can easily monitor whether or not your application is actually failing there.
To enable MicroProfile Fault Tolerance 1.1 and Metrics 1.1, add the definitions to your server.xml:<featureManager>    <feature>mpFaultTolerance-1.1</feature>    <feature>mpMetrics-1.1</feature></featureManager>Also new in Fault Tolerance 1.1 is the ability to disable individual Fault Tolerance annotations using the MicroProfile Config feature.
For example, if you have a method MyClient.methodA() which is annotated with @CircuitBreaker, you can disable the circuit breaker by adding this configuration entry:com.acme.test.MyClient/methodA/CircuitBreaker/enabled=falseHere's an example use of fault tolerance annotations:@ApplicationScopedpublic class Example {  @Retry  @CircuitBreaker  public Result getResult(String userId) {    List<Widgets> widgets = fetchWidgets(userId);    return new Result(userId, widgets);  }}Learn about fault tolerance in microservices:Failing fast and recovering from errors (interactive guide)Limiting the number of concurrent requests to microservices (interactive guide)Building fault-tolerant microservices with the @Fallback annotationPreventing repeated failed calls to microservicesGet Liberty Component Metrics From /metrics Endpoint (Monitor 1.0 With MP Metrics 1.1)You can now access metrics from individual Liberty components (e.g.
No more!
You no longer need to specify the @Dependentannotation on REST client interfaces when used with CDI.
You can also use URIs, which can be constructed without throwing a MalformedURLException, saving a few lines of unnecessary try/catch blocks.
For example, add to the microprofile-config.properties file one of the following entries:mp.jwt.verify.publickey.location=/META-INF/orange.pemormp.jwt.verify.publickey=(pkcs#8 key goes here)mp.jwt.verify.issuer=https://server.example.comFor UNIX and Linux platforms (which do not accept dots in environment variables), use _.
In most cases, developers just worry about instantiating a tracer and letting the instrumentation libraries capture interesting spans.
The code repository with this example is available on GitLab, but here’s the most relevant part:package com.example.demo;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import io.jaegertracing.Configuration;import io.opentracing.Scope;import io.opentracing.Tracer;import io.vertx.core.AbstractVerticle;import io.vertx.core.Vertx;public class MainVerticle extends AbstractVerticle {    private static final Logger logger = LoggerFactory.getLogger(MainVerticle.class);    public static void main(String[] args) {        Vertx.vertx().deployVerticle(new MainVerticle());    }    @Override    public void start() throws Exception {        String serviceName = System.getenv("JAEGER_SERVICE_NAME");        if (null == serviceName || serviceName.isEmpty()) {            serviceName = "vertx-create-span";        }        System.setProperty("JAEGER_SERVICE_NAME", serviceName);        Tracer tracer = Configuration.fromEnv().getTracer();        vertx.createHttpServer().requestHandler(req -> {            try (Scope ignored = tracer.buildSpan("operation").startActive(true)) {                logger.debug("Request received");                req.response().putHeader("content-type", "text/plain").end("Hello from Vert.x!
Needless to say, no more work happens in the “main” thread for this span.
It’s worth noting that, from this point and on, we stop referring to our span as “span”: for almost all cases after this, it is treated as a fully fledged trace, that happens to be composed by a single span.
These days, it is commonplace to use MyS QL, Redis as a key-value store, MongoDB, Postgress, and InfluxDB — and that is all just for the database — let alone the multiple services that make up other parts of the application.
We need to make sure that the integration with Redis, MongoDB or a microservice works as expected, not just that the mock works as we wrote it.
In this article, I will show you how to use a testcontainer to write integration tests in Go with very low overhead.
it is not a mandatory command, but testcontainers-go uses it to remove the containers at some point.
Log4j, Logj42, java.util.logging, Commons Logging, Logback, and slf4j, the choice can be confusing.
src/main/resurces/logback-spring.xml<?xml version="1.0" encoding="UTF-8"?><configuration>    <include resource="org/springframework/boot/logging/logback/defaults.xml"/>    <springProperty scope="context" name="springAppName" source="spring.application.name"/>    <!-- Example for logging into a 'logs' folder -->    <property name="LOG_FILE" value="${LOG_FILE:-${LOG_PATH:-${LOG_TEMP:-${java.io.tmpdir:-/tmp}}}}/${springAppName}.log"/>    <!-- You can override this to have a custom pattern -->    <property name="CONSOLE_LOG_PATTERN"              value="%clr(%d{yyyy-MM-dd HH:mm:ss.SSS}){faint} %clr(${LOG_LEVEL_PATTERN:-%5p}) %clr(${PID:- }){magenta} %clr(---){faint} %clr([%15.15t]){faint} %clr(%-40.40logger{39}){cyan} %clr(:){faint} %m%n${LOG_EXCEPTION_CONVERSION_WORD:-%wEx}"/>    <!-- Appender to log to console -->    <appender name="console" class="ch.qos.logback.core.ConsoleAppender">        <filter class="ch.qos.logback.classic.filter.ThresholdFilter">            <!-- Minimum logging level to be presented in the console logs-->            <level>DEBUG</level>        </filter>        <encoder>            <pattern>${CONSOLE_LOG_PATTERN}</pattern>            <charset>utf8</charset>        </encoder>    </appender>    <!-- Appender to log to file -->    <appender name="flatfile" class="ch.qos.logback.core.rolling.RollingFileAppender">        <file>${LOG_FILE}</file>        <!-- Daily rollovers, keep a maximum of 90 log files -->        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">            <fileNamePattern>${LOG_FILE}.%d{yyyy-MM-dd}.gz</fileNamePattern>            <maxHistory>90</maxHistory>        </rollingPolicy>        <encoder>            <pattern>${CONSOLE_LOG_PATTERN}</pattern>            <charset>utf8</charset>        </encoder>    </appender>    <!-- Appender to log to file in a JSON format -->    <appender name="logstash" class="ch.qos.logback.core.rolling.RollingFileAppender">        <file>${LOG_FILE}.json</file>        <!-- Daily rollovers, keep a maximum of 90 log files -->        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">            <fileNamePattern>${LOG_FILE}.json.%d{yyyy-MM-dd}.gz</fileNamePattern>            <maxHistory>90</maxHistory>        </rollingPolicy>        <encoder class="net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder">            <providers>                <timestamp>                    <timeZone>UTC</timeZone>                </timestamp>                <pattern>                    <pattern>                        {                        "severity": "%level",                        "service": "${springAppName:-}",                        "trace": "%X{X-B3-TraceId:-}",                        "span": "%X{X-B3-SpanId:-}",                        "parent": "%X{X-B3-ParentSpanId:-}",                        "exportable": "%X{X-Span-Export:-}",                        "pid": "${PID:-}",                        "thread": "%thread",                        "class": "%logger{40}",                        "rest": "%message"                        }                    </pattern>                </pattern>            </providers>        </encoder>    </appender>    <root level="INFO">        <appender-ref ref="console"/>        <appender-ref ref="flatfile"/>        <!-- uncomment this to have also JSON logs -->        <!--<appender-ref ref="logstash"/>-->    </root></configuration>TracingAn often overlooked concern is how to log distributed calls across multiple services.
If not addressed early, it can lead to a difficult manual effort of correlating logs based on timestamps to recreate an entire call flow.
To start instrumenting your logs, add the following dependencies to build.gradle:// Enable distributed tracing with Sleuthcompile group: 'org.springframework.cloud', name: 'spring-cloud-starter-zipkin'// Enable JSON loggingruntime( group: 'net.logstash.logback', name: 'logstash-logback-encoder', version: '4.11' ) {    exclude group: 'ch.qos.logback', module: 'logback-core'}Since we're using slf4j we don't need to do anything further to get trace information to show up in our logs.
In order to expose actuator metrics to JMX, you need to register a specific bean.
akka {    actor {        provider = "cluster"    }    remote {        log-remote-lifecycle-events = off        netty.tcp {            hostname = "127.0.0.1"            port = 0        }    }    cluster {        seed-nodes = [            "akka.tcp://ClusterSystem@127.0.0.1:2551",            "akka.tcp://ClusterSystem@127.0.0.1:2552"]        # auto downing is NOT safe for production deployments.
akka {    actor {        provider = "cluster"    }    remote {        log-remote-lifecycle-events = off        netty.tcp {            hostname = "127.0.0.1"            port = 0        }    }    cluster {        seed-nodes = [            "akka.tcp://ClusterSystem@127.0.0.1:2551",            "akka.tcp://ClusterSystem@127.0.0.1:2552"]        # auto downing is NOT safe for production deployments.
Here, we cut to the chase and just run the two Docker instances.
No code changes.
No annotations.
It started with Jonah Kowall, a former Gartner APM analyst, and today VP of market development at AppDynamics, writing about Misunderstanding "Open Tracing" for the Enterprise.
His criticism is that OpenTracing is not a broadly adopted open standard and explains that Enterprises have highly heterogeneous environments that require automated, agent-based tracing.
Alois Reitbauer, Chief Technology Strategist at dynaTrace, reacts in his blog A CTO's strategy towards OpenTracing that OpenTracing is not a standard and agrees with Jonah about the need for standardization of TraceContext.
In microservices architectures, understanding dynamic dependencies using topology and graph analysis is more difficult than in traditional architecture.
Instana's AI leverages our unique Dynamic Graph which provides the context to pinpoint to the root cause of problems and ultimately arrive at accurate causation.
We predict that there will be no ultimate reason to drive to a single tracing standard.
However, every architecture comes with various pain areas and microservices architecture is no different.
So maintain these configurations outside the application, central location is required where configuration can be managed and if any changes in configuration then there is no change in the code.
The problem is how do clients of service know about the available instance of services which will be different for each environment.
Distributed tracing, aka request tracing, meanwhile follows operations inside and over a range of systems to pinpoint where failures occur and what is behind poor performance.
It could lead to resource exhaustion and make calling service unable to handle other request.
So prevent network or service failure cascading to other service, circuit breaker comes into the picture.
Fault tolerance.
Each load balancer is part of an ensemble of components that work together to contact a remote server on demand, and the ensemble has a name that we give it as an application developer.
We have defined a custom Event class called SowEvent representing a single sow event while user clicks on specific pit index:/*    This event is fired when user clicks on any pit to sow the game.
We can enhance this feature by adding Micrometer API to our Spring boot application and even further enabling our Spring boot application to send those collected metrics to Prometheus server by adding below dependencies to pom.xml:<!-- Spring boot actuator to expose metrics endpoint --><dependency>  <groupId>org.springframework.boot</groupId>  <artifactId>spring-boot-starter-actuator</artifactId></dependency><!-- Micrometer core  --><dependency>  <groupId>io.micrometer</groupId>  <artifactId>micrometer-core</artifactId></dependency><!-- Micrometer Prometheus registry  --><dependency>  <groupId>io.micrometer</groupId>  <artifactId>micrometer-registry-prometheus</artifactId></dependency>10 — ELK-StackThe same as our 'mancala-api' project, in order to add Elasticsearch integration for our spring boot application, we need to add below dependencies into pom.xml file:<!-- Dependencies for LogStash --><dependency>  <groupId>ch.qos.logback</groupId>  <artifactId>logback-core</artifactId>  <version>1.2.3</version></dependency><dependency>  <groupId>net.logstash.logback</groupId>  <artifactId>logstash-logback-encoder</artifactId>  <version>6.1</version></dependency>We also need to configure the spring logger to store all logs into specific folder "/logs" through our logback-spring.xml file and then we configure the Filebeat module to ingest logs from that folder and send it to Elasticsearch server where we can search and query about the details through its web interface.
11 — Integration TestsTo implement a complete set of integration tests for our two microservices, I have used Wiremock to provide comprehensive testing by mocking our 'mancala-api' API through Wiremock service virtualization facility.
If we forget to register a specific tracer instance, then the tracing feature would use NoopTracer.
No changes to the application or business logic!
If you missed Part 1, you can check it out here.
Both of them expose an endpoint ping which lists the application’s name and version.
This output will be exposed inside the Minikube node under Port 8091.
FROM openjdk:8-jre-alpineENV APP_FILE caller-service-1.0.0-SNAPSHOT.jarENV APP_HOME /usr/appEXPOSE 8090COPY target/$APP_FILE $APP_HOME/WORKDIR $APP_HOMEENTRYPOINT [“sh”, “-c”]CMD [“exec java -jar $APP_FILE”]The callme-service has a similar Docker file.
The following code splits traffic in proportions 20:80 between the versions while also adding a 5-second delay in 10% of the requests, and returns an HTTP 500 error code for 10% of the requests.
How to Photograph a Black Hole — Observing Microservices With OpenTelemetryMicroservices architecture is a software development architectural style, whereby a complex business problem is solved by a suite of small services.
Event Horizon Telescope releases first-ever black hole imageYou don’t have to be a unicorn, like Amazon or Netflix, in order for your microservices architecture to turn into the death star or a murder mystery for every outage.
Metcalfe’s Law is related to the fact that the number of unique possible connections in a network of n nodes can be expressed mathematically as the n(n-1)/2:Metcalfe’s LawAs the chart above depicts, as the number of nodes increases, the explosion of interconnections occurs quickly and any service could fail or behave unexpectedly.
You must operate with the mantra that all systems eventually fail.
A single microservice may fail, leading one to the false conclusion that this microservice is the culprit for an outage; but in fact, it is likely due to one of its dependencies which could be another microservice(s), file system, database, cloud service, etc.
Every interaction between a service and its dependencies is a potential cause for failure, compounding the points of failure in the architecture even further beyond the graph of microservice nodes.
OpenTelemetryGiven all the potential failure locations how do you separate the signal amid the noise so that you can do preventive/proactive maintenance of the system?
Distributed tracing helps pinpoint where failures occur and what causes poor performance.
It took half a ton of hard drives to store the Black Hole image which adds up to five petabytes.
Ops would say that a microservice architecture without distributed tracing is “embellished dark source” as they’ve to cope with the death star.
this functionality works generically for microservices, no matter in which language they have been implemented and independent from the application logic.
unfortunately, that image doesn't seem to support microprofile 2.2 yet (at least i haven't found it).
additionally, i have created another variation of the   docker image  so that my sample application can be installed even by people who don't have java and maven installed locally (or who have wrong java/maven versions).
One of the problems with this example is that you have to configure the OIDC properties in each application.
This can be a real pain if you have hundreds of microservices.
However, if you have different microservices stacks using different OIDC client IDs, this approach will be difficult.
The most common way to install JHipster is using npm:npm install -g generator-jhipster@6.0.1You can run the command above without the version number to get the latest version of JHipster.
jhipster import-jdl apps.jhCreate Docker Images for Microservice AppsWhen the configuration is generated for Docker Compose, a warning is spat out to the console.
WARNING!
Configure JHipster Microservices to Use Okta for IdentityOne of the problems you saw in the bare-bones Spring Boot + Spring Cloud setup is you have to configure okta.oauth2.
You don't always want to do that.
This helps in re-processing a message that initially threw an error.
Asynchronous communication solves the problem by causing services to enqueue requests into a message queue.
Azure FunctionService Deployment PlatformDocker Swarm + KubernetesElastic BeanstackCross Cutting MS ChasisSpring Boot and Spring CloudExternal ConfigurationsSpring Cloud Config ServerKubernetes Config ServerAPIAPI GatewaySpring Cloud API GatewayMule Soft API GatewayAWS GatewayI won't explain each and every component and software mentioned above.
Following things can be done to resolve these concerns  Prepare Test suite that test services in the isolation  Deploy all micro-services and perform end to end testing with all live servicesMock other micro-services during unit/integration testingWrite the Stubs Above mentioned solution has its own advantages and disadvantages and should be               considered while applying one of them.
Spans also have other data, such as descriptions, timestamped events, key-value annotations (tags), the ID of the span that caused them, and process ID’s (normally IP address).Spans are started and stopped, and they keep track of their timing information.
Once you create a span, you must stop it at some point in the future.Trace: A set of spans forming a tree-like structure.
Some of the core annotations used to define the start and stop of a request are:cs - Client Sent - The client has made a request.
As we write in the documentation:The name should be low cardinality (e.g.
not include identifiers).Finding the name for the span is not that big of a problem from a library’s perspective.
Microservices With Observability on KubernetesAre you looking to implement observable microservices but clueless as to how to make them work with Kubernetes?
Observability - Third Pillar - MetricsTo implement the third pillar, i.e., metrics, we can use the APM Services dashboard where Latency, Throughput, and Error rates are captured.
It's a common misunderstanding that when tracing with your service mesh, there aren't any code changes.
Why microservices are difficult to debug.
Microservices Architecture: Centralized Configuration and Config ServerMicroservices Architecture: API GatewaysThe Need for VisibilityIn a microservices architecture, there are a number of small microservices talking to each other:In the above example, let's assume there is a problem with Microservice5, due to which Microservice1 throws an error.
How does a developer debug the problem?
From such a trace, it should be possible to identify that something went wrong at Microservice5.
Otherwise, a lot of time and effort needs to be spent in debugging problems.
Spring Reactive Microservices: A ShowcaseIntroduction and ScopeThe Servlet Specification was built with the blocking semantics or one-request-per-thread model.
Reactive programming is trending at the moment but trivial "hello-world" examples with Mono & Flux cannot simply capture the demands of building 12-factor apps for production usage.
A mix is also possible in case we have some endpoints and services which cannot become reactive for a number of reasons such as: blocking dependencies with no reactive alternatives or we may have an existing legacy app which we want to migrate gradually etc.
Of course, these will be mocked for simplicity.
22                     // If an error occurs in one of the Monos, execution stops immediately.
23                     // If we want to delay errors and execute all Monos, then we can use zipDelayError instead              24                     Mono<Tuple2<CustomerDTO, String>> zippedCalls = Mono.zip(customerInfo, msisdnStatus);              25                           26                     // Perform additional actions after the combined mono has returned              27                     return zippedCalls.flatMap(resultTuple -> {              28                           29                         // After the calls have completed, generate a random pin              30                         int pin = 100000 + new Random().nextInt(900000);              31                           32                         // Save the OTP to local DB, in a reactive manner              33                         Mono<OTP> otpMono = otpRepository.save(OTP.builder()              34                                 .customerId(resultTuple.getT1().getAccountId())              35                                 .msisdn(form.getMsisdn())              36                                 .pin(pin)              37                                 .createdOn(ZonedDateTime.now())              38                                 .expires(ZonedDateTime.now().plus(Duration.ofMinutes(1)))              39                                 .status(OTPStatus.ACTIVE)              40                                 .applicationId("PPR")              41                                 .attemptCount(0)              42                                 .build());              43                           44                         // External notification service invocation              45                         Mono<NotificationResultDTO> notificationResultDTOMono = webclient.build()              46                           .post()              47                           .uri(notificationServiceUrl)              48                           .accept(MediaType.APPLICATION_JSON)              49                           .body(BodyInserters.fromValue(NotificationRequestForm.builder()              50                                 .channel(Channel.AUTO.name())              51                                 .destination(form.getMsisdn())              52                                 .message(String.valueOf(pin))              53                                 .build()))              54                           .retrieve()              55                           .bodyToMono(NotificationResultDTO.class);              56                           57                         // When this operation is complete, the external notification service              58                        // will be invoked.
If an error occurs in one of the Monos, the execution stops immediately.
If we want to delay errors and execute all Monos, then we can use zipDelayError instead.
Validate OTPBusiness RequirementGiven an existing OTP id and a valid pin previously delivered to user's device:fetch the corresponding OTP record from the DB by querying "otp" table by idif found, fetch information of maximum attempts allowed from configuration table "application", otherwise return errorperform validations: check if maximum attempts exceeded, check for matching pin, if OTP has expired etc.
if validation checks fail, then return error, otherwise update the OTP status to VERIFIED and return successin case of error we need to finally save the updated counter of maximum attempts and OTP status back to the databaseWe assume here that we can have OTPs associated with applications and we can have different time-to-live periods, different number of maximum attempts allowed etc.
Notice that for such simple queries no implementation is neededWe then use the switchIfEmpty and Mono.error methods to throw an Exception if no record found.
As the name says, it acts as an error handler so we can put in there related actions.
It's like a finally clause but for errors.
and concurrently (in parallel):fetch the corresponding OTP record from the DB by querying "otp" table by idif not found or its status is no longer valid (e.g.
EXPIRED), return errorif found, proceed re-sending it to the customer via multiple channels and simultaneously via the notification-serviceReturn to the caller the OTP sentSolutionYou can check the implementation here:     Java      xxxxxxxxxx             1                         23                                                  1             return otpRepository.findById(otpId)              2                             .switchIfEmpty(Mono.error(new OTPException("Error resending OTP", FaultReason.NOT_FOUND)))              3                             .zipWhen(otp -> {              4                                 // perform various status checks here...              5                                             6                                 List<Mono<NotificationResultDTO>> monoList = channels.stream()              7                                         .filter(Objects::nonNull)              8                                         .map(method -> webclient.build()              9                                                 .post()              10                                                 .uri(notificationServiceUrl)              11                                                 .accept(MediaType.APPLICATION_JSON)              12                                     .body(BodyInserters.fromValue(NotificationRequestForm.builder()              13                                                         .channel(method)              14                                                         .destination(Channel.EMAIL.name().equals(method) ?
Its primary target is to be a simple, limited, opinionated object mapper.
However, when we are using File Appenders for logging then we have an issue since this I/O operation is blocking.
neverBlock – Setting it to true will prevent any blocking on the application threads but it comes at the cost of lost log events if the AsyncAppender’s internal buffer fills up.
For example, tracing database calls with R2DBC is not yet supported.
Sometimes it may be hard to detect blocking code in Reactor thread.
And this is because we don't need to use block to make things blocking but we can unconsciously introduce blocking by using a library which can block the current thread.
The only thing you need for this is to include the following dependency:     XML      xxxxxxxxxx             1                                                  1             <dependency>              2               <groupId>io.projectreactor.tools</groupId>              3               <artifactId>blockhound-junit-platform</artifactId>              4               <version>1.0.4.RELEASE</version>              5             </dependency>Keep in mind that if go with Java 11 and above, the following JVM argument is needed for the tool to work properly:-XX:+AllowRedefinitionToAddDeleteMethodsIntegration TestingIn our sample project we show an example Integration Test covering our most "complicated" endpoint which is the one that generates an OTP.
We use HoverFly for mocking responses of the two "external" services (i.e.
This client can connect to any server over HTTP, or to a WebFlux application via mock request and response objects.
Moreover, we need to "bind" it with HoverFly so when it is invoked to return the mocked response we want.
Async SOAPNowadays most of the systems we integrate with expose REST endpoints.
The most significant difference (4 times faster than blocking Servlet) comes when underlying service is slow (500ms).
Also, it does not create a lot of threads comparing with Servlet (20 vs 220).
Certain groups of steps or spans in-between may be repeatable, but never indefinitely like a “do loop” without an exit condition.
Unfortunately, the project I’m working has a different context:The transaction ID is not created by the first microservice in the call chain — a mandatory façade proxy doesThe transaction ID is not numeric — and Sleuth handles only numeric valuesAnother header is required.
Unfortunately, the current context doesn’t allow for that.
Blame me all you want, I couldn’t find a more descriptive name.
Regarding scope mismatch, the same injection trick as for the interceptor handler above is used.
Observability — Once the API is deployed in production, testing in production provides the overall health of live APIs and alert us if any problem occurs.
We will illustrate the usage of tools on APIs exposed by a web application as we elaborate on each phase of API development.
It exposes REST APIs to perform CRUD operations on a product catalog.
There is no separation between design and implementation.
This allows us to intercept the request and the reponse and modify them to create scenarios that are otherwise difficult to test without changing the client.
With caching, there is a problem of a client having an out-dated resource or two clients to have different versions of the same resource.
If this is not acceptable or if there are personalized resources that cannot be cached, e.g., auth tokens, HTTP provides validation caching.
A routing policy may then be configured for weighted routing, latency routing or for fault tolerance.
Some proxy servers do not allow this.
The basic idea behind performance testing is to send lots of requests to the API at the same time and see at what point performance degrades and ultimately fails.
To get a baseline performance of API, different kinds of load tests can be run with increasing loads, measured by the number of requests per second, to find out performance figures quantified by errors and response times, forSoak test — average load for long periods, e.g., run for 48 hours @1 request per second.
Stress test — way-over peak load, e.g., run10K requests per second for 10 minutes.
ObservabilityOnce API is deployed in production, it does not mean we can forget about the API.
For example, one log statement for every 10 lines of code or more if the code is complex with log levels split as - 60 percent DEBUG, 25 percent INFO, 10 percent WARN and 5 percent ERROR.
While logging explicitly tells us what is going on with the API, monitoring provides the overall health of API using generic metrics exposed by the platform and the API itself.
Without the ability to correlate downstream service requests it can be very difficult to understand how requests are being handled within your platform.
This might be fine for some use cases, but no so much for others, especially when you are building microservices.
The problem with using ThreadLocal variables within the asynchronous approach is that the Thread that initially handles the request (and creates the DeferredResult/Future) will not be the Thread doing the actual processing.
This can be achieved by extending Callable with the required functionality: (don’t worry if example Calling Class code doesn’t look intuitive – this adaption between DeferredResults and Futures is a necessary evil within Spring, and the full code including the boilerplate ListenableFutureAdapter is in my GitHub repo):public class CorrelationCallable<V> implements Callable<V> {    private String correlationId;    private Callable<V> callable;    public CorrelationCallable(Callable<V> targetCallable) {        correlationId = RequestCorrelation.getId();        callable = targetCallable;    }    @Override    public V call() throws Exception {        RequestCorrelation.setId(correlationId);        return callable.call();    }}//...
If you don't already have a config file, you can generate one with the following command:telegraf --sample-config --input-filter prometheus --aggregator-filter --output-filter influxdbThe --sample-config argument creates a new config file, while the various filter options filter the configuration sections which will be added to the config file.
It also exposed a REST endpoint for search policies.
Each product had a code, name, image, description, cover-list, and question-list, which affected the price defined by the tariff.
On Windows, it’s a bit problematic because you have to do it through a bash (cygwin/GIT bash) or Linux subsystem for Windows.
Currently, we have two options for configuring MongoDB: non-blocking or blocking.
pom.xml        <dependency>            <groupId>io.micronaut.configuration</groupId>            <artifactId>mongo-reactive</artifactId>        </dependency>application.ymlmongodb:    uri: "mongodb://${MONGO_HOST:localhost}:${MONGO_PORT:27017}/products-demo"    cluster:      maxWaitQueueSize: 5    connectionPool:      maxSize: 20Then the non-blocking MongoClient will be available for injection and can be used in our repository:import com.mongodb.client.model.Filters;import com.mongodb.reactivestreams.client.MongoClient;import com.mongodb.reactivestreams.client.MongoCollection;import io.reactivex.Flowable;import io.reactivex.Maybe;import io.reactivex.Single;import lombok.RequiredArgsConstructor;import pl.altkom.asc.lab.micronaut.poc.product.service.domain.Product;import pl.altkom.asc.lab.micronaut.poc.product.service.domain.Products;import javax.inject.Singleton;import java.util.List;@Singleton@RequiredArgsConstructorpublic class ProductsRepository implements Products {    private final MongoClient mongoClient;    @Override    public Single add(Product product) {        return Single.fromPublisher(                getCollection().insertOne(product)        ).map(success -> product);    }    @Override    public Single<List> findAll() {        return Flowable.fromPublisher(                getCollection().find()        ).toList();    }    @Override    public Maybe findOne(String productCode) {        return Flowable.fromPublisher(                getCollection()                        .find(Filters.eq("code", productCode))                        .limit(1)        ).firstElement();    }    private MongoCollection getCollection() {        return mongoClient                .getDatabase("products-demo")                .getCollection("product", Product.class);    }}Exposing REST EndpointsREST endpoints are a basic way of communicating between the server application and the client application.
It is horizontally scalable, fault-tolerant, wicked fast, and runs in production in thousands of companies.
Unfortunately, this is impossible, but in the next version (RC1), this should already be possible.
For now, we have solved this problem by adding a method to an existing client and overwriting defined paths.
@Client(id = "policy-search-service", path = "/policies")@Retryable(attempts = "2", delay = "3s")public interface PolicySearchGatewayClient extends PolicySearchOperations {}This results in a retry two times with a delay of three seconds between each.
We should have an emergency plan called a fallback mechanism.
A fallback mechanism is a second way of doing things, in case the first way fails.
Instead, each microservice needs a dynamic port allocation to avoid collisions during replication.
You’ll find the answer in the Micronaut FAQ: The majority of Consul and Eureka clients that exist are blocking and include a mountain of external dependencies that inflate your JAR files...Micronaut’s DiscoveryClient uses Micronaut’s native HTTP client, thus greatly reducing the need for external dependencies and providing a reactive API onto both discovery servers.
Scheduling can be configured at a fixed rate (fixedRate), with a fixed delay (fixedDelay), or as a cron task (cron).
Sensitive data must be restricted to authenticated users.
ProsYou can access various data stores, both in blocking and non-blocking ways, connect your services via REST HTTP calls or asynchronously through a message broker, and secure your system with JWT.
Despite their popularity, they still may require you to use other products-or even pair with Istio itself to match Istio's feature set.
These processes are themselves prone to error, in addition to being potentially costly and time-consuming.
However, they will likely incur costs and experience delays in the process, and they are more likely to make mistakes in the implementation of either functionality or compliance.
These gateway abstractions can be configured to allow you to define policies for retries and timeouts, to inject faults into the system at will to test its resilience, to direct traffic to legacy services, or even to add services in another service mesh through a multicluster configuration.
Observability: A Deeper Dive Service meshes offer centralized, platform-level solutions to the general problems surrounding the observability of applications.
These benefits do not come for free, however, and service meshes such as Istio are famous for their management complexities.
Cloud ProvidersThe usual suspects are present: Amazon Web Services Lambda, Google Cloud Functions, Microsoft Azure Function Apps and recently IBM has entered the space with a hosted version of OpenWhisk.
This level of automation makes moving functions from one provider to another less painful.
However, functions are not truly portable as there is currently not any standard for function entry points, returning data or for the libraries that will be available at runtime.
Event GatewayWhile each cloud provider has their own API Gateway they do not typically provide much convenience for multiple provider solutions nor ease of portability.
The Future for ServerlessIt is still a wild frontier out there with many offerings and no real standards.
Inbound proxy architectureProxies and MicroservicesReverse proxies are a critical component of microservices applications.
The Jaeger Client typically sends spans via UDP to the agent, avoiding the TCP overhead and reducing the CPU and memory pressure upon the instrumented application.
With that in mind, the Jaeger Agent should be deployed as close as possible to the instrumented application, reducing the risks inherent to UDP delivery.
Keep your eyes on the Spring Initializr or you’ll miss it!
If you’re in any of those places, don’t hesitate to reach out!
The SpringOne Tour is stopping in Boston, MA March 13 and 14.
Quickly though, using naive I/O for debugging becomes really tedious.
You have a hard time finding the information you need.
Things are just a mess.
Now let's add some log4cpp code:     C++     xxxxxxxxxx             1                                                  1             class printer {              2             private:              3                 log4cpp::Category& log;              4                 const std::string message;              5                           6             public:              7                 printer();              8                 void print() const;              9             };and     C++     xxxxxxxxxx             1                                                  1             class fancy_printer {              2             private:              3                 log4cpp::Category& log;              4                           5             public:              6                 fancy_printer();              7                 void print(const std::string message) const;              8             };This has forced a couple of changes on us.
Overall, kind of a naive approach.
We're going to use a simple properties file that copies our previous functionality and configures the loggers at different levels:     Properties files     xxxxxxxxxx             1                                                  1             log4cpp.rootCategory=WARN, rootAppender              2             log4cpp.category.printer=DEBUG              3                           4             log4cpp.appender.rootAppender=ConsoleAppender              5             log4cpp.appender.rootAppender.layout=BasicLayoutHere, we've configured the logging system as a whole to log at the WARN level, while the printer logger is configured to log at DEBUG.
“Distributed tracing” allows DevOps to automatically follow the path that any and every request takes across microservices; with ordinary transactions involving hundreds or thousands of distinct services in the span of a quarter second, there’s no other way to understand and explain the behavior of today’s intricate distributed systems.
Sigelman is now the co-founder of LightStep, which is still in stealth.
However, the solutions that engineering departments have depended on for the last decade – logging, metrics and conventional APM (and even how they deploy and do security) – depend on built-in assumptions about system architecture that no longer hold.
With the adoption of DevOps, distributed tracing needs to be automated so companies are able to explain any and every request, sort the signal from the noise, and know where the problem is that needs to be fixed.
If you’re around, don’t hesitate to say hi.
Many of the services are old and too brittle to update, so we can't update the original service, plus it would take to long to update each service.
It exposes an index route which just returns a 'hello' message.
Let's fire up our deployment, and our service: $ kubectl create -f ./deployment.yml,./service.yml.
No?
Then we look at some comparison points: plain Java apps, apps that use Spring but not Spring Boot, an app that uses Spring Boot but no autoconfiguration, and some Ratpack sample apps.Vanilla Spring Boot AppAs a baseline we build a static app with a few webjars and spring.resources.enabled=true.
All the Spring Boot webapps we analyse have this same configuration.We might have to worry about how big the classpath is, in order to estimate what happens to the memory.
A completely minimal Spring Boot application including Spring and some logging but no web server would be around 5MB of jars.JVM ToolsTo measure memory usage there are some tools in the JVM.
There are 6200 classes and 25 threads, including a few that are added by the monitoring tool that we use to measure them.Here’s a graph of the heap usage from a quiescent app under load,followed by a manual garbage collection (the double nick in themiddle) and a new equilibrium with a lower heap usage.Some tools in the JVM other than JConsole might also beinteresting.
type<bootstrap>21233609965  null  live<internal>0x00000000f4b0d730114760x00000000f495c890deadsun/reflect/DelegatingClassLoader@0x0000000100009df80x00000000f5a26120114830x00000000f495c890deadsun/reflect/DelegatingClassLoader@0x0000000100009df80x00000000f52ba3a811472  null  deadsun/reflect/DelegatingClassLoader@0x0000000100009df80x00000000f5a3052018800x00000000f495c890deadsun/reflect/DelegatingClassLoader@0x0000000100009df80x00000000f495c890397263629020x00000000f495c8f0deadorg/springframework/boot/loader/LaunchedURLClassLoader@0x00000001000608280x00000000f5b639b0114730x00000000f495c890deadsun/reflect/DelegatingClassLoader@0x0000000100009df80x00000000f4b80a30114730x00000000f495c890deadsun/reflect/DelegatingClassLoader@0x0000000100009df8...total = 93630010405986    N/A    alive=1, dead=92    N/AThere are loads of “dead” entries, but there is also a warning that the liveness information is not accurate.
A manual GC doesn’t get rid of them.Kernel Memory ToolsYou would think that a Linux OS would provide plenty of insight into a running process, and it does, but Java processes are notoriously hard to analyse.
The JVM is very jealous of its memory.A lower level tool is pmap, where we can look at the memoryallocations assigned to a process.
Once they all start they serve their home pages quite efficiently (51ms latency over a crappy LAN at 99th percentile).
Once they are up and running, stopping and starting one of the processes is relatively quick (a few seconds not a few minutes).The VSZ numbers from ps are off the scale (as expected).
Hardly any classes loaded so no surprise really.Do Nothing Spring Boot AppNow suppose we do the same thing but load a Spring application context as well:@SpringBootApplicationpublic class MainApplication implements ApplicationRunner {  @Override  public void run(ApplicationArguments args) throws Exception {    System.in.read();  }  public static void main(String[] args) throws Exception {    SpringApplication.run(MainApplication.class, args);  }}Heap 12MB (but drops to 6MB after a manual GC), non-heap 26MB (Code Cache 7MB, Compressed Class Space 2MB, Metaspace 17MB), 3200 classes.
The big drop in the middle is the manual GC, and you can see that after this the app stabilizes at a different saw tooth.Does Spring Boot itself (as opposed to just Spring) add a lot of overhead to this application?
Also makes startup a bit faster: less than 5s compared to as much as 7s when memory is constrained with the fat jar.A slimmed down version of the app with no static resources or webjars runs at 23MB heap and 41MB non-heap as exploded archive (starts in less than 3s).
You can customize it to zero and forgo the parachute to save an extra MB of heap, e.g:@SpringBootApplicationpublic class SlimApplication implements EmbeddedServletContainerCustomizer {  @Override  public void customize(ConfigurableEmbeddedServletContainer container) {    if (container instanceof TomcatEmbeddedServletContainerFactory) {      TomcatEmbeddedServletContainerFactory tomcat = (TomcatEmbeddedServletContainerFactory) container;      tomcat.addConnectorCustomizers(connector -> {        ProtocolHandler handler = connector.getProtocolHandler();        if (handler instanceof Http11NioProtocol) {          Http11NioProtocol http = (Http11NioProtocol) handler;          http.getEndpoint().setOomParachute(0);        }      });    }  }...}Using Jetty instead of Tomcat makes no difference whatsoever to the overall memory or heap, even though the NioEndpoint is high on the “Biggest objects” list in YourKit (takes about 1MB), and there is no corresponding blip for Jetty.
Slightly slower startup maybe.Tomcat ContainerInstead of using the emedded container in Spring Boot, what if we deploy a traditional war file to a Tomcat container?The container starts and warms up a bit and uses of order 50MB heap, and 40MB non-heap.
Then we deploy a war of the vanilla Spring Boot app, and there’s a spike in heap usage, which settles down to about 100MB.
So we have no reason to believe this app is really using much if any additional heap compared to the container.
Also any app that manages its own thread pool (not uncommon in real life Spring applications) will incur an additional non-heap memory penalty for the threads it needs.Rule of Thumb Process SizesA very rough estimate for actual memory usage would be the heap size plus 20 times the stack size (for the 20 threads typical in a servlet container), plus a bit, so maybe 40MB per process in our vanilla app.
That estimate is a bit low, given the JConsole numbers (50MB plus the heap, or 82MB).
The estimate is accurate for the vanilla app and the do nothing Java app.Adding Spring Cloud Eureka discovery only loads about another 1500 classes, and uses about 40 threads, so it should use a bit more non-heap memory, but not a lot (and indeed it does use about 70MB with 256KB stacks, where the rule of thumb would predict 63MB).The performance of this model for the apps we measured is shown below:Summary of DataApplication     Heap (MB)     Non Heap (MB)     Threads     Classes               Vanilla     22     50     25     6200          Plain Java     6     14     11     1500          Spring Boot     6     26     11     3200          No Actuator     5     22     11     2700          Spring Only     5     20     11     2400          Eureka Client     80*     70     40     7600          Ratpack Groovy     22     43     24     5300          Ratpack Java     16     28     22     3000       * Only the Eureka client has a larger heap: all the others are setexplicitly to -Xmx32m.ConclusionsThe effect Spring Boot on its own has on a Java application is to use a bit more heap and non-heap memory, mostly because of extra classes it has to load.
When we compare memory usage for apps deployed in a single Tomcat container with the same apps deployed as independent processes, not surprisingly the single container packs the apps more densely in memory.
The penalty for a standalone process is mainly related to non-heap usage though, which adds up to maybe 30MB per app when the number of apps is much larger than the number of containers (and less otherwise).
As a final note, we observe that the native tools in the operating system are not nearly as good as the ones provided by the JVM, when you want to inspect a process and find out about its memory usage.
We have taken a cautious approach and kept the Camel API backwards compatible.
if you're unsure what a microservice is, i suggest reading martin fowler's   article  about them.
benchmarking programming languages against each other in a meaningful way can be quite difficult.
if you're coming from the world of jvms (as i do myself), the go garbage collector is perhaps not as mature but it does seem to be very reliable after changes introduced somewhere after go 1.2 or so.
from iron/baseexpose 6868add eventservice-linux-amd64 /entrypoint ["./eventservice-linux-amd64", "-profile=test"] in other words - no jvm or other runtime component is required except for the standard c library (libc) which is included in the base image.
Spring Security lead Rob Winch just announced Spring LDAP 2.3.1, which is a quick follow-up to Spring LDAP 2.3.0 due to an issue discovered in the earlier release.
Dashboards don't scale because they usually reveal the information you need until only after an outage is experienced, and, at some point, looking for spikes in your graphs becomes a straining eye exercise.
And that's not monitoring, it is just a "not very" smart way to understand how something isn't working.
Every one of the actors that handle a request can fail, and when that happens, you need a way to answer the basic question: "What just happened?"
It was set up on a daemon in an isolated environment on AWS for every customer with InfluxDB, Chronograf, Kapacitor, and other add-ons that our service offers.
Cluster termination: used when a customer stops paying for the service.
The second time the plan executes it should return no steps because all of them should already be executed, inferring that the plan is solved.
func (s *Scheduler) react(ctx context.Context, steps []Procedure) error {	for _, step := range steps {		span, _ := opentracing.StartSpanFromContext(ctx, step.Identifier())		step.WithSpan(span)		logger := s.logger		f := []zapcore.Field{zap.String("step", step.Identifier())}		zipkinSpan, ok := span.Context().
Yet, a few days ago, we had a problem.
Comparing this trace with one that failed makes it easy to see that in the second screen there is only one request to AWS.ELBv2.
It ended up being an error not well-handled.
Alternatively, monitoring is only important in production, and you normally don't care about triggering alerts during your development cycle.
Circuit breakers and retry policies make the debugging a lot more complicated, and some failures are unpredictable or very expensive to handle if they don't happen often.
With this in mind, for some failures, it is better to have a fast way to understand the new issue than try to predict and avoid every possible failure because that's simply not possible anymore.
Operating from one codebase can be done but is not without its own challenges.
This shows the dependency declarations as specified by the developer.Figure 1: A portion of POM.xml showing application dependencies<parent>    <groupId>org.springframework.boot</groupId>    <artifactId>spring-boot-starter-parent</artifactId>    <version>1.3.7.RELEASE</version>    <relativePath/> <!-- lookup parent from repository    --></parent><dependencies>    <dependency>        <groupId>org.springframework.cloud</groupId>        <artifactId>spring-cloud-starter-config</artifactId>    </dependency>    <dependency>        <groupId>org.springframework.cloud</groupId>        <artifactId>spring-cloud-starter-eureka</artifactId>    </dependency>    <dependency>        <groupId>org.springframework.cloud</groupId>        <artifactId>spring-cloud-starter-zipkin</artifactId>    </dependency>Figure 2 is a portion of listed dependencies within the same application, showing JARs bundled into the application’s uberjar, which isolates those dependencies from variations in the underlying environment.
The application will rely upon these dependencies rather than potentially conflicting libraries present in the deployment target.Figure 2: A portion of mvn dependency:tree for a sample application[INFO] Scanning for projects...[INFO][INFO] ------------------------------------------------------------------------[INFO] Building quote-service 0.0.1-SNAPSHOT[INFO] ------------------------------------------------------------------------[INFO][INFO] --- maven-dependency-plugin:2.10:tree (default-cli) @quote-service ---[INFO] com.example:quote-service:jar:0.0.1-SNAPSHOT[INFO] +- org.springframework.cloud:spring-cloud-starterconfig:jar:1.1.3.RELEASE:compile[INFO] | +- org.springframework.cloud:spring-cloud-starter:jar:1.1.1.RELEASE:compile[INFO] | | +- org.springframework.cloud:spring-cloud-context:jar:1.1.1.RELEASE:compile[INFO] | | | \- org.springframework.security:springsecurity-crypto:jar:4.0.4.RELEASE:compile[INFO] | | +- org.springframework.cloud:spring-cloud-commons:jar:1.1.1.RELEASE:compile[INFO] | | \- org.springframework.security:spring-securityrsa:jar:1.0.1.RELEASE:compile[INFO] | | \- org.bouncycastle:bcpkixjdk15on:jar:1.47:compile[INFO] | | \- org.bouncycastle:bcprovjdk15on:jar:1.47:compile[INFO] | +- org.springframework.cloud:spring-cloud-configclient:jar:1.1.2.RELEASE:compile[INFO] | | \- org.springframework.boot:spring-boot-autoconfigure:jar:1.3.7.RELEASE:compile[INFO] | \- com.fasterxml.jackson.core:jacksondatabind:jar:2.6.7:compile[INFO] | \- com.fasterxml.jackson.core:jacksoncore:jar:2.6.7:compile[INFO] +- org.springframework.cloud:spring-cloud-startereureka:jar:1.1.5.RELEASE:compile[INFO] | +- org.springframework.cloud:spring-cloud-netflixcore:jar:1.1.5.RELEASE:compile[INFO] | | \- org.springframework.boot:springboot:jar:1.3.7.RELEASE:compile3.
The notion of Build leads naturally to continuous integration (CI), since those systems provide a single location that assemble artifacts in a repeatable way.Modern Java frameworks can produce uberjars, or the more traditional WAR file, as a single CI-friendly artifact.
A cloud process is disposable — it can be destroyed and created at any time.
Cloud platforms keep multiple app environments consistent and eliminate the pain of debugging environment discrepancies.9.
In the cloud you avoid this micromanagement — the cloud provider will manage port assignment along with routing, scaling, etc.While it is possible to rely upon external mechanisms to provide traffic to your app, these mechanisms vary among containers, machines, and platforms.
We just need to set some configuration properties to expose a predefined set of metrics provided by the Spring Boot Actuator.
Prometheus periodically retrieves data from endpoint exposed by the application, while InfluxDB provides REST API that has to be called by the application.
Logging is something that is not very important during development but is the key point during maintenance.
To expose the Swagger HTML site with API documentation we need to include the following dependencies.
The guideline should contain instructions on how to build your API, which headers need to be set on the request and response, how to generate error codes etc.
For a more detailed explanation of generating Swagger documentation for Spring Boot microservices including exposing it for all the applications on API gateway, you may refer to my article Microservices API Documentation with Swagger2.
Java     xxxxxxxxxx             1                         10                                                  1             @RunWith(SpringRunner.class) @SpringBootTest(webEnvironment =               2                                                          WebEnvironment.NONE)               3             @AutoConfigureStubRunner(ids = {"pl.piomin.services:person-              4               service:+:stubs:8090"}, consumerName = "letter-consumer", stubsPerConsumer =               5                 true, stubsMode = StubsMode.REMOTE, repositoryRoot =               6                 "http://192.168.99.100:8081/artifactory/libs-snapshot-local")               7                 @DirtiesContext public class PersonConsumerContractTest { @Autowired               8                   private PersonClient personClient; @Test public void verifyPerson() {               9                     Person p = personClient.findPersonById(1); Assert.assertNotNull(p);               10                     Assert.assertEquals(1, p.getId().intValue());               11                     Assert.assertNotNull(p.getFirstName());               12                     Assert.assertNotNull(p.getLastName());               13                     Assert.assertNotNull(p.getAddress());               14                     Assert.assertNotNull(p.getAddress().getCity());               15                     Assert.assertNotNull(p.getAddress().getCountry());               16                     Assert.assertNotNull(p.getAddress().getPostalCode());               17                     Assert.assertNotNull(p.getAddress().getStreet());               18                     Assert.assertNotEquals(0, p.getAddress().getHouseNo()); } }   Contract testing will not verify sophisticated use cases in your microservices-based system.
Spring Cloud releases new versions of projects using release train pattern, to simplify dependencies management and avoid problems with conflicts between incompatible versions of libraries.
As a developer or administrator, if you want to troubleshoot any issue, that leaves you clueless.
We also avoid the overhead of excessive logging that way.
That's because:If the system is highly scalable, with an autoscaling feature, the instances will be created and destroyed based on need.
In that case, if you go with the second option, there might be a loss of log files if the host is destroyed.
For some reason, if those agents stop working, we may not get the logs from that host.
Again, we are losing the log information.
Since it is written in C++ there is no garbage collection cycle to slow down processing and no worries about JVM heap memory management.
To change this behavior, open entry.component.html and change the following line:<td>{{entry.content}}</td>To:<td [innerHTML]="entry.content"></td>After making this change, you’ll see that the HTML is no longer escaped.
NoDo you want to use a Data Transfer Object (DTO)?
NoDo you want to use separate service class for your business logic?
NoDo you want pagination on your entity?
When you run this command in the blog application, you’ll likely get a test failure.
Results :Failed tests:  BlogResourceIntTest.getAllBlogs:184 Status expected:<200> but was:<500>Tests run: 157, Failures: 1, Errors: 0, Skipped: 0The reason this happens is shown in a stack trace in your terminal.
Running org.jhipster.blog.web.rest.BlogResourceIntTest2017-06-19 10:29:17.288 ERROR 4168 --- [           main] o.j.b.w.rest.errors.ExceptionTranslator  : An unexpected error occured: Authentication object cannot be null; nested exception is java.lang.IllegalArgumentException: Authentication object cannot be null2017-06-19 10:29:17.472 ERROR 4168 --- [           main] o.j.blog.web.rest.util.HeaderUtil        : Entity processing failed, A new blog cannot already have an IDTo fix this, you can use Spring Security Test’s @WithMockUser.
You can also run kubectl get po -o wide --watch to see the status of each pod.
To remove all deployed containers, run the following command:kubectl delete deployment --allTo stop Minikube, run minikube stop.
The logs have an error like the following:  2017-06-19 18:08:06.930 ERROR 17597 --- [ XNIO-2 task-62] o.j.b.w.rest.errors.ExceptionTranslator  :  An unexpected error occured: Unable to access lob stream; nested exception is org.hibernate.HibernateException:  Unable to access lob streamThis is a known issue with PostgreSQL.
Adding @Transactional to the EntryResource.java class definition solved this problem.
When running everything in Minikube, adding new products fails.
what happens if the right-most service “service y” fails?
the waiting requests of the consumer services (service n & service a) will eventually time out, but if you have a system handling tens or hundreds of requests per second, you’ll have thread pools filling up, memory usage skyrocketing and irritated end consumers (those who called service 1) waiting for their response.
figure 2 - cascading failure.
while a properly implemented   healthcheck  will eventually trigger a service restart of the failing service through mechanisms in the container orchestrator, that may take several minutes.
meanwhile, an application under heavy load will suffer from   cascading failures  unless we’ve actually implemented patterns to handle this situation.
thomas edison filed a patent application back in 1879. the circuit breaker is designed to open when a failure is detected, making sure cascading side effect such as your house burning down or microservices crashing doesn’t happen.
open: whenever a failure has been detected (n number of failed requests within a time span, request(s) taking too long, a massive spike of current), the circuit     opens    , making sure the consumer service short-circuits instead of waiting for the failing producer service.
sometimes, we can’t do without the data or service of the broken producer - but just as often, our fallback method can provide a default result, a well-structured error message or perhaps calling a backup service.
stopping cascading failures.
no thread pools filling up with pending requests, no timeouts, and hopefully fewer annoyed end-consumers.
why consider the first attempt as a failure inside the circuit breaker if you have many instances where perhaps just a single one has problems?
so instead of failing inside the breaker, why not have a mechanism that automatically performs a configurable number of retries including some kind of backoff?
the circuit breaker only considers the request failed if all retry attempts failed.
actually, the circuit breaker has no notion of what’s going on inside it - it only cares about whether the operation it encapsulates returns an error or not.
errors := hystrix.go(breakername,    // pass the name of the circuit breaker as first parameter.
func() error {                    // create the request.
in this case, we just do a bit of logging and return the error.
func(err error) error {                    logrus.errorf("in fallback function for breaker %v, error: %v", breakername, err.error())                    circuit, _, _ := hystrix.getcircuit(breakername)                    logrus.errorf("circuit state is: %v", circuit.isopen())                    return err        })        // response and error handling.
otherwise,        // the errors channel gives us the error.
blocking)   output  channel to the   select  code snippet, which will effectively block until   either  the   output  or   errors  channels recieves a message 5.2 retrier code next, the   callwithretries(…)  func that uses the retrier package of go-resilience:func callwithretries(req *http.request, output chan []byte) error {    // create a retrier with constant backoff, retries number of attempts (3) with a 100ms sleep between retries.
err := r.run(func() error {            attempt++            // do http request and handle response.
resp, err := client.do(req)            if err == nil && resp.statuscode < 299 {                    responsebody, err := ioutil.readall(resp.body)                    if err == nil {                            output <- responsebody                            return nil                    }                    return err            } else if err == nil {                    err = fmt.errorf("status was %v", resp.statuscode)            }            logrus.errorf("retrier failed, attempt %v", attempt)            return err    })    return err} 5.3 unit testing i’ve created three unit tests in the   /goblog/common/circuitbreaker/hystrix_test.go  file which runs the   callusingcircuitbreaker()  func.
in this test we use   gock  to mock responses to three outgoing http requests, two failed and at last one successful:func testcallusingresiliencelastsucceeds(t *testing.t) {        defer gock.off()        buildgockmatchertimes(500, 2)        // first two requests respond with 500 server error        body := []byte("some response")        buildgockmatcherwithbody(200, string(body))   // next (3rd) request respond with 200 ok        hystrix.flush()     // reset circuit breaker state        convey("given a call request", t, func() {                convey("when", func() {                        // call single time (will become three requests given that we retry thrice)                        bytes, err := callusingcircuitbreaker("test", "http://quotes-service", "get")                        convey("then", func() {                                // assert no error and expected response                                so(err, shouldbenil)                                so(bytes, shouldnotbenil)                                so(string(bytes), shouldequal, string(body))                        })                })        })} the console output of the test above looks like this:erro[2017-09-03t10:26:28.106] retrier failed, attempt 1                    erro[2017-09-03t10:26:28.208] retrier failed, attempt 2                    debu[2017-09-03t10:26:28.414] call in breaker test successful               the other   tests  assert that hystrix fallback func runs if all retries fail and another test makes sure that the hhystrix circuit breaker is opened if a sufficient number of requests fail.
a simple example below where we specify the number of failed requests that should open the circuit and the retry timeout:hystrix.configurecommand("quotes-service", hystrix.commandconfig{    sleepwindow:            5000,requestvolumethreshold: 10,}) see the   docs  for details.
","ipaddress":"10.0.0.25"},"imageurl":"http://imageservice:7777/file/cake.jpg"} if we kill the quotes-service:> docker service scale quotes-service=0 we’ll see almost right away (due to connection refused) how the fallback function has kicked in and are returning the fallbackquote:{name":"person_23","servedby":"10.255.0.19","quote":{"quote":"may the source be with you, always.
6.2.1 disabled circuit breaker no circuit breaker, just using the standard   http.get(url string)  :  the very first request needs slightly less than a second, but then latencies increases, topping out at 15-20   seconds  per request.
then - the increasing latencies trip the breaker and we immediately see how response times drop back to a few milliseconds instead of ~5 seconds for the majority of the requests.
for each circuit breaker, we see breaker state, req/s, average latencies, number of connected hosts per breaker name and error percentages.
there’s also a thread pools section below, though i’m not sure they work correctly when the root statistics producer is the go-hystrix library rather than a hystrix-enabled spring boot application.
i guess there are two options apart from doubling up and using eureka in addition to the orchestrator’s service discovery mechanism - something i thought was a pretty bad idea back in   part 7  .
unsure if hystrix streams are produced by go-hystrix if there’s been no or no ongoing traffic passing through.
Cloud computing enables some of the largest companies in the world, such as Airbnb, Netflix, and Uber, to reinvent and dominate their industries.
Understanding and choosing the correct cloud-native technologies are critical for increasing development velocity and spending less time and money developing and maintaining tooling and infrastructure.
Additionally, cloud-native tools rely heavily on abstractions, making them more generic and allowing teams to run their services without agreeing on a shared runtime across the company.
Resources always adapt to the current demand, which saves money over traditional, statically scaled resources.
Companies that leverage the full suite of tools can often deliver faster, with less friction, and lower development and maintenance costs.
Engineering teams can store container images in a container registry, which in most cases, also provides vulnerability analysis and fine-grained access control.
Manual configuration, however, makes it hard to keep track of changes.
SecretsSecret management is essential for cloud-native solutions but is often neglected at smaller scales.
Ultimately, organizations that ignore secret management could increase the surface area for credential leakage.
CertificatesSecure communication over TLS is not only best practice but a must-have.
What is the result of the operations (success, failure, or status codes)?
Without alerts, you don't get notified of incidents, which in the worst case means that companies don't know that there have been problems.
When performance issues arise, teams can see what service errors are occurring and how long each phase of the transaction is taking.
The critical thing is event producers, consumers, and brokers are decoupled, which allows them to be scaled, updated, and deployed independently.
Pre-built connectors to produce and consume events from a variety of event sources — SourceDesigned To Handle Highly Scalable and Resilient DeploymentsEvent-driven applications demand on-demand scaling and high resiliency in the face of a failure.
When you have more entry points, you have more places to worry about securing.
The more entry points to an application, the broader the attack surface is.
Once a request is inside the application layer, you don’t need to worry about security when one component talks to another.
The Broader the Attack Surface, the Higher the Risk of AttackIn a monolithic application, communication among internal components happens within a single process — in a Java application, for example, within the same Java Virtual Machine (JVM).
As the number of entry points to the system increases, the attack surface broadens too.
These repetitive, distributed security checks and remote connections could contribute heavily to latency and considerably degrade the performance of the system.
Jack Kleeman, a backend engineer at Monzo, explains in a blog (http://mng.bz/gyAx) how they built network isolation for 1,500 services to make Monzo more secure.
Also, you need to be able to revoke certificates (in case the corresponding private key gets compromised) and rotate certificates (change the certificates periodically to minimize any risks in losing the keys unknowingly).
A high number probably indicates that the system is under attack or the first-level defense is weak.
At any point, you can kill a running container and create a new one with the base configuration without worrying about runtime data.
Nothing is shared among microservices (or only a very limited set of resources), and the user context has to be passed explicitly from one microservice to another.
minimizing all links between modules to a minimum avoids a complex architecture.
the mentioned components support an authorization process — that is why they are able to throw a forbidden exception.
conclusion as we can see, java developers do not have to create a whole microservices architecture from scratch.
No changes are needed to the application itself.
No.
Since fluentd is shipping logs across the Kubernetes cluster, I’m using a search to narrow down on Istio logs only:kubernetes.namespace_name : "istio-system"Metric visualizationNo.
The technology itself is still relatively immature, so there is some risk involved.
As the technology matures, and costs and risks gradually go down, the tipping point for adopting service mesh is fast approaching.
Here in this section, you will learn about resources, CRUD implementation, error handling, HTTP status codes, versioning, pagination, partial responses, etc.
Learn what the problems are with SOAP — which REST solves — and make a decision between SOAP and REST for future projects.
1.2September, 2017Addition of Health Check 1.0,Metrics 1.0, Fault Tolerance 1.0,and JWT Propagation 1.0.Updating to Configuration 1.1.
1.4June 2018Updating to Open Tracing 1.1,REST Client 1.1, Fault Tolerance1.1, JWT Propagation 1.1, andConfiguration 1.3.
2.2February, 2019Updating to Open Tracing 1.3, Open API 1.1., REST client 1.2, and Fault Tolerance 2.0.
Fault Tolerance 2.0 was upgraded to CDI 2.0.
This is a lower level SPI style technology that will likely be incorporated into other parts of MicroProfile going forward.
The Open Tracing specification solves this problem by providing a standard for instrumenting microservices for distributed tracing in a technology agnostic manner.
Rest Client JAX-RS provides a powerful client API, but it can be hard to use and not really type safe.
Fault ToleranceMicroservices, especially running on the cloud, are inherently unreliable.
SECONDS)@Timeout(value=3, unit=ChronoUnit.SECONDS)@Bulkhead(2)public Membership getMembership(   @NotNull @PathParam(value = "id") int id) {In the code example, the @Timeout annotation causes the method call to fail unless it executes within three seconds.
Once a failure occurs, the method can be accessed again after 10 seconds.
In addition to a destination, we may also define a source of the request in order to restrict a rule only to a specific caller.
The next rule callme-service-v2(1) has a lower priority (2).
The rule callme-service-v1-default(1) visible in the code fragment below has a lower priority (2) than the two previously described rules.
In practice, it means that it is executed only if conditions defined in two previous rules were not fulfilled.
It prints the version of caller-service taken from pom.xml and calls the method GET callme/ping exposed by callme-service.
There are some issues with the monolithic architecture, such asLarge codebases become a mess over time.
Challenges With MicroservicesGetting the correct sub-domain boundaries, in the beginning, is hard.
Instead of reinventing the wheel, we can simply take advantage of various Spring Cloud modules and focus on our main business problem rather than worrying about infrastructural concerns.
Circuit Breaker: In microservices-based architecture, one service might depend on another service, and if one service goes down, then failures may cascade to other services as well.
Distributed Tracing: One of the pain points with microservices is the ability to debug issues.
(Spoiler: There is no sense.
)Things do however quickly become more difficult when tracing asynchronous executions.
If a request is received by one thread but is answered from within another thread, it does no longer suffice to only trace entries and exits.
And the same way that debugging asynchronous execution is difficult, this is quite a bit of work for us too.
It is obvious that the easiest way of avoiding garbage is to avoid object allocation altogether.
However, object allocation in itself isn’t too bad either.
But since our agent runs mostly isolated from the application threads and at first, this did at first not make sense to me.When digging deeper, I found that our analysis of user objects triggered some additional escapes of objects but the impact was minimal.
Each worker thread of a fork join pool applies work stealing and might grab tasks out of the queue of any other task.
This is not true for all collection algorithms but for many of them such as for all default collectors of HotSpot.
In order to avoid this, I started to pool the objects I was using.
Traditionally, pooling was used to avoid the costs of allocation which became cheap in our days.
But as a matter of fact, the problem that I describe applies to a large number of Java applications.
Just as in the above case, the garbage collection algorithm does not group objects by application as it has no notion of this deployment model.
Therefore, object allocations by two isolated applications that share a container do interfere with the anticipated collection patterns of one another.
As a matter of fact, I think they are a bad idea for most applications.
And even if isolated applications ease development, you quickly pay the price in operations.
I am just mentioning this to avoid a misinterpretation of the moral of the above experience.What this experience taught me was that deploying several applications in a single Java process can be a bad idea if those applications are heterogeneous.
Yet, many enterprise frameworks still advertise all-in-one solutions for tackling such problems which should not share a process to begin with.
Otherwise, you might end up with collection patterns that you did not anticipate when developing, running, and testing your applications in isolation.
This Week in Spring: Spring Fu and MongoDBHi Spring fans and welcome to another installment of This Week in Spring!
Don’t miss this webinar on MongoDB by Spring Data’s Christoph Strobl and Mat Keep.
Spring Fu is here and it’s awesome!
Spring Fu is explicit, not implicit.
It's functional, it’s fast, it’s Spring Fu.
Don’t miss the upcoming Pivotal Paris event with many of the Spring team’s local engineers.
Here is the Chaos Monkey for Spring Boot now works with Russ Miles’ Chaos Toolkit.
Chaos Toolkit LOVES Chaos Monkey for Spring Boot – Chaos Toolkit – Medium.
Service-based Application:These applications expose business functionalities via service API using microservices architecture as a common approach.
Image building is a core building block that plays a significant role in the automatic deployment pipeline.
Service Mess manages the network between the service’s proxy(sidecar).
Istio, Consul, and Linkerd are some commonly used Service Mess tools.
Tolls are CNI, Network Service Mess, Kube-OVN, Ligato, etc.
Logging - logStash, Splunk, LOGIQTracing – Spring Cloud Sleuth, ZipkinChaos Engineering – Litmus, Chaos Mesh, GremlinPlease refer CNCF landscape link to check the complete list of tools under each category.
- High availability and fault tolerance.
since i’m too lazy to do things manually, i’d prefer the stubs to be automatically downloaded for me, the wiremock servers started and fed with the stub definitions.
any exceptions at this point are ignored (so assuming that   notawiremockmapping.json  is not a valid wiremock definition, the exception will be suppressed).
defaults to 'stubs' (default: stubs) -wo (--workoffline)            : switch to work offline.
if however that’s not the case - don’t worry, you’ll just have to map it yourself.
The Spring Cloud Config Server then exposes this information to all the microservices.
Fault ToleranceWhat if a microservice is down?
Each of these solves a particular problem associated with microservices.
Important problems include service registration, service discovery, load distribution, event tracing, service monitoring and fault tolerance.
Customer responsivenessLimited knowledge wasteFeedback loopExamples of Issue-Tracking Tools:Atlassian’s JiraJira is a proprietary issue-tracking product developed by Atlassian that allows bug tracking and Agile project management.
It focuses on query-based issue searching — with autocompleting, manipulating issues in batches, customizing the set of issue attributes, and creating custom workflows.
Asset controlLimit transportation wasteEmpower teamsExamples of SCM Tools:GitGit is a distributed version-control system for tracking changes in source code during software development.
JFrog offers high availability, replication, disaster recovery, scalability, and works with many on-prem and cloud-storage offerings.
Fast feedbackReduce defect waste and waiting wasteExamples of CI Tools:JenkinsJenkins is a free and open-source automation server.
With AWS CodePipeline, you only pay for what you use.
There are no upfront fees or long-term commitments.
It also performs monitoring, failure recovery, and software updates with zero-to-minimal downtime.
It has to handle failures by doing automatic failovers, and it needs to be able to scale containers when there’s too much data to process/compute for a single instance.
ZooKeeper helps Marathon to look up the address of the Mesos master — multiple instances are available to handle failure.
The Kubernetes scheduler’s task is to watch for pods having an empty PodSpec.
Fast recoveryResponsivenessTransparencyLimited human involvement during incidentsExamples of Monitoring/Logging Tools:ELK StackThe ELK Stack is a collection of three open-source products — Elasticsearch, Logstash, and Kibana.
The nodes expose these over the endpoints that the Prometheus server scrapes.
Reduce knowledge wasteIncrease new-hire productivityLimit repeat mistakesExamples of Knowledge-Sharing Tools:GitHub PagesGitHub Pages is a static site-hosting service that takes HTML, CSS, and JavaScript files straight from a repository on GitHub, optionally runs the files through a build process, and publishes a website.
It was an intense start to the year, but we learned so much about how our community was using Grakn to solve some of the most complex problems in their industries.
From financial analytics to drug discovery, cyber threat detection to robotics disaster recovery, Grakn was being used to tackle a higher order of complexity in data and knowledge, and it inspired all of us at Grakn Labs.
We knew that we had to do something big to overcome this massive hurdle that was holding everyone back, and we made it the priority of 2020 — no excuses.
[1] About the only thing we kept from the old Grakn’s codebase was our test framework, which was essential to guarantee stability and no regression.
We expect to release a production-ready Grakn 2.0 at the end of this month (end of January 2021) after we go through a series of planned stress testing and benchmarking regime.
[6] Pay attention to the nightly releases of Grakn, Graql, Client Java, Client Python, and Client Node.js as we’ll be frequently submitting patches throughout this Alpha phase.
It’s optimized for fast, low latency storage, and it exploits the full potential of high read/write rates offered by flash or RAM — exactly what we want.
This is natively parallelized to exploit the graph storage being thread-safe.
Most of them would be too costly for the traversal engine to execute as they produce millions of permutations in the execution.
This is an inherent problem with databases, especially graph databases, as they expose the ability to query multiple degrees of relationships.
Well, it’s the “reactive stream” problem.
So the last trick was to bundle multiple query answers into a single server RPC “response”.
The Raft algorithm is asynchronous by nature, but we ensured our implementation had no locks and mutexes, and we built Grakn Cluster as a fully event-driven and non-blocking system.
Developing with Performance in Mind by Jennifer Marsh — Testing, testing, testing—is it stuck in your head yet?
New to the StreetsError Handling in Go that Every Beginner Should Know by Hussachai PuripunpinyoFor anyone new to the Golang game, check this rundown of how to handle some of the most common errors you might encounter using the multiple value return value and the panic methods.
Garbage Collection: A Brief Introduction by Umang KesariGarbage in the real and digital world both stink, take up space unnecessarily, and are a pain to get rid of, as this article illustrates.
The 6 Most Common Performance Testing Mistakes, and How to Fix Them by Jason RizioYou're sure to make some mistakes along the way, and if you've been programming for a while, you've likely already made some of the mistakes on this list.
Both projects are cutting edge and very competitive, making it a tough choice to select one.
A service typically offers service discovery, load balancing, failure recovery, metrics, and monitoring.
ResilienceCircuit breaking, Retries and Timeouts, fault-injection, delay injection.
No circuit breaking and no delay injection support.
ConclusionService meshes are becoming an essential building block in cloud-native solutions and microservice architectures.
When choosing technology as complex and as critical as a service mesh, think about more than just the technology – consider the context in which it will be used.
Kubernetes can be installed on any virtualization platform without requiring any special tools, and can be spawned on AWS, Google Cloud, Azure, IBM Cloud, and Oracle Cloud as managed services by only paying for the virtual machines required for running the workloads.
Some key features such as user management and credential management are missing in the open-source version.
Last year, IBM, Google, and Lyft joined together to implement a solution for this problem with the Istio project, by combining IBM's Amalgam8 project, Google's Service Control implementation, and Lyft's Envoy proxy.
If the system grows over time, it would require a considerable amount of effort and repetitive work by introducing a considerable amount of integrations.
Monitoring involves observing the health of the applications, including socket status, resource usage, request counts, latencies, etc., and generating alerts for the operations teams to take actions on actual system failures (excluding false positives).
This Codecentric post by Benjamin Wilm is a must-read manifesto for chaos engineering and a useful background for anybody considering the Chaos Monkey for Spring Boot.
One of the common obstacles when it comes to learning a new technology is figuring out how to start.
Developing for cloud has its own problems and Spring Cloud's goal is to make it as easy as developing a local application, if not easier.
When you develop for the cloud, you don't personally need to worry about many things like hardware, installing OS, or installing the database, but your application still needs those.
As hardware failure became the common case rather than the exception, Google engineers needed to design accordingly.
Today, we might start by going to GitHub to look for code that others have developed to tackle similar problems, but in 2001 this simply wasn't an option.
As a result, Google had no choice but to build things from scratch.
As Google built out its infrastructure, measuring performance was critical.
The solution to this problem is (of course) a cache, but caching isn't compatible with a serverless model.
Serverless has its place, especially for offline processing where latency is less of a concern, but often a small amount of context (in the form of a cache) can make all the difference.
"Skipping" right to a serverless-based architecture might leave you in a situation where you need to step back to a more performant, services-based one.
Lesson 3: What Independence Should MeanAbove, I argued that organizations should adopt microservices to enable teams to work more independently.
Failing to do so will result in lots of redundant work, as each team evaluates or even builds tools to solve these problems.
Worse, without standards, it will become impossible to measure or reason about the performance or security of the application as a whole.
Lesson 4: Beware of Giant DashboardsTime series data is great for determining when there is a regression but it's nearly impossible to use it to determine which service was the cause of the problem.
You aren't going to be able to enumerate all possible root causes, however — let alone build graphs to measure them.
Ultimately, observability boils down to two activities: measuring these critical signals and then refining the search space of possible root causes.
I use the word "refining" because root cause analysis is usually an iterative process: establishing a theory for what went wrong, then digging in more to validate or refute that theory.
Google's solution to this problem was simple: 99.99% of traces were discarded before they were recorded in long-term durable storage.
Unfortunately, this approach misses many rare yet interesting transactions.
As usual, we’ve got a ton of things to dive into, so I’ll leave you to it.
Don’t miss the training for Spring Boot and Spring Cloud at this year’s SpringOne Platform 2018Check out this upcoming Cloud Foundry webinar on July 18, Bring Your Own Code vs. Bring Your Own Container
Istio Tracing and Monitoring: Where Are You and How Fast Are You Going?The Heisenberg Uncertainty Principle states that you cannot measure an object's position and velocity at the same time.
If it's in a location, then it has no velocity.
That sounds pretty broad, but in fact, one of the basic rules of tracing is that data are dumped into a tracing data store without regard to formatting.
Using the Jaeger UI, we can view traces, see how far and deep they go, and get an idea of where performance might be lagging.
Depending on your language (Java, for example) and framework (Spring Boot), you could implement all this with no changes to your source code.
Top 10 Open-Source Monitoring Tools for KubernetesEngineers list monitoring as one of the main obstacles for adopting Kubernetes.
Not surprisingly, when asked, engineers list monitoring as one of the main obstacles for adopting Kubernetes.
After all, monitoring distributed environments have never been easy and Kubernetes adds additional complexity.
Fluentd uses disk or memory for buffering and queuing to handle transmission failures or data overload and supports multiple configuration options to ensure a more resilient data pipeline.
Both Fluentd and Fluent Bit are also CNCF projects and Kubernetes-native - they are designed to seamlessly integrate with Kubernetes, enrich data with relevant pod and container metadata, and as mentioned - all this with a low resource footprint.
Pros: Huge plugin ecosystem, performance, reliability Cons: Difficult to configurecAdvisorcAdvisor is an open-source agent designed for collecting, processing, and exporting resource usage and performance information about running containers.
The service is built to handle a specific domain and declares its boundaries accordingly; these boundaries are exposed as a contract, which technically can be implemented by API.
Loose Coupling, High CohesionChanging one microservice does not impact the other microservices, so altering our solution's behavior can be done only in one place while leaving the other parts intact.
In short, it means shortening the features’ time-to-market while reducing risks.
Therefore, the SDLC agility is affected; releasing a new feature incurs more testing time, prudent releases, and overall prolonged time-to-market.
Adding more services when the load rises is simple, and similarly, when the load is low, the number of running services can be reduced.
Tackling and troubleshooting can be isolated if the source of the problem is known.
After that, resolving the problem is an isolated chirurgical fix.
Technology-binding is limited to the service level.
This has its toll; lack of standardization costs more in testing, deploying, and maintaining the system.
Standardization should include the API the service exposes and the response structure, and the error handling.
It also enables the service consumers to be aware when the service has changed and avoided breaking the existing contract and the communication between them.
Naturally, this can be prioritized low in the tasks list of the developer.
However, if the environment is dynamic and frequent changes of services' paths are required, choosing DNS may not be the best solution since updating the DNS entries can be painful.
Using load-balancers as a buffer between the service and its consumers is one option to bypass this problem.
Instances are automatically assigned to the network location, so maintaining a central configuration is not efficient and almost not practical.
However, the disadvantages of the client-side discovery approach are:The clients are coupled to the service registry; without it, new clients may be blind to the existence of other services.
It hurts if the services differ by their tech-stack, which means this logic should be implemented across different technologies.
In this approach, the client is more naive, as it communicates only with a load-balancer.
However, this is also a disadvantage.
Only the registrar interacts with the service registry; the new microservice does not play actively in the registration process.
Therefore, this approach is not recommended.
In the monolithic application, transactions are ACID compliant: Atomic, Consistent, Isolated, and Durable.
One alternative is to implement a distributed transactions mechanism, but it is hard to develop and maintain.
There is no guarantee for immediate and isolated transactions in this model, but there is a goal to achieve consistency eventually.
We need to gauge the required hardware to supply a varying demand.
Firstly, even the cloud resources are limited in their compute and storage.
Planning for ResilienceThe microservices solution is composed of many isolated but communicative parts, so the nature of problems differs from the monolith world.
Failures happen in every system, all the more so in a distributes interconnected system; therefore, planning how to tackle these problems on the design stage is needed.
Some of these patterns are:TimeoutsThis is an effective way to abort calls that take too long or fail without returning to the caller promptly.
When a timeout happens, it needs to be logged as it might be caused by a network problem or some other issue besides the service itself.
RetriesFailing to receive a valid response can be due to an error or timeout.
It protects the service itself from being loaded and keeps the clients from sending futile requests.
It can reduce the latency by signaling; there is no point in executing retries.
Second, the service can disregard requests from clients it deems as nonessential or not-important in a given time; it prioritizes other services.
Lastly, to consume fewer resources and supply the demand, the service can continue providing service but with reduced quality; for example, lower resolution video streaming rather than cease the streaming totally.
Another benefit of using throttling is enhanced security; it limits a potential DDOS attack, prevents data harvesting, and blocks irresponsible requests.
BulkheadsThis pattern ensures that faults in one part of the system do not take the other parts down.
In a microservices architecture, when each service is isolated, naturally, it is limited when a service is down.
Besides partitioning the services, consumers can be partitioned,  meaning isolating critical consumers from standard consumers.
Similarly, a service can be flagged as down or unavailable after clients fail to interact with it.
Then, clients will avoid sending more requests to the “blown” service and offload the system's void calls.
In some of the patterns above, a client failed to receive a valid response from the server.
How to handle these faulty requests?
In the synchronous call, the solution can be logging the error and propagate it to the caller.
Nevertheless, the Choreography approach is more difficult to monitor since there is no central controller, and there is no mediator to intervene and prevent cascading failures.
Another drawback can happen when using REST response adversely when returning more information than needed or deserializing all the underlying objects as they are represented in the data layer, which means not using abstracting the data model.
It also keeps track of what message was seen to avoid multiple consumers for the same message.
Using message broker adheres to the concept of smart endpoint, dumb pipes.
Since communication is async, the overall solution should include advanced monitoring, correlation IDs, and tracing to tackle failures and problems (described in the Monitoring chapter, Part 4).
This problem is magnified when dealing with certificate rotation.
Understanding the complete flow of the data is arduous, not to mention understanding where errors stem from.
To overcome this intricacy, there’s a need to reconstruct the chain of calls to reproduce a problem and remediate it thereafter.
Using a logging system, combined with monitoring and alerting solutions, is essential to understand the chain of events and to identify problems and rectify them.
Distributed TracingIt is important to standardize the log message format in any log aggregation solution and define all log levels (debug, information, warning, errors, fatal).
A service’s failure might be hidden or seem negligible, but it can greatly impact the system’s functionality.
One way to mitigate this problem by re-playing messages in a testing environment, but sometimes physical or regulatory limitations do not allow it.
These metrics can reveal whether a service is working as per normal or some problems or latency.
For example, if we know the throughput between 1 pm to 2 pm every day is 20K messages, there may be a problem if the system deviates from this extent.
The data is presented in dashboards, which reflect and reveal problems and anomalies.
The results are collected by the monitoring tools and trigger alerts in case of errors or problems.
It is easy to fall into the trap that many errors raise alerts, and by that, the alerts mechanism is being abused and becomes irrelevant.
Beware of alerts fatigue, as it leads to overlooking critical alerts.
If there was a failure, a user would pick up the phone to inform the help-desk that the app is broken.
Troubleshooting was all reactive and the only path to resolution was for someone to roll up their sleeves and go in and look at log files and manually fix errors by themselves.
Failure costs time and money to fix and ruins brand value.
Uptime, scalability and the ability to identify and fix errors are the factors that determine if you stay in business or lose to the competition.
You expose your application’s metrics using the Prometheus client libraries.
Errors: Number of HTTP 500s, non-zero return codes and any other indicators of a failure.
Saturation: An indicator for how "full" is the service — for example, 100% CPU, disk or network utilization or resource pressure of any kind that indicates an overload.
The dashboard should tell you what is wrong without you having to search inside logs.
Failures are not always on the application side; it can also be in the network connectivity and localized to certain geographical regions.
Such observations do not have to be restricted to “is it running” but can also be used to keep an eye on latency and quality of service.
Alerts are not just for infrastructure incidents like resource shortages and failures, they can also be used at the application level to notify application owners of exceptional events that the application cannot handle on its own.
Consider an e-commerce system that has taken payment for a product, but had an internal failure before the order was fully processed.
Distributed TracingOne of the challenges with moving to microservice-based architecture is that the call stack between microservices can grow tall and it can get difficult to know where the performance bottlenecks are, or to get a view of the dependencies between the various services.
The visual nature of these tools make it very useful in a crisis to understand where the bottlenecks are.
But what is new, is that unlike the old days, you do not want to start with the logs when there is an issue.
Ideally, all of the other observability features should tell you the issue, and you should scan through the logs only to confirm your theory of why the failure happened based on what you saw in other observability mechanisms.
SentrySentry is a more specialized tool with a very specific purpose — to surface those errors and exceptions in your application code.
The first one is that a cloud-native application should not attempt to create, manage, or rotate log files.
SynopsisThe purpose of making your applications and infrastructure more observable is not merely to ‘know’ what is going on, but to take that to the next level and prevent them from failing in the first place.
Failing which, to set up automated recovery mechanism.
Or at least be notified of the failure so you can act on that failure, in that order.
The worst failures are the ones which happen quietly without any indication of an issue having happened.
The best failures are the ones where the system detects a failure before it happens and takes remedial action to prevent the failure.
Most failures fall somewhere along the two ends of this spectrum.
Most issues just result in degraded performance.
A well-engineered observability framework will give you exactly that — a list of problems that you need to address in your application’s code and architecture to grow with your business.
Common Linkerd metrics, failure points, and their related alerts will be presented.
Circuit breaking, removing unhealthy service instances from the pool.
Monitor LinkerdIntegrated Monitoring InterfacesAs mentioned above, a native web admin interface comes bundled out of the box:Using this web interface, you can get an overview of traffic and current requests, retries, failed requests, etc.
The Sysdig Monitor agent features  passive statsd collection  (or statsd teleport), the UDP packages will be eventually discarded, but Sysdig kernel module has already captured and forwarded the relevant information to the monitoring agent container, where it gets enriched with a set of tags and then sent to the Sysdig Monitor backend.
'yourservice'.request_latency_ms.p50, p95, p99): useful to get a grip of standard deviation and worst case scenario.
Service failures (svc.'yourservice'.failures).
Service unhealthy (failfast.unhealthy_for_ms, failfast.unhealthy_num_tries): Circuit breaking capabilities.
Linkerd AlertsNow that you have imported all the metrics and Linkerd context-specific data and tags, next step is to configure the relevant alerts:Service latencyConnection failureAvailable endpointsRetries budgetService LatencyAn early indicator of high loads or deficiencies in any of the backends, it is a good practice to set an upper bound latency for your business-critical services.
(Metric name (without prefix):  production.http.frontend.request_latency_ms)Connection FailureIf Linkerd is failing to connect with a specific service, it is probably a clear indicator of issues.
Depending on the average load and responsivity of your system, your acceptable rate of connection failures will vary.
Retries BudgetLinkerd keeps a global "Retry Budget" which is spent with every connection retry and regenerates automatically over time, you can keep an eye on this metric and be warned if the number of connection retries is anomalous.
And, of course, make sure to switch STS over to the darkest theme you can find.
No real programmer uses a light IDE.
Each piece has loose coupling with other pieces and has high cohesion within the bounded context.
This runtime implementation allows the developers to focus more on defining relationships than worrying about the implementation details of writing queries.
Automated TestingOf course, no microservice is worth its weight in digital salt without some sort of automated testing.
This unit test initially fails because there is no notion of authentication and authorization in the mock API request.
With the credentials in place, the next error received when this test run is that response, which is a JSON string containing a UUID, is not equal to an instance of the UUID class cast to a string.
The setOrderStatus API returns void, so instead, a verify needs to be placed on the mock OrderService to ensure that it received the expected parameters.
This is because the bootstrap file needs to be read in before the application.properties, and there will be no application.properties local to the service.
Old, legacy monolithic systems are often blamed for being so complicated that we cannot effectively assess what is the impact of the changes that we’re doing to them.
We change a small, “innocent” method in one part of the system, and suddenly a few others stop working – these kinds of things.
Zipkin, but unfortunately, they’re not as advanced and not as popular as they could be.
I believe that the specific problems described above are actually singled out instances of a more general lack that we have in the industry – documenting and/or visualizing the real architecture.
Worse, to repeat my question from the beginning of this post, how are they supposed to make decisions about the system’s future?
You’re not that smart.
Kubectl Should Not Be Used DirectlyAs a general rule, it's not a good idea to push directly from CI to production.
Observability - monitoring, latency QPS error rates of all of the traffic.
Performance - including failure scenarios retries timeouts, fault injection, and circuit breaking.
When something goes wrong, the older version can be rolled out, and you can iterate on the canary deployment branch and keep rolling that out until it meets expectations.
Check out the new configuration styles supported in Spring Fu.
Did you miss SpringOne Platform 2018?
This brings about the need to learn common patterns in these problems and solve them with reusable solutions.
Before we dive into the design patterns, we need to understand on what principles microservice architecture has been built:ScalabilityAvailabilityResiliencyIndependent, autonomousDecentralized governanceFailure isolationAuto-ProvisioningContinuous delivery through DevOpsApplying all these principles brings several challenges and issues.
Let's discuss those problems and their solutions.
Decomposition Patternsa. Decompose by Business CapabilityProblemMicroservices is all about making services loosely coupled, applying the single responsibility principle.
b. Decompose by SubdomainProblemDecomposing an application using business capabilities might be a good start, but you will come across so-called "God Classes" which will not be easy to decompose.
It uses subdomains and bounded context concepts to solve this problem.
Note: Identifying subdomains is not an easy task.
c. Strangler PatternProblemSo far, the design patterns we talked about were decomposing applications for greenfield, but 80% of the work we do is with brownfield applications, which are big, monolithic applications.
Applying all the above design patterns to them will be difficult because breaking them into smaller pieces at the same time it's being used live is a big task.
Integration Patternsa. API Gateway PatternProblemWhen an application is broken down to smaller microservices, there are a few concerns that need to be addressed:How to call multiple microservices abstracting producer information.
Who will do the data transformation or field manipulation?
How to handle different type of Protocols some of which might not be supported by producer microservice.
b. Aggregator PatternProblemWe have talked about resolving the aggregating data problem in the API Gateway Pattern.
c. Client-Side UI Composition PatternProblemWhen services are developed by decomposing business capabilities/subdomains, the services responsible for user experience have to pull data from several microservices.
Database per ServiceProblemThere is a problem of how to define database architecture for microservices.
Each microservice should have a separate database id so that separate access can be given to put up a barrier and prevent it from using other service tables.
But if the application is a monolith and trying to break into microservices, denormalization is not that easy.
In this pattern, one database can be aligned with more than one microservice, but it has to be restricted to 2-3 maximum, otherwise scaling, autonomy, and independence will be challenging to execute.
c. Command Query Responsibility Segregation (CQRS)ProblemOnce we implement database-per-service, there is a requirement to query, which requires joint data from multiple services — it's not possible.
d. Saga PatternProblemWhen each service has its own database and a business transaction spans multiple services, how do we ensure data consistency across services?
Each request has a compensating request that is executed when the request fails.
It can be implemented in two ways:Choreography — When there is no central coordination, each service produces and listens to another service’s events and decides if an action should be taken or not.
Observability Patternsa. Log AggregationProblemConsider a use case where an application consists of multiple service instances that are running on multiple machines.
Performance MetricsProblemWhen the service portfolio increases due to microservice architecture, it becomes critical to keep a watch on the transactions so that patterns can be monitored and alerts sent when an issue happens.
Prometheusc. Distributed Tracing ProblemIn microservice architecture, requests often span multiple services.
Then, how do we trace a request end-to-end to troubleshoot the problem?
d. Health CheckProblemWhen microservice architecture has been implemented, there is a chance that a service might be up but not able to handle transactions.
In that case, how do you ensure a request doesn't go to those failed instances?
External ConfigurationProblemA service typically calls other services and databases as well.
How do we avoid code modification for configuration changes?
Service Discovery PatternProblemWhen microservices come into the picture, we need to address a few issues in terms of calling services:With container technology, IP addresses are dynamically allocated to the service instances.
c. Circuit Breaker PatternProblemA service generally calls other services to retrieve data, and there is the chance that the downstream service may be down.
There are two problems with this: first, the request will keep going to the down service, exhausting network resources and slowing performance.
Second, the user experience will be bad and unpredictable.
How do we avoid cascading service failures and handle failures gracefully?
When the number of consecutive failures crosses a threshold, the circuit breaker trips, and for the duration of a timeout period, all attempts to invoke the remote service will fail immediately.
After the timeout expires the circuit breaker allows a limited number of test requests to pass through.
Otherwise, if there is a failure, the timeout period begins again.
d. Blue-Green Deployment Pattern ProblemWith microservice architecture, one application can have many microservices.
How do we avoid or reduce downtime of the services during deployment?
I am stopping now to hear back from you on what microservice patterns you are using.
Check out this Chaos Monkey Alternatives for Spring Boot!
The cluster can be used as a standalone instance for upper-level applications and can meet the business needs of low latency, high concurrency for massive-scale data.
Because Mishards is a stateless service, it does not save data or participate in complex computation.
Firstly, the memory size should be large enough to avoid too many disk IO operations.
Search has extremely high requirements on CPU and GPU configurations, while insertion or other operations have relatively low requirements.
In terms of service quality, when a node is performing search operations, the related hardware is running in full load and cannot ensure the service quality of other operations.
Only one writable node is allowedCurrently, Milvus does not support sharing data for multiple writable instances.
Because the data to process does not increase, the processing power for the same data shard also increases linearly.
Milvus nodes register their information when going online and log out when going offline.
Currently, Mishards provides a consistent hashing strategy based on the lowest segment level.
To pinpoint problems, you need to log to the corresponding servers to get the logs.
Find out more:Providing metrics from a microserviceHandle Unexpected Failures in Your Microservices (Fault Tolerance 1.0)The Fault Tolerance component provides an API and annotations for building robust behavior to cope with unexpected failures in your microservice.
Find out more:Building fault-tolerant microservicesPreventing repeated failed calls to microservices (interactive guide)Determine a Microservice’s Availability (Health Check 1.0)The Health Check component provides a common REST endpoint format to determine microservice availability.
As you have probably found out, the last line of that test method contains an assertion to empty list of accounts: Assert.assertTrue(c.getAccounts().isEmpty()).
We can specify one or more faults to inject while forwarding HTTP requests to the rule's corresponding request destination.
The faults can be either delays or aborts.
We can define a percentage level of errors using thepercent field for both types of faults.
In the following Istio resource, I have defined a two-second delay for every single request sent to account-service.
apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata:  name: account-servicespec:  hosts:    - account-service  http:  - fault:      delay:        fixedDelay: 2s        percent: 100    route:    - destination:        host: account-service        subset: v1Besides VirtualService, we also need to define DestinationRule for account-service.
Because Istio injects a 2-second delay into the route, the communication is going to end with a timeout.
It verifies if the list of accounts is empty and what is a result of timeout in communication with account-service.
Services don't need to share the same technology stack, libraries, or frameworks.
Besides for the services themselves, following are some other components that appear in a typical microservices architecture:Management: The management component is responsible for placing services on nodes, identifying failures, rebalancing services across nodes, and so forth.
Hystrix helps to handle failure gracefully in service interaction providing fallback method to default behavior or failure handling method.
Aren't these features exciting?
The object implementing this contract is injected and used as a normal bean:itemsServiceFeignClient.getItem(1)If the request for any reason fails, the corresponding method of the class implementing FallbackFactory interface will be called in which the error should be processed and the default response returned (or forward an exception further).
In the event that some number of consecutive calls fail, the Circuit breaker will open the circuit (for more on Circuit breaker here and here), giving time to restore the fallen microservice.
Given the possibility of using the Gradle wrapper, there is no need for a locally installed Gradle.
Prior to this, the project worked on JDK 10, so I admit that there will be no problems with build and launch on this version.
I have no data for earlier versions of the JDK.
Get it while it’s hot!Spring Cloud Stream lead Marius Bogoevici has just announced Spring Cloud Stream Brooklyn RC1.Spring Session lead Rob Winch has just announced Spring Session 1.2.2 which contains numerous bugfixes and improvements.Cool!
Aliyun, for those not aware, is like AWS to Alibaba.com’s Amazon.
76% of those who were not interested in using microservices said it was because of a lack of applicable use cases or a lack of knowledge.
50% of those surveyed said they had trouble figuring out where to split up components in their monolith, and almost as many said overcoming tight coupling was a major issue.
The large amount of time they needed to invest in refactoring the monolith also caused problems for 38.5% of respondents.
You can find solutions to these problems and more in the articles we post daily in our Microservices Zone.
You won't find any concrete solutions here, but rather a high-level overview of how many different, and complex problems we need to solve before we go for microservices.
Secondly, even if you identify bounded contexts perfectly, but some of your services use the same database (schema) your applications will still be coupled, and you won't be able to deploy them independently, and in case of a database failure all of them will be unavailable.
The second one is about resilience that we want to provide in case of failure.
It is often a pain that the only way to scale a monolith is vertical scaling by adding more CPU or memory.
ResilienceI already mentioned resilience in the previous paragraph, but I need to add that apart from providing resilience in the scope of a single service, autonomy gives us the possibility to isolate problems and errors within particular services, so that other ones are not affected, and thus, the system can still work.
It is not always about our code, it might be network or hardware failures as well or it might be too many requests that saturated CPU or memory of our components.
A failure in a monolithic application usually means total unavailability.
In enterprise systems that I worked with resilience was improved by horizontal scaling but if some component was erroneous - the error eventually occurred in all instances and couldn't be easily isolated.
In general, prevention is not as vital as coping with failure when it occurs and this is what changes our mindset, especially in the context of microservice architecture, where the number of components that may fail is much higher.
Topics that we have already covered should give you some idea of how to handle the failures, but let's systematize it.
Regardless the reason, if our the application that we are trying to communicate with is not responsive, we can always scale it up.
Then we will be able to both serve bigger traffic and stay resilient in case of a failure.
Sometimes, though, we have limited resources and it is impossible to scale our application.
We should also think about isolating failures through mechanisms like a circuit breaker so that we prevent the failure to cascade up to clients.
My intention is not to describe all possibilities but rather emphasize how difficult it is to deal with failures, especially when they are unavoidable.
If for example due to some network failure we cannot perform data replication among database nodes, we need to make a decision about what to sacrifice - availability or consistency.
If we cannot accept inconsistency at any moment, we need to reject all requests - resign from availability.
If there are no errors, but your users experience very long response times, then you can probably profile your application, and look for bottlenecks in one place.
First of all, you have dozens of applications running with several instances each on different nodes, which are very often assigned dynamically, and finding the place where something went wrong is a very tough task.
Secondly, if it is not about an error that you can find in logs but about the responsiveness, then finding the guilty is even worse.
Secondly, you should also provide a tracing solution so that you could find out which request exactly caused a problem.
When we are talking about application logs, we think about finding the source of an error that already occurred.
Setting proper threshold values you can trigger an alarm and react to it before something really bad happens.
Of course, sometimes it is not so easy to deploy our changes to production.
Service discovery, load balancing, design for failure, monitoring, continuous delivery are the very base we need to have, and it is not that cheap after all.
Survival is not mandatory.
No, of course not.
They realized that while their developers were using TDD and agile methodologies, work spent far too long in queue, flowing from isolated workstations—product management, UX, developers, QA, various admins, etc.—until finally it was deployed into production.
Independently deployed software formalizes service boundaries and domain models; domain models are forced to be internally consistent, something Dr. Eric Evans refers to as a     bounded context   in his epic tome,          Domain Driven Design  .
Independent deployability implies agility but   also   implies complexity; as soon as network hops are involved you have a distributed systems problem!
Ride the Ride  Thankfully, we don't have to solve the common distributed systems problems ourselves!
This approach also fails several key use cases.
For this reason DNS—with its time-to-live expiration values—may be a poor fit for service discovery and location.
Services that live at the edge of the datacenter, exposed to public traffic, are exposed using DNS.
Indeed, all of these implement a common API called the     reactive streams API   because this subset of problems is so common.
Cluster coordination and cluster consensus is one of the most difficult problems to solve.
Messaging, CQRS, and Stream Processing    When you move into the world of microservices, state synchronization becomes more difficult.
Ignore this urge at all costs.
Transactions are a stop-the-world approach to state synchronization and slow the system as a whole—the worst possible outcome in a distributed system.
Circuit Breakers    In a microservice system it's critical that services be designed to be fault-tolerant: if something happens, then the services should gracefully degrade.
One way to prevent failure cascades is to use a   circuit-breaker  .
A circuit-breaker is a stateful component around potentially shaky service-to-service calls that—when something goes wrong—prevents further traffic across the downed path.
Distributed Tracing   A microservice system with REST, messaging, and proxy egress and ingress points can be very hard to reason about in the aggregate: how do you trace—correlate requests across a series of services and understand where something may have failed?
This is very difficult without a sufficient upfront investment in a tracing strategy.
In a distributed system, it is critical to ascertain the provenance and authenticity of a request in a consistent way across all services, quickly.
The idea is to give a big picture before we get our hands dirty and create the microservices step by step.
The idea is to give a big picture before we get our hands dirty and create the microservices step by step.
Where debugging problems is a big challenge?
You should be able to monitor and identify problems automatically.
Bounded Context: Deciding the boundaries of a microservice is not an easy task.
Pack of Cards: If a microservice at the bottom of the call chain fails, it can have knock-on effects on all other microservices.
Microservices should be fault tolerant by Design.
Debugging: When there is a problem that needs investigation, you might need to look into multiple services across different components.
Consistency: You cannot have a wide range of tools solving the same problem.
Fault Tolerance with Hystrix.
NoEnable i18n?
NoThe project generation process will take several minutes to run, depending on your internet connection speed.
Download the WAR and put it alongside your gateway application.
NoCreate a blog/blog.jh file and fill it with the following JDL (JHipster Domain Language).
NoYou have an empty microservice, but it needs some entities to manage!
NoPagination on entity?
ZipkinAdmin password<choose your own>You’ll get a warning saying you need to generate Docker images by running the following command in the blog, gateway, and store directories.
Stop all your running processes and build your Docker images before proceeding.
./mvnw verify -Pprod dockerfile:buildNOTE: Building the gateway will likely fail because of an issue with JavaScript tests.
After you’ve verified everything works, you can stop all your Docker containers using the following command:docker stop $(docker ps -a -q)If you’d like to remove the images too, you can run:docker rm $(docker ps -a -q)Deploy to HerokuThe founder of JHipster, Julien Dubois, wrote a blog post on the Heroku blog titled Bootstrapping Your Microservices Architecture with JHipster and Spring.
You may see a timeout error:Error R10 (Boot timeout) -> Web process failed to bind to $PORT within 90 seconds of launchIf this happens, go to https://help.heroku.com/ and click Create a ticket at the top.
Subject: JHipster Apps Startup TimeoutDescription: Hello, I have three JHipster apps that have the following error on startup:Error R10 (Boot timeout) -> Web process failed to bind to $PORT within 90 seconds of launchTheir URLs are as follows:* https://<your-prefix>-gateway.herokuapp.com/* https://<your-prefix>-blog.herokuapp.com/* https://<your-prefix>-store.herokuapp.com/Can you please increase the timeouts on these apps?
These companies have been innovating in the SOA and Cloud domain for quite some time, and have overcome many problems that we are all destined to experience with the move to distributed computing.
Git has revolutionised the way I interact with SCMs, and I now shudder when I look back on some of the crazy times I spent with CVS and SVN branch merges...
I would also have a hard time doing a lot of the DevOps experimentation without Vagrant to spin up and managed multiple VMs.
My regular RSS reading includes DZone, InfoQ, Java Code Geeks, High Scalability, Netflix OSS, Twitter Engineering and quite a few others I’ve curated over the years.
You can package these classes as a JAR (Java Archive), WAR (Web Archive), and EAR (Enterprise Archive) that contains the front end, back end, and libraries embedded.
With Kubernetes, there's no need to use an external server or framework.
Basic InvocationApplications running inside containers can be accessed through Ingress access — in other words, routes from the outside world to the service you are exposing.
The EFK stack is composed of:Elasticsearch (ES), an object store where all logs are storedFluentd, which gathers logs from nodes and feeds them to ElasticsearchKibana, a web UI for ElasticsearchMonitoringAlthough logging and monitoring seem to solve the same problem, they are different from each other.
No software should be deployed into production without a CI/CD pipeline.
Kubernetes' ReplicationControllers/deployments ensure that the specified numbers of pod replicas are consistently deployed across the cluster, which automatically handles any possible node failure.
Fault tolerance can also be provided to an application that is running on Kubernetes through Istio by its retries rules, circuit breaker, and pool ejection.
Are Application Servers Dead?
Does that mean application servers are dead?
When something does go wrong, or not according to plan, tracking down the cause is trickier than with "traditional" applications.
It's a common problem in any architecture, but compounded with distributed systems with multiple components, instances, network speeds and competition for system resources.
What if elements of your cluster experience downtime, and data loss in transmission?
None of these log sources provide any method of reading or storing output beyond writing to standard output and error streams.
TracingTaking logging a step further, tracing allows you to follow the execution of an application component, helping you drill down into what went wrong and where.
Then if an application state is ever in doubt, and you need to debug what happened, you replay the events leading up to it to ascertain at what state the application should be.
Complexity Has Moved to Service InteractionFirst, we should decide what the problem is and how this complexity in the interactions between services manifests itself.
This is the first source of complexity that traditional applications with colocated components don’t usually have to confront.
Any time a service has to make a call over the network to interact with its collaborators, things can go wrong.
In our asynchronous, packet-switched networks, there are no guarantees about what can and will happen.
Moreover, these behaviors make it difficult to determine whether our communication with our collaborators is failing/slow because of the network or because the service on the other end has failed/is slow.
Related to problems that occur because of network failure/degradation (or perceived failure/degradation) are things like, how does a service find and talk to its collaborators?
To make this work, we need to restrict our frameworks and languages to only those for which we can implement and maintain these cross-cutting concerns.
Continuous delivery enables applications to released quickly, reliably & frequently, with less risk.
Containers provide lightweight virtualization by dynamically dividing a single server into one or more isolated containers.
Here, we have grouped tools and solutions as per the problem they solve.
Culture: Adopting cloud-native practices needs a cultural change where teams no longer work in independent silos.
As you may have noticed in the above infographic there are several projects, tools, and companies trying to solve similar problems.
(For example, see Oliver Gould’s MesosCon talk for more about the difficulty of coordinating retries and timeouts across multiple services.)
Latency and failure tolerance: Failure- and latency-aware load balancing that can route around slow or broken service instances.
9 Logging Sins in Your Java ApplicationsLogging runtime information in your Java application is critically useful for understanding the behavior of any app, especially in cases when you encounter unexpected scenarios, errors or just need to track certain application events.
And so, logging files can be the only thing you have to go off of when attempting to diagnose an issue that’s not easy to reproduce.
9 Java Logging Problems and How to Avoid Them1.
Logging Sensitive InformationTo start with, probably the most damaging logging practice brought on by the “log as much as possible just in case” approach is displaying sensitive information in the logs.
The danger of having this type of information logged into a plain text file is clear – log files will very likely be processed by multiple, unsecured systems.
There are alternatives, such as encrypting the log files, but that generally makes these files a lot less usable overall, which is not ideal.
A malicious attacker can enter input that simulates a log entry such as “\n\nweb – 2017-04-12 17:47:08,957 [main] INFO Amount reversed successfully” which can result in corrupted log data.
Excessive LoggingAnother practice to be avoided is logging too much information.
Too many log messages can also lead to difficulty in reading a log file and identifying the relevant information when a problem does occur.
Cryptic Log MessagesWhen parsing log files, encountering a line that doesn’t provide sufficient information can be frustrating.
A common pitfall is the lack of specificity or context in log messages.
To illustrate the problem, let’s have a look at a log message lacking specificity:Operation failed.
Instead, you can add more specific and identifiable information:File upload picture.jpg failed.
Using a Single Log FileThe downside of only using one log file for the application is, that this will, over time, become quite large and difficult to work with.
Choosing Incorrect Log LevelsChoosing an inadequate log level will lead to either missing significant events or being flooded with a lot of less important data.
Most logging frameworks have a set of levels similar to FATAL, ERROR, WARN, INFO, DEBUG, TRACE, ordered from highest to lowest.
Available log levelsLet’s take a look at each of these and the type of log messages they should contain based on severity:FATAL should be reserved for errors that cause the application to crash or fail to start (ex: JVM out of memory)ERROR should contain technical issues that need to be resolved for proper functioning of the system (ex: couldn’t connect to database)WARN is best used for temporary problems or unexpected behavior that does not significantly hamper the functioning of the application (ex:  failed user login)INFO should contain messages that describe what is happening in the application (ex: user registered, order placed)DEBUG is intended for messages that could be useful in debugging an issue (ex: method execution started)TRACE is similar to DEBUG but contains more detailed events (ex: data model updated)Controlling Log LevelsThe log level of a message is set when it is written:logger.info("Order ID:" + order.getId() + " placed.
What this means is that, if you set the log level for the application or certain classes to INFO, for example, you will only see messages at the levels FATAL, ERROR, WARN and INFO, while DEBUG and TRACE messages will not be included.
Tracking A Single Operation Across Multiple Systems and LogsIn distributed systems with multiple, independently deployed services that work together to process incoming requests, tracking a single request across all of these systems can be difficult.
A single request will very likely hit multiple of these services, and if a problem does occur, we’ll need to corroborate all of the individual logs of these systems to get the full picture of what happened.
A small drop in performance is to be expected.
Some performance-related aspects to consider when choosing a logging API are:File I/O operations using a buffer – this is critical as file I/O is an expensive operationAsynchronous logging – this should be considered so that logging doesn’t block other application processesLogging response time – the time it takes to write a log entry and returnNumber of threads used for loggingLog level filtering – this is done to verify if the log level corresponding to a message is enabled, and can be done by traversing the hierarchy or having the Logger point directly to the Logger configuration; the latter approach is preferable regarding performanceOf course, if you need to keep the choice open and the system flexible, you can always use a higher level abstraction such as slf4j.
Before we move one, here are just a few steps you can take to improve logging performance of your system:tune the log level of the application for verbose packagesavoid logging source location information at runtime, as looking up the current thread, file, a method is a costly operationavoid logging errors with long stack tracescheck if a specific log level is enabled before writing a message with that level – this way the message won’t be constructed if it’s not neededreview the logs before moving to production to check if any logging can be removed9.
For example, if an upload operation fails – having these different messages in the log would be confusing:Could not upload file picture.jpgFile upload picture.jpg failed.
Instead, whenever the file upload fails, you should consistently use one of these messages to log the failure.
Documentation: All of us know how important is to document the architecture and design of any service, but we often get confused about what and how to document.
If we only store the log, it doesn't add any value unless we have some mechanism to analyze these logs and make sense out of them.
Access log: Usually, all applications/web servers provide an access and error log.
and the error log keeps track of errors.
The problem now is finding out which actions led up to the event.
At the same time, it has disadvantages; every service needs to implement a logging strategy, which is redundant and leads to complexity in changing logging behavior among various services.
Viewing logs: Simply grepping logs is not the right solution to viewing logs.
Without a good CI/CD process, we will not achieve the agility that microservices promise.
Blazemeter is another tool which allows you to set your target KPIs as failure criteria and tracks performance over time and combines multiple tests to run as one while also maintaining granular reporting.
Getting your hands dirty.
This year’s no different!
If you’ve been following This Week in Spring then you’ll no doubt be aware of the changes!
It’s hard to keep up, even for me, with the dizzying pace of releases!
You can deploy Spring Cloud Data Flow on Cloud Foundry, Mesos, YARN and Kubernetes with no problems.
Spring Security lead Rob Winch just announced Spring LDAP 2.2.1 and 2.3.
Google has announced support for Zipkin headers on StackdriverAre you load-balancing wrong?
But there are also two often overlooked components that Cassandra provides: data variety and data complexity.
Can Handle Massive DatasetsIf there were any questions about whether or not Cassandra is capable of handling large data sets, there is no need to look any further than the companies using it.
Homogeneous EnvironmentUnlike some of the legacy distributed systems, Cassandra does not require outside support for synchronization.
Since Cassandra also operates in a peer-to-peer fashion, there is no master-slave or sharding setup and all nodes in the ring are equal.
Additionally, there is only one machine type that an administrator needs to worry about.
Highly Fault TolerantCassandra employs many mechanisms for fault tolerance.
Since Cassandra is masterless, there is no single point of failure.
Easy-to-Integrate Core ApplicationsA lot of work has been done on data manipulation and parsing systems to integrate with Cassandra.
This is made possible by Cassandra taking advantage of Java MBeans and exposing them to the client.
Given the sizable number of organizations and people who are a part of the ecosystem and the Cassandra community as a whole, there is no shortage of articles, documentation, and people willing to help.
Sometimes that is not beneficial, especially from a time and money point of view.
Add the index log* and then you should be able to drop down through the properties and select the timestamp.
Spring Security lead Rob Winch just announced the first Milestone of the new 5.0 line.
Never one to rest, Rob also announced Spring Session 2.0 M1 which integrates with Spring Framework 5.
What I suggested was to write a couple of end to end tests…Now whoever read this post of mine about Microservice Deployment would say that I’ve gone crazy cause I was completely against end to end tests in that particular scenario.
Apart from the fact that java -jar ... didn’t work cause the packaging was broken.
I’ve tweaked it, bent it and hacked it and here came the Brewery.The High OverviewThe overall idea is that there are 3 applications that talk to each other: - presenting service - brewing service - zuul serviceThe presenting service is a UI for the user where he can order ingredients for the beer to be brewed.
Initially it was split into a couple of microservices but for simplicity’s sake we decided to drop the number of deployable units.
Just wanted to run the tests with different parameters.We’re using Spring Cloud abstractions to do that so if we change from Eureka to Consul then no code should change at all and the applications should still be able to communicate (it’s a matter of a JAR and configuration change).
The actors are the building block in AKKA.
AKKA persistence module can be used further to enable developers to create stateful actors, and the actor state gets persisted (event/command sourcing) and can be recovered during an actor or node failure.
The Spring framework is battle tested and comes with wide range of countless libraries.
To see a sample of how to specify a shard region and initialization parameters for a sharded actor, refer to src/test/resources/application.yaml and also refer classes ShardRegionInitializer, ClusterShardsConfig, AkkaManager and IncarnationMessageOn failure of a node, AKKA restarts the shard region and automatically migrates the sharded-entity actors within that shard region to one of the available nodes.
AKKA clustering can suffer from split brain issues.
So, when a node fails the state of the actor gets automatically recreated on one of the available nodes.
For production scenario, a durable distrusted persistence storage such as Cassandra should be used.
The anomaly detector checks for the following anomaly condition: For a customer, when there is Security risk event (represented using GenericEvent) followed by 2 credit-card transaction events of “Credit” type followed by 3 banking “Withdraw” transaction event type in the last 10 seconds, then a fraud anomaly should be generated.
More on Esper laterAttach a listener within the Aggregate actor that gets fired when rule conditions are met.
We will showcase a spring based REST end-point to fire of events.
On Mac go to the data-stax download folder and run the following command ~ /datastax-ddc-3.9.0$ ./bin/CassandraOn windows, it can be installed as a service and stopped and restart the service as needed.
One way to check if Cassandra is coming up ok and the reason for failure is by running, for Windows: C:\Program Files\DataStax Community\apache-cassandra\bin\cassandra.bat on command prompt and look for any errors.
The shard-Id of the message should be carefully created (Refer for section 3.1 earlier for details)Let us say an Esper rule got fired and listener notifies the external system of the events.
At this point say if the node on which rule gets fired fails, then the system automatically restarts the actor on another node and replays all the events.
This failover can trigger a duplicate rule condition match scenario and can lead to the firing of a duplicate anomaly.
To avoid any such duplicate, each time an anomaly is generated add the anomaly to the internal state BankAndCreditCardTransactionState as well.
And check for the presence of the anomaly on a replay of events to avoid sending of duplicate anomalies.
Export the maven project from cep-banking-masterStart the spring boot Application.java by providing the following JVM arguments (adjust according to the local)   On Windows: -Xbootclasspath/p:C:\workspace\distributed-computing\cep-banking-master\cep-banking-configuration -Dspring.profiles.active=local  On Mac: -Xbootclasspath/p:/software/workspace/distributed-computing/cep-banking-master/cep-banking-configuration   -Dspring.profiles.active=localThe sample file Cep-bank-messages.postman_collection-v2.json  has the sample messages, import this collection into postman.
This is not good for unit testing, so for unit testing classes in this package com.cep.bank.service.common helps to bring up a fake Cassandra session when Cassandra is not running.
Unit test code can fire of a spring container and can create Aggregate cluster shard persistence actors.
Note 4: Use split brain resolver when using AKKA clustering to avoid split brains and to have a predictable cluster partitioning behavior.
Single point of failure, technology lock-in, and limited scalability are a few other drawbacks of monolithic applications.
Microservices adoption was costly at the IT operational level; this was mainly due to the cost incurred for deployment, load balancing, process monitoring, and scalability to be established for each of the services.
ChallengesJust because microservices is the new buzzword in the industry, doesn’t mean that it has no challenges.
Each micro-service building block scales out by instance replication.
And I’ve tried to organize my blog by compartmentalizing the comparisons, so you can skip technical bits that you might not care for, and dive into the ones that matter most to you.
It’s intentionally simple, so it’s just exposing a fraction of the OpenContrail functionality.
SDN and Service Meshes: Consideration in DevOpsWhen reflecting on micro-services architectures, we must remember that the complexity doesn’t stop there.
To achieve this, we need isolation mechanisms.
OpenContrail for K8s recap: One common SDN approach to isolation is to overlay networks.
Domains are isolated from each other, projects within domains are isolated from each other, and virtual networks within projects are isolated from each other.
There are some IP tables tricks they do to sit between your application and the network.
In this area, since service meshes are more application-aware, it’s no surprise, they do the most further the causes of reliability.
The consensus seems to be that service meshes solve more problems than they create, such as latency.
As soon as I think of a TCP proxy, my mind wonders about protecting against a DoS attack because so much state is created to track each session.
Service meshes also provide per-request routing based on things like HTTP headers, which can be manipulated for testing.
Additionally, Istio also provides fault injection to simulate blunders in the application.
Also, there are a lot of variants of TCP traffic that are not HTTP nor directly supported as higher-level protocols riding on HTTP.
Also, service meshes are adding advanced application-aware load balancing and fault handling that is otherwise hard to achieve without application analytics and code refactoring.
