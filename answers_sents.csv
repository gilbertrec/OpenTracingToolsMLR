toolname,answer_id,question_id,comment_count,creation_date,is_accepted,last_activity_date,owner_reputation,owner_id,score,sentence
AppDynamics,13532221,13524580,1,"2012/11/23, 17:48:01",False,"2012/11/23, 17:48:01",17.0,1847969.0,0,"I suggest to look into Gartners magic quadrant and get dynaTrace since it has negligible overhead , less than 1% in production under load."
AppDynamics,13570380,13524580,1,"2012/11/26, 19:52:11",True,"2012/11/26, 19:52:11",74.0,1854219.0,5,Full Disclosure: I currently work for AppDynamics.
AppDynamics,13570380,13524580,1,"2012/11/26, 19:52:11",True,"2012/11/26, 19:52:11",74.0,1854219.0,5,AppDynamics was designed from the ground up for high volume production environments but works equally well in both prod and non-prod.
AppDynamics,13570380,13524580,1,"2012/11/26, 19:52:11",True,"2012/11/26, 19:52:11",74.0,1854219.0,5,"It's currently running in production in some of the worlds largest mission critical application environments at Netflix, Exact Target, Edmunds, and many others."
AppDynamics,13570380,13524580,1,"2012/11/26, 19:52:11",True,"2012/11/26, 19:52:11",74.0,1854219.0,5,Here are a few quotes from existing customers…
AppDynamics,13570380,13524580,1,"2012/11/26, 19:52:11",True,"2012/11/26, 19:52:11",74.0,1854219.0,5,"""It's like a profiler that you can run in production"" -- Leonid Igolnik, Taleo"
AppDynamics,13570380,13524580,1,"2012/11/26, 19:52:11",True,"2012/11/26, 19:52:11",74.0,1854219.0,5,"""We found that the overhead was negligible"" -- Jacob Marcus, Care.com"
AppDynamics,13570380,13524580,1,"2012/11/26, 19:52:11",True,"2012/11/26, 19:52:11",74.0,1854219.0,5,"""We wanted a monitoring solution that wouldn't impact our production runway"" -- John Martin, Edmunds"
AppDynamics,13570380,13524580,1,"2012/11/26, 19:52:11",True,"2012/11/26, 19:52:11",74.0,1854219.0,5,AppDynamics overhead is extremely low but I suggest you test it and see for yourself.
AppDynamics,13570380,13524580,1,"2012/11/26, 19:52:11",True,"2012/11/26, 19:52:11",74.0,1854219.0,5,You can download and use it for free from the AppDynamics website.
AppDynamics,13570380,13524580,1,"2012/11/26, 19:52:11",True,"2012/11/26, 19:52:11",74.0,1854219.0,5,Good luck in your search for the right APM tool.
AppDynamics,13806782,13524580,0,"2012/12/10, 20:18:05",False,"2012/12/10, 20:18:05",207.0,153728.0,1,Yes it will if the application is sensitive to extra GC cycles caused by call stack sampling.
AppDynamics,13806782,13524580,0,"2012/12/10, 20:18:05",False,"2012/12/10, 20:18:05",207.0,153728.0,1,The impact will depend on the number of threads and typical call stack depth.
AppDynamics,13806782,13524580,0,"2012/12/10, 20:18:05",False,"2012/12/10, 20:18:05",207.0,153728.0,1,"This is not specific to AppDynamics, other call stack sampling solutions such as NewRelic and VisualVM Sampler will have a similar impact."
AppDynamics,13806782,13524580,0,"2012/12/10, 20:18:05",False,"2012/12/10, 20:18:05",207.0,153728.0,1,http://www.jinspired.com/wp-content/uploads/2011/11/JXInsightOpenCore_Good_APM_vs_AppDynamics_Bad_APM.pdf
AppDynamics,13806782,13524580,0,"2012/12/10, 20:18:05",False,"2012/12/10, 20:18:05",207.0,153728.0,1,http://www.jinspired.com/wp-content/uploads/2011/11/JXInsightOpenCore_vs_AppDynamics.pdf
AppDynamics,13820531,13524580,0,"2012/12/11, 14:32:58",False,"2012/12/11, 14:32:58",207.0,153728.0,4,There are a number of assumptions made by a vendor but the following are most common:
AppDynamics,13820531,13524580,0,"2012/12/11, 14:32:58",False,"2012/12/11, 14:32:58",207.0,153728.0,4,TRANSLATION: We assume you have a slow performing database backend.
AppDynamics,13820531,13524580,0,"2012/12/11, 14:32:58",False,"2012/12/11, 14:32:58",207.0,153728.0,4,TRANSLATION: We assume that you already know your performance hotspots.
AppDynamics,13820531,13524580,0,"2012/12/11, 14:32:58",False,"2012/12/11, 14:32:58",207.0,153728.0,4,TRANSLATION: We assume that you will not notice tricks used to hide our overhead.
AppDynamics,13820531,13524580,0,"2012/12/11, 14:32:58",False,"2012/12/11, 14:32:58",207.0,153728.0,4,TRANSLATION: We assume that you know little about performance engineering.
AppDynamics,13820531,13524580,0,"2012/12/11, 14:32:58",False,"2012/12/11, 14:32:58",207.0,153728.0,4,And my favorite (5) is the restriction within a vendors software license on the publication of benchmark results.
AppDynamics,13820531,13524580,0,"2012/12/11, 14:32:58",False,"2012/12/11, 14:32:58",207.0,153728.0,4,TRANSLATION: We assume that you blindly accept our claims – unquestionably.
AppDynamics,19518863,13524580,0,"2013/10/22, 16:10:28",False,"2013/10/22, 16:10:28",3374.0,2508411.0,1,"Appdynamics will not slow down your system significant, I was on a usermeeting and they said that they always try to be under 2% cpu usage, thats nothing compared to what you get from them."
AppDynamics,19518863,13524580,0,"2013/10/22, 16:10:28",False,"2013/10/22, 16:10:28",3374.0,2508411.0,1,"They are working with samples per time, so if you have 10 requests per second or 100, they will still take the same amout of your cpu / bandwith / whatever."
AppDynamics,17416710,17412297,0,"2013/07/02, 05:56:59",True,"2013/07/02, 05:56:59",415.0,621964.0,6,The way that these products generally work is by doing bytecode injection / function interposition / monkey-patching on commonly used libraries and methods.
AppDynamics,17416710,17412297,0,"2013/07/02, 05:56:59",True,"2013/07/02, 05:56:59",415.0,621964.0,6,"For instance, you might hook into JDBC query methods, servlet base classes, and HTTP client libraries."
AppDynamics,17416710,17412297,0,"2013/07/02, 05:56:59",True,"2013/07/02, 05:56:59",415.0,621964.0,6,"When a request enters the application, track all the important methods/calls it makes, and log them in some way."
AppDynamics,17416710,17412297,0,"2013/07/02, 05:56:59",True,"2013/07/02, 05:56:59",415.0,621964.0,6,"Take the data and crunch it into analytics, charts, and alerts."
AppDynamics,17416710,17412297,0,"2013/07/02, 05:56:59",True,"2013/07/02, 05:56:59",415.0,621964.0,6,"On top of that, you can start to add in statistical profiling or other options."
AppDynamics,17416710,17412297,0,"2013/07/02, 05:56:59",True,"2013/07/02, 05:56:59",415.0,621964.0,6,The tricky things are tracking requests across process boundaries and dealing with the volume of performance data you'll gather.
AppDynamics,17416710,17412297,0,"2013/07/02, 05:56:59",True,"2013/07/02, 05:56:59",415.0,621964.0,6,(I work on this problem at AppNeta)
AppDynamics,17416710,17412297,0,"2013/07/02, 05:56:59",True,"2013/07/02, 05:56:59",415.0,621964.0,6,"One thing to check out is Twitter Zipkin ( https://github.com/twitter/zipkin ), doesn't support much and pretty early-stage but interesting project."
AppDynamics,17418261,17412297,0,"2013/07/02, 08:52:04",False,"2013/07/02, 08:52:04",101.0,1792169.0,10,Both AppDynamics and New Relic use Standard BCI to monitor the common interfaces (entry and exit points) developers use to build applications (e.g.
AppDynamics,17418261,17412297,0,"2013/07/02, 08:52:04",False,"2013/07/02, 08:52:04",101.0,1792169.0,10,"Servlet, struts, SOAP, JMS, JDBC, ...)."
AppDynamics,17418261,17412297,0,"2013/07/02, 08:52:04",False,"2013/07/02, 08:52:04",101.0,1792169.0,10,This provides a basic skeleton of code execution (call graphs) with timing information which represents less than 5% of code that is executed.
AppDynamics,17418261,17412297,0,"2013/07/02, 08:52:04",False,"2013/07/02, 08:52:04",101.0,1792169.0,10,The secret is to then uncover the timing of the remaining 95% code execution during slowdowns without incurring too much overhead in a production JVM.
AppDynamics,17418261,17412297,0,"2013/07/02, 08:52:04",False,"2013/07/02, 08:52:04",101.0,1792169.0,10,AppDynamics uses a combination of in-memory agent analytics and Java API calls to then extract the remaining code execution in real-time.
AppDynamics,17418261,17412297,0,"2013/07/02, 08:52:04",False,"2013/07/02, 08:52:04",101.0,1792169.0,10,This means no custom instrumentation is required or explicit declaration of what classes/methods you want the monitoring solution to instrument.
AppDynamics,17418261,17412297,0,"2013/07/02, 08:52:04",False,"2013/07/02, 08:52:04",101.0,1792169.0,10,AppDynamics data collection is very different to that of New Relic.
AppDynamics,17418261,17412297,0,"2013/07/02, 08:52:04",False,"2013/07/02, 08:52:04",101.0,1792169.0,10,"For example, with AppDynamics you can get a complete distributed call graph across multiple JVMs for a specific user request, rather than say an aggregate of requests."
AppDynamics,17418261,17412297,0,"2013/07/02, 08:52:04",False,"2013/07/02, 08:52:04",101.0,1792169.0,10,"BCI is a commodity these days, the difference is in the analytics and algorithms used by vendors that trigger diagnostics/call graph information so you end up with the right visibility at the right time to solve problems."
AppDynamics,17418261,17412297,0,"2013/07/02, 08:52:04",False,"2013/07/02, 08:52:04",101.0,1792169.0,10,Steve.
AppDynamics,31063347,31001196,2,"2015/06/26, 04:13:54",True,"2015/06/26, 04:13:54",86.0,5051173.0,7,That error means that gradle is unable to resolve the dependency on  com.appdynamics:appdynamics-runtime .
AppDynamics,31063347,31001196,2,"2015/06/26, 04:13:54",True,"2015/06/26, 04:13:54",86.0,5051173.0,7,The easiest way to fix this problem is to use the AppDynamics libraries from maven central rather than the  adeum-maven-repo  directory.
AppDynamics,31063347,31001196,2,"2015/06/26, 04:13:54",True,"2015/06/26, 04:13:54",86.0,5051173.0,7,You can do that by editing your top level gradle file to look like this:
AppDynamics,31063347,31001196,2,"2015/06/26, 04:13:54",True,"2015/06/26, 04:13:54",86.0,5051173.0,7,Then your project-level gradle file would look like:
AppDynamics,31063347,31001196,2,"2015/06/26, 04:13:54",True,"2015/06/26, 04:13:54",86.0,5051173.0,7,"Note that I have removed the references to  adeum-maven-repo , and changed the version numbers on the AppDynamics artifacts to refer to them as they exist in maven central."
AppDynamics,31063347,31001196,2,"2015/06/26, 04:13:54",True,"2015/06/26, 04:13:54",86.0,5051173.0,7,"Once you've done this, you no longer need  adeum-maven-repo  in your project, since gradle is now downloading these dependencies automatically."
AppDynamics,58801626,56138542,0,"2019/11/11, 14:45:04",False,"2019/11/11, 14:45:04",26.0,12355048.0,0,"JAVA_OPTS=""$JAVA_OPTS -Djboss.modules.system.pkgs=org.jboss.byteman,com.singularity,org""
If you do not initialize the JVM, the installation throws a ""class not found"" exception."
AppDynamics,58801626,56138542,0,"2019/11/11, 14:45:04",False,"2019/11/11, 14:45:04",26.0,12355048.0,0,"-Djava.util.logging.manager=org.jboss.logmanager.LogManager -Xbootclasspath/p:
/jboss-logmanager-.jar"
AppDynamics,58801626,56138542,0,"2019/11/11, 14:45:04",False,"2019/11/11, 14:45:04",26.0,12355048.0,0,"JDK9 and above, -Xbootclasspath/p option has been removed; use -Xbootclasspath/a instead."
AppDynamics,58801626,56138542,0,"2019/11/11, 14:45:04",False,"2019/11/11, 14:45:04",26.0,12355048.0,0,"-Djava.util.logging.manager=org.jboss.logmanager.LogManager -Xbootclasspath/a:
/jboss-logmanager-.jar"
AppDynamics,58801626,56138542,0,"2019/11/11, 14:45:04",False,"2019/11/11, 14:45:04",26.0,12355048.0,0,https://docs.appdynamics.com/display/PRO45/JBoss+and+Wildfly+Startup+Settings#JBossandWildflyStartupSettings-InitializetheJVM
AppDynamics,16171037,16161856,0,"2013/04/23, 16:48:17",False,"2013/04/23, 16:48:17",11.0,2047636.0,1,A great place to ask this question would be on the AppDynamics discussion forums so that AppDynamics support can answer you directly...  http://appsphere.appdynamics.com/t5/Discussions/ct-p/Discussions
AppDynamics,16171037,16161856,0,"2013/04/23, 16:48:17",False,"2013/04/23, 16:48:17",11.0,2047636.0,1,I'm guessing there is a permissions issue somewhere.
AppDynamics,18347965,16161856,0,"2013/08/21, 05:35:52",False,"2013/08/21, 05:35:52",17506.0,524588.0,1,"You should copy all files to run the agent, not just javaagent.jar."
AppDynamics,18347965,16161856,0,"2013/08/21, 05:35:52",False,"2013/08/21, 05:35:52",17506.0,524588.0,1,This is a thread about it.
AppDynamics,18347965,16161856,0,"2013/08/21, 05:35:52",False,"2013/08/21, 05:35:52",17506.0,524588.0,1,http://appsphere.appdynamics.com/t5/Lite-for-Java/Keep-getting-a-quot-Invalid-Agent-Installation-Directory-quot-on/td-p/1151
AppDynamics,26749167,25891322,0,"2014/11/05, 05:23:23",True,"2014/11/05, 05:23:23",5502.0,376340.0,4,The definition can be found in AppDynamics docs:  Slow and Stalled Transactions
AppDynamics,26749167,25891322,0,"2014/11/05, 05:23:23",True,"2014/11/05, 05:23:23",5502.0,376340.0,4,"By default AppDynamics considers a slow transaction one that lasts longer than 3 times the standard deviation for the last two hours, a very slow transaction 4 times the standard deviation for the last two hours, and a stalled transaction one that lasts longer than 300 deviations above the average for the last two hours."
AppDynamics,26749167,25891322,0,"2014/11/05, 05:23:23",True,"2014/11/05, 05:23:23",5502.0,376340.0,4,"Of course, you can modify the default rules by providing your own:  Configure Thresholds"
AppDynamics,29123098,29122786,0,"2015/03/18, 15:14:41",False,"2015/03/18, 15:14:41",505.0,4382794.0,0,"Even it is not the desired behaviour, if filtering by URL and specifying only createNewActivity worked for me (as long as there are no other REST URLs matching this)."
AppDynamics,33141659,33134494,1,"2015/10/15, 09:47:16",True,"2015/10/15, 09:52:42",42504.0,936832.0,2,"Generally, monitoring tools cannot record method-level data continuously, because they have to operate at a much lower level of overhead compared to profiling tools."
AppDynamics,33141659,33134494,1,"2015/10/15, 09:47:16",True,"2015/10/15, 09:52:42",42504.0,936832.0,2,"They focus on ""business transactions"" that show you high-level performance measurements with associated semantic information, such as the processing of an order in your web shop."
AppDynamics,33141659,33134494,1,"2015/10/15, 09:47:16",True,"2015/10/15, 09:52:42",42504.0,936832.0,2,Method level data only comes in when these business transactions are too slow.
AppDynamics,33141659,33134494,1,"2015/10/15, 09:47:16",True,"2015/10/15, 09:52:42",42504.0,936832.0,2,The monitoring tool will then start sampling the executing thread and show you a call tree or hot spots.
AppDynamics,33141659,33134494,1,"2015/10/15, 09:47:16",True,"2015/10/15, 09:52:42",42504.0,936832.0,2,"However, you will not get this information for the entire VM for a continuous interval like you're used to from a profiler."
AppDynamics,33141659,33134494,1,"2015/10/15, 09:47:16",True,"2015/10/15, 09:52:42",42504.0,936832.0,2,"You mentioned JProfiler, so if you are already familiar with that tool, you might be interested in  perfino  as a monitoring solution."
AppDynamics,33141659,33134494,1,"2015/10/15, 09:47:16",True,"2015/10/15, 09:52:42",42504.0,936832.0,2,It shows you samples on the method level and has cross-over functionality into profiling with the native JVMTI interface.
AppDynamics,33141659,33134494,1,"2015/10/15, 09:47:16",True,"2015/10/15, 09:52:42",42504.0,936832.0,2,It allows you to do  full sampling of the entire JVM  for a selected amount of time and look at the results in the JProfiler GUI.
AppDynamics,33141659,33134494,1,"2015/10/15, 09:47:16",True,"2015/10/15, 09:52:42",42504.0,936832.0,2,Disclaimer: My company develops JProfiler and perfino.
AppDynamics,42376943,41130233,1,"2017/02/21, 22:06:15",True,"2017/02/21, 22:06:15",207.0,992280.0,-7,Information I got from another website.
AppDynamics,42376943,41130233,1,"2017/02/21, 22:06:15",True,"2017/02/21, 22:06:15",207.0,992280.0,-7,Application Insights (AI) is a very simplistic APM tool today.
AppDynamics,42376943,41130233,1,"2017/02/21, 22:06:15",True,"2017/02/21, 22:06:15",207.0,992280.0,-7,"It does do transaction tracing, it has very limited code and runtime diagnostics, and it doesn’t go very deep into analyzing the browser performance."
AppDynamics,42376943,41130233,1,"2017/02/21, 22:06:15",True,"2017/02/21, 22:06:15",207.0,992280.0,-7,Application Insights is much more like Google Analytics than like a typical APM tool.
AppDynamics,47278626,47278327,1,"2017/11/14, 07:56:39",True,"2017/11/14, 07:56:39",22298.0,7005159.0,1,We could install the AppDynamics extension use Azure portal or Kudu tool( https://functionAppname.scm.azurewebsites.net/ ).
AppDynamics,47278626,47278327,1,"2017/11/14, 07:56:39",True,"2017/11/14, 07:56:39",22298.0,7005159.0,1,Azure Portal:
AppDynamics,47278626,47278327,1,"2017/11/14, 07:56:39",True,"2017/11/14, 07:56:39",22298.0,7005159.0,1,Kudu UI
AppDynamics,47278626,47278327,1,"2017/11/14, 07:56:39",True,"2017/11/14, 07:56:39",22298.0,7005159.0,1,After installation
AppDynamics,48631895,48631700,0,"2018/02/05, 23:36:59",False,"2018/02/05, 23:36:59",1274.0,7716468.0,0,I have used Crashlytics and Google Analytics together without any issue.
AppDynamics,48631895,48631700,0,"2018/02/05, 23:36:59",False,"2018/02/05, 23:36:59",1274.0,7716468.0,0,All the logging is done in a background process so I don't think you will notice any speed degradation but the app is technically doing more work so there is some sort of performance hit.
AppDynamics,48631895,48631700,0,"2018/02/05, 23:36:59",False,"2018/02/05, 23:36:59",1274.0,7716468.0,0,I haven't seen any issues with the crashlog.
AppDynamics,48631895,48631700,0,"2018/02/05, 23:36:59",False,"2018/02/05, 23:36:59",1274.0,7716468.0,0,Analytics libraries just write the crashlogs to a file and then send them the next time the user opens your app.
AppDynamics,48631895,48631700,0,"2018/02/05, 23:36:59",False,"2018/02/05, 23:36:59",1274.0,7716468.0,0,They are not affecting how the actually crashes are handled by the operating system so there shouldn't be any issue with them conflicting.
AppDynamics,48631903,48631700,0,"2018/02/05, 23:37:27",True,"2018/02/05, 23:53:27",12972.0,541786.0,4,First  : I would strongly suggest removing the unused one (or the one you don't prefer) from your code.
AppDynamics,48631903,48631700,0,"2018/02/05, 23:37:27",True,"2018/02/05, 23:53:27",12972.0,541786.0,4,"For reasons, like : 
1."
AppDynamics,48631903,48631700,0,"2018/02/05, 23:37:27",True,"2018/02/05, 23:53:27",12972.0,541786.0,4,It will increase project size which in turn will increase your bundle size.
AppDynamics,48631903,48631700,0,"2018/02/05, 23:37:27",True,"2018/02/05, 23:53:27",12972.0,541786.0,4,2.
AppDynamics,48631903,48631700,0,"2018/02/05, 23:37:27",True,"2018/02/05, 23:53:27",12972.0,541786.0,4,Messy code.
AppDynamics,48631903,48631700,0,"2018/02/05, 23:37:27",True,"2018/02/05, 23:53:27",12972.0,541786.0,4,3.
AppDynamics,48631903,48631700,0,"2018/02/05, 23:37:27",True,"2018/02/05, 23:53:27",12972.0,541786.0,4,There is no point checking two different analytics.
AppDynamics,48631903,48631700,0,"2018/02/05, 23:37:27",True,"2018/02/05, 23:53:27",12972.0,541786.0,4,4.
AppDynamics,48631903,48631700,0,"2018/02/05, 23:37:27",True,"2018/02/05, 23:53:27",12972.0,541786.0,4,"While third person is understanding the code, he would waste his time in understanding which will lead to confusion."
AppDynamics,48631903,48631700,0,"2018/02/05, 23:37:27",True,"2018/02/05, 23:53:27",12972.0,541786.0,4,I might be missing other reasons.
AppDynamics,48631903,48631700,0,"2018/02/05, 23:37:27",True,"2018/02/05, 23:53:27",12972.0,541786.0,4,"Second  : To answer your question, it should work fine."
AppDynamics,48631903,48631700,0,"2018/02/05, 23:37:27",True,"2018/02/05, 23:53:27",12972.0,541786.0,4,"I did the same in one of my projects, where initially I was using  Hockey Crash reporting ."
AppDynamics,48631903,48631700,0,"2018/02/05, 23:37:27",True,"2018/02/05, 23:53:27",12972.0,541786.0,4,But then client asked to use  Crashlytics .
AppDynamics,48631903,48631700,0,"2018/02/05, 23:37:27",True,"2018/02/05, 23:53:27",12972.0,541786.0,4,I didn't remove Hockey SDK immediately.
AppDynamics,48631903,48631700,0,"2018/02/05, 23:37:27",True,"2018/02/05, 23:53:27",12972.0,541786.0,4,"Though this worked fine and both reported the issues, but soon I removed Hockey SDK from the code."
AppDynamics,53074197,52994676,0,"2018/10/31, 01:18:54",True,"2018/10/31, 01:18:54",1182.0,1622880.0,2,There are integration tools available between influxdb/AppDynamics and grafana/AppDynamics.
AppDynamics,53074197,52994676,0,"2018/10/31, 01:18:54",True,"2018/10/31, 01:18:54",1182.0,1622880.0,2,https://github.com/Appdynamics/MetricMover
AppDynamics,53074197,52994676,0,"2018/10/31, 01:18:54",True,"2018/10/31, 01:18:54",1182.0,1622880.0,2,https://grafana.com/plugins/dlopes7-appdynamics-datasource/installation ).
AppDynamics,53074197,52994676,0,"2018/10/31, 01:18:54",True,"2018/10/31, 01:18:54",1182.0,1622880.0,2,There's nothing that integrates between Prometheus and AppDynamics at the moment
AppDynamics,53074197,52994676,0,"2018/10/31, 01:18:54",True,"2018/10/31, 01:18:54",1182.0,1622880.0,2,"I'm not sure there will be one going forward, seeing how they are competing in the same space from different vantage points (Open Source vs Enterprise)"
AppDynamics,58803167,53397807,1,"2019/11/11, 16:20:49",False,"2019/11/11, 17:18:30",26.0,12355048.0,0,"If you want to monitor only your worker processes of IIS and Standalone Service, You can use CLR Crash event on your policy configuration."
AppDynamics,58803167,53397807,1,"2019/11/11, 16:20:49",False,"2019/11/11, 17:18:30",26.0,12355048.0,0,AppDynamics automaticaly creates CLR Crash Events If your IIS or Standalone Service are crashed.
AppDynamics,58803167,53397807,1,"2019/11/11, 16:20:49",False,"2019/11/11, 17:18:30",26.0,12355048.0,0,"You can find the details of CLR Crash Events:
 https://docs.appdynamics.com/display/PRO45/Monitor+CLR+Crashes"
AppDynamics,58803167,53397807,1,"2019/11/11, 16:20:49",False,"2019/11/11, 17:18:30",26.0,12355048.0,0,"Also, Sample policy configuration: 
 Policy Configuration Screen"
AppDynamics,65946975,65946794,2,"2021/01/29, 01:58:56",False,"2021/01/29, 01:58:56",483.0,4895267.0,0,AppDynamics have a fork of contrib project  here  where there is an exporter but it isn't clear if it has been finished
AppDynamics,66421315,65946794,0,"2021/03/01, 13:29:48",True,"2021/03/01, 13:29:48",3170.0,1237595.0,1,there is a Beta program now running for using AppDynamics with the OpenTelemetry Collector.
AppDynamics,66421315,65946794,0,"2021/03/01, 13:29:48",True,"2021/03/01, 13:29:48",3170.0,1237595.0,1,More details are available in the public docs:  https://docs.appdynamics.com/display/PRO21/Ingest+OpenTelemetry+Trace+Data
AppDynamics,13182788,13182423,0,"2012/11/01, 19:52:02",True,"2012/11/01, 19:52:02",49920.0,32453.0,1,"Looks like you create the metric, then edit a dashboard, then click on a widget -  add metric -  (browse, but choose ""Individual Nodes"" instead of JMX, then select your metric."
AppDynamics,13182788,13182423,0,"2012/11/01, 19:52:02",True,"2012/11/01, 19:52:02",49920.0,32453.0,1,voila.
AppDynamics,24808336,24797525,2,"2014/07/17, 18:58:39",False,"2014/07/17, 18:58:39",11.0,2047636.0,0,The best place for you to get this question answered would be on the AppDynamics community discussion boards.
AppDynamics,24808336,24797525,2,"2014/07/17, 18:58:39",False,"2014/07/17, 18:58:39",11.0,2047636.0,0,Here's a link for you...  http://community.appdynamics.com/t5/Forums-Community-AppDynamics/ct-p/Discussions
AppDynamics,24808336,24797525,2,"2014/07/17, 18:58:39",False,"2014/07/17, 18:58:39",11.0,2047636.0,0,The AppDynamics documentation site is also a great resource and you don't even need a login to access them...  http://docs.appdynamics.com/
AppDynamics,24809185,24797525,3,"2014/07/17, 19:45:22",False,"2014/07/17, 19:45:22",66.0,3850108.0,0,"Installation instructions for the controller can be found in:
 http://docs.appdynamics.com/display/PRO14S/Install+the+Controller 
Assuming you are using App Server Agent for Java, installation instructions for that and the machine agent can be found in the above site on the Left Nav table of contents."
AppDynamics,24809185,24797525,3,"2014/07/17, 19:45:22",False,"2014/07/17, 19:45:22",66.0,3850108.0,0,"The controller is the management server, the app server agent monitors the JVM and the machine agent collects system metrics (such as CPU, Memory, Disk &amp; Network)."
AppDynamics,24809185,24797525,3,"2014/07/17, 19:45:22",False,"2014/07/17, 19:45:22",66.0,3850108.0,0,Very minimal configuration is required.
AppDynamics,30294344,29858961,0,"2015/05/18, 05:39:08",True,"2015/05/18, 05:39:08",16.0,4910351.0,0,"From what I understand from their documentation, AppD do not have a way to capture heap dumps."
AppDynamics,30294344,29858961,0,"2015/05/18, 05:39:08",True,"2015/05/18, 05:39:08",16.0,4910351.0,0,They suggest using Memory Leak detection feature in such scenarios.
AppDynamics,30294344,29858961,0,"2015/05/18, 05:39:08",True,"2015/05/18, 05:39:08",16.0,4910351.0,0,"On a different note, I know we can get thread dumps which maybe helpful in some cases (Agents -  Request Agent Log Files)"
AppDynamics,31398932,29858961,0,"2015/07/14, 09:12:20",False,"2015/07/14, 09:12:20",51.0,3269857.0,3,Heap dumps in appdynamics can be taken for JRockit JVM by the below method(Note : This does not work for IBM JVM)
AppDynamics,31398932,29858961,0,"2015/07/14, 09:12:20",False,"2015/07/14, 09:12:20",51.0,3269857.0,3,p0 – Path to generate heap dump(/path/dump.hprof)
AppDynamics,31398932,29858961,0,"2015/07/14, 09:12:20",False,"2015/07/14, 09:12:20",51.0,3269857.0,3,p1 -  True – GC before Heap dump ; False - No GC before heap dump
AppDynamics,31398932,29858961,0,"2015/07/14, 09:12:20",False,"2015/07/14, 09:12:20",51.0,3269857.0,3,Note : If you want heap dump to be generated in the case of out of memory give
AppDynamics,31398932,29858961,0,"2015/07/14, 09:12:20",False,"2015/07/14, 09:12:20",51.0,3269857.0,3,p0 : HeapDumpOnOutOfMemoryError
AppDynamics,31398932,29858961,0,"2015/07/14, 09:12:20",False,"2015/07/14, 09:12:20",51.0,3269857.0,3,Also note that these values will be lost on JVM restart.
AppDynamics,33780274,33210833,1,"2015/11/18, 14:36:17",True,"2015/11/18, 14:36:17",66.0,1355536.0,3,"Data retrieval by APM tools is done in several ways, each one with its pros and cons"
AppDynamics,33780274,33210833,1,"2015/11/18, 14:36:17",True,"2015/11/18, 14:36:17",66.0,1355536.0,3,"Bytecode injection  (for both Java and .NET) is one technique, which is somewhat intrusive but allows you to get data from places the application owner (or even 3rd party frameworks) did not intend to allow."
AppDynamics,33780274,33210833,1,"2015/11/18, 14:36:17",True,"2015/11/18, 14:36:17",66.0,1355536.0,3,"Native function interception  is similar to bytecode injection, but allows you to intercept unmanaged code"
AppDynamics,33780274,33210833,1,"2015/11/18, 14:36:17",True,"2015/11/18, 14:36:17",66.0,1355536.0,3,Application plugins  - some applications (e.g.
AppDynamics,33780274,33210833,1,"2015/11/18, 14:36:17",True,"2015/11/18, 14:36:17",66.0,1355536.0,3,"Apache, IIS) give access to monitoring and application information via a well-documented APIs and plugin architecture"
AppDynamics,33780274,33210833,1,"2015/11/18, 14:36:17",True,"2015/11/18, 14:36:17",66.0,1355536.0,3,Network sniffing  allows you to see all the communication to/from the monitored machine
AppDynamics,33780274,33210833,1,"2015/11/18, 14:36:17",True,"2015/11/18, 14:36:17",66.0,1355536.0,3,"OS specific un/documented APIs  - just like application plugins, but for the Windows/*nix"
AppDynamics,33780274,33210833,1,"2015/11/18, 14:36:17",True,"2015/11/18, 14:36:17",66.0,1355536.0,3,"Disclaimer : I work for Correlsense, provider of APM software SharePath, which uses all of the above methods to give you complete end-to-end transaction visibility."
AppDynamics,42301899,41933094,0,"2017/02/17, 17:40:40",False,"2017/02/17, 17:48:24",4877.0,1140237.0,0,I've tried add multi dex without giving any minimum or maximum of method count per dex file wise.I've tried with simply just adding multidex and able to build.And Yes!!
AppDynamics,42301899,41933094,0,"2017/02/17, 17:40:40",False,"2017/02/17, 17:48:24",4877.0,1140237.0,0,I am able to build app too.
AppDynamics,42301899,41933094,0,"2017/02/17, 17:40:40",False,"2017/02/17, 17:48:24",4877.0,1140237.0,0,major change is in  afterEvaluate  &amp;  incremental true  in  dexoption .
AppDynamics,42301899,41933094,0,"2017/02/17, 17:40:40",False,"2017/02/17, 17:48:24",4877.0,1140237.0,0,build.gradle
AppDynamics,42301899,41933094,0,"2017/02/17, 17:40:40",False,"2017/02/17, 17:48:24",4877.0,1140237.0,0,application's parent gradle
AppDynamics,42301899,41933094,0,"2017/02/17, 17:40:40",False,"2017/02/17, 17:48:24",4877.0,1140237.0,0,If above thing have still issue just check your dependecies hierarchy if any other extra dependecies are added (Based on your  build.gradle    packagingOptions  there should be some other dependecies there).Not sure but it may possible because of internal library conflicts its not proceeding further to create dexfile or build.
AppDynamics,42301899,41933094,0,"2017/02/17, 17:40:40",False,"2017/02/17, 17:48:24",4877.0,1140237.0,0,Let me know if anything
AppDynamics,43318354,43315501,1,"2017/04/10, 11:34:41",True,"2017/04/10, 11:34:41",324.0,7220665.0,3,You can use the REST API of the Controller described here  https://docs.appdynamics.com/display/PRO42/Using+the+Controller+APIs
AppDynamics,43318354,43315501,1,"2017/04/10, 11:34:41",True,"2017/04/10, 11:34:41",324.0,7220665.0,3,"To access the REST API Browser, in a Web browser, go to:"
AppDynamics,43318354,43315501,1,"2017/04/10, 11:34:41",True,"2017/04/10, 11:34:41",324.0,7220665.0,3,this will give you a nice swagger UI description of the available resources.
AppDynamics,43318354,43315501,1,"2017/04/10, 11:34:41",True,"2017/04/10, 11:34:41",324.0,7220665.0,3,Alternatively you can create reports directly out of the box in AppDynamics via the  Dashboards &amp; Reports  section.
AppDynamics,44308592,44283136,0,"2017/06/01, 16:15:26",False,"2017/06/01, 20:52:24",324.0,7220665.0,1,"First you need to setup the AppDynamics Controller (should be hosted on a separate  machine), then you need java agents on the machine with the application."
AppDynamics,44308592,44283136,0,"2017/06/01, 16:15:26",False,"2017/06/01, 20:52:24",324.0,7220665.0,1,"You need to wire the application with the Java agent, e.g."
AppDynamics,44308592,44283136,0,"2017/06/01, 16:15:26",False,"2017/06/01, 20:52:24",324.0,7220665.0,1,like this
AppDynamics,44308592,44283136,0,"2017/06/01, 16:15:26",False,"2017/06/01, 20:52:24",324.0,7220665.0,1,Now the Java agent sends application information to the controller.
AppDynamics,44308592,44283136,0,"2017/06/01, 16:15:26",False,"2017/06/01, 20:52:24",324.0,7220665.0,1,Take a look at the documentation.
AppDynamics,44308592,44283136,0,"2017/06/01, 16:15:26",False,"2017/06/01, 20:52:24",324.0,7220665.0,1,https://docs.appdynamics.com/display/PRO43/Getting+Started
AppDynamics,44308592,44283136,0,"2017/06/01, 16:15:26",False,"2017/06/01, 20:52:24",324.0,7220665.0,1,The application will automatically be available in AppDynamics and you can see the dashboard.
AppDynamics,44338258,44283136,0,"2017/06/03, 01:27:31",False,"2017/06/03, 01:27:31",68.0,1394813.0,0,"You don't need to host the controller, you can get a SAAS account here."
AppDynamics,44338258,44283136,0,"2017/06/03, 01:27:31",False,"2017/06/03, 01:27:31",68.0,1394813.0,0,https://www.appdynamics.com/free-trial/  .
AppDynamics,44338258,44283136,0,"2017/06/03, 01:27:31",False,"2017/06/03, 01:27:31",68.0,1394813.0,0,Once you get a SAAS account lookup the account name and account key in the welcome email and use the instructions in the email to add JVM args to your app as shown.
AppDynamics,44338258,44283136,0,"2017/06/03, 01:27:31",False,"2017/06/03, 01:27:31",68.0,1394813.0,0,Don't forget to set the account name and account access key args: -Dappdynamics.agent.accountName -Dappdynamics.agent.accountAccessKey .
AppDynamics,49007036,49004859,0,"2018/02/27, 12:58:41",False,"2018/02/27, 12:58:41",239035.0,2711488.0,2,See  4.3.x Documentation⇒POJO Entry Points⇒Monitor Java Interface Static and Default Methods :
AppDynamics,49007036,49004859,0,"2018/02/27, 12:58:41",False,"2018/02/27, 12:58:41",239035.0,2711488.0,2,"Note that another Java language feature introduced in Java 8, lambda method interfaces, are not supported by the AppDynamics Java Agent."
AppDynamics,49007036,49004859,0,"2018/02/27, 12:58:41",False,"2018/02/27, 12:58:41",239035.0,2711488.0,2,It’s possible that this is due to technical difficulties with  JDK-8145964  as you suspect.
AppDynamics,49007036,49004859,0,"2018/02/27, 12:58:41",False,"2018/02/27, 12:58:41",239035.0,2711488.0,2,But I’d also point out that this kind of Instrumentation would be questionable.
AppDynamics,49007036,49004859,0,"2018/02/27, 12:58:41",False,"2018/02/27, 12:58:41",239035.0,2711488.0,2,"It’s not this JRE generated class that implements any specific behavior, it’s the invoked target method."
AppDynamics,49029706,49004859,1,"2018/02/28, 14:33:14",False,"2018/02/28, 14:33:14",474.0,4682632.0,0,Support for Lambda expressions has been in the product since 4.1 which shipped in 2015 I believe.
AppDynamics,49029706,49004859,1,"2018/02/28, 14:33:14",False,"2018/02/28, 14:33:14",474.0,4682632.0,0,That being said we are always enhancing support.
AppDynamics,49029706,49004859,1,"2018/02/28, 14:33:14",False,"2018/02/28, 14:33:14",474.0,4682632.0,0,These do have some limitations after initialization of the classes using them (dynamic instrumentation limits).
AppDynamics,49029706,49004859,1,"2018/02/28, 14:33:14",False,"2018/02/28, 14:33:14",474.0,4682632.0,0,"The product should support them, we have added some additional capabilities and features for Lambda expressions in our next major release."
AppDynamics,49029706,49004859,1,"2018/02/28, 14:33:14",False,"2018/02/28, 14:33:14",474.0,4682632.0,0,Have you tried to contact help @ appdynamics.com
AppDynamics,49033394,49004859,0,"2018/02/28, 17:45:13",False,"2018/02/28, 17:45:13",9111.0,4647853.0,0,"Looks like it was not mentioned in release notes, but instead  Support Advisory 56039  was raised."
AppDynamics,49033394,49004859,0,"2018/02/28, 17:45:13",False,"2018/02/28, 17:45:13",9111.0,4647853.0,0,They indeed mention JDK-8145964 as a reason for removing the support.
AppDynamics,50713654,50713335,2,"2018/06/06, 09:34:40",False,"2018/06/06, 09:34:40",44.0,2480398.0,1,I got this too...
AppDynamics,50713654,50713335,2,"2018/06/06, 09:34:40",False,"2018/06/06, 09:34:40",44.0,2480398.0,1,I was running from command-line as a non-root user:
AppDynamics,50713654,50713335,2,"2018/06/06, 09:34:40",False,"2018/06/06, 09:34:40",44.0,2480398.0,1,I added the shell expand(-x) switch and log to the command(s) like so:
AppDynamics,50713654,50713335,2,"2018/06/06, 09:34:40",False,"2018/06/06, 09:34:40",44.0,2480398.0,1,"If we tail the last bit of that log you get, this response in debug mode:"
AppDynamics,50713654,50713335,2,"2018/06/06, 09:34:40",False,"2018/06/06, 09:34:40",44.0,2480398.0,1,and the script checkLibaio.sh isn't left there... so you cannot figure it out easily.
AppDynamics,50713654,50713335,2,"2018/06/06, 09:34:40",False,"2018/06/06, 09:34:40",44.0,2480398.0,1,I also have a RedHat variant with the packages installed:
AppDynamics,50713654,50713335,2,"2018/06/06, 09:34:40",False,"2018/06/06, 09:34:40",44.0,2480398.0,1,"Strangely enough I have one VM from the same image that will install the distribution just fine, and one that will not, so on the broken install (where I really want to install this)."
AppDynamics,50713654,50713335,2,"2018/06/06, 09:34:40",False,"2018/06/06, 09:34:40",44.0,2480398.0,1,"I ran another command from the expanded view of the install.log, which was a really long JVM command line."
AppDynamics,50713654,50713335,2,"2018/06/06, 09:34:40",False,"2018/06/06, 09:34:40",44.0,2480398.0,1,Anyways I got it to work and then made a looping script to retrieve the file (Because AppD for some reason removes the check script before you can look at it).
AppDynamics,50713654,50713335,2,"2018/06/06, 09:34:40",False,"2018/06/06, 09:34:40",44.0,2480398.0,1,The script is as follows:
AppDynamics,50713654,50713335,2,"2018/06/06, 09:34:40",False,"2018/06/06, 09:34:40",44.0,2480398.0,1,I you run this script like me on the faulty platform what you will discover is that your version of Linux has both:
AppDynamics,50713654,50713335,2,"2018/06/06, 09:34:40",False,"2018/06/06, 09:34:40",44.0,2480398.0,1,and
AppDynamics,50713654,50713335,2,"2018/06/06, 09:34:40",False,"2018/06/06, 09:34:40",44.0,2480398.0,1,installed.
AppDynamics,50713654,50713335,2,"2018/06/06, 09:34:40",False,"2018/06/06, 09:34:40",44.0,2480398.0,1,To work around this you should temporarily make a name change to one of these two package manager executables so it cannot be found (by your shell environment).
AppDynamics,50713654,50713335,2,"2018/06/06, 09:34:40",False,"2018/06/06, 09:34:40",44.0,2480398.0,1,Most common here will be that you are running a RedHat variant where someone chose to install dpkg (For who knows what reason).
AppDynamics,50713654,50713335,2,"2018/06/06, 09:34:40",False,"2018/06/06, 09:34:40",44.0,2480398.0,1,If so desired remove that package and the install should be successful.
AppDynamics,53539438,53537738,0,"2018/11/29, 14:47:54",False,"2018/11/29, 14:47:54",6049.0,3156333.0,2,As stated on the AppD docs regarding  Kubernetes and AppDynamics APM
AppDynamics,53539438,53537738,0,"2018/11/29, 14:47:54",False,"2018/11/29, 14:47:54",6049.0,3156333.0,2,Install a Standalone Machine Agent (1) in a Kubernetes node.
AppDynamics,53539438,53537738,0,"2018/11/29, 14:47:54",False,"2018/11/29, 14:47:54",6049.0,3156333.0,2,Install an APM Agent (2) inside each container in a pod you want to monitor.
AppDynamics,53539438,53537738,0,"2018/11/29, 14:47:54",False,"2018/11/29, 14:47:54",6049.0,3156333.0,2,"The Standalone Machine Agent then collects hardware metrics for each monitored container, as well as Machine and Server metrics for the host (3), and forwards the metrics to the Controller."
AppDynamics,53539438,53537738,0,"2018/11/29, 14:47:54",False,"2018/11/29, 14:47:54",6049.0,3156333.0,2,ContainerID and UniqueHostID can be taken from  /proc/self/cgroup
AppDynamics,53539438,53537738,0,"2018/11/29, 14:47:54",False,"2018/11/29, 14:47:54",6049.0,3156333.0,2,ContainerID  cat /proc/self/cgroup | awk -F '/' '{print $NF}'  | head -n 1
AppDynamics,53539438,53537738,0,"2018/11/29, 14:47:54",False,"2018/11/29, 14:47:54",6049.0,3156333.0,2,UniqueHostID  sed -rn '1s#.
AppDynamics,53539438,53537738,0,"2018/11/29, 14:47:54",False,"2018/11/29, 14:47:54",6049.0,3156333.0,2,*/##; 1s/(.{12}).
AppDynamics,53539438,53537738,0,"2018/11/29, 14:47:54",False,"2018/11/29, 14:47:54",6049.0,3156333.0,2,*/\1/p' /proc/self/cgroup
AppDynamics,54083177,54046191,0,"2019/01/08, 01:13:19",True,"2019/01/08, 01:13:19",474.0,4682632.0,2,Thanks for the reply to my question.
AppDynamics,54083177,54046191,0,"2019/01/08, 01:13:19",True,"2019/01/08, 01:13:19",474.0,4682632.0,2,The .NET agent in most APM tools works the same way which leverages the profiler APIs in the .NET SDK and allows for data collection and also callbacks and other interception.
AppDynamics,54083177,54046191,0,"2019/01/08, 01:13:19",True,"2019/01/08, 01:13:19",474.0,4682632.0,2,"Most of the tools also use performance counter data, and other sources aside from inside the .NET runtime."
AppDynamics,54083177,54046191,0,"2019/01/08, 01:13:19",True,"2019/01/08, 01:13:19",474.0,4682632.0,2,This allows you to do several things similar to Java in terms of data collection.
AppDynamics,54083177,54046191,0,"2019/01/08, 01:13:19",True,"2019/01/08, 01:13:19",474.0,4682632.0,2,ref:  https://docs.microsoft.com/en-us/visualstudio/profiling/walkthrough-using-profiler-apis?view=vs-2017
AppDynamics,54083177,54046191,0,"2019/01/08, 01:13:19",True,"2019/01/08, 01:13:19",474.0,4682632.0,2,http://www.blong.com/conferences/dcon2003/internals/profiling.htm
AppDynamics,55150379,55048208,0,"2019/03/13, 22:07:48",True,"2019/03/13, 22:07:48",41.0,2625630.0,0,"The solution was adding ""appdynamics"" to the ""externals"" in the Webpack configuration:  https://webpack.js.org/configuration/externals/"
AppDynamics,55150379,55048208,0,"2019/03/13, 22:07:48",True,"2019/03/13, 22:07:48",41.0,2625630.0,0,This allows AppDynamics to use the default Node.js require import.
AppDynamics,58210199,58208802,0,"2019/10/03, 01:37:31",True,"2019/10/03, 01:37:31",955.0,284480.0,1,The most important thing to understand about this error is the meaning of this line:
AppDynamics,58210199,58208802,0,"2019/10/03, 01:37:31",True,"2019/10/03, 01:37:31",955.0,284480.0,1,Caused by: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target
AppDynamics,58210199,58208802,0,"2019/10/03, 01:37:31",True,"2019/10/03, 01:37:31",955.0,284480.0,1,"SSL certificates work by establishing a certificate chain, or a hierarchy of trust."
AppDynamics,58210199,58208802,0,"2019/10/03, 01:37:31",True,"2019/10/03, 01:37:31",955.0,284480.0,1,"For example, if I go to  https://www.google.com  and look at their cert, this is what I see:"
AppDynamics,58210199,58208802,0,"2019/10/03, 01:37:31",True,"2019/10/03, 01:37:31",955.0,284480.0,1,"There is the google cert, which sits on their servers/CDN, then an intermediate cert which also sits on their servers/CDN, then a trusted root CA cert which is in the  client  keystore and is implicitly trusted."
AppDynamics,58210199,58208802,0,"2019/10/03, 01:37:31",True,"2019/10/03, 01:37:31",955.0,284480.0,1,"So when someone browses to google, b/c they have the root CA cert and have trusted it, the browser (client) will trust that the server is actually who they say they are and will establish a secure connection to the site."
AppDynamics,58210199,58208802,0,"2019/10/03, 01:37:31",True,"2019/10/03, 01:37:31",955.0,284480.0,1,"So getting back to your error, whatever CA issued the server cert being used by RabbitMQ, the monitor is not recognizing it as trusted."
AppDynamics,58210199,58208802,0,"2019/10/03, 01:37:31",True,"2019/10/03, 01:37:31",955.0,284480.0,1,"To troubleshoot this error, here are the things to do:"
AppDynamics,58210199,58208802,0,"2019/10/03, 01:37:31",True,"2019/10/03, 01:37:31",955.0,284480.0,1,There are a couple other gotchas to watch out for:
AppDynamics,66101093,63647198,0,"2021/02/08, 13:51:32",False,"2021/02/08, 13:51:32",3170.0,1237595.0,0,This issue was solved here:  https://community.appdynamics.com/t5/Java-Java-Agent-Installation-JVM/monitor-URL-page-availability/td-p/40904
AppDynamics,66101093,63647198,0,"2021/02/08, 13:51:32",False,"2021/02/08, 13:51:32",3170.0,1237595.0,0,Summary:
AppDynamics,13198174,13198172,0,"2012/11/02, 17:21:28",True,"2018/05/25, 01:03:27",49920.0,32453.0,0,"Turns out if you go to configure -  instrumentation -  JMX tab then voila, you can now delete metrics and modify/edit them."
AppDynamics,13198174,13198172,0,"2012/11/02, 17:21:28",True,"2018/05/25, 01:03:27",49920.0,32453.0,0,But nowhere else.
AppDynamics,13198174,13198172,0,"2012/11/02, 17:21:28",True,"2018/05/25, 01:03:27",49920.0,32453.0,0,Odd.
AppDynamics,18914810,18895293,0,"2013/09/20, 13:44:18",False,"2013/09/20, 13:44:18",3.0,2795488.0,0,I've instrumented  org.apache.ode.bpel.engine.BpelProcess.handleJobDetails and it is showing me transactions going through Carbon out to the backends.
AppDynamics,19523960,19518737,4,"2013/10/22, 19:58:09",False,"2013/10/22, 19:58:09",1.0,2908075.0,0,It is a bit hard to completely answer your question and solve the issue with the provided information.
AppDynamics,19523960,19518737,4,"2013/10/22, 19:58:09",False,"2013/10/22, 19:58:09",1.0,2908075.0,0,"However, I hope my questions below help you to get on the right track."
AppDynamics,19523960,19518737,4,"2013/10/22, 19:58:09",False,"2013/10/22, 19:58:09",1.0,2908075.0,0,1.)
AppDynamics,19523960,19518737,4,"2013/10/22, 19:58:09",False,"2013/10/22, 19:58:09",1.0,2908075.0,0,"After making the configuration change, did you also restart the AppDynamics.AgentCoordinator_WindowsService?"
AppDynamics,19523960,19518737,4,"2013/10/22, 19:58:09",False,"2013/10/22, 19:58:09",1.0,2908075.0,0,Without restarting it the new configuration will not be applied to the agent itself.
AppDynamics,19523960,19518737,4,"2013/10/22, 19:58:09",False,"2013/10/22, 19:58:09",1.0,2908075.0,0,2.)
AppDynamics,19523960,19518737,4,"2013/10/22, 19:58:09",False,"2013/10/22, 19:58:09",1.0,2908075.0,0,"Also important is your windows service hosting any OOTB support entry point like WCF, ASP.NET MVC4 WebAPI, web services etc.?"
AppDynamics,19523960,19518737,4,"2013/10/22, 19:58:09",False,"2013/10/22, 19:58:09",1.0,2908075.0,0,"If not, you need to setup a custom entry point."
AppDynamics,19523960,19518737,4,"2013/10/22, 19:58:09",False,"2013/10/22, 19:58:09",1.0,2908075.0,0,If you  check out the AppDynamics documentation and search for'POCO Entry Points' you should get onto the right track
AppDynamics,19523960,19518737,4,"2013/10/22, 19:58:09",False,"2013/10/22, 19:58:09",1.0,2908075.0,0,3.)
AppDynamics,19523960,19518737,4,"2013/10/22, 19:58:09",False,"2013/10/22, 19:58:09",1.0,2908075.0,0,"In case No.1 &amp; No.2 did not do the trick, could you please attach the config.xml file for review?"
AppDynamics,19523960,19518737,4,"2013/10/22, 19:58:09",False,"2013/10/22, 19:58:09",1.0,2908075.0,0,Or directly reach out to the AppDynamics customer success team.
AppDynamics,19523960,19518737,4,"2013/10/22, 19:58:09",False,"2013/10/22, 19:58:09",1.0,2908075.0,0,"Kind regards,
Theo"
AppDynamics,19523960,19518737,4,"2013/10/22, 19:58:09",False,"2013/10/22, 19:58:09",1.0,2908075.0,0,Disclaimer: I work for AppDynamics as part of the Customer Success team.
AppDynamics,19524167,19518737,3,"2013/10/22, 20:08:31",False,"2013/10/22, 20:08:31",1.0,2908075.0,0,Just to add to you answer and to clarify a little.
AppDynamics,19524167,19518737,3,"2013/10/22, 20:08:31",False,"2013/10/22, 20:08:31",1.0,2908075.0,0,"Until release 3.7.7 the .NET agent from AppDynamics used the web.config (for IIS based application), App.config for standalone and windows services or the global.config plus some environment variables to configure the agent."
AppDynamics,19524167,19518737,3,"2013/10/22, 20:08:31",False,"2013/10/22, 20:08:31",1.0,2908075.0,0,With the 3.7.8 or later release we replaced this with a cleaner truly singly configuration file approach.
AppDynamics,19524167,19518737,3,"2013/10/22, 20:08:31",False,"2013/10/22, 20:08:31",1.0,2908075.0,0,The configuration file is called config.xml and located in the %ProgramData%\AppDynamics... directory.
AppDynamics,19524167,19518737,3,"2013/10/22, 20:08:31",False,"2013/10/22, 20:08:31",1.0,2908075.0,0,For any version after 3.7.8 all settings have to be in the config.xml.
AppDynamics,19599763,19518737,0,"2013/10/26, 00:32:59",False,"2013/10/26, 00:32:59",11.0,2047636.0,0,You really should take this up with AppDynamics support by filing a ticket or posting in the support forums...  http://www.appdynamics.com/support/#helptab
AppDynamics,28696479,28692916,2,"2015/02/24, 14:58:43",False,"2015/02/24, 14:58:43",72.0,3641322.0,-1,There are many things you should analyse.
AppDynamics,28696479,28692916,2,"2015/02/24, 14:58:43",False,"2015/02/24, 14:58:43",72.0,3641322.0,-1,It depends on what your LR portal uses most.
AppDynamics,28696479,28692916,2,"2015/02/24, 14:58:43",False,"2015/02/24, 14:58:43",72.0,3641322.0,-1,"You may want to analyse web content , user, groups, calnder, theme related services."
AppDynamics,30730071,30706344,0,"2015/06/09, 13:57:33",False,"2015/06/09, 13:57:33",474.0,4682632.0,0,I would ask support on this if you can't get it to work.
AppDynamics,30730071,30706344,0,"2015/06/09, 13:57:33",False,"2015/06/09, 13:57:33",474.0,4682632.0,0,Here is the configuration for custom exit and entry points in the product:  https://docs.appdynamics.com/display/PRO14S/Configure+Custom+Exit+Points
AppDynamics,31233300,31219797,0,"2015/07/05, 20:54:05",False,"2015/07/05, 20:54:05",474.0,4682632.0,0,"Love to help you out here, but there are no details about the error, please email help@appdynamics.com for assistance."
AppDynamics,31233300,31219797,0,"2015/07/05, 20:54:05",False,"2015/07/05, 20:54:05",474.0,4682632.0,0,"I assume this is C#, but wouldn't know based on this."
AppDynamics,31233300,31219797,0,"2015/07/05, 20:54:05",False,"2015/07/05, 20:54:05",474.0,4682632.0,0,AppDynamics supports many languages and technologies.
AppDynamics,31309166,31219797,0,"2015/07/09, 08:49:40",True,"2015/07/09, 08:49:40",117.0,2384622.0,1,For whom it might be of interest I have found a workaround and more details about this issue.
AppDynamics,31309166,31219797,0,"2015/07/09, 08:49:40",True,"2015/07/09, 08:49:40",117.0,2384622.0,1,This occurs only in the following scenario:
AppDynamics,31309166,31219797,0,"2015/07/09, 08:49:40",True,"2015/07/09, 08:49:40",117.0,2384622.0,1,If you use the MsBuild 14 that is installed along Microsoft Visual Studio 2015 RC then this issue does not occur anymore.
AppDynamics,31309166,31219797,0,"2015/07/09, 08:49:40",True,"2015/07/09, 08:49:40",117.0,2384622.0,1,From my first findings there is an issue in ServiceStack's way of caching the endpoints and wrapping the execute method using Linq but I don't understand why this happens only when AppDynamics agent is installed.
AppDynamics,31309166,31219797,0,"2015/07/09, 08:49:40",True,"2015/07/09, 08:49:40",117.0,2384622.0,1,@mythz if you want to dig deeper into this issue I'm available to help but with the above solution everything is ok.
AppDynamics,31309166,31219797,0,"2015/07/09, 08:49:40",True,"2015/07/09, 08:49:40",117.0,2384622.0,1,Hope this helps
AppDynamics,38459237,38454851,0,"2016/07/19, 15:55:16",False,"2016/07/19, 15:55:16",474.0,4682632.0,0,"There are lots of APIs available out of the box, you can find the docs at  https://docs.appdynamics.com/  not everything has an API."
AppDynamics,38459237,38454851,0,"2016/07/19, 15:55:16",False,"2016/07/19, 15:55:16",474.0,4682632.0,0,You should find the user management as part of the configuration API here :  https://docs.appdynamics.com/display/PRO42/Configuration+API
AppDynamics,39897499,39540605,0,"2016/10/06, 16:34:01",False,"2016/10/06, 16:34:01",1247.0,1232692.0,1,They do different things.
AppDynamics,39897499,39540605,0,"2016/10/06, 16:34:01",False,"2016/10/06, 16:34:01",1247.0,1232692.0,1,ELK will give you log aggregation that you can add in other functionality.
AppDynamics,39897499,39540605,0,"2016/10/06, 16:34:01",False,"2016/10/06, 16:34:01",1247.0,1232692.0,1,Appdynamics is great for real time monitoring and profiling.
AppDynamics,39897499,39540605,0,"2016/10/06, 16:34:01",False,"2016/10/06, 16:34:01",1247.0,1232692.0,1,I think it depends on what you're going for.
AppDynamics,39897499,39540605,0,"2016/10/06, 16:34:01",False,"2016/10/06, 16:34:01",1247.0,1232692.0,1,Logging a distributed system and capturing error messages in one place might be very helpful with ELK.
AppDynamics,39897499,39540605,0,"2016/10/06, 16:34:01",False,"2016/10/06, 16:34:01",1247.0,1232692.0,1,"Not just that, but ELK can be used in a number of other ways."
AppDynamics,39897499,39540605,0,"2016/10/06, 16:34:01",False,"2016/10/06, 16:34:01",1247.0,1232692.0,1,Elasticsearch can be used stand alone as a search engine or data cache.
AppDynamics,39897499,39540605,0,"2016/10/06, 16:34:01",False,"2016/10/06, 16:34:01",1247.0,1232692.0,1,TL;DR  It depends on what you're doing.
AppDynamics,39897499,39540605,0,"2016/10/06, 16:34:01",False,"2016/10/06, 16:34:01",1247.0,1232692.0,1,Maybe yes...maybe no...
AppDynamics,39715544,39652512,0,"2016/09/27, 06:32:35",True,"2016/09/27, 06:32:35",506.0,5006681.0,0,"On that particular server, we have the .NET agent running as well."
AppDynamics,39715544,39652512,0,"2016/09/27, 06:32:35",True,"2016/09/27, 06:32:35",506.0,5006681.0,0,"After shutting off the .NET agent, we were no longer having this issue with 1-2 hour gaps in the metric browser."
AppDynamics,39715544,39652512,0,"2016/09/27, 06:32:35",True,"2016/09/27, 06:32:35",506.0,5006681.0,0,"Apparently, there is some conflict in having multiple machine agents installed on the same server."
AppDynamics,40455823,40452664,0,"2016/11/07, 01:40:56",False,"2016/11/07, 01:40:56",5633.0,4033292.0,0,You need both unit tests and integration tests.
AppDynamics,40455823,40452664,0,"2016/11/07, 01:40:56",False,"2016/11/07, 01:40:56",5633.0,4033292.0,0,"Unit tests should not use in database or File, ect."
AppDynamics,40455823,40452664,0,"2016/11/07, 01:40:56",False,"2016/11/07, 01:40:56",5633.0,4033292.0,0,I like to use Spring profiles for my tests.
AppDynamics,40455823,40452664,0,"2016/11/07, 01:40:56",False,"2016/11/07, 01:40:56",5633.0,4033292.0,0,"For instance, if I have a profile called integeration_test."
AppDynamics,40455823,40452664,0,"2016/11/07, 01:40:56",False,"2016/11/07, 01:40:56",5633.0,4033292.0,0,"(I'm using xml) then in your context do something like:  &lt;beans profile=""test""&gt;TODO &lt;/beans&gt;  And configure your data-source in there."
AppDynamics,40455823,40452664,0,"2016/11/07, 01:40:56",False,"2016/11/07, 01:40:56",5633.0,4033292.0,0,"I know there are ways to rollback all your transactions after running a test, but I like this better."
AppDynamics,40455823,40452664,0,"2016/11/07, 01:40:56",False,"2016/11/07, 01:40:56",5633.0,4033292.0,0,"Just don't delete everything in your real database haha, could even put some safety code in the clearDatabase to make sure that doesn't happen."
AppDynamics,40455823,40452664,0,"2016/11/07, 01:40:56",False,"2016/11/07, 01:40:56",5633.0,4033292.0,0,"For performance testing you will really need to figure out what you want to achieve, and what is meaningful to display."
AppDynamics,40455823,40452664,0,"2016/11/07, 01:40:56",False,"2016/11/07, 01:40:56",5633.0,4033292.0,0,"If you have a specific question about performance testing you can ask that, otherwise it is too broader topic."
AppDynamics,40455823,40452664,0,"2016/11/07, 01:40:56",False,"2016/11/07, 01:40:56",5633.0,4033292.0,0,Maybe you can make a mini-webapp which does performance testing for you and has the results exposed as URL requests for displaying in HTML.
AppDynamics,40455823,40452664,0,"2016/11/07, 01:40:56",False,"2016/11/07, 01:40:56",5633.0,4033292.0,0,Really just depends on how much effort you are willing to spend on it and what you want to test.
AppDynamics,64280827,40452664,0,"2020/10/09, 16:13:06",False,"2020/10/09, 16:13:06",556.0,1086679.0,0,Once the agent is attached you can use the  AppDynamics Database Queries Window
AppDynamics,41757511,41755184,1,"2017/01/20, 08:38:17",True,"2017/01/20, 08:38:17",2681.0,506813.0,1,"Reflection (inherently  TypeVisitor  and  TypeToken  classes) is always costly in Java, try not using it."
AppDynamics,41757511,41755184,1,"2017/01/20, 08:38:17",True,"2017/01/20, 08:38:17",2681.0,506813.0,1,Rendering time seems OK.
AppDynamics,41757511,41755184,1,"2017/01/20, 08:38:17",True,"2017/01/20, 08:38:17",2681.0,506813.0,1,"There can be thousand reasons for high latency in an application, but you only gave this much information so that's about the best answer you can get."
AppDynamics,41773005,41764607,2,"2017/01/20, 23:58:40",False,"2017/01/20, 23:58:40",474.0,4682632.0,1,That's the beauty of APM is you don't need to deal with logging to get performance data.
AppDynamics,41773005,41764607,2,"2017/01/20, 23:58:40",False,"2017/01/20, 23:58:40",474.0,4682632.0,1,"APM tools instrument the applications themselves regardless of what the code does (logging, generating metrics, etc)."
AppDynamics,41773005,41764607,2,"2017/01/20, 23:58:40",False,"2017/01/20, 23:58:40",474.0,4682632.0,1,"AppDynamics can collect log data, and provide similar capabilities to what a log analytics tool can do, but it's of less value than the transaction tracing and instrumentation you get."
AppDynamics,41773005,41764607,2,"2017/01/20, 23:58:40",False,"2017/01/20, 23:58:40",474.0,4682632.0,1,"Your application has to be built in a supported language (Java, .NET, PHP, Python, C++, Node.js) and if it's web or mobile based you can also collect client side performance data and unify between both frontend and backend."
AppDynamics,41773005,41764607,2,"2017/01/20, 23:58:40",False,"2017/01/20, 23:58:40",474.0,4682632.0,1,If you have questions just reach out and I can answer them for you.
AppDynamics,41773005,41764607,2,"2017/01/20, 23:58:40",False,"2017/01/20, 23:58:40",474.0,4682632.0,1,Good luck!
AppDynamics,42998075,41764607,0,"2017/03/24, 13:14:03",False,"2017/03/24, 13:14:03",324.0,7220665.0,1,You basically need the AppDynamics Controller and a AppDynamics Machine-Agent which will be installed on the machine to monitor.
AppDynamics,42998075,41764607,0,"2017/03/24, 13:14:03",False,"2017/03/24, 13:14:03",324.0,7220665.0,1,In the Machine-Agent configuration you set the URI of the controller and the agent starts to report machine metrics to the controller.
AppDynamics,42998075,41764607,0,"2017/03/24, 13:14:03",False,"2017/03/24, 13:14:03",324.0,7220665.0,1,"Then you can configure alarms, see metrics, create dashboards, etc."
AppDynamics,42998075,41764607,0,"2017/03/24, 13:14:03",False,"2017/03/24, 13:14:03",324.0,7220665.0,1,"I can deliver you more information if you want, but as Jonah Kowall said, take a look at the documentation as well  AppDynamics Machine Agent Doc"
AppDynamics,43040838,42274447,0,"2017/03/27, 10:58:07",False,"2017/03/27, 10:58:07",324.0,7220665.0,1,"Typically, you don't need to change anything in your code to capture JMX metrics except your Java Beans have to fulfill the Management Beans (MBeans) requirements you have to enable JMX monitoring in each Java process that should be monitored and the monitored system runs on Java 1.5 or later."
AppDynamics,43040838,42274447,0,"2017/03/27, 10:58:07",False,"2017/03/27, 10:58:07",324.0,7220665.0,1,See also  here  and  here .
AppDynamics,43040838,42274447,0,"2017/03/27, 10:58:07",False,"2017/03/27, 10:58:07",324.0,7220665.0,1,Then you can navigate to  Tiers &amp; Nodes -&gt; Select a tier -&gt; JMX tab
AppDynamics,42996563,42361990,0,"2017/03/24, 12:05:58",True,"2017/06/07, 09:23:41",324.0,7220665.0,2,"I am not exactly sure what the problem is, but you can share a Dashboard and open this shared Dashboard URL and make the Browser fullscreen."
AppDynamics,42996563,42361990,0,"2017/03/24, 12:05:58",True,"2017/06/07, 09:23:41",324.0,7220665.0,2,Thats how we do it and it works perfectly (on AppDynamics controller version 4.2).
AppDynamics,42996563,42361990,0,"2017/03/24, 12:05:58",True,"2017/06/07, 09:23:41",324.0,7220665.0,2,We use the 'revolver tab' add-on for the browser to switch between desktops.
AppDynamics,52864552,42719255,0,"2018/10/18, 01:37:57",False,"2018/10/18, 01:37:57",1.0,10521093.0,0,Try adding the line below in the crx-quickstart/conf/sling.properties:
AppDynamics,52864552,42719255,0,"2018/10/18, 01:37:57",False,"2018/10/18, 01:37:57",1.0,10521093.0,0,"org.osgi.framework.bootdelegation=com.singularity.*,com.yourkit."
AppDynamics,52864552,42719255,0,"2018/10/18, 01:37:57",False,"2018/10/18, 01:37:57",1.0,10521093.0,0,"*, ${org.apache.sling.launcher.bootdelegation}"
AppDynamics,42982843,42950752,0,"2017/03/23, 19:21:05",True,"2017/03/23, 19:21:05",474.0,4682632.0,1,You can build multiple baselines within AppDynamics if you'd like to.
AppDynamics,42982843,42950752,0,"2017/03/23, 19:21:05",True,"2017/03/23, 19:21:05",474.0,4682632.0,1,The thresholds should be auto calculated off deviation from baseline.
AppDynamics,42982843,42950752,0,"2017/03/23, 19:21:05",True,"2017/03/23, 19:21:05",474.0,4682632.0,1,This makes it so you don't need to configure them manually.
AppDynamics,42982843,42950752,0,"2017/03/23, 19:21:05",True,"2017/03/23, 19:21:05",474.0,4682632.0,1,"If you want to do SLA tracking, Business iQ (analytics) can do this very well."
AppDynamics,42982843,42950752,0,"2017/03/23, 19:21:05",True,"2017/03/23, 19:21:05",474.0,4682632.0,1,We also are building additional features around SLA use cases we can share.
AppDynamics,42982843,42950752,0,"2017/03/23, 19:21:05",True,"2017/03/23, 19:21:05",474.0,4682632.0,1,Feel free to email me or support for a hand.
AppDynamics,43535736,43496426,0,"2017/04/21, 09:47:35",False,"2017/04/21, 09:47:35",324.0,7220665.0,0,"I think everything you need is described  here  
Basically you need to do the following:"
AppDynamics,43535736,43496426,0,"2017/04/21, 09:47:35",False,"2017/04/21, 09:47:35",324.0,7220665.0,0,probably in your case it is
AppDynamics,43535736,43496426,0,"2017/04/21, 09:47:35",False,"2017/04/21, 09:47:35",324.0,7220665.0,0,via CURL it looks like this
AppDynamics,43535736,43496426,0,"2017/04/21, 09:47:35",False,"2017/04/21, 09:47:35",324.0,7220665.0,0,You can find more information about the AppDynamics API  here
AppDynamics,44338324,44269450,0,"2017/06/03, 01:35:15",True,"2017/06/03, 01:35:15",68.0,1394813.0,1,"Inside the widget settings make sure you select the ""Stack Areas or Columns"" checkbox."
AppDynamics,44338324,44269450,0,"2017/06/03, 01:35:15",True,"2017/06/03, 01:35:15",68.0,1394813.0,1,This works in every version of AppD I've used including 4.3.0.2.
AppDynamics,44411981,44391470,1,"2017/06/07, 14:55:46",True,"2017/06/07, 14:55:46",474.0,4682632.0,1,"SQL 2005 is supported, but this was a bug which was introduced in version 4.3.0."
AppDynamics,44411981,44391470,1,"2017/06/07, 14:55:46",True,"2017/06/07, 14:55:46",474.0,4682632.0,1,There is currently a diagnostic patch for this issue for supported customers.
AppDynamics,44411981,44391470,1,"2017/06/07, 14:55:46",True,"2017/06/07, 14:55:46",474.0,4682632.0,1,The fix should be in the next patch level once we isolate the issue.
AppDynamics,44411981,44391470,1,"2017/06/07, 14:55:46",True,"2017/06/07, 14:55:46",474.0,4682632.0,1,If you'd like support just email help@appdynamics.com and they can assist.
AppDynamics,44411981,44391470,1,"2017/06/07, 14:55:46",True,"2017/06/07, 14:55:46",474.0,4682632.0,1,Thanks.
AppDynamics,44606002,44569260,1,"2017/06/17, 17:54:45",False,"2017/06/17, 17:54:45",474.0,4682632.0,1,In order to capture this data follow these steps:
AppDynamics,44606002,44569260,1,"2017/06/17, 17:54:45",False,"2017/06/17, 17:54:45",474.0,4682632.0,1,"Pointer, you can also adjust the time periods in the metric browser to adjust the time."
AppDynamics,44606002,44569260,1,"2017/06/17, 17:54:45",False,"2017/06/17, 17:54:45",474.0,4682632.0,1,Hope this is helpful.
AppDynamics,45979323,44776999,0,"2017/08/31, 13:33:52",False,"2017/08/31, 13:33:52",676.0,7761024.0,0,Couple of things:
AppDynamics,45979323,44776999,0,"2017/08/31, 13:33:52",False,"2017/08/31, 13:33:52",676.0,7761024.0,0,I think the general URL format for app dynamics applications are (notice the '#'):
AppDynamics,45979323,44776999,0,"2017/08/31, 13:33:52",False,"2017/08/31, 13:33:52",676.0,7761024.0,0,"Also, I think the requests.get method needs an additional parameter for the 'account'."
AppDynamics,45979323,44776999,0,"2017/08/31, 13:33:52",False,"2017/08/31, 13:33:52",676.0,7761024.0,0,"For instance, my auth format looks like:"
AppDynamics,45979323,44776999,0,"2017/08/31, 13:33:52",False,"2017/08/31, 13:33:52",676.0,7761024.0,0,I am able to get a right response code back with this config.
AppDynamics,45979323,44776999,0,"2017/08/31, 13:33:52",False,"2017/08/31, 13:33:52",676.0,7761024.0,0,Let me know if this works for you.
AppDynamics,50472163,44776999,0,"2018/05/22, 19:09:38",False,"2018/05/22, 21:41:45",6.0,8195498.0,-1,You could also use native python code for more control:
AppDynamics,50472163,44776999,0,"2018/05/22, 19:09:38",False,"2018/05/22, 21:41:45",6.0,8195498.0,-1,example:
AppDynamics,50472163,44776999,0,"2018/05/22, 19:09:38",False,"2018/05/22, 21:41:45",6.0,8195498.0,-1,If you prefer it in JSON simply specify it in the request.
AppDynamics,44805282,44805162,1,"2017/06/28, 17:23:30",False,"2017/06/28, 17:23:30",333.0,2534221.0,0,"You can get severity information by appending &amp;severities=INFO,WARN,ERROR to your URL."
AppDynamics,44805282,44805162,1,"2017/06/28, 17:23:30",False,"2017/06/28, 17:23:30",333.0,2534221.0,0,"So your url must be like :  http://:/controller/rest/applications//business-transactions?output=JSON&amp;severities=INFO,WARN,ERROR"
AppDynamics,45253438,44805162,0,"2017/07/22, 13:24:47",False,"2017/07/22, 13:24:47",11.0,8349426.0,1,Severity is associated with specific events/entities in AppDynamics.
AppDynamics,45253438,44805162,0,"2017/07/22, 13:24:47",False,"2017/07/22, 13:24:47",11.0,8349426.0,1,Based on your API call I can see that you are trying to retrieve information about Business Transactions (BTs).
AppDynamics,45253438,44805162,0,"2017/07/22, 13:24:47",False,"2017/07/22, 13:24:47",11.0,8349426.0,1,Severity param is not associated with BTs.
AppDynamics,45253438,44805162,0,"2017/07/22, 13:24:47",False,"2017/07/22, 13:24:47",11.0,8349426.0,1,e.g.
AppDynamics,45253438,44805162,0,"2017/07/22, 13:24:47",False,"2017/07/22, 13:24:47",11.0,8349426.0,1,"You can pull Severity for Health rule violations in AppDynamics by making the following API call:
http:///controller/rest/applications//problems/healthrule-violations
Result:"
AppDynamics,45253438,44805162,0,"2017/07/22, 13:24:47",False,"2017/07/22, 13:24:47",11.0,8349426.0,1,You can find further information about using AppD controller API in the following documentation pages:
AppDynamics,45253438,44805162,0,"2017/07/22, 13:24:47",False,"2017/07/22, 13:24:47",11.0,8349426.0,1,https://docs.appdynamics.com/display/PRO42/AppDynamics+APIs
AppDynamics,45253438,44805162,0,"2017/07/22, 13:24:47",False,"2017/07/22, 13:24:47",11.0,8349426.0,1,https://docs.appdynamics.com/display/PRO42/Alert+and+Respond+API
AppDynamics,44960915,44939278,0,"2017/07/07, 03:51:06",False,"2017/07/07, 03:51:06",7.0,1858160.0,0,The Issue is resolved.
AppDynamics,44960915,44939278,0,"2017/07/07, 03:51:06",False,"2017/07/07, 03:51:06",7.0,1858160.0,0,"The controller-info of Machine Agent need not to have any Application,Node and Tier name."
AppDynamics,44960915,44939278,0,"2017/07/07, 03:51:06",False,"2017/07/07, 03:51:06",7.0,1858160.0,0,It should include unique host id which should be same as controller-info of JavaAgent.
AppDynamics,46592166,46591986,0,"2017/10/05, 21:07:15",False,"2017/10/05, 21:07:15",938.0,1640095.0,1,It looks like you can use environment variables to configure the python appdynamics agent as well.
AppDynamics,46592166,46591986,0,"2017/10/05, 21:07:15",False,"2017/10/05, 21:07:15",938.0,1640095.0,1,Open up your repl
AppDynamics,46592166,46591986,0,"2017/10/05, 21:07:15",False,"2017/10/05, 21:07:15",938.0,1640095.0,1,"For the usual configuration values (APP_NAME, TIER_NAME, NODE_NAME, etc) you can configure them via the environment variables."
AppDynamics,46592166,46591986,0,"2017/10/05, 21:07:15",False,"2017/10/05, 21:07:15",938.0,1640095.0,1,You just need to prefix them with 'APPD_'.
AppDynamics,46592166,46591986,0,"2017/10/05, 21:07:15",False,"2017/10/05, 21:07:15",938.0,1640095.0,1,For for APP_NAME it would be:
AppDynamics,47352287,46591986,0,"2017/11/17, 15:57:08",False,"2017/11/17, 15:57:08",1248.0,2339348.0,0,You can configure the python agent in your code like so:
AppDynamics,47352287,46591986,0,"2017/11/17, 15:57:08",False,"2017/11/17, 15:57:08",1248.0,2339348.0,0,"Alternatively, you can pass in the location of your appdynamics.cfg file."
AppDynamics,47352287,46591986,0,"2017/11/17, 15:57:08",False,"2017/11/17, 15:57:08",1248.0,2339348.0,0,"That is to say, setting environment variables is not enough."
AppDynamics,47352287,46591986,0,"2017/11/17, 15:57:08",False,"2017/11/17, 15:57:08",1248.0,2339348.0,0,"Then you need to manually start the proxy (after you  appd.init ) by running
 pyagent proxy start"
AppDynamics,47352287,46591986,0,"2017/11/17, 15:57:08",False,"2017/11/17, 15:57:08",1248.0,2339348.0,0,The agent configuration from your code will be automatically used by the proxy.
AppDynamics,47352287,46591986,0,"2017/11/17, 15:57:08",False,"2017/11/17, 15:57:08",1248.0,2339348.0,0,For a full list of config keys see the  setting docs
AppDynamics,53704958,46591986,0,"2018/12/10, 13:41:43",False,"2018/12/10, 13:41:43",13.0,4981560.0,0,I managed to define just environment variables without changing application code.
AppDynamics,53704958,46591986,0,"2018/12/10, 13:41:43",False,"2018/12/10, 13:41:43",13.0,4981560.0,0,Note that variable name for controller host is APPD_CONTROLLER_HOST.
AppDynamics,53704958,46591986,0,"2018/12/10, 13:41:43",False,"2018/12/10, 13:41:43",13.0,4981560.0,0,You can also pass command line parameters to the process.
AppDynamics,47149088,47129599,0,"2017/11/07, 04:33:37",False,"2017/11/07, 04:33:37",474.0,4682632.0,0,"Yes, it does transaction tracking for every intra-component call across languages."
AppDynamics,47149088,47129599,0,"2017/11/07, 04:33:37",False,"2017/11/07, 04:33:37",474.0,4682632.0,0,Without code changes.
AppDynamics,47149088,47129599,0,"2017/11/07, 04:33:37",False,"2017/11/07, 04:33:37",474.0,4682632.0,0,"When there is a slow transaction detail down to code level will show up in snapshots, this is what you'd use for diagnostics."
AppDynamics,47149088,47129599,0,"2017/11/07, 04:33:37",False,"2017/11/07, 04:33:37",474.0,4682632.0,0,Depending on your language you can also configure data collectors which do runtime instrumentation of custom data from the code.
AppDynamics,47149088,47129599,0,"2017/11/07, 04:33:37",False,"2017/11/07, 04:33:37",474.0,4682632.0,0,"These show up on every call, and you can turn them into metrics too."
AppDynamics,47149088,47129599,0,"2017/11/07, 04:33:37",False,"2017/11/07, 04:33:37",474.0,4682632.0,0,Once again no code changes.
AppDynamics,47149111,47135535,0,"2017/11/07, 04:37:05",False,"2017/11/07, 04:37:05",474.0,4682632.0,3,Zipkin only does tracing.
AppDynamics,47149111,47135535,0,"2017/11/07, 04:37:05",False,"2017/11/07, 04:37:05",474.0,4682632.0,3,"APM tools like Appdynamics do other monitoring (browser, mobile, database, server, network)."
AppDynamics,47149111,47135535,0,"2017/11/07, 04:37:05",False,"2017/11/07, 04:37:05",474.0,4682632.0,3,Code-level diagnostics with automated overhead controls and limiters.
AppDynamics,47149111,47135535,0,"2017/11/07, 04:37:05",False,"2017/11/07, 04:37:05",474.0,4682632.0,3,Don't forget log analytics and transaction analytics.
AppDynamics,47149111,47135535,0,"2017/11/07, 04:37:05",False,"2017/11/07, 04:37:05",474.0,4682632.0,3,It also collects metrics.
AppDynamics,47149111,47135535,0,"2017/11/07, 04:37:05",False,"2017/11/07, 04:37:05",474.0,4682632.0,3,"There is a lot more to APM than just tracing, which is what Zipkin does."
AppDynamics,47149111,47135535,0,"2017/11/07, 04:37:05",False,"2017/11/07, 04:37:05",474.0,4682632.0,3,"You could do this with a stack of 20 open source tools, but you'd have to deal with disjointed UIs and data models not to mention the work associated with keeping them all working."
AppDynamics,47382243,47378215,1,"2017/11/19, 23:34:58",False,"2017/11/19, 23:34:58",127.0,7526329.0,-2,"It's actually more the other way around - Crittercism does crash logging, network monitoring, and performance timing whereas AppDynamics is more focused on server monitoring."
AppDynamics,47398459,47378215,0,"2017/11/20, 20:08:39",True,"2017/11/20, 20:08:39",474.0,4682632.0,0,"Both products essentially do the same thing, if anything AppDynamics has advanced beyond Crittercism (now called Apteligent)."
AppDynamics,47398459,47378215,0,"2017/11/20, 20:08:39",True,"2017/11/20, 20:08:39",474.0,4682632.0,0,"They had the lead when the mAPM market started, but now there is not much innovating happening, especially after they sold the company to VMware earlier this year."
AppDynamics,47398459,47378215,0,"2017/11/20, 20:08:39",True,"2017/11/20, 20:08:39",474.0,4682632.0,0,"Here are the mRUM docs for AppD:  https://docs.appdynamics.com/display/PRO44/Mobile+Real+User+Monitoring  some of the more advanced features in AppD you will not find in Appteligent would be things like screen recording, breadcrumb/navigation for every click and interaction."
AppDynamics,47398459,47378215,0,"2017/11/20, 20:08:39",True,"2017/11/20, 20:08:39",474.0,4682632.0,0,"The most valuable feature, of course, is the measurement outside of a straight mobile use case."
AppDynamics,47398459,47378215,0,"2017/11/20, 20:08:39",True,"2017/11/20, 20:08:39",474.0,4682632.0,0,"AppD does measure and ties together the mobile transaction with the backend, server (and network), Docker containers, AWS or other cloud providers."
AppDynamics,47398459,47378215,0,"2017/11/20, 20:08:39",True,"2017/11/20, 20:08:39",474.0,4682632.0,0,"Additionally, it monitors performance and usage of browsers."
AppDynamics,47398459,47378215,0,"2017/11/20, 20:08:39",True,"2017/11/20, 20:08:39",474.0,4682632.0,0,"Disclosure: I used to be the lead for these products at Gartner, and have been working at AppDynamics for the last 3 years."
AppDynamics,50738776,47378215,0,"2018/06/07, 13:19:23",False,"2018/06/07, 13:25:10",2304.0,1162044.0,0,For Apple platforms I recommend you to avoid any third party crash log frameworks and use  Xcode crashes organizer.
AppDynamics,52641688,49509430,0,"2018/10/04, 10:52:28",False,"2018/10/04, 10:52:28",21.0,3348861.0,0,It seems that you schould be able to define your custom  Asynchronous Transaction Demarcator  as described in:   https://docs.appdynamics.com/display/PRO44/Asynchronous+Transaction+Demarcators
AppDynamics,52641688,49509430,0,"2018/10/04, 10:52:28",False,"2018/10/04, 10:52:28",21.0,3348861.0,0,which will point to the last method of Runnable that you passes to the Executor.
AppDynamics,52641688,49509430,0,"2018/10/04, 10:52:28",False,"2018/10/04, 10:52:28",21.0,3348861.0,0,Then according to the documentation all you need is to attach the Demarcator to your Business Transaction and it will collect the asynchronous call.
AppDynamics,50270677,50218209,3,"2018/05/10, 13:22:05",False,"2018/05/10, 13:22:05",16918.0,1047788.0,0,The first step is to add a username and password in the etc/users.properties file.
AppDynamics,50270677,50218209,3,"2018/05/10, 13:22:05",False,"2018/05/10, 13:22:05",16918.0,1047788.0,0,"For most purposes, it is ok to just 
use the default settings provided out of the box."
AppDynamics,50270677,50218209,3,"2018/05/10, 13:22:05",False,"2018/05/10, 13:22:05",16918.0,1047788.0,0,"For this, just uncomment the following line:"
AppDynamics,50270677,50218209,3,"2018/05/10, 13:22:05",False,"2018/05/10, 13:22:05",16918.0,1047788.0,0,"Then, you must bypass credential checks on BrokeViewMBean by adding it to the whitelist ACL configuration."
AppDynamics,50270677,50218209,3,"2018/05/10, 13:22:05",False,"2018/05/10, 13:22:05",16918.0,1047788.0,0,You can do so by replacing this line:
AppDynamics,50270677,50218209,3,"2018/05/10, 13:22:05",False,"2018/05/10, 13:22:05",16918.0,1047788.0,0,with this:
AppDynamics,50270677,50218209,3,"2018/05/10, 13:22:05",False,"2018/05/10, 13:22:05",16918.0,1047788.0,0,"In addition to being the correct way, it also enables several different configuration options (eg: port, listen address, etc) by just changing the file org.apache.karaf.management.cfg on broker's etc directory."
AppDynamics,50270677,50218209,3,"2018/05/10, 13:22:05",False,"2018/05/10, 13:22:05",16918.0,1047788.0,0,"Please keep in mind that JMX access is made through a different JMX connector root in this case: it uses  karaf-root  instead of  jmxrmi , which was previously used in the older method."
AppDynamics,50270677,50218209,3,"2018/05/10, 13:22:05",False,"2018/05/10, 13:22:05",16918.0,1047788.0,0,"It also uses port 1099 by default, instead of 1616."
AppDynamics,50270677,50218209,3,"2018/05/10, 13:22:05",False,"2018/05/10, 13:22:05",16918.0,1047788.0,0,"Therefore, the uri should be"
AppDynamics,52869901,52825100,0,"2018/10/18, 11:25:36",False,"2018/10/18, 11:25:36",294930.0,208809.0,3,"From my current knowledge: no, AppDynamics doesn't support OpenTracing yet."
AppDynamics,52869901,52825100,0,"2018/10/18, 11:25:36",False,"2018/10/18, 11:25:36",294930.0,208809.0,3,"Usually, APM vendors have their own OpenTracing tracers build off the official specification and then get them listed at  http://opentracing.io ."
AppDynamics,52869901,52825100,0,"2018/10/18, 11:25:36",False,"2018/10/18, 11:25:36",294930.0,208809.0,3,But as of this writing there is no mention of any AppDynamics Tracers at  https://opentracing.io/docs/supported-tracers/  nor  https://github.com/opentracing-contrib/meta .
AppDynamics,52869901,52825100,0,"2018/10/18, 11:25:36",False,"2018/10/18, 11:25:36",294930.0,208809.0,3,"Full disclosure: I work for Instana, a competitor that does support OpenTracing."
AppDynamics,52989775,52825100,0,"2018/10/25, 15:53:20",False,"2018/10/25, 15:53:20",474.0,4682632.0,0,No AppD doesn't support OpenTracing at this time.
AppDynamics,52989775,52825100,0,"2018/10/25, 15:53:20",False,"2018/10/25, 15:53:20",474.0,4682632.0,0,The question is why do you want this functionality when you can already extract custom data from transactions dynamically with most AppDynamics agents?
AppDynamics,52989775,52825100,0,"2018/10/25, 15:53:20",False,"2018/10/25, 15:53:20",474.0,4682632.0,0,Do you really want to hardcode your APM tool into your software application?
AppDynamics,52989775,52825100,0,"2018/10/25, 15:53:20",False,"2018/10/25, 15:53:20",474.0,4682632.0,0,"AppDynamics is building a unique way to support OpenTracing, which is currently in testing, but the approach is not by hardcoding libraries into the code."
AppDynamics,52989775,52825100,0,"2018/10/25, 15:53:20",False,"2018/10/25, 15:53:20",474.0,4682632.0,0,"If you'd like to learn more please reach out to support, or you can contact me directly as I work for AppDynamics."
AppDynamics,52989775,52825100,0,"2018/10/25, 15:53:20",False,"2018/10/25, 15:53:20",474.0,4682632.0,0,Thanks.
AppDynamics,58802450,54003615,0,"2019/11/11, 15:36:01",False,"2019/11/11, 15:36:01",26.0,12355048.0,0,You need to add:
AppDynamics,58802450,54003615,0,"2019/11/11, 15:36:01",False,"2019/11/11, 15:36:01",26.0,12355048.0,0,into   wrapper.conf  file.
AppDynamics,58802450,54003615,0,"2019/11/11, 15:36:01",False,"2019/11/11, 15:36:01",26.0,12355048.0,0,https://docs.appdynamics.com/display/PRO45/Tanuki+Service+Wrapper+Settings
AppDynamics,58802165,55234879,0,"2019/11/11, 15:18:20",False,"2019/11/11, 15:18:20",26.0,12355048.0,0,You can export the dashboard and you will get the json version of your dashboard.
AppDynamics,58802165,55234879,0,"2019/11/11, 15:18:20",False,"2019/11/11, 15:18:20",26.0,12355048.0,0,(The export button is located top of the your dashboard page)
AppDynamics,58802165,55234879,0,"2019/11/11, 15:18:20",False,"2019/11/11, 15:18:20",26.0,12355048.0,0,This json file can be easily editable with any editor and you can replace the Application and/or other properties which contain on of your dashboards.
AppDynamics,58802165,55234879,0,"2019/11/11, 15:18:20",False,"2019/11/11, 15:18:20",26.0,12355048.0,0,https://docs.appdynamics.com/display/PRO45/Import+and+Export+Custom+Dashboards+and+Templates+Using+the+UI
AppDynamics,55811490,55332537,1,"2019/04/23, 15:42:30",False,"2019/04/23, 15:42:30",53.0,2309810.0,0,Due to its proprietary nature i don't think the internals are available for anyone to view or discuss.
AppDynamics,55811490,55332537,1,"2019/04/23, 15:42:30",False,"2019/04/23, 15:42:30",53.0,2309810.0,0,Below link might give an idea of how the product works at a high level.
AppDynamics,55811490,55332537,1,"2019/04/23, 15:42:30",False,"2019/04/23, 15:42:30",53.0,2309810.0,0,https://www.appdynamics.com/product/how-it-works/
AppDynamics,55700067,55697936,0,"2019/04/16, 06:18:36",False,"2019/04/16, 06:18:36",11138.0,9132707.0,0,"You are on the right track, but you are not saying that you are having errors or showing the."
AppDynamics,55700067,55697936,0,"2019/04/16, 06:18:36",False,"2019/04/16, 06:18:36",11138.0,9132707.0,0,"Yet, based on your experience to date, I am sure you already know that PowerShell provides cmdlets for working with XML."
AppDynamics,55700067,55697936,0,"2019/04/16, 06:18:36",False,"2019/04/16, 06:18:36",11138.0,9132707.0,0,See them using ---
AppDynamics,55700067,55697936,0,"2019/04/16, 06:18:36",False,"2019/04/16, 06:18:36",11138.0,9132707.0,0,or get other XML modules from the MS PowerShellGallery.com using ---
AppDynamics,55700067,55697936,0,"2019/04/16, 06:18:36",False,"2019/04/16, 06:18:36",11138.0,9132707.0,0,--- and install the one(s) that fit your needed goals.
AppDynamics,55700067,55697936,0,"2019/04/16, 06:18:36",False,"2019/04/16, 06:18:36",11138.0,9132707.0,0,And of course there are lot's of examples and videos on the topic.
AppDynamics,55700067,55697936,0,"2019/04/16, 06:18:36",False,"2019/04/16, 06:18:36",11138.0,9132707.0,0,"Searching for 'PowerShell working with XML', gives you plenty of hits."
AppDynamics,55700067,55697936,0,"2019/04/16, 06:18:36",False,"2019/04/16, 06:18:36",11138.0,9132707.0,0,PowerShell Data Basics: XML
AppDynamics,55700067,55697936,0,"2019/04/16, 06:18:36",False,"2019/04/16, 06:18:36",11138.0,9132707.0,0,"For the most part, what you will find is very similar to what you already have posted."
AppDynamics,55700067,55697936,0,"2019/04/16, 06:18:36",False,"2019/04/16, 06:18:36",11138.0,9132707.0,0,"Yet, you say you want add nodes / elements, but that is not in your post, so, something like the below should help."
AppDynamics,55700067,55697936,0,"2019/04/16, 06:18:36",False,"2019/04/16, 06:18:36",11138.0,9132707.0,0,Or even just using the  WebAdministration module  on the IIS server directly.
AppDynamics,55709010,55697936,0,"2019/04/16, 16:11:04",False,"2019/04/16, 16:11:04",101.0,2586735.0,0,You should check out the AppDynamics agent installation PowerShell Extension:  https://www.appdynamics.com/community/exchange/extension/dotnet-agent-installation-with-remote-management-powershell-extension/
AppDynamics,55709010,55697936,0,"2019/04/16, 16:11:04",False,"2019/04/16, 16:11:04",101.0,2586735.0,0,"It should be able to handle what you are trying to do without having to generate the xml manually, check the pdf for advanced options and read about the Add-IISApplicationMonitoring cmdlet"
AppDynamics,55709010,55697936,0,"2019/04/16, 16:11:04",False,"2019/04/16, 16:11:04",101.0,2586735.0,0,What it can’t do is the newer stuff like “multiple business application” and “custom node name” configuration.
AppDynamics,55709010,55697936,0,"2019/04/16, 16:11:04",False,"2019/04/16, 16:11:04",101.0,2586735.0,0,(Which you can’t do in the install wizard either)
AppDynamics,55810979,55787843,0,"2019/04/23, 15:12:33",False,"2019/04/23, 15:24:27",53.0,2309810.0,0,"AppDynamics itself powerful enough to provide all sort of information, However if you still want actuator data to be captured and displayed then you may need to use AppD extension."
AppDynamics,55810979,55787843,0,"2019/04/23, 15:12:33",False,"2019/04/23, 15:24:27",53.0,2309810.0,0,Please refer below official link to AppDyanmics Exchange.
AppDynamics,55810979,55787843,0,"2019/04/23, 15:12:33",False,"2019/04/23, 15:24:27",53.0,2309810.0,0,https://www.appdynamics.com/community/exchange/
AppDynamics,55810979,55787843,0,"2019/04/23, 15:12:33",False,"2019/04/23, 15:24:27",53.0,2309810.0,0,if the relevant is not available you may need write your own.
AppDynamics,57925017,57912322,0,"2019/09/13, 16:56:48",False,"2019/09/13, 16:56:48",21.0,5846608.0,0,I found this solution  https://github.com/Appdynamics/flowmap-builder
AppDynamics,57925017,57912322,0,"2019/09/13, 16:56:48",False,"2019/09/13, 16:56:48",21.0,5846608.0,0,Seems to be working so far.
AppDynamics,57925017,57912322,0,"2019/09/13, 16:56:48",False,"2019/09/13, 16:56:48",21.0,5846608.0,0,Doesn't rely (directly) on APIs but it works!
AppDynamics,58010383,57958059,0,"2019/09/19, 14:50:45",True,"2019/09/19, 14:50:45",41.0,7611518.0,0,For anyone who is looking for an answer here.
AppDynamics,58010383,57958059,0,"2019/09/19, 14:50:45",True,"2019/09/19, 14:50:45",41.0,7611518.0,0,This is resolved by unchecking the Extended  tracking option for kafka in AppDynamics Console.
AppDynamics,58010383,57958059,0,"2019/09/19, 14:50:45",True,"2019/09/19, 14:50:45",41.0,7611518.0,0,The option will be there in Automatic Backend Discovery.
AppDynamics,58010383,57958059,0,"2019/09/19, 14:50:45",True,"2019/09/19, 14:50:45",41.0,7611518.0,0,Hope this helps.
AppDynamics,58800924,57958059,0,"2019/11/11, 13:57:56",False,"2019/11/11, 13:57:56",26.0,12355048.0,0,"Disable default Kafka configuration on ""Backend Detection"" page"
AppDynamics,58800924,57958059,0,"2019/11/11, 13:57:56",False,"2019/11/11, 13:57:56",26.0,12355048.0,0,"Define a new ""Custom Exit Point"" with the last class and method which you see on the call graphs before Kafka exit call"
AppDynamics,58800924,57958059,0,"2019/11/11, 13:57:56",False,"2019/11/11, 13:57:56",26.0,12355048.0,0,"Don't forget to click on ""Is High Volume"" checkbox."
AppDynamics,58801167,57977882,0,"2019/11/11, 14:16:20",True,"2019/11/11, 14:16:20",26.0,12355048.0,1,If we take the example of an ECommerce Application:
AppDynamics,58801167,57977882,0,"2019/11/11, 14:16:20",True,"2019/11/11, 14:16:20",26.0,12355048.0,1,"Business Transactions  are Checkout, Landing Page, Add to Cart etc."
AppDynamics,58801167,57977882,0,"2019/11/11, 14:16:20",True,"2019/11/11, 14:16:20",26.0,12355048.0,1,which are known by every end user of the application.
AppDynamics,58801167,57977882,0,"2019/11/11, 14:16:20",True,"2019/11/11, 14:16:20",26.0,12355048.0,1,"These business transactions cover all the method executions, database calls, web service calls etc."
AppDynamics,58801167,57977882,0,"2019/11/11, 14:16:20",True,"2019/11/11, 14:16:20",26.0,12355048.0,1,Service End Points  are the sub calls(method call or web service call) execute inside of the Business Transactions.
AppDynamics,58801167,57977882,0,"2019/11/11, 14:16:20",True,"2019/11/11, 14:16:20",26.0,12355048.0,1,"Such as ""Check Inventory"" service which is executed in Checkout and Add to Cart transactions as well."
AppDynamics,58801167,57977882,0,"2019/11/11, 14:16:20",True,"2019/11/11, 14:16:20",26.0,12355048.0,1,"Information Points  are the key business or technical metric counts, such as Checkout amount, Add to Cart item count."
AppDynamics,58801167,57977882,0,"2019/11/11, 14:16:20",True,"2019/11/11, 14:16:20",26.0,12355048.0,1,"Service End Points and Information Points only give you performance metrics but Business Transactions also give you full code visibility with ""call graphs"""
AppDynamics,58801167,57977882,0,"2019/11/11, 14:16:20",True,"2019/11/11, 14:16:20",26.0,12355048.0,1,"Also, there are some limitations like max 200 Business Transaction on the default but you can change these rules."
AppDynamics,58801167,57977882,0,"2019/11/11, 14:16:20",True,"2019/11/11, 14:16:20",26.0,12355048.0,1,"While configuring the BTs &amp; SEs, you must focus on the needs of AppDynamics users."
AppDynamics,58801167,57977882,0,"2019/11/11, 14:16:20",True,"2019/11/11, 14:16:20",26.0,12355048.0,1,"If you configure AppDynamics for mostly business teams, I can use BTs like I described above."
AppDynamics,58801167,57977882,0,"2019/11/11, 14:16:20",True,"2019/11/11, 14:16:20",26.0,12355048.0,1,"But If you target the Dev and Ops teams, you can configure your BTs based on method or service calls."
AppDynamics,58801167,57977882,0,"2019/11/11, 14:16:20",True,"2019/11/11, 14:16:20",26.0,12355048.0,1,There is no only single approach on BT &amp; SE configuration.
AppDynamics,58801167,57977882,0,"2019/11/11, 14:16:20",True,"2019/11/11, 14:16:20",26.0,12355048.0,1,You must shape that with the needs of your AppDynamics users.
AppDynamics,58800607,58644710,2,"2019/11/11, 13:37:12",False,"2019/11/11, 13:55:20",26.0,12355048.0,0,Configuration- Instrumentation- Transaction Detection- Add
AppDynamics,58800607,58644710,2,"2019/11/11, 13:37:12",False,"2019/11/11, 13:55:20",26.0,12355048.0,0,"On the ""Split Transactions Using Request Data"" section you must choose "" Specific URI Segments ""
Segment Numbers: 1,2,4"
AppDynamics,58800607,58644710,2,"2019/11/11, 13:37:12",False,"2019/11/11, 13:55:20",26.0,12355048.0,0,"In your case transaction name will be ""/data/scenario/job"""
AppDynamics,58800607,58644710,2,"2019/11/11, 13:37:12",False,"2019/11/11, 13:55:20",26.0,12355048.0,0,Sample Configuration:
AppDynamics,60203721,60082614,0,"2020/02/13, 10:51:30",False,"2020/02/13, 10:51:30",11.0,2131858.0,0,"On analyzing the heap dump taken during system idle state, we only see various WebAppClassLoaders holding instances of different library classes."
AppDynamics,60203721,60082614,0,"2020/02/13, 10:51:30",False,"2020/02/13, 10:51:30",11.0,2131858.0,0,This pattern is also explained in official blogs of APM experts like  Plumbr  and  Datadog  as a sign of healthy JVM where regular GC activity is occurring and they explain that it means none of the objects will stay in memory forever.
AppDynamics,60203721,60082614,0,"2020/02/13, 10:51:30",False,"2020/02/13, 10:51:30",11.0,2131858.0,0,From Plumbr blog:
AppDynamics,60203721,60082614,0,"2020/02/13, 10:51:30",False,"2020/02/13, 10:51:30",11.0,2131858.0,0,Seeing the following pattern is a confirmation that the JVM at question is definitely not leaking memory.
AppDynamics,60203721,60082614,0,"2020/02/13, 10:51:30",False,"2020/02/13, 10:51:30",11.0,2131858.0,0,The reason for the double-sawtooth pattern is that the JVM needs to allocate memory on the heap as new objects are created as a part of the normal program execution.
AppDynamics,60203721,60082614,0,"2020/02/13, 10:51:30",False,"2020/02/13, 10:51:30",11.0,2131858.0,0,Most of these objects are short-lived and quickly become garbage.
AppDynamics,60203721,60082614,0,"2020/02/13, 10:51:30",False,"2020/02/13, 10:51:30",11.0,2131858.0,0,These short-lived objects are collected by a collector called “Minor GC” and represent the small drops on the sawteeth.
AppDynamics,60129360,60127980,4,"2020/02/08, 19:36:49",False,"2020/02/09, 10:51:22",2705.0,1501456.0,0,The reason you don't see the browser name in HTTP protocol is because there is no browser.
AppDynamics,60129360,60127980,4,"2020/02/08, 19:36:49",False,"2020/02/09, 10:51:22",2705.0,1501456.0,0,The protocol is a transport level protocol which means it simulates the traffic of the browser without running the actual browser.
AppDynamics,60129360,60127980,4,"2020/02/08, 19:36:49",False,"2020/02/09, 10:51:22",2705.0,1501456.0,0,This allows the protocol to simulate many more virtual users than a client level protocol such as TruClient.
AppDynamics,60129360,60127980,4,"2020/02/08, 19:36:49",False,"2020/02/09, 10:51:22",2705.0,1501456.0,0,EDIT: There is no dedicated API and you must use the user agent.
AppDynamics,60129360,60127980,4,"2020/02/08, 19:36:49",False,"2020/02/09, 10:51:22",2705.0,1501456.0,0,Please refer to this article for more details:  https://www.appdynamics.com/blog/engineering/how-to-use-appdynamics-with-loadrunner-for-load-testing/
AppDynamics,62775806,61730657,0,"2020/07/07, 15:55:48",False,"2020/07/07, 15:55:48",1.0,11862547.0,0,I'd try setting &quot;Use data from last&quot; to  60 minutes .
AppDynamics,62775806,61730657,0,"2020/07/07, 15:55:48",False,"2020/07/07, 15:55:48",1.0,11862547.0,0,In Criteria-tab use &quot;Single Metric&quot; with  Sum  of &quot;Calls per Minute&quot; and define your threshold.
AppDynamics,65580996,63517278,0,"2021/01/05, 16:39:58",False,"2021/01/05, 16:39:58",3170.0,1237595.0,0,The Drop-off rate is the percentage of user sessions which reach the specific page and then end their session (they do not reach any further pages as part of the session and therefore &quot;drop-off&quot; the map).
AppDynamics,65580996,63517278,0,"2021/01/05, 16:39:58",False,"2021/01/05, 16:39:58",3170.0,1237595.0,0,This can be seen in the example (image) in the  documentation
AppDynamics,66421794,65134789,0,"2021/03/01, 14:02:52",False,"2021/03/01, 14:02:52",3170.0,1237595.0,0,Yes - you can do this using ADQL / Analytics searches (assuming you have this licensed).
AppDynamics,66421794,65134789,0,"2021/03/01, 14:02:52",False,"2021/03/01, 14:02:52",3170.0,1237595.0,0,Without specifics I can only give you a general guide:
AppDynamics,66623443,65710328,0,"2021/03/14, 12:32:34",False,"2021/03/20, 18:19:18",3170.0,1237595.0,0,Generally the workflow for sending reports from AppDynamics is as follows:
AppDynamics,66623443,65710328,0,"2021/03/14, 12:32:34",False,"2021/03/20, 18:19:18",3170.0,1237595.0,0,Note: There is a sample custom dashboard .json available here to get you started:  https://community.appdynamics.com/t5/Knowledge-Base/Sample-Custom-Dashboard-for-Business-Transaction-Report/ta-p/21264
AppDynamics,66623443,65710328,0,"2021/03/14, 12:32:34",False,"2021/03/20, 18:19:18",3170.0,1237595.0,0,Note: There are already many &quot;Standard Reports&quot; which could make things easier depending on use case.
AppDynamics,66623443,65710328,0,"2021/03/14, 12:32:34",False,"2021/03/20, 18:19:18",3170.0,1237595.0,0,"Note: If you instead want to export data and then analyse with your own tooling, then see the docs on Public API's available here:  https://docs.appdynamics.com/display/PRO21/AppDynamics+APIs"
AppDynamics,66421156,66096667,0,"2021/03/01, 13:19:27",False,"2021/03/01, 13:19:27",3170.0,1237595.0,0,All currently available AppDynamics APIs are documented here:  https://docs.appdynamics.com/display/PRO21/AppDynamics+APIs
AppDynamics,66421156,66096667,0,"2021/03/01, 13:19:27",False,"2021/03/01, 13:19:27",3170.0,1237595.0,0,The main page includes a summary of what is available.
AppDynamics,31367542,31358408,0,"2015/07/12, 15:01:07",False,"2015/07/12, 15:01:07",474.0,4682632.0,0,Business transactions should accomplish this.
AppDynamics,31367542,31358408,0,"2015/07/12, 15:01:07",False,"2015/07/12, 15:01:07",474.0,4682632.0,0,If you want to report on each web service you can build a report or custom dashboard.
AppDynamics,31367542,31358408,0,"2015/07/12, 15:01:07",False,"2015/07/12, 15:01:07",474.0,4682632.0,0,If you need more assistance just email help@appdynamics.com
AppDynamics,29093613,29083565,0,"2015/03/17, 09:37:21",True,"2015/03/17, 09:37:21",5170.0,1242093.0,0,AFAIK the critical point for AppDynamics (or profilers like that) it is essential to find an entry point.
AppDynamics,29093613,29083565,0,"2015/03/17, 09:37:21",True,"2015/03/17, 09:37:21",5170.0,1242093.0,0,"Usually the prefered way is to have an Servlet ""Endpoint"" that starts a threat and can be followed."
AppDynamics,29093613,29083565,0,"2015/03/17, 09:37:21",True,"2015/03/17, 09:37:21",5170.0,1242093.0,0,"For the scenario you are describing this wouldn't work as it's missing the ""trigger"" to start the following."
AppDynamics,29093613,29083565,0,"2015/03/17, 09:37:21",True,"2015/03/17, 09:37:21",5170.0,1242093.0,0,Most likely you'll need to build your own app-dynamics monitoring extension for it.
AppDynamics,29100312,29083565,2,"2015/03/17, 15:27:06",False,"2015/03/17, 15:27:06",26.0,2378795.0,0,By default much of the Apache stuff is excluded.
AppDynamics,29100312,29083565,2,"2015/03/17, 15:27:06",False,"2015/03/17, 15:27:06",26.0,2378795.0,0,"Try adding Call Graph Settings (Configure    Instrumentation    Call Graph Settings), to include specific transports, like org.apache.camel.component.file."
AppDynamics,29100312,29083565,2,"2015/03/17, 15:27:06",False,"2015/03/17, 15:27:06",26.0,2378795.0,0,* in the Specific sub-packages / classes from the Excluded Packages to be included in call graphs section.
AppDynamics,29100312,29083565,2,"2015/03/17, 15:27:06",False,"2015/03/17, 15:27:06",26.0,2378795.0,0,Do not include org.apache.camel.
AppDynamics,29100312,29083565,2,"2015/03/17, 15:27:06",False,"2015/03/17, 15:27:06",26.0,2378795.0,0,* as it will instrument all the camel code which is very expensive.
AppDynamics,29100312,29083565,2,"2015/03/17, 15:27:06",False,"2015/03/17, 15:27:06",26.0,2378795.0,0,"You may want to do it at first to detect what you want to watch, but make sure to change it back."
AppDynamics,29398487,29083565,1,"2015/04/01, 21:48:07",False,"2015/04/01, 21:48:07",26.0,2378795.0,0,Edit AppServerAgent\conf\app-agent-config.xml:
AppDynamics,29398487,29083565,1,"2015/04/01, 21:48:07",False,"2015/04/01, 21:48:07",26.0,2378795.0,0,From the Controller web site:
AppDynamics,29398487,29083565,1,"2015/04/01, 21:48:07",False,"2015/04/01, 21:48:07",26.0,2378795.0,0,"Configure    Instrumentation    Call Graph Settings
Add Always Shown Package/Class: org.apache.camel."
AppDynamics,29398487,29083565,1,"2015/04/01, 21:48:07",False,"2015/04/01, 21:48:07",26.0,2378795.0,0,*
AppDynamics,29398487,29083565,1,"2015/04/01, 21:48:07",False,"2015/04/01, 21:48:07",26.0,2378795.0,0,"Servers    App Servers    {tiername}    {nodename}    Agents
App Server Agent
Configure
Use Custom Configuration
find-entry-points: true"
AppDynamics,29370213,29346979,0,"2015/03/31, 16:51:25",True,"2015/03/31, 16:51:25",602.0,3453371.0,0,Can I ask if your trial is beyond 15 days?
AppDynamics,29370213,29346979,0,"2015/03/31, 16:51:25",True,"2015/03/31, 16:51:25",602.0,3453371.0,0,According to their site the trial only includes monitoring for one agent beyond that time - that would explain why only your first agent delivers data.
AppDynamics,29370213,29346979,0,"2015/03/31, 16:51:25",True,"2015/03/31, 16:51:25",602.0,3453371.0,0,"Other tools in that space, e.g: Dynatrace free trial - gives you a bit more time if that's what you need and also allow you to extend the free trial if you need more time"
AppDynamics,30553280,29346979,0,"2015/05/31, 05:34:32",False,"2015/05/31, 05:34:32",474.0,4682632.0,0,You have gone beyond the 15 day pro trial and you are using the free product.
AppDynamics,30553280,29346979,0,"2015/05/31, 05:34:32",False,"2015/05/31, 05:34:32",474.0,4682632.0,0,"If you want to buy the product you can get an extension on the trial, if you do not want to buy the product it will not trace across JVMs."
AppDynamics,16543274,14993309,2,"2013/05/14, 15:33:59",True,"2013/05/14, 15:33:59",1970.0,131903.0,15,"Not sure if you already have resolved this, but: SNI is SQL Server Network Interface, and the mentioned method exists in most ADO.NET full call stacks that wait for data from SQL Server."
AppDynamics,16543274,14993309,2,"2013/05/14, 15:33:59",True,"2013/05/14, 15:33:59",1970.0,131903.0,15,"This is regardless of whether the higher-level implementation is EF, raw ADO.NET or whatever."
AppDynamics,16543274,14993309,2,"2013/05/14, 15:33:59",True,"2013/05/14, 15:33:59",1970.0,131903.0,15,"I'm not sure which metric or signal AppDynamics uses to capture the completion of a stored procedure execution, but you could be seeing this kind of behavior if your stored procedure completes relatively fast, but transmitting the query result from the server to your client takes a while."
AppDynamics,16543274,14993309,2,"2013/05/14, 15:33:59",True,"2013/05/14, 15:33:59",1970.0,131903.0,15,"Without knowing more about your infrastructure, it is very hard to help further."
AppDynamics,16543274,14993309,2,"2013/05/14, 15:33:59",True,"2013/05/14, 15:33:59",1970.0,131903.0,15,"If the problem still persists, I would recommend running the same query in SQL Server Management studio with SET STATISTICS TIME ON and ""Include Client Statistics"" switched to on."
AppDynamics,16543274,14993309,2,"2013/05/14, 15:33:59",True,"2013/05/14, 15:33:59",1970.0,131903.0,15,Perhaps those numbers would give you an idea on whether data transfer is actually the problem.
AppDynamics,42025178,14993309,0,"2017/02/03, 15:12:58",False,"2017/02/03, 15:12:58",2131.0,585819.0,1,"In my case it was indeed, as Jouni mentions, very slow transmitting of the query results."
AppDynamics,42025178,14993309,0,"2017/02/03, 15:12:58",False,"2017/02/03, 15:12:58",2131.0,585819.0,1,I use Automapper to prepare data for sending to client.
AppDynamics,42025178,14993309,0,"2017/02/03, 15:12:58",False,"2017/02/03, 15:12:58",2131.0,585819.0,1,"So, it's unclear what exact property caused the load but to be sure I've cut all compound ones I don't need to show on client side."
AppDynamics,42025178,14993309,0,"2017/02/03, 15:12:58",False,"2017/02/03, 15:12:58",2131.0,585819.0,1,(I originally needed a collection to show in grid on client side.)
AppDynamics,42025178,14993309,0,"2017/02/03, 15:12:58",False,"2017/02/03, 15:12:58",2131.0,585819.0,1,The execution became very fast.
AppDynamics,49904327,14993309,0,"2018/04/18, 19:08:53",False,"2018/04/18, 19:08:53",1707.0,2308106.0,0,I've came across similar issue - it turns out that SqlDataReader.Dispose can get stuck for very lengthy time if you break early from large select.
AppDynamics,49904327,14993309,0,"2018/04/18, 19:08:53",False,"2018/04/18, 19:08:53",1707.0,2308106.0,0,see:  https://github.com/dotnet/corefx/issues/29181
AppDynamics,14983064,14330329,3,"2013/02/20, 17:05:14",False,"2016/08/20, 12:41:24",424.0,2091508.0,7,"please see my reply in 
 G1GC long pause with initial-mark"
AppDynamics,14983064,14330329,3,"2013/02/20, 17:05:14",False,"2016/08/20, 12:41:24",424.0,2091508.0,7,your every setting should has a solid reason to be here...and unfortunately some of them don't have e.g.
AppDynamics,14983064,14330329,3,"2013/02/20, 17:05:14",False,"2016/08/20, 12:41:24",424.0,2091508.0,7,"-XX:+UseBiasedLocking (used for combination of tenured and young generation GCs, but G1GC is capable to handle both) -XX:+DisableExplicitGC (its unpredictable, in my experience its never restrict explicit gc calls)"
AppDynamics,14983064,14330329,3,"2013/02/20, 17:05:14",False,"2016/08/20, 12:41:24",424.0,2091508.0,7,"please use/tweak accordingly below mentioned settings to get optimum results, I'm giving you baseline to move forward, hope so these settings will work for you:
 Java G1 garbage collection in production"
AppDynamics,16782833,14330329,1,"2013/05/28, 05:08:52",True,"2013/05/28, 05:08:52",1275.0,1335717.0,5,"We recorded this bug on 1.7._u06 and upgraded to  1.7.0_21-b11  just a couple of days ago and we haven't seen any Full GC's since upgrade, so it seems like this bug was fixed in JVM."
AppDynamics,16782833,14330329,1,"2013/05/28, 05:08:52",True,"2013/05/28, 05:08:52",1275.0,1335717.0,5,The code cache memory profiles look much nicer now too in the profiler.
AppDynamics,16782833,14330329,1,"2013/05/28, 05:08:52",True,"2013/05/28, 05:08:52",1275.0,1335717.0,5,"In the past, this problem used to be a daily one, one to more times per day."
AppDynamics,16782833,14330329,1,"2013/05/28, 05:08:52",True,"2013/05/28, 05:08:52",1275.0,1335717.0,5,"If the situation changes, I will report back."
AppDynamics,16782833,14330329,1,"2013/05/28, 05:08:52",True,"2013/05/28, 05:08:52",1275.0,1335717.0,5,"Until then, I consider this problem solved with the upgrade."
AppDynamics,28458957,28436858,1,"2015/02/11, 18:07:23",False,"2015/02/11, 18:13:31",49324.0,168493.0,0,"Your query profile shows that the ""Query end"" time is very large."
AppDynamics,28458957,28436858,1,"2015/02/11, 18:07:23",False,"2015/02/11, 18:13:31",49324.0,168493.0,0,This may be caused by a very (too) large  query cache .
AppDynamics,28458957,28436858,1,"2015/02/11, 18:07:23",False,"2015/02/11, 18:13:31",49324.0,168493.0,0,"Every time you perform an update statement (INSERT, DELETE, UPDATE), the query cache must be updated (every query that reads from the updated tables is invalidated)."
AppDynamics,28481902,28436858,1,"2015/02/12, 17:54:03",True,"2018/08/13, 11:49:47",525.0,3132445.0,20,I got in touch with RDS engineers from amazon and they gave me the solution.
AppDynamics,28481902,28436858,1,"2015/02/12, 17:54:03",True,"2018/08/13, 11:49:47",525.0,3132445.0,20,Such a high latency was due to a very low performing storage type.
AppDynamics,28481902,28436858,1,"2015/02/12, 17:54:03",True,"2018/08/13, 11:49:47",525.0,3132445.0,20,"Indeed, I was using the default 5GB SSD (which they call GP2) which gives 3 IOPS per GB of storage, resulting in 15 IOPS when my application required about 50 IOPS or even more."
AppDynamics,28481902,28436858,1,"2015/02/12, 17:54:03",True,"2018/08/13, 11:49:47",525.0,3132445.0,20,"Therefore, they suggested me to change the storage type to  Magnetic  which provides 100 IOPS as baseline."
AppDynamics,28481902,28436858,1,"2015/02/12, 17:54:03",True,"2018/08/13, 11:49:47",525.0,3132445.0,20,"Moreover, I've also been able to decrease the instance type because the bottleneck was only the disk."
AppDynamics,28481902,28436858,1,"2015/02/12, 17:54:03",True,"2018/08/13, 11:49:47",525.0,3132445.0,20,The migration took about 3h due to the very low performance of the source disk (GP2).
AppDynamics,28481902,28436858,1,"2015/02/12, 17:54:03",True,"2018/08/13, 11:49:47",525.0,3132445.0,20,Hope it may help someone out there!
AppDynamics,49953075,35971200,0,"2018/04/21, 09:34:37",True,"2018/04/21, 09:34:37",639.0,812093.0,0,We never were able to properly solve the issue but at some point it vanished.
AppDynamics,49953075,35971200,0,"2018/04/21, 09:34:37",True,"2018/04/21, 09:34:37",639.0,812093.0,0,If I'm not wrong one the client change the appdynamics configuration / removed it which seemed to have solved the issue.
AppDynamics,28937115,24261110,0,"2015/03/09, 09:38:07",False,"2015/03/09, 09:38:07",10216.0,854207.0,0,did you take a look to this google group ticket issue?
AppDynamics,28937115,24261110,0,"2015/03/09, 09:38:07",False,"2015/03/09, 09:38:07",10216.0,854207.0,0,https://groups.google.com/forum/#!topic/hazelcast/ivk6hzk2YwA
AppDynamics,28937115,24261110,0,"2015/03/09, 09:38:07",False,"2015/03/09, 09:38:07",10216.0,854207.0,0,Here in particular looks the reason of the issue.
AppDynamics,28937115,24261110,0,"2015/03/09, 09:38:07",False,"2015/03/09, 09:38:07",10216.0,854207.0,0,https://github.com/hazelcast/hazelcast/issues/553
AppDynamics,32718584,32717415,1,"2015/09/22, 16:44:26",False,"2015/09/22, 16:44:26",nan,nan,0,Have you tried to Minify your code?
AppDynamics,32718584,32717415,1,"2015/09/22, 16:44:26",False,"2015/09/22, 16:44:26",nan,nan,0,Minifying unneccesary characters from your code without removing any functionality.
AppDynamics,32718584,32717415,1,"2015/09/22, 16:44:26",False,"2015/09/22, 16:44:26",nan,nan,0,This could help speed up the download times.
AppDynamics,32718584,32717415,1,"2015/09/22, 16:44:26",False,"2015/09/22, 16:44:26",nan,nan,0,Take a look at the following link:  http://www.htmlgoodies.com/beyond/reference/7-tips-to-make-your-websites-load-faster.html
AppDynamics,32719029,32717415,0,"2015/09/22, 17:02:56",False,"2015/09/22, 17:18:53",55753.0,761202.0,5,Ill take a look at serving assets from webroot
AppDynamics,32719029,32717415,0,"2015/09/22, 17:02:56",False,"2015/09/22, 17:18:53",55753.0,761202.0,5,From the book  (emphasis added):
AppDynamics,32719029,32717415,0,"2015/09/22, 17:02:56",False,"2015/09/22, 17:18:53",55753.0,761202.0,5,It’s a well known fact that  serving assets through PHP is guaranteed to be slower than serving those assets without invoking PHP .
AppDynamics,32719029,32717415,0,"2015/09/22, 17:02:56",False,"2015/09/22, 17:18:53",55753.0,761202.0,5,"And while the core team has taken steps to make plugin and theme asset serving as fast as possible, there may be situations where more performance is required."
AppDynamics,32719029,32717415,0,"2015/09/22, 17:02:56",False,"2015/09/22, 17:18:53",55753.0,761202.0,5,In these situations it’s recommended that you either symlink or copy out plugin/theme assets to directories in app/webroot with paths matching those used by CakePHP.
AppDynamics,32719029,32717415,0,"2015/09/22, 17:02:56",False,"2015/09/22, 17:18:53",55753.0,761202.0,5,"Depending on many factors, &quot;slower&quot; can be anywhere between barely-noticeable to barely-usable."
AppDynamics,32719029,32717415,0,"2015/09/22, 17:02:56",False,"2015/09/22, 17:18:53",55753.0,761202.0,5,"This advice is not version specific, and pretty much always applies."
AppDynamics,32719029,32717415,0,"2015/09/22, 17:02:56",False,"2015/09/22, 17:18:53",55753.0,761202.0,5,"To make assets load faster, let the webserver take care of them for you."
AppDynamics,21035147,20975043,0,"2014/01/10, 04:24:32",False,"2016/07/23, 21:57:52",1025.0,1777388.0,0,"Yes, it is possible for the behaviour of GCs to change over time due to JIT optimisation."
AppDynamics,21035147,20975043,0,"2014/01/10, 04:24:32",False,"2016/07/23, 21:57:52",1025.0,1777388.0,0,One example is 'Escape Analysis' which has been enabled by default since Java 6u23.
AppDynamics,21035147,20975043,0,"2014/01/10, 04:24:32",False,"2016/07/23, 21:57:52",1025.0,1777388.0,0,This type of optimisation can prevent some objects from being created in the heap and therefore not require garbage collection at all.
AppDynamics,21035147,20975043,0,"2014/01/10, 04:24:32",False,"2016/07/23, 21:57:52",1025.0,1777388.0,0,For more information see  Java 7's HotSpot Performance Enhancements .
AppDynamics,16108561,16090702,0,"2013/04/19, 18:49:57",False,"2013/04/19, 18:49:57",625.0,2298085.0,1,It is really difficult to find the exact cause of your problem without more information.
AppDynamics,16108561,16090702,0,"2013/04/19, 18:49:57",False,"2013/04/19, 18:49:57",625.0,2298085.0,1,"But I can try to answer to your question : 
 Can the OS block the garbage collection ?"
AppDynamics,16108561,16090702,0,"2013/04/19, 18:49:57",False,"2013/04/19, 18:49:57",625.0,2298085.0,1,It is very unlikely than your OS blocks the thread garbage collector and let the other threads run.
AppDynamics,16108561,16090702,0,"2013/04/19, 18:49:57",False,"2013/04/19, 18:49:57",625.0,2298085.0,1,You should not investigate that way.
AppDynamics,16108561,16090702,0,"2013/04/19, 18:49:57",False,"2013/04/19, 18:49:57",625.0,2298085.0,1,Can the OS block the JVM ?
AppDynamics,16108561,16090702,0,"2013/04/19, 18:49:57",False,"2013/04/19, 18:49:57",625.0,2298085.0,1,"Yes it perflecty can and it do it a lot, but so fast than you think that the processes are all running at the same time."
AppDynamics,16108561,16090702,0,"2013/04/19, 18:49:57",False,"2013/04/19, 18:49:57",625.0,2298085.0,1,jvm is a process like the other and his under the control of the OS.
AppDynamics,16108561,16090702,0,"2013/04/19, 18:49:57",False,"2013/04/19, 18:49:57",625.0,2298085.0,1,You have to check the cpu used by the application when it hangs (with monitoring on the server not in the jvm).
AppDynamics,16108561,16090702,0,"2013/04/19, 18:49:57",False,"2013/04/19, 18:49:57",625.0,2298085.0,1,If it is very low then I see 2 causes (but there are more) :
AppDynamics,16108770,16090702,0,"2013/04/19, 18:59:28",False,"2013/04/19, 18:59:28",243.0,2096203.0,0,"In theory, YES, it can."
AppDynamics,16108770,16090702,0,"2013/04/19, 18:59:28",False,"2013/04/19, 18:59:28",243.0,2096203.0,0,"But it practice, it never should."
AppDynamics,16108770,16090702,0,"2013/04/19, 18:59:28",False,"2013/04/19, 18:59:28",243.0,2096203.0,0,"In most Java virtual machines, application threads are not the only threads that are running."
AppDynamics,16108770,16090702,0,"2013/04/19, 18:59:28",False,"2013/04/19, 18:59:28",243.0,2096203.0,0,"Apart from the application threads, there are compilation threads, finalizer threads, garbage collection threads, and some more."
AppDynamics,16108770,16090702,0,"2013/04/19, 18:59:28",False,"2013/04/19, 18:59:28",243.0,2096203.0,0,"Scheduling decisions for allocating CPU cores to these threads and other threads from other programs running on the machine are based on many parameters (thread priorities, their last execution time, etc), which try be fair to all threads."
AppDynamics,16108770,16090702,0,"2013/04/19, 18:59:28",False,"2013/04/19, 18:59:28",243.0,2096203.0,0,"So, in practice no thread in the system should be waiting for CPU allocation for an unreasonably long time and the operating system should not block any thread for an unlimited amount of time."
AppDynamics,16108770,16090702,0,"2013/04/19, 18:59:28",False,"2013/04/19, 18:59:28",243.0,2096203.0,0,There is minimal activity that the garbage collection threads (and other VM threads) need to do.
AppDynamics,16108770,16090702,0,"2013/04/19, 18:59:28",False,"2013/04/19, 18:59:28",243.0,2096203.0,0,They need to check periodically to see if a garbage collection is needed.
AppDynamics,16108770,16090702,0,"2013/04/19, 18:59:28",False,"2013/04/19, 18:59:28",243.0,2096203.0,0,"Even if the application threads are all suspended, there could be other VM threads, such as the JIT compiler thread or the finalizer thread, that do work and ,hence, allocate objects and trigger garbage collection."
AppDynamics,16108770,16090702,0,"2013/04/19, 18:59:28",False,"2013/04/19, 18:59:28",243.0,2096203.0,0,This is particularly true for meta-circular JVM that implement VM threads in Java and not in a C/C++;
AppDynamics,16108770,16090702,0,"2013/04/19, 18:59:28",False,"2013/04/19, 18:59:28",243.0,2096203.0,0,"Moreover, most modern JVM use a generational garbage collector (A garbage collector that partitions the heap into separate spaces and puts objects with different ages in different parts of the heap) This means as objects get older and older, they need to be moved to other older spaces."
AppDynamics,16108770,16090702,0,"2013/04/19, 18:59:28",False,"2013/04/19, 18:59:28",243.0,2096203.0,0,"Hence, even if there is no need to collect objects, a generational garbage collector may move objects from one space to another."
AppDynamics,16108770,16090702,0,"2013/04/19, 18:59:28",False,"2013/04/19, 18:59:28",243.0,2096203.0,0,Of course the details of each garbage collector in different from JVM to JVM.
AppDynamics,16108770,16090702,0,"2013/04/19, 18:59:28",False,"2013/04/19, 18:59:28",243.0,2096203.0,0,"To put more salt on the injury, some JVMs support more than one type of garbage collector."
AppDynamics,16108770,16090702,0,"2013/04/19, 18:59:28",False,"2013/04/19, 18:59:28",243.0,2096203.0,0,But seeing a minimal garbage collection activity in an idle application is no surprise.
AppDynamics,16108782,16090702,3,"2013/04/19, 19:00:01",True,"2013/04/19, 19:32:49",59821.0,12943.0,1,Does your OS have swapping enabled.
AppDynamics,16108782,16090702,3,"2013/04/19, 19:00:01",True,"2013/04/19, 19:32:49",59821.0,12943.0,1,"I've noticed HUGE problems with Java once it fills up all the ram on an OS with swapping enabled--it will actually devistate windows systems, effictevly locking them up and causing a reboot."
AppDynamics,16108782,16090702,3,"2013/04/19, 19:00:01",True,"2013/04/19, 19:32:49",59821.0,12943.0,1,My theory is this:
AppDynamics,16108782,16090702,3,"2013/04/19, 19:00:01",True,"2013/04/19, 19:32:49",59821.0,12943.0,1,"At first it doesn't effect the system much, but if you try to launch an app that wants a bunch of memory it can take a really long time, and your system just keeps degrading."
AppDynamics,16108782,16090702,3,"2013/04/19, 19:00:01",True,"2013/04/19, 19:32:49",59821.0,12943.0,1,"Multiple large VMs can make this worse, I run 3 or 4 huge ones and my system now starts to sieze when I get over 60-70% RAM usage."
AppDynamics,16108782,16090702,3,"2013/04/19, 19:00:01",True,"2013/04/19, 19:32:49",59821.0,12943.0,1,This is conjecture but it describes the behavior I've seen after days of testing.
AppDynamics,16108782,16090702,3,"2013/04/19, 19:00:01",True,"2013/04/19, 19:32:49",59821.0,12943.0,1,"The effect is that all the swapping seems to ""Prevent"" gc."
AppDynamics,16108782,16090702,3,"2013/04/19, 19:00:01",True,"2013/04/19, 19:32:49",59821.0,12943.0,1,More accurately the OS is spending most of the GC time swapping which makes it look like it's hanging doing nothing during GC.
AppDynamics,16108782,16090702,3,"2013/04/19, 19:00:01",True,"2013/04/19, 19:32:49",59821.0,12943.0,1,"A fix--set -Xmx to a lower value, drop it until you allow enough room to avoid swapping."
AppDynamics,16108782,16090702,3,"2013/04/19, 19:00:01",True,"2013/04/19, 19:32:49",59821.0,12943.0,1,"This has always fixed my problem, if it doesn't fix yours then I'm wrong about the cause of your problem :)"
AppDynamics,18276354,18276208,5,"2013/08/16, 18:00:12",True,"2013/08/16, 18:39:33",177788.0,1103872.0,3,One major GC per minute doesn't at all seem troublesome.
AppDynamics,18276354,18276208,5,"2013/08/16, 18:00:12",True,"2013/08/16, 18:39:33",177788.0,1103872.0,3,"Usually it takes about half a second, so that's 1/120th of your overall CPU usage."
AppDynamics,18276354,18276208,5,"2013/08/16, 18:00:12",True,"2013/08/16, 18:39:33",177788.0,1103872.0,3,It is also quite natural that heavy application load results in more memory allocation.
AppDynamics,18276354,18276208,5,"2013/08/16, 18:00:12",True,"2013/08/16, 18:39:33",177788.0,1103872.0,3,Apparently you are allocating some objects that live on for a while (could be caching).
AppDynamics,18276354,18276208,5,"2013/08/16, 18:00:12",True,"2013/08/16, 18:39:33",177788.0,1103872.0,3,My conclusion: the demonstrated GC behavior is not a proof that there is something wrong with your application's memory allocation.
AppDynamics,18276354,18276208,5,"2013/08/16, 18:00:12",True,"2013/08/16, 18:39:33",177788.0,1103872.0,3,I have looked more carefully at your diagrams (unfortunately they are quite difficult to read).
AppDynamics,18276354,18276208,5,"2013/08/16, 18:00:12",True,"2013/08/16, 18:39:33",177788.0,1103872.0,3,"You don't have one GC per min; you have 60  seconds  of major GC per minute, which would mean it's happening all the time."
AppDynamics,18276354,18276208,5,"2013/08/16, 18:00:12",True,"2013/08/16, 18:39:33",177788.0,1103872.0,3,That  does  look like major trouble; in fact in those conditions you usually get an OOME due to &quot;GC time percentage threshold crossed&quot;.
AppDynamics,18276354,18276208,5,"2013/08/16, 18:00:12",True,"2013/08/16, 18:39:33",177788.0,1103872.0,3,Note that the CMS collector which you are using is actually slower than the default one; its advantage is only that it doesn't &quot;stop the world&quot; as much.
AppDynamics,18276354,18276208,5,"2013/08/16, 18:00:12",True,"2013/08/16, 18:39:33",177788.0,1103872.0,3,It may not be the best choice for you.
AppDynamics,18276354,18276208,5,"2013/08/16, 18:00:12",True,"2013/08/16, 18:39:33",177788.0,1103872.0,3,"But you do look to either have a memory leak, or a general issue in program design."
AppDynamics,18276716,18276208,0,"2013/08/16, 18:16:46",False,"2013/08/16, 18:16:46",470.0,2623382.0,1,Are you always waiting for GC to take care of removing unused references?
AppDynamics,18276716,18276208,0,"2013/08/16, 18:16:46",False,"2013/08/16, 18:16:46",470.0,2623382.0,1,"Is there some places in your application that you know a reference to a heavy weight object won't be used anymore from that point, but it is not manually nulled?"
AppDynamics,18276716,18276208,0,"2013/08/16, 18:16:46",False,"2013/08/16, 18:16:46",470.0,2623382.0,1,Perhaps setting such heavy weight objects to null manually at the right places could prevent them growing till they hit the end of the heap....
AppDynamics,18277366,18276208,0,"2013/08/16, 18:49:24",False,"2013/08/16, 18:56:19",4061.0,2031799.0,2,"When JVM reaches heap maximum size, GC is called more frequently to prevent OOM exception."
AppDynamics,18277366,18276208,0,"2013/08/16, 18:49:24",False,"2013/08/16, 18:56:19",4061.0,2031799.0,2,Normal program execution should not happen at such circumstances.
AppDynamics,18277366,18276208,0,"2013/08/16, 18:49:24",False,"2013/08/16, 18:56:19",4061.0,2031799.0,2,"When a new object is allocated and JVM cant get enough of free space, GC is called."
AppDynamics,18277366,18276208,0,"2013/08/16, 18:49:24",False,"2013/08/16, 18:56:19",4061.0,2031799.0,2,This might postpone object allocation process and thus slowdown overall performance.
AppDynamics,18277366,18276208,0,"2013/08/16, 18:49:24",False,"2013/08/16, 18:56:19",4061.0,2031799.0,2,Garbage collection happens not concurrently in this case and you do not benefit from CMS collector.
AppDynamics,18277366,18276208,0,"2013/08/16, 18:49:24",False,"2013/08/16, 18:56:19",4061.0,2031799.0,2,"Try to play with  CMSInitiatingOccupancyFraction , its default value is about 90%."
AppDynamics,18277366,18276208,0,"2013/08/16, 18:49:24",False,"2013/08/16, 18:56:19",4061.0,2031799.0,2,"Setting this parameter to lower values, will force garbage collection before application reaches heap maximum."
AppDynamics,18277366,18276208,0,"2013/08/16, 18:49:24",False,"2013/08/16, 18:56:19",4061.0,2031799.0,2,Thus GC will work in parallel with application not clashing with it.
AppDynamics,18277366,18276208,0,"2013/08/16, 18:49:24",False,"2013/08/16, 18:56:19",4061.0,2031799.0,2,Have a look at article  Starting a Concurrent Collection Cycle .
AppDynamics,53270016,53269696,2,"2018/11/12, 22:59:13",True,"2018/11/12, 23:07:57",64495.0,413337.0,2,"Looking at the  source code  of  UTF8JsonGenerator._flushBuffer() , there is no indication of  LockSupport.parkNanos() ."
AppDynamics,53270016,53269696,2,"2018/11/12, 22:59:13",True,"2018/11/12, 23:07:57",64495.0,413337.0,2,So it has probably been inlined by the JIT compiler from  OutputStream.write() .
AppDynamics,53270016,53269696,2,"2018/11/12, 22:59:13",True,"2018/11/12, 23:07:57",64495.0,413337.0,2,My guess is it's the place where – for your application – Tomcat typically waits until the client has accepted all the output (expect for the last piece that fits into the typical connection buffer size) before it can close the connection.
AppDynamics,53270016,53269696,2,"2018/11/12, 22:59:13",True,"2018/11/12, 23:07:57",64495.0,413337.0,2,We have had bad experience with slow clients in the past.
AppDynamics,53270016,53269696,2,"2018/11/12, 22:59:13",True,"2018/11/12, 23:07:57",64495.0,413337.0,2,"Until they have retrieved all the output, they block a thread in Tomcat."
AppDynamics,53270016,53269696,2,"2018/11/12, 22:59:13",True,"2018/11/12, 23:07:57",64495.0,413337.0,2,And blocking a few dozens threads in Tomcat seriously reduces the throughput of a busy web app.
AppDynamics,53270016,53269696,2,"2018/11/12, 22:59:13",True,"2018/11/12, 23:07:57",64495.0,413337.0,2,Increasing the number of threads isn't the best option as the blocked threads also occupy a considerable amount of memory.
AppDynamics,53270016,53269696,2,"2018/11/12, 22:59:13",True,"2018/11/12, 23:07:57",64495.0,413337.0,2,So what you want is that Tomcat can handle a request as quickly as possible and then move on to the next request.
AppDynamics,53270016,53269696,2,"2018/11/12, 22:59:13",True,"2018/11/12, 23:07:57",64495.0,413337.0,2,"We have solved the problem by configuring our reverse proxy, which we always had in front of Tomcat, to immediately consume all output from Tomcat and deliver it to the client at the client's speed."
AppDynamics,53270016,53269696,2,"2018/11/12, 22:59:13",True,"2018/11/12, 23:07:57",64495.0,413337.0,2,The reverse proxy is very efficient at handling slow clients.
AppDynamics,53270016,53269696,2,"2018/11/12, 22:59:13",True,"2018/11/12, 23:07:57",64495.0,413337.0,2,"In our case, we have used  nginx ."
AppDynamics,53270016,53269696,2,"2018/11/12, 22:59:13",True,"2018/11/12, 23:07:57",64495.0,413337.0,2,We also looked at  Apache httpd .
AppDynamics,53270016,53269696,2,"2018/11/12, 22:59:13",True,"2018/11/12, 23:07:57",64495.0,413337.0,2,"But at the time, it wasn't capable of doing it."
AppDynamics,53270016,53269696,2,"2018/11/12, 22:59:13",True,"2018/11/12, 23:07:57",64495.0,413337.0,2,Additional Note
AppDynamics,53270016,53269696,2,"2018/11/12, 22:59:13",True,"2018/11/12, 23:07:57",64495.0,413337.0,2,Clients that unexpectedly disconnect also look like slow clients to the server as it takes some time until it has been fully established that the connection is broken.
AppDynamics,7413670,7409112,7,"2011/09/14, 11:52:55",False,"2011/09/15, 05:34:03",706.0,795321.0,1,"You could use Javamelody, but you have to:"
AppDynamics,7413670,7409112,7,"2011/09/14, 11:52:55",False,"2011/09/15, 05:34:03",706.0,795321.0,1,Generate war file from your play framework web
AppDynamics,7413670,7409112,7,"2011/09/14, 11:52:55",False,"2011/09/15, 05:34:03",706.0,795321.0,1,Edit web.xml in war file (http://code.google.com/p/javamelody/wiki/UserGuide?tm=6)
AppDynamics,60742827,60741008,0,"2020/03/18, 17:37:23",False,"2020/03/18, 17:37:23",6820.0,1709216.0,7,You need to set Multi-Release to true in the jar's MANIFEST.MF.
AppDynamics,60742827,60741008,0,"2020/03/18, 17:37:23",False,"2020/03/18, 17:37:23",6820.0,1709216.0,7,In the assembly plugin you should be able to do that by adding
AppDynamics,60742827,60741008,0,"2020/03/18, 17:37:23",False,"2020/03/18, 17:37:23",6820.0,1709216.0,7,to the configuration section of your assembly configuration.
AppDynamics,60742827,60741008,0,"2020/03/18, 17:37:23",False,"2020/03/18, 17:37:23",6820.0,1709216.0,7,You could also use the jar plugin to create your jar.
AppDynamics,60742827,60741008,0,"2020/03/18, 17:37:23",False,"2020/03/18, 17:37:23",6820.0,1709216.0,7,For that you would do
AppDynamics,43779475,43771883,0,"2017/05/04, 12:49:36",False,"2017/05/04, 12:49:36",383.0,2305216.0,2,Use Stackify Prefix for this kind of things:
AppDynamics,43779475,43771883,0,"2017/05/04, 12:49:36",False,"2017/05/04, 12:49:36",383.0,2305216.0,2,https://stackify.com/prefix/
AppDynamics,43779475,43771883,0,"2017/05/04, 12:49:36",False,"2017/05/04, 12:49:36",383.0,2305216.0,2,"View SQL queries: Including SQL parameters, affected records and how long it took to download the result set."
AppDynamics,22447659,19772523,0,"2014/03/17, 07:40:09",True,"2014/03/17, 07:40:09",391.0,1801817.0,2,"use tracer plugin where u can import the information in csv/html/xml.This is for jvisualvm
 http://visualvm.java.net/plugins.html"
AppDynamics,6588384,6588296,1,"2011/07/05, 23:19:08",True,"2011/07/05, 23:19:08",37594.0,192444.0,3,Start the application and run jconsole on the PID.
AppDynamics,6588384,6588296,1,"2011/07/05, 23:19:08",True,"2011/07/05, 23:19:08",37594.0,192444.0,3,While its running look at the heap in the console.
AppDynamics,6588384,6588296,1,"2011/07/05, 23:19:08",True,"2011/07/05, 23:19:08",37594.0,192444.0,3,When it near maxes get a heap dump.
AppDynamics,6588384,6588296,1,"2011/07/05, 23:19:08",True,"2011/07/05, 23:19:08",37594.0,192444.0,3,Download Eclipse MAT and parse the heap dump.
AppDynamics,6588384,6588296,1,"2011/07/05, 23:19:08",True,"2011/07/05, 23:19:08",37594.0,192444.0,3,If you notice the retained heap size is vastly less then the actual binary file parse the heap dump with -keep_unreachable_objects being set.
AppDynamics,6588384,6588296,1,"2011/07/05, 23:19:08",True,"2011/07/05, 23:19:08",37594.0,192444.0,3,If the latter is true and you are doing a full GC often you probably have some kind of leak going on.
AppDynamics,6588384,6588296,1,"2011/07/05, 23:19:08",True,"2011/07/05, 23:19:08",37594.0,192444.0,3,"Keep in mind when I say leak I don't mean a leak where the GC cannot retain memory, rather some how you are building large objects and making them unreachable often enough to cause the GC to consume a lot of CPU time."
AppDynamics,6588384,6588296,1,"2011/07/05, 23:19:08",True,"2011/07/05, 23:19:08",37594.0,192444.0,3,If you were seeing true memory leaks you would see GC Over head reached errors
AppDynamics,52681154,52680807,5,"2018/10/06, 19:38:02",False,"2018/10/08, 13:21:09",465.0,6645806.0,4,You can get  POD_NAME  and  POD_NAMESPACE  passing them as environment variables via  fieldRef .
AppDynamics,52681154,52680807,5,"2018/10/06, 19:38:02",False,"2018/10/08, 13:21:09",465.0,6645806.0,4,EDIT :  Added example env  REFERENCE_EXAMPLE  to show how to reference variables.
AppDynamics,52681154,52680807,5,"2018/10/06, 19:38:02",False,"2018/10/08, 13:21:09",465.0,6645806.0,4,Thanks to  this  answer for pointing out the  $()  interpolation.
AppDynamics,52681154,52680807,5,"2018/10/06, 19:38:02",False,"2018/10/08, 13:21:09",465.0,6645806.0,4,"You can reference  supports metadata.name, metadata.namespace, metadata.labels, metadata.annotations, spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP  as mentioned in the documentation  here ."
AppDynamics,52681154,52680807,5,"2018/10/06, 19:38:02",False,"2018/10/08, 13:21:09",465.0,6645806.0,4,"However,  CLUSTERNAME  is not a standard property available."
AppDynamics,52681154,52680807,5,"2018/10/06, 19:38:02",False,"2018/10/08, 13:21:09",465.0,6645806.0,4,"According to this  PR #22043 , the  CLUSTERNAME  should be injected to the  .metadata  field if using GCE."
AppDynamics,52681154,52680807,5,"2018/10/06, 19:38:02",False,"2018/10/08, 13:21:09",465.0,6645806.0,4,"Otherwise, you'll have to specific the  CLUSTERNAME  manually in the  .metadata  field  and then use  fieldRef  to inject it as an environment variable."
AppDynamics,52696997,52680807,0,"2018/10/08, 10:02:32",False,"2018/10/08, 10:08:15",415.0,6220162.0,1,"Below format helped me, suggested by ewok2030 and Praveen."
AppDynamics,52696997,52680807,0,"2018/10/08, 10:02:32",False,"2018/10/08, 10:08:15",415.0,6220162.0,1,Only one thing to make sure that the variable should be declared before they are used as JAVA_OPTS.
AppDynamics,52696997,52680807,0,"2018/10/08, 10:02:32",False,"2018/10/08, 10:08:15",415.0,6220162.0,1,containers:
AppDynamics,60428806,50355471,1,"2020/02/27, 10:11:48",False,"2020/02/27, 10:11:48",85.0,2574522.0,0,Are you sure you're not getting the FileNotFound when trying to do getInputStream on the connection later (or in your real code in case this is a simplified example)?
AppDynamics,60428806,50355471,1,"2020/02/27, 10:11:48",False,"2020/02/27, 10:11:48",85.0,2574522.0,0,Since HttpUrlConnection implements http protocol and 4xx codes are error codes trying to call getInputStream generates FNF.
AppDynamics,60428806,50355471,1,"2020/02/27, 10:11:48",False,"2020/02/27, 10:11:48",85.0,2574522.0,0,Instead you should call getErrorStream.
AppDynamics,60428806,50355471,1,"2020/02/27, 10:11:48",False,"2020/02/27, 10:11:48",85.0,2574522.0,0,I ran your code (without the auth part) and I don't get any FilenotFoundException testing on url's that return 404.
AppDynamics,60428806,50355471,1,"2020/02/27, 10:11:48",False,"2020/02/27, 10:11:48",85.0,2574522.0,0,So in this case HttpUrlConnection correctly implements the http protocol and appdynamics correctly catches the error.
AppDynamics,60428806,50355471,1,"2020/02/27, 10:11:48",False,"2020/02/27, 10:11:48",85.0,2574522.0,0,"I'm facing the same issue with jersey wrapping the FnF in a UniformResourceException but after some analyzing it's actually either jersey that should provide ways of checking the status code before returning output or correctly use httpurlconnection, and in our case - the webservice should not return 404 for requests that yields no found results but rather an empty collection."
AppDynamics,47301009,47300483,0,"2017/11/15, 08:46:45",False,"2017/11/15, 19:23:51",89858.0,1945651.0,-1,"Not sure whether it would be the cause of memory leaks, but your function definitely has a ton of unnecessary cruft that could be cleaned up with Bluebird's  Promise.mapSeries()  and other Bluebird helpers."
AppDynamics,47301009,47300483,0,"2017/11/15, 08:46:45",False,"2017/11/15, 19:23:51",89858.0,1945651.0,-1,Doing so may very well help cut down on memory leaks.
AppDynamics,47301009,47300483,0,"2017/11/15, 08:46:45",False,"2017/11/15, 19:23:51",89858.0,1945651.0,-1,doSomething  function reduced to 8 lines:
AppDynamics,38628646,38600280,0,"2016/07/28, 09:30:52",False,"2016/07/28, 09:30:52",9374.0,5493302.0,2,After this operations
AppDynamics,38628646,38600280,0,"2016/07/28, 09:30:52",False,"2016/07/28, 09:30:52",9374.0,5493302.0,2,will be
AppDynamics,38628646,38600280,0,"2016/07/28, 09:30:52",False,"2016/07/28, 09:30:52",9374.0,5493302.0,2,and
AppDynamics,38628646,38600280,0,"2016/07/28, 09:30:52",False,"2016/07/28, 09:30:52",9374.0,5493302.0,2,will be
AppDynamics,29033676,29023156,4,"2015/03/13, 15:37:34",True,"2015/03/13, 15:37:34",129048.0,1240763.0,3,"Since you are a Spring Framework user, consider using  Spring AMQP ."
AppDynamics,29033676,29023156,4,"2015/03/13, 15:37:34",True,"2015/03/13, 15:37:34",129048.0,1240763.0,3,The  RabbitTemplate  uses cached channels over a single connection with the channel being checked out of the cache (and returned) for each operation.
AppDynamics,29033676,29023156,4,"2015/03/13, 15:37:34",True,"2015/03/13, 15:37:34",129048.0,1240763.0,3,"The default cache size is 1, so it generally needs to be configured for an environment like yours."
AppDynamics,26594982,26555066,0,"2014/10/27, 21:06:16",False,"2014/10/27, 21:06:16",41.0,702498.0,2,I'm leaning towards it being an issue with the number of channels and the channel cache size.
AppDynamics,26594982,26555066,0,"2014/10/27, 21:06:16",False,"2014/10/27, 21:06:16",41.0,702498.0,2,Does anyone know if there's a limit on the number of channels on a queue?
AppDynamics,26594982,26555066,0,"2014/10/27, 21:06:16",False,"2014/10/27, 21:06:16",41.0,702498.0,2,It seems like specifying connections rather than channels might help here.
AppDynamics,26594982,26555066,0,"2014/10/27, 21:06:16",False,"2014/10/27, 21:06:16",41.0,702498.0,2,"If anyone has any information it would really help,  getting stuck for time :)"
AppDynamics,23488652,23480106,2,"2014/05/06, 10:35:58",False,"2014/05/06, 10:35:58",670.0,2080975.0,1,I think you should use  @PersistenceContext  annotation to obtain  EntityManager  from Spring context and  @Transactional  annotation to drive your transactions.
AppDynamics,23488652,23480106,2,"2014/05/06, 10:35:58",False,"2014/05/06, 10:35:58",670.0,2080975.0,1,"This way you won't need to worry about closing Hibernate sessions manually, and number of connections will not increase until there are  too many connections ."
AppDynamics,23682727,23480106,1,"2014/05/15, 18:33:56",True,"2014/05/15, 18:33:56",962.0,360903.0,4,"The problem did not lay in the sessions but in the connection pool, sessions where being managed correctly, but the connection pool in the JDBC layer wasn't closing the connections."
AppDynamics,23682727,23480106,1,"2014/05/15, 18:33:56",True,"2014/05/15, 18:33:56",962.0,360903.0,4,These are the things that I did to fix them.
AppDynamics,23682727,23480106,1,"2014/05/15, 18:33:56",True,"2014/05/15, 18:33:56",962.0,360903.0,4,JDBC context configuration
AppDynamics,23682727,23480106,1,"2014/05/15, 18:33:56",True,"2014/05/15, 18:33:56",962.0,360903.0,4,1.- Changed the JDBC connection factory from tomcat's old BasicDataSourceFactory to tomcat's new DataSourceFactory
AppDynamics,23682727,23480106,1,"2014/05/15, 18:33:56",True,"2014/05/15, 18:33:56",962.0,360903.0,4,"2.-  Tuned the JDBC settings based on this article: 
 http://www.tomcatexpert.com/blog/2010/04/01/configuring-jdbc-pool-high-concurrency"
AppDynamics,23682727,23480106,1,"2014/05/15, 18:33:56",True,"2014/05/15, 18:33:56",962.0,360903.0,4,Session factory xml configuration
AppDynamics,23682727,23480106,1,"2014/05/15, 18:33:56",True,"2014/05/15, 18:33:56",962.0,360903.0,4,3.- Deleted this line from the session factory configuration:
AppDynamics,23682727,23480106,1,"2014/05/15, 18:33:56",True,"2014/05/15, 18:33:56",962.0,360903.0,4,This is how my configuration ended up:
AppDynamics,23682727,23480106,1,"2014/05/15, 18:33:56",True,"2014/05/15, 18:33:56",962.0,360903.0,4,JDBC context configuration
AppDynamics,23682727,23480106,1,"2014/05/15, 18:33:56",True,"2014/05/15, 18:33:56",962.0,360903.0,4,Session factory xml configuration
AppDynamics,23100111,23099992,2,"2014/04/16, 07:39:32",False,"2014/04/16, 07:39:32",755.0,3535576.0,0,"Hey there, Just crawl in to this URL."
AppDynamics,23100111,23099992,2,"2014/04/16, 07:39:32",False,"2014/04/16, 07:39:32",755.0,3535576.0,0,https://help.ubuntu.com/community/EnvironmentVariables
AppDynamics,23100111,23099992,2,"2014/04/16, 07:39:32",False,"2014/04/16, 07:39:32",755.0,3535576.0,0,"It will better help you.The above URL gives all informations about
  Environment Variable Ubuntu."
AppDynamics,23100111,23099992,2,"2014/04/16, 07:39:32",False,"2014/04/16, 07:39:32",755.0,3535576.0,0,"ANd the above POST is updated in 3 Jan
  2014."
AppDynamics,23100276,23099992,0,"2014/04/16, 07:54:30",False,"2014/04/16, 07:54:30",6589.0,3145373.0,4,Put the environment variables into the global /etc/environment file:
AppDynamics,23100276,23099992,0,"2014/04/16, 07:54:30",False,"2014/04/16, 07:54:30",6589.0,3145373.0,4,...
AppDynamics,23100276,23099992,0,"2014/04/16, 07:54:30",False,"2014/04/16, 07:54:30",6589.0,3145373.0,4,...
AppDynamics,23100276,23099992,0,"2014/04/16, 07:54:30",False,"2014/04/16, 07:54:30",6589.0,3145373.0,4,"Execute ""source /etc/environment"" in every shell where you want the variables to be updated:"
AppDynamics,23100276,23099992,0,"2014/04/16, 07:54:30",False,"2014/04/16, 07:54:30",6589.0,3145373.0,4,Check that it works:
AppDynamics,23100276,23099992,0,"2014/04/16, 07:54:30",False,"2014/04/16, 07:54:30",6589.0,3145373.0,4,Here is a other link from mkyong
AppDynamics,23100418,23099992,0,"2014/04/16, 08:06:09",False,"2014/04/16, 08:06:09",526.0,1494927.0,2,"Here is some info on environment variables, setting your path and where to install things that I hope is useful for you to get your environment setup."
AppDynamics,23100418,23099992,0,"2014/04/16, 08:06:09",False,"2014/04/16, 08:06:09",526.0,1494927.0,2,.bashrc : is specific for the  bash  shell.
AppDynamics,23100418,23099992,0,"2014/04/16, 08:06:09",False,"2014/04/16, 08:06:09",526.0,1494927.0,2,".profile : is used by several shells, and was originally used by the bourne shell (from memory)."
AppDynamics,23100418,23099992,0,"2014/04/16, 08:06:09",False,"2014/04/16, 08:06:09",526.0,1494927.0,2,.profile  might not be loaded by bash if there is a  .bashrc  present.
AppDynamics,23100418,23099992,0,"2014/04/16, 08:06:09",False,"2014/04/16, 08:06:09",526.0,1494927.0,2,Some shells read it only if there is no shell specific configuration present.
AppDynamics,23100418,23099992,0,"2014/04/16, 08:06:09",False,"2014/04/16, 08:06:09",526.0,1494927.0,2,"If you happen to be using a different shell, you will need to look at how best to configure environment variables for that shell."
AppDynamics,23100418,23099992,0,"2014/04/16, 08:06:09",False,"2014/04/16, 08:06:09",526.0,1494927.0,2,"Note that adding to the above files only effect the user that you set them for, since they live in  /home/username/ ."
AppDynamics,23100418,23099992,0,"2014/04/16, 08:06:09",False,"2014/04/16, 08:06:09",526.0,1494927.0,2,"Also remember to source the file again, or reload the shell so that your settings take effect."
AppDynamics,23100418,23099992,0,"2014/04/16, 08:06:09",False,"2014/04/16, 08:06:09",526.0,1494927.0,2,You can achieve this with something like  source .bashrc  after you edit it at the command line to avoid having to restart or reopen terminal.
AppDynamics,23100418,23099992,0,"2014/04/16, 08:06:09",False,"2014/04/16, 08:06:09",526.0,1494927.0,2,"If you would like to set system wide variables, you can do that in  /etc/environment ."
AppDynamics,23100418,23099992,0,"2014/04/16, 08:06:09",False,"2014/04/16, 08:06:09",526.0,1494927.0,2,"If you would like to execute java / ant / maven, etc."
AppDynamics,23100418,23099992,0,"2014/04/16, 08:06:09",False,"2014/04/16, 08:06:09",526.0,1494927.0,2,"from the command line, or enable applications that require the  PATH  environment variable to be set correctly to work, you will also need to add the  ./bin  directories to your PATH."
AppDynamics,23100418,23099992,0,"2014/04/16, 08:06:09",False,"2014/04/16, 08:06:09",526.0,1494927.0,2,Depending on your preference regarding system wide or user specific path settings:
AppDynamics,23100418,23099992,0,"2014/04/16, 08:06:09",False,"2014/04/16, 08:06:09",526.0,1494927.0,2,etc.
AppDynamics,23100418,23099992,0,"2014/04/16, 08:06:09",False,"2014/04/16, 08:06:09",526.0,1494927.0,2,in the relevant file.
AppDynamics,23100418,23099992,0,"2014/04/16, 08:06:09",False,"2014/04/16, 08:06:09",526.0,1494927.0,2,"A side point and entirely optional, the correct place to install java, ant, maven, etc if not from .deb's would be in  /opt , according to the  FilesystemHierarchyStandard"
AppDynamics,11917597,11916710,1,"2012/08/11, 23:56:01",False,"2012/08/11, 23:56:01",2045.0,1469663.0,0,You might check out Spring Insight.
AppDynamics,11917597,11916710,1,"2012/08/11, 23:56:01",False,"2012/08/11, 23:56:01",2045.0,1469663.0,0,"Spring-insight source, design and alternatives"
AppDynamics,11917597,11916710,1,"2012/08/11, 23:56:01",False,"2012/08/11, 23:56:01",2045.0,1469663.0,0,http://www.springsource.org/insight
AppDynamics,12173401,11916710,0,"2012/08/29, 11:08:08",False,"2012/08/29, 11:08:08",275412.0,40342.0,0,Java Melody  might be relevant for you.
AppDynamics,61748113,60872992,0,"2020/05/12, 12:22:07",True,"2020/05/12, 12:33:22",56.0,9641196.0,1,This is an issue with the AppDynamics plugin and gradle plugin tools 3.6.x.
AppDynamics,61748113,60872992,0,"2020/05/12, 12:22:07",True,"2020/05/12, 12:33:22",56.0,9641196.0,1,You can see this link:  https://community.appdynamics.com/t5/End-User-Monitoring-EUM/AppDynamic-EUM-setup-for-Android-Cordova-project/td-p/38864
AppDynamics,61748113,60872992,0,"2020/05/12, 12:22:07",True,"2020/05/12, 12:33:22",56.0,9641196.0,1,I have downgrade gradle plugin tools to 3.5.x and solve it
AppDynamics,59252943,58873513,0,"2019/12/09, 18:27:29",False,"2019/12/09, 18:27:29",392.0,2801554.0,0,"You can create a cron job to run from Monday to Saturday, here for each hour:"
AppDynamics,59252943,58873513,0,"2019/12/09, 18:27:29",False,"2019/12/09, 18:27:29",392.0,2801554.0,0,"And another to cover the interval you want on Sunday, here one by one hour from 10:00 AM to 03:00 PM:"
AppDynamics,50644421,50618877,0,"2018/06/01, 16:20:18",False,"2018/06/01, 16:20:18",23.0,9436164.0,0,Looks like httprequest plugin does not support uploading zip file.
AppDynamics,50644421,50618877,0,"2018/06/01, 16:20:18",False,"2018/06/01, 16:20:18",23.0,9436164.0,0,This is my observation.
AppDynamics,50857929,50618877,0,"2018/06/14, 15:47:37",False,"2018/06/14, 15:47:37",61.0,6101424.0,0,I think upload will use  Content-Type: multipart/form-data .
AppDynamics,50857929,50618877,0,"2018/06/14, 15:47:37",False,"2018/06/14, 15:47:37",61.0,6101424.0,0,But httpRequest plugin is not supporting this type.
AppDynamics,50857929,50618877,0,"2018/06/14, 15:47:37",False,"2018/06/14, 15:47:37",61.0,6101424.0,0,However it supports  APPLICATION_OCTETSTREAM(ContentType.APPLICATION_OCTET_STREAM)
AppDynamics,50857929,50618877,0,"2018/06/14, 15:47:37",False,"2018/06/14, 15:47:37",61.0,6101424.0,0,Could you post output from your curl?
AppDynamics,57322375,50618877,3,"2019/08/02, 11:13:00",False,"2019/08/02, 11:13:00",177.0,277547.0,1,Following Code worked for me :
AppDynamics,47084941,47024920,0,"2017/11/02, 23:29:17",True,"2017/11/02, 23:29:17",190.0,550621.0,1,It turns out the code I was given was completely wrong for angular 2 implementation.
AppDynamics,47084941,47024920,0,"2017/11/02, 23:29:17",True,"2017/11/02, 23:29:17",190.0,550621.0,1,The code they gave me is for running on the web server's side with node js.
AppDynamics,47084941,47024920,0,"2017/11/02, 23:29:17",True,"2017/11/02, 23:29:17",190.0,550621.0,1,"Since angular 2 is an SPA that runs on the browser, it would never work."
AppDynamics,47084941,47024920,0,"2017/11/02, 23:29:17",True,"2017/11/02, 23:29:17",190.0,550621.0,1,I did some research and found this example application that I added a few tweaks to:  https://github.com/derrekyoung/appd-sampleapp-angular2
AppDynamics,47319681,45650349,0,"2017/11/16, 02:37:15",False,"2017/11/16, 02:37:15",4583.0,756095.0,1,"in your appdynamics.cfg, change"
AppDynamics,47319681,45650349,0,"2017/11/16, 02:37:15",False,"2017/11/16, 02:37:15",4583.0,756095.0,1,to
AppDynamics,43387027,41357203,0,"2017/04/13, 11:20:58",False,"2017/04/13, 11:20:58",324.0,7220665.0,0,"I am not exactly sure what the problem is, but I just tried it with AppDynamics Controller Version 4.2 to share a custom dashboard and open the shared Link in a different browser and I didn't need to put in any credentials."
AppDynamics,43387027,41357203,0,"2017/04/13, 11:20:58",False,"2017/04/13, 11:20:58",324.0,7220665.0,0,Maybe AppDynamics Version 3.9 had a bug (assuming you used version 3.9 according to the docs link you posted)
AppDynamics,43387027,41357203,0,"2017/04/13, 11:20:58",False,"2017/04/13, 11:20:58",324.0,7220665.0,0,Here is what I did:
AppDynamics,38639601,38635466,3,"2016/07/28, 17:40:52",False,"2016/07/28, 17:40:52",211.0,5400918.0,1,"CloudForms is not trying to substitute OpenShift UI, but complement it, giving you more information about the environment and providing additional capabilities on top of OPenShift."
AppDynamics,38639601,38635466,3,"2016/07/28, 17:40:52",False,"2016/07/28, 17:40:52",211.0,5400918.0,1,You can see information about what is being done and demos in videos:
AppDynamics,38639601,38635466,3,"2016/07/28, 17:40:52",False,"2016/07/28, 17:40:52",211.0,5400918.0,1,https://www.youtube.com/watch?v=FVLo9Nc_10E&amp;list=PLQAAGwo9CYO-4tQsnC6oWgzhPNOykOOMd&amp;index=15
AppDynamics,38639601,38635466,3,"2016/07/28, 17:40:52",False,"2016/07/28, 17:40:52",211.0,5400918.0,1,"And you can find the presentations here
 http://www.slideshare.net/ManageIQ/presentations"
AppDynamics,30225687,30191131,1,"2015/05/14, 00:43:25",True,"2015/05/14, 00:43:25",185615.0,2016562.0,0,Just follow this istructions:
AppDynamics,30225687,30191131,1,"2015/05/14, 00:43:25",True,"2015/05/14, 00:43:25",185615.0,2016562.0,0,"Clone a local copy of this project with
git clone  https://github.com/Appdynamics/ECommerce-Android"
AppDynamics,30225687,30191131,1,"2015/05/14, 00:43:25",True,"2015/05/14, 00:43:25",185615.0,2016562.0,0,Open Android Studio and Import Project (select app/build.gradle)
AppDynamics,30225687,30191131,1,"2015/05/14, 00:43:25",True,"2015/05/14, 00:43:25",185615.0,2016562.0,0,Android Studio will ask you to build the project with gradle.
AppDynamics,30225687,30191131,1,"2015/05/14, 00:43:25",True,"2015/05/14, 00:43:25",185615.0,2016562.0,0,Gradle will use the  build.gradle  inside the project.
AppDynamics,30225687,30191131,1,"2015/05/14, 00:43:25",True,"2015/05/14, 00:43:25",185615.0,2016562.0,0,The version of gradle is inside the  gradle/wrapper/gradle-wrapper.properties  file:
AppDynamics,30225687,30191131,1,"2015/05/14, 00:43:25",True,"2015/05/14, 00:43:25",185615.0,2016562.0,0,Check for example the file inside the  app :
AppDynamics,30268298,30191131,0,"2015/05/15, 23:34:56",False,"2015/05/15, 23:34:56",166.0,4226112.0,0,"You need put a directory called ""adeum-maven-repo"" in your project setup ."
AppDynamics,30268298,30191131,0,"2015/05/15, 23:34:56",False,"2015/05/15, 23:34:56",166.0,4226112.0,0,https://docs.appdynamics.com/display/PRO39/Instrument+an+Android+Application#InstrumentanAndroidApplication-SetupforGradle
AppDynamics,27420289,27415368,0,"2014/12/11, 12:14:49",True,"2014/12/11, 12:20:03",628370.0,139985.0,1,Whether this is an issue or not is really up to you to decide.
AppDynamics,27420289,27415368,0,"2014/12/11, 12:14:49",True,"2014/12/11, 12:20:03",628370.0,139985.0,1,"On the one hand, a full GC that runs for one second every 10 minutes is not a significant issue for throughput."
AppDynamics,27420289,27415368,0,"2014/12/11, 12:14:49",True,"2014/12/11, 12:20:03",628370.0,139985.0,1,"On the other hand, the full GC is probably going to dramatically reduce response times during that those one second windows."
AppDynamics,27420289,27415368,0,"2014/12/11, 12:14:49",True,"2014/12/11, 12:20:03",628370.0,139985.0,1,But that may not be important or even relevant to your application.
AppDynamics,27420289,27415368,0,"2014/12/11, 12:14:49",True,"2014/12/11, 12:20:03",628370.0,139985.0,1,The thing I would be worried about is whether your load testing is a realistic test.
AppDynamics,27420289,27415368,0,"2014/12/11, 12:14:49",True,"2014/12/11, 12:20:03",628370.0,139985.0,1,"The application appears to need 4Gb of heap space under test, but is it also going to need that in a real-world use?"
AppDynamics,27420289,27415368,0,"2014/12/11, 12:14:49",True,"2014/12/11, 12:20:03",628370.0,139985.0,1,I'd be worried that a memory leak might show up when it is deployed into production.
AppDynamics,27420289,27415368,0,"2014/12/11, 12:14:49",True,"2014/12/11, 12:20:03",628370.0,139985.0,1,Or that the load in your load testing is causing the application's in-memory caching to reach a steady state in that won't be reproduced in production.
AppDynamics,27420289,27415368,0,"2014/12/11, 12:14:49",True,"2014/12/11, 12:20:03",628370.0,139985.0,1,"As a general rule, it is a bad thing for the heap to be running close to full, so an increase in the heap is probably advisable."
AppDynamics,27420289,27415368,0,"2014/12/11, 12:14:49",True,"2014/12/11, 12:20:03",628370.0,139985.0,1,"Your application's performance doesn't appear to be suffering, but you could be ""on the cusp""."
AppDynamics,27420289,27415368,0,"2014/12/11, 12:14:49",True,"2014/12/11, 12:20:03",628370.0,139985.0,1,"I thought that once old gen utilization is above certain level, GC will try to free/compact old gen area and if nothing is freed it would either fail with OOM and start crazy full GC cycles just before it."
AppDynamics,27420289,27415368,0,"2014/12/11, 12:14:49",True,"2014/12/11, 12:20:03",628370.0,139985.0,1,I suspect that the monitoring reports might be misleading you.
AppDynamics,27420289,27415368,0,"2014/12/11, 12:14:49",True,"2014/12/11, 12:20:03",628370.0,139985.0,1,"If the full GC cycles were  really  not reclaiming anything, I'd expect the behaviour to be different."
AppDynamics,27420289,27415368,0,"2014/12/11, 12:14:49",True,"2014/12/11, 12:20:03",628370.0,139985.0,1,"Try turning on JVM GC log message, and see what they tell you about the amount of memory that the full GC cycles are managing to reclaim."
AppDynamics,25416702,25134646,0,"2014/08/21, 03:50:57",True,"2014/08/21, 03:50:57",525.0,2479518.0,1,My answer with links got deleted by another SO user so I'm listing the steps here.
AppDynamics,25416702,25134646,0,"2014/08/21, 03:50:57",True,"2014/08/21, 03:50:57",525.0,2479518.0,1,First uninstall using this
AppDynamics,25416702,25134646,0,"2014/08/21, 03:50:57",True,"2014/08/21, 03:50:57",525.0,2479518.0,1,"$ npm uninstall -g strong-cli
$ npm uninstall -g loopback-sdk-angular-cli"
AppDynamics,25416702,25134646,0,"2014/08/21, 03:50:57",True,"2014/08/21, 03:50:57",525.0,2479518.0,1,and then install
AppDynamics,25416702,25134646,0,"2014/08/21, 03:50:57",True,"2014/08/21, 03:50:57",525.0,2479518.0,1,npm install -g strongloop
AppDynamics,25416702,25134646,0,"2014/08/21, 03:50:57",True,"2014/08/21, 03:50:57",525.0,2479518.0,1,You can now run slc strongops
AppDynamics,25416702,25134646,0,"2014/08/21, 03:50:57",True,"2014/08/21, 03:50:57",525.0,2479518.0,1,and let us know how it goes.
AppDynamics,27635551,25134646,0,"2014/12/24, 12:38:57",False,"2014/12/24, 12:38:57",924.0,427300.0,1,"I created a Linux init.d Daemon script which you can use to run your app with slc as service:
 https://gist.github.com/gurdotan/23311a236fc65dc212da"
AppDynamics,27635551,25134646,0,"2014/12/24, 12:38:57",False,"2014/12/24, 12:38:57",924.0,427300.0,1,Might be useful to some of you.
AppDynamics,19326222,19191781,1,"2013/10/11, 22:56:19",False,"2013/10/11, 22:56:19",11.0,2872398.0,1,You should try Compuware's dynaTrace
AppDynamics,11687150,11687049,8,"2012/07/27, 14:47:42",True,"2012/07/27, 14:47:42",2990.0,1554255.0,3,"i don't think that you can do full-featured profiling of distributed request across number of JVM's - AppDynamics from what i can remember understands the EE stuff - like calling DB, EJB, RMI, or remote webservice - however it still works in scope of JVM."
AppDynamics,11687150,11687049,8,"2012/07/27, 14:47:42",True,"2012/07/27, 14:47:42",2990.0,1554255.0,3,"Isn't it suffient in your case just to use java profiler (like yourkit, jprofiler)?"
AppDynamics,20809292,11687049,0,"2013/12/28, 01:37:03",False,"2013/12/28, 01:37:03",4378.0,341191.0,1,you can try 24x7monitoring
AppDynamics,20809292,11687049,0,"2013/12/28, 01:37:03",False,"2013/12/28, 01:37:03",4378.0,341191.0,1,https://code.google.com/p/monitor-24x7/
AppDynamics,20809292,11687049,0,"2013/12/28, 01:37:03",False,"2013/12/28, 01:37:03",4378.0,341191.0,1,"it provides method level monitoring, SQL queries, business transactions..."
AppDynamics,21438317,11687049,0,"2014/01/29, 19:29:28",False,"2014/01/29, 19:29:28",31.0,3249955.0,3,Did you try the free version of AppDynamics.
AppDynamics,21438317,11687049,0,"2014/01/29, 19:29:28",False,"2014/01/29, 19:29:28",31.0,3249955.0,3,It's called AppDynamics LITE.
AppDynamics,21438317,11687049,0,"2014/01/29, 19:29:28",False,"2014/01/29, 19:29:28",31.0,3249955.0,3,You can take a look also to EXTRAHOP free version.
AppDynamics,21438317,11687049,0,"2014/01/29, 19:29:28",False,"2014/01/29, 19:29:28",31.0,3249955.0,3,Maybe it is good enough for your needs.
AppDynamics,21438317,11687049,0,"2014/01/29, 19:29:28",False,"2014/01/29, 19:29:28",31.0,3249955.0,3,Also you can try using a SaaS solutions such as NewRelic or Boundary.
AppDynamics,21438317,11687049,0,"2014/01/29, 19:29:28",False,"2014/01/29, 19:29:28",31.0,3249955.0,3,They have free accounts that could also be good enough for your needs.
AppDynamics,21438317,11687049,0,"2014/01/29, 19:29:28",False,"2014/01/29, 19:29:28",31.0,3249955.0,3,"Finally if you want to monitor the performance of any specific JAVA application, you can use  http://www.moskito.org/ ."
AppDynamics,21438317,11687049,0,"2014/01/29, 19:29:28",False,"2014/01/29, 19:29:28",31.0,3249955.0,3,It's totaly FREE.
AppDynamics,7264935,7077885,0,"2011/09/01, 03:19:24",False,"2011/09/01, 03:19:24",251.0,914642.0,0,can you tell me the methodology for deriving those stats?
AppDynamics,7264935,7077885,0,"2011/09/01, 03:19:24",False,"2011/09/01, 03:19:24",251.0,914642.0,0,"In most cases, this behavior comes down to poor code implementation in a custom portlet or a JVM configuration issue and more likely the latter."
AppDynamics,7264935,7077885,0,"2011/09/01, 03:19:24",False,"2011/09/01, 03:19:24",251.0,914642.0,0,Take a look at the  Ehcache Garbage Collection Tuning Documentation .
AppDynamics,7264935,7077885,0,"2011/09/01, 03:19:24",False,"2011/09/01, 03:19:24",251.0,914642.0,0,"When you run  jstat , how do the garbage collection times look?"
AppDynamics,64358686,63590062,0,"2020/10/14, 20:36:25",False,"2020/10/14, 20:36:25",1.0,14450632.0,0,"I realize it's been a while, but I'll contribute anyway as it may help other people too."
AppDynamics,64358686,63590062,0,"2020/10/14, 20:36:25",False,"2020/10/14, 20:36:25",1.0,14450632.0,0,"In my case, importing Instrumentation in iOS caused this error; it seems to be a problem in the latest version of @appdynamics/react-native-agent (version 20.7.0 as of writing)."
AppDynamics,64358686,63590062,0,"2020/10/14, 20:36:25",False,"2020/10/14, 20:36:25",1.0,14450632.0,0,"I instead initialized AppDynamics in native code (in the AppDelegate.m file), as follows:"
AppDynamics,64358686,63590062,0,"2020/10/14, 20:36:25",False,"2020/10/14, 20:36:25",1.0,14450632.0,0,"For more info, check the iOS guide:
 https://docs.appdynamics.com/display/PRO45/Instrument+an+iOS+Application"
AppDynamics,64358686,63590062,0,"2020/10/14, 20:36:25",False,"2020/10/14, 20:36:25",1.0,14450632.0,0,"Additionally, I avoided importing AppDynamics in javascript by requiring it in runtime, in Android only."
AppDynamics,63117665,62998835,0,"2020/07/27, 17:38:53",False,"2020/07/27, 17:38:53",213.0,7933630.0,1,"There's a lot of different questions here, and some of them don't have answers without more information about your specific setup, but I'll try to give you a good overview."
AppDynamics,63117665,62998835,0,"2020/07/27, 17:38:53",False,"2020/07/27, 17:38:53",213.0,7933630.0,1,Why Tracing?
AppDynamics,63117665,62998835,0,"2020/07/27, 17:38:53",False,"2020/07/27, 17:38:53",213.0,7933630.0,1,You've already intuited that there are a lot of similarities between &quot;APM&quot; and &quot;tracing&quot; - the differences are fairly minimal.
AppDynamics,63117665,62998835,0,"2020/07/27, 17:38:53",False,"2020/07/27, 17:38:53",213.0,7933630.0,1,"Distributed Tracing is a superset of capabilities marketed as APM (application performance monitoring) and RUM (real user monitoring), as it allows you to capture performance information about the work being done in your services to handle a single, logical request both at a per-service level, and at the level of an entire request (or transaction) from client to DB and back."
AppDynamics,63117665,62998835,0,"2020/07/27, 17:38:53",False,"2020/07/27, 17:38:53",213.0,7933630.0,1,"Trace data, like other forms of telemetry, can be aggregated and analyzed in different ways - for example, unsampled trace data can be used to generate RED (rate, error, duration) metrics for a given API endpoint or function call."
AppDynamics,63117665,62998835,0,"2020/07/27, 17:38:53",False,"2020/07/27, 17:38:53",213.0,7933630.0,1,"Conventionally, trace data is annotated (tagged) with properties about a request or the underlying infrastructure handling a request (things like a customer identifier, or the host name of the server handling a request, or the DB partition being accessed for a given query) that allows for powerful exploratory queries in a tool like Jaeger or a commercial tracing tool."
AppDynamics,63117665,62998835,0,"2020/07/27, 17:38:53",False,"2020/07/27, 17:38:53",213.0,7933630.0,1,Sampling
AppDynamics,63117665,62998835,0,"2020/07/27, 17:38:53",False,"2020/07/27, 17:38:53",213.0,7933630.0,1,The overall performance impact of generating traces varies.
AppDynamics,63117665,62998835,0,"2020/07/27, 17:38:53",False,"2020/07/27, 17:38:53",213.0,7933630.0,1,"In general, tracing libraries are designed to be fairly lightweight - although there are a lot of factors that influence this overhead, such as the amount of attributes on a span, the log events attached to it, and the request rate of a service."
AppDynamics,63117665,62998835,0,"2020/07/27, 17:38:53",False,"2020/07/27, 17:38:53",213.0,7933630.0,1,"Companies like Google will aggressively sample due to their scale, but to be honest, sampling is more beneficial to consider from a long-term storage perspective rather than an up-front overhead perspective."
AppDynamics,63117665,62998835,0,"2020/07/27, 17:38:53",False,"2020/07/27, 17:38:53",213.0,7933630.0,1,"While the additional overhead per-request to create a span and transmit it to your tracing backend might be small, the cost to store trace data over time can quickly become prohibitive."
AppDynamics,63117665,62998835,0,"2020/07/27, 17:38:53",False,"2020/07/27, 17:38:53",213.0,7933630.0,1,"In addition, most traces from most systems aren't terribly interesting."
AppDynamics,63117665,62998835,0,"2020/07/27, 17:38:53",False,"2020/07/27, 17:38:53",213.0,7933630.0,1,This is why dynamic and tail-based sampling approaches have become more popular.
AppDynamics,63117665,62998835,0,"2020/07/27, 17:38:53",False,"2020/07/27, 17:38:53",213.0,7933630.0,1,"These systems move the sampling decision from an individual service layer to some external process, such as the  OpenTelemetry Collector , which can analyze an entire trace and determine if it should be sampled in or out based on user-defined criteria."
AppDynamics,63117665,62998835,0,"2020/07/27, 17:38:53",False,"2020/07/27, 17:38:53",213.0,7933630.0,1,"You could, for example, ensure that any trace where an error occurred is sampled in, while 'baseline' traces are sampled at a rate of 1%, in order to preserve important error information while giving you an idea of steady-state performance."
AppDynamics,63117665,62998835,0,"2020/07/27, 17:38:53",False,"2020/07/27, 17:38:53",213.0,7933630.0,1,Proprietary APM vs. OSS
AppDynamics,63117665,62998835,0,"2020/07/27, 17:38:53",False,"2020/07/27, 17:38:53",213.0,7933630.0,1,One important distinction between something like AppDynamics or New Relic and tools like Jaeger is that Jaeger does not rely on proprietary instrumentation agents in order to generate trace data.
AppDynamics,63117665,62998835,0,"2020/07/27, 17:38:53",False,"2020/07/27, 17:38:53",213.0,7933630.0,1,"Jaeger supports OpenTelemetry, allowing you to use open source tools like the  OpenTelemetry Java Automatic Instrumentation  libraries, which will automatically generate spans for many popular Java frameworks and libraries, such as Spring."
AppDynamics,63117665,62998835,0,"2020/07/27, 17:38:53",False,"2020/07/27, 17:38:53",213.0,7933630.0,1,"In addition, since OpenTelemetry is available in multiple languages with a shared data format and trace context format, you can guarantee that your traces will work properly in a polyglot environment (so, if you have Node.JS or Golang services in addition to your Java services, you could use OpenTelemetry for each language, and trace context propagation would work seamlessly between all of them)."
AppDynamics,63117665,62998835,0,"2020/07/27, 17:38:53",False,"2020/07/27, 17:38:53",213.0,7933630.0,1,"Even more advantageous, though, is that your instrumentation is decoupled from a specific vendor or tool."
AppDynamics,63117665,62998835,0,"2020/07/27, 17:38:53",False,"2020/07/27, 17:38:53",213.0,7933630.0,1,"You can instrument your service with OpenTelemetry and then send data to one - or more - analysis tools, both commercial and open source."
AppDynamics,63117665,62998835,0,"2020/07/27, 17:38:53",False,"2020/07/27, 17:38:53",213.0,7933630.0,1,"This frees you from vendor lock-in, and allows you to select the best tool for the job."
AppDynamics,63117665,62998835,0,"2020/07/27, 17:38:53",False,"2020/07/27, 17:38:53",213.0,7933630.0,1,"If you'd like to learn more about OpenTelemetry, observability, and other topics I wrote a longer series that you can find  here  (look for the other 'OpenTelemetry 101' posts)."
AppDynamics,59198597,59191709,1,"2019/12/05, 17:40:40",False,"2019/12/09, 14:27:54",722.0,8826818.0,0,Windows 10 64-bit.
AppDynamics,59198597,59191709,1,"2019/12/05, 17:40:40",False,"2019/12/09, 14:27:54",722.0,8826818.0,0,Powershell 5.
AppDynamics,59198597,59191709,1,"2019/12/05, 17:40:40",False,"2019/12/09, 14:27:54",722.0,8826818.0,0,Does not require admin privileges.
AppDynamics,59198597,59191709,1,"2019/12/05, 17:40:40",False,"2019/12/09, 14:27:54",722.0,8826818.0,0,How to quickly and efficiently extract text from a large logfile from input1 to input2 using powershell 5?
AppDynamics,59198597,59191709,1,"2019/12/05, 17:40:40",False,"2019/12/09, 14:27:54",722.0,8826818.0,0,Sample logfile below.
AppDynamics,59198597,59191709,1,"2019/12/05, 17:40:40",False,"2019/12/09, 14:27:54",722.0,8826818.0,0,For testing purposes copy your logfile to the desktop and name it logfile.txt
AppDynamics,59198597,59191709,1,"2019/12/05, 17:40:40",False,"2019/12/09, 14:27:54",722.0,8826818.0,0,What is the default text editor in Windows Server 2012 R2 Standard 64-bit?
AppDynamics,59198597,59191709,1,"2019/12/05, 17:40:40",False,"2019/12/09, 14:27:54",722.0,8826818.0,0,See line 42.
AppDynamics,59198597,59191709,1,"2019/12/05, 17:40:40",False,"2019/12/09, 14:27:54",722.0,8826818.0,0,What program do you have associated with .txt files?
AppDynamics,59198597,59191709,1,"2019/12/05, 17:40:40",False,"2019/12/09, 14:27:54",722.0,8826818.0,0,See line 42.
AppDynamics,59198597,59191709,1,"2019/12/05, 17:40:40",False,"2019/12/09, 14:27:54",722.0,8826818.0,0,logfile.txt:
AppDynamics,59198597,59191709,1,"2019/12/05, 17:40:40",False,"2019/12/09, 14:27:54",722.0,8826818.0,0,Results of running script on logfile.txt:
AppDynamics,59198597,59191709,1,"2019/12/05, 17:40:40",False,"2019/12/09, 14:27:54",722.0,8826818.0,0,Powershell in four hours at Youtube
AppDynamics,59198597,59191709,1,"2019/12/05, 17:40:40",False,"2019/12/09, 14:27:54",722.0,8826818.0,0,Parse and extract text with powershell at Bing
AppDynamics,59198597,59191709,1,"2019/12/05, 17:40:40",False,"2019/12/09, 14:27:54",722.0,8826818.0,0,How to quickly and efficiently extract text from a large logfile from line x to line y using powershell 5?
AppDynamics,59198597,59191709,1,"2019/12/05, 17:40:40",False,"2019/12/09, 14:27:54",722.0,8826818.0,0,How to quickly and efficiently extract text from a large logfile from date1 to date2 using powershell 5?
AppDynamics,55505955,55505087,0,"2019/04/04, 02:58:03",True,"2019/04/04, 02:58:03",1220.0,796375.0,2,curl's  --user  command line option sets up HTTP authentication for the request ( username:password ).
AppDynamics,55505955,55505087,0,"2019/04/04, 02:58:03",True,"2019/04/04, 02:58:03",1220.0,796375.0,2,"In the case of AppDynamics' Configuration API (which is a subset of their Controller API),  user1@customer1:secret  are your account credentials in the format documented  here :"
AppDynamics,55002450,54993328,1,"2019/03/05, 14:04:18",False,"2019/03/05, 14:04:18",116702.0,2897748.0,0,"You should not be retrieving any embedded resources in your load test, you need to limit the scope of the embedded resources scanning to  your application under test domain only"
AppDynamics,55002450,54993328,1,"2019/03/05, 14:04:18",False,"2019/03/05, 14:04:18",116702.0,2897748.0,0,"Any external domains like  googleapis  or   appdynamics  must be excluded via  URLs must match  input (lives at ""Advanced"" tab of the  HTTP Request  sampler, or even better  HTTP Request Defaults )"
AppDynamics,55002450,54993328,1,"2019/03/05, 14:04:18",False,"2019/03/05, 14:04:18",116702.0,2897748.0,0,More information:  Excluding Domains from the Load Test
AppDynamics,44071970,44038577,0,"2017/05/19, 16:57:39",False,"2017/05/19, 16:57:39",474.0,4682632.0,0,The problem with this approach is you'll have to manually do that each time.
AppDynamics,44071970,44038577,0,"2017/05/19, 16:57:39",False,"2017/05/19, 16:57:39",474.0,4682632.0,0,I would highly recommend just configuring your app server to automatically load the AppDynamics agent.
AppDynamics,44071970,44038577,0,"2017/05/19, 16:57:39",False,"2017/05/19, 16:57:39",474.0,4682632.0,0,"Another option is using the universal agent, which does auto-attach:  https://docs.appdynamics.com/display/PRO43/Install+the+Universal+Agent  Doing this one off attach is never really a good idea, as you'll have to get the PID each time."
AppDynamics,44071970,44038577,0,"2017/05/19, 16:57:39",False,"2017/05/19, 16:57:39",474.0,4682632.0,0,"The error indicates that you are probably not running the attach as the same user the JVM is running under, but it could also be permissions or something else as well, hence I would use the methods that work all the time :)"
AppDynamics,39731234,39730780,3,"2016/09/27, 20:49:55",False,"2016/09/27, 20:49:55",6408.0,5004822.0,0,Use Dependency Injection instead of creating the actual WCF endpoints and passing them around.
AppDynamics,39731234,39730780,3,"2016/09/27, 20:49:55",False,"2016/09/27, 20:49:55",6408.0,5004822.0,0,Then mocking them up is trivial.
AppDynamics,39731234,39730780,3,"2016/09/27, 20:49:55",False,"2016/09/27, 20:49:55",6408.0,5004822.0,0,You would then use the interface and let DI take care of the rest!
AppDynamics,26850353,26778994,2,"2014/11/10, 20:22:08",False,"2014/11/10, 20:22:08",4817.0,3817025.0,0,The best way to logging the requests to an external logging provider.
AppDynamics,26850353,26778994,2,"2014/11/10, 20:22:08",False,"2014/11/10, 20:22:08",4817.0,3817025.0,0,Check out  http://docs.cloudfoundry.org/devguide/services/log-management-thirdparty-svc.html .
AppDynamics,26850353,26778994,2,"2014/11/10, 20:22:08",False,"2014/11/10, 20:22:08",4817.0,3817025.0,0,You can actually log to any http endpoint that supports POST.
AppDynamics,26850353,26778994,2,"2014/11/10, 20:22:08",False,"2014/11/10, 20:22:08",4817.0,3817025.0,0,You can use Splunk to calculate your response time and requests per second.
AppDynamics,26850353,26778994,2,"2014/11/10, 20:22:08",False,"2014/11/10, 20:22:08",4817.0,3817025.0,0,The logs that come out of that are in real time and are streamed to your logging endpoint.
AppDynamics,26850353,26778994,2,"2014/11/10, 20:22:08",False,"2014/11/10, 20:22:08",4817.0,3817025.0,0,It contains information about the requests as well as log messages from your app.
AppDynamics,26850353,26778994,2,"2014/11/10, 20:22:08",False,"2014/11/10, 20:22:08",4817.0,3817025.0,0,ex.
AppDynamics,27005164,26778994,2,"2014/11/19, 00:15:20",False,"2014/11/19, 00:15:20",10201.0,1585136.0,0,There are basic stats available when you run  cf app &lt;app-name&gt; .
AppDynamics,27005164,26778994,2,"2014/11/19, 00:15:20",False,"2014/11/19, 00:15:20",10201.0,1585136.0,0,"These include the memory, cpu and disk utilization of your app."
AppDynamics,27005164,26778994,2,"2014/11/19, 00:15:20",False,"2014/11/19, 00:15:20",10201.0,1585136.0,0,"You can also access these via the REST api, documented here."
AppDynamics,27005164,26778994,2,"2014/11/19, 00:15:20",False,"2014/11/19, 00:15:20",10201.0,1585136.0,0,https://s3.amazonaws.com/cc-api-docs/41397913/apps/get_detailed_stats_for_a_started_app.html
AppDynamics,27005164,26778994,2,"2014/11/19, 00:15:20",False,"2014/11/19, 00:15:20",10201.0,1585136.0,0,That's not going to help with requests per second or response time though.
AppDynamics,27005164,26778994,2,"2014/11/19, 00:15:20",False,"2014/11/19, 00:15:20",10201.0,1585136.0,0,"@jsloyer's solution would work for that, or you could use an APM like NewRelic, which will give you a wealth of data for virtually nothing."
AppDynamics,27018574,26778994,1,"2014/11/19, 15:53:11",True,"2014/11/19, 15:53:11",4817.0,3817025.0,1,Yes this is pretty easy to use.
AppDynamics,27018574,26778994,1,"2014/11/19, 15:53:11",True,"2014/11/19, 15:53:11",4817.0,3817025.0,1,"You can use Logstash as the ingesting engine, you just need the correct parser."
AppDynamics,27018574,26778994,1,"2014/11/19, 15:53:11",True,"2014/11/19, 15:53:11",4817.0,3817025.0,1,Check out  http://scottfrederick.cfapps.io/blog/2014/02/20/cloud-foundry-and-logstash  for the parser and config for ingesting cloud foundry logs.
AppDynamics,27018574,26778994,1,"2014/11/19, 15:53:11",True,"2014/11/19, 15:53:11",4817.0,3817025.0,1,I was playing around with this before and it worked quite well.
AppDynamics,27018574,26778994,1,"2014/11/19, 15:53:11",True,"2014/11/19, 15:53:11",4817.0,3817025.0,1,Let me know if you have any issues.
AppDynamics,67137278,66872969,0,"2021/04/17, 13:46:07",False,"2021/04/17, 13:46:07",3170.0,1237595.0,0,From checking out the code for Istio -  https://github.com/istio  - this appears to be an application with components written in Go and C++ which are deployed to containers using Kubernetes.
AppDynamics,67137278,66872969,0,"2021/04/17, 13:46:07",False,"2021/04/17, 13:46:07",3170.0,1237595.0,0,This would mean that for monitoring you should be looking at:
AppDynamics,67137278,66872969,0,"2021/04/17, 13:46:07",False,"2021/04/17, 13:46:07",3170.0,1237595.0,0,You have you work cut out for you here - both SDK's would require changes to the code being ran to get visibility.
AppDynamics,65073399,65072887,0,"2020/11/30, 14:14:21",False,"2020/11/30, 14:14:21",116702.0,2897748.0,0,It's hard to say what's wrong without seeing your Jenkins job and JMeter  Thread Group  configuration.
AppDynamics,65073399,65072887,0,"2020/11/30, 14:14:21",False,"2020/11/30, 14:14:21",116702.0,2897748.0,0,"In order to apply external settings in JMeter you need to define threads, ramp-up and the number of iterations using JMeter Properties via  __P() function  like:"
AppDynamics,65073399,65072887,0,"2020/11/30, 14:14:21",False,"2020/11/30, 14:14:21",116702.0,2897748.0,0,"Once done you will be able to  override the values of these properties using  -J  command line argument , for example in Jenkins:"
AppDynamics,65073399,65072887,0,"2020/11/30, 14:14:21",False,"2020/11/30, 14:14:21",116702.0,2897748.0,0,This way you will be able to pass whatever number of virtual users/iterations without having to change your script.
AppDynamics,65073399,65072887,0,"2020/11/30, 14:14:21",False,"2020/11/30, 14:14:21",116702.0,2897748.0,0,More information:  Apache JMeter Properties Customization Guide
AppDynamics,64909534,64881926,0,"2020/11/19, 12:13:48",False,"2020/11/19, 12:13:48",615.0,2319439.0,0,"just to make it clear, a  live connection  is (in the world of Power BI 😉) a connection to either a Power BI dataset or a SSAS tabular model."
AppDynamics,64909534,64881926,0,"2020/11/19, 12:13:48",False,"2020/11/19, 12:13:48",615.0,2319439.0,0,"I think what you are looking for is  DirectQuery , but it is currently not supported for URL GET commands."
AppDynamics,64909534,64881926,0,"2020/11/19, 12:13:48",False,"2020/11/19, 12:13:48",615.0,2319439.0,0,MS docs - Power BI data sources
AppDynamics,64909534,64881926,0,"2020/11/19, 12:13:48",False,"2020/11/19, 12:13:48",615.0,2319439.0,0,To get realtime data you need to use one of the supported sources.
AppDynamics,64909534,64881926,0,"2020/11/19, 12:13:48",False,"2020/11/19, 12:13:48",615.0,2319439.0,0,maybe AppDynamics supports direct DB access.
AppDynamics,64909534,64881926,0,"2020/11/19, 12:13:48",False,"2020/11/19, 12:13:48",615.0,2319439.0,0,DirectQuery is supported by SQL databases.
AppDynamics,64909534,64881926,0,"2020/11/19, 12:13:48",False,"2020/11/19, 12:13:48",615.0,2319439.0,0,Another way is to offload the data to a supported source eg.
AppDynamics,64909534,64881926,0,"2020/11/19, 12:13:48",False,"2020/11/19, 12:13:48",615.0,2319439.0,0,SQL-db or CDS-service and then connect your pbi to that source.
AppDynamics,64443455,64442861,0,"2020/10/20, 13:40:45",True,"2020/10/20, 13:49:24",2274.0,6266192.0,1,The correct and hard way is to amend the SELinux boolean on Tomcat directories to allow Tomcat amend files created by other users.
AppDynamics,64443455,64442861,0,"2020/10/20, 13:40:45",True,"2020/10/20, 13:49:24",2274.0,6266192.0,1,Read  here .
AppDynamics,64443455,64442861,0,"2020/10/20, 13:40:45",True,"2020/10/20, 13:49:24",2274.0,6266192.0,1,The easy and dirty solution is to  start-up process includes a step for renaming yesterdays AppDynamics logs  as user  sysXYZ  .
AppDynamics,64443455,64442861,0,"2020/10/20, 13:40:45",True,"2020/10/20, 13:49:24",2274.0,6266192.0,1,And that way you avoid the problem.
AppDynamics,64443455,64442861,0,"2020/10/20, 13:40:45",True,"2020/10/20, 13:49:24",2274.0,6266192.0,1,Use  su - sysXYZ &lt;script&gt;  command or  sudo -iu sysXYZ &lt;script&gt;  command or  sudo -t &lt;Tomcat role&gt; &lt;script&gt;
AppDynamics,64443455,64442861,0,"2020/10/20, 13:40:45",True,"2020/10/20, 13:49:24",2274.0,6266192.0,1,Good luck.
AppDynamics,55963734,55948183,0,"2019/05/03, 08:23:29",False,"2019/05/03, 08:23:29",4861.0,192923.0,0,Add  CheckedParameter=false  in the connection string to fix the issue.
AppDynamics,53293120,53271297,0,"2018/11/14, 06:16:21",False,"2018/11/14, 06:16:21",342.0,1394897.0,0,I found my answer on the Flow Force online help ( https://manual.altova.com/flowforceserver/flowforceserver/ )
AppDynamics,53293120,53271297,0,"2018/11/14, 06:16:21",False,"2018/11/14, 06:16:21",342.0,1394897.0,0,"The Flow Force is deployed as two servers, which in a window env, can be started and stopped as windows services (can be found via ""Control Panel"" ""Administrative Tools"" Services)."
AppDynamics,53293120,53271297,0,"2018/11/14, 06:16:21",False,"2018/11/14, 06:16:21",342.0,1394897.0,0,"With this information, I can monitor them via NAGIOS."
AppDynamics,51469112,51468443,0,"2018/07/22, 23:45:36",False,"2018/07/22, 23:45:36",45031.0,213136.0,0,"One possibility is that you've got a cached execution plan which works fine for most parameter values, or combination of parameter values, but which fails badly for certain values/combinations."
AppDynamics,51469112,51468443,0,"2018/07/22, 23:45:36",False,"2018/07/22, 23:45:36",45031.0,213136.0,0,You can try adding a non-filtering predicate such as  1 = 1  to your WHERE clause.
AppDynamics,51469112,51468443,0,"2018/07/22, 23:45:36",False,"2018/07/22, 23:45:36",45031.0,213136.0,0,"I've read  but haven't tested that this can be used to force a hard parse, but it may be that you need to change the value (e.g."
AppDynamics,51469112,51468443,0,"2018/07/22, 23:45:36",False,"2018/07/22, 23:45:36",45031.0,213136.0,0,"1 = 1 ,  2 = 2 ,  3 = 3 , etc) for each execution of your query."
AppDynamics,48974838,48974684,0,"2018/02/25, 16:57:04",False,"2018/02/25, 17:12:50",49358.0,6779307.0,1,"Here's a recursive solution that yields key ""paths""."
AppDynamics,48974838,48974684,0,"2018/02/25, 16:57:04",False,"2018/02/25, 17:12:50",49358.0,6779307.0,1,"The  (*keys, k)  syntax is available in Python versions  = 3.5, you can also use  keys + (k,)"
AppDynamics,48983005,48974684,0,"2018/02/26, 08:59:10",False,"2018/02/26, 08:59:10",2251.0,5550284.0,0,"As a little modifcation to @Jon Clements code, this is what gives me what I need."
AppDynamics,48983245,48974684,0,"2018/02/26, 09:16:56",False,"2018/02/26, 09:16:56",1513.0,7352806.0,0,Try this:-
AppDynamics,47754925,47754856,0,"2017/12/11, 16:20:02",True,"2017/12/11, 16:20:02",5644.0,9908141.0,1,Application Performance Management(APM)
AppDynamics,47754925,47754856,0,"2017/12/11, 16:20:02",True,"2017/12/11, 16:20:02",5644.0,9908141.0,1,In simpler terms:
AppDynamics,47754925,47754856,0,"2017/12/11, 16:20:02",True,"2017/12/11, 16:20:02",5644.0,9908141.0,1,"APM monitors the speed at which transactions are performed both by
  end-users and by the systems and network infrastructure that support a
  software application, providing an end-to-end overview of potential
  bottlenecks and service interruptions."
AppDynamics,47754925,47754856,0,"2017/12/11, 16:20:02",True,"2017/12/11, 16:20:02",5644.0,9908141.0,1,"In pragmatic terms, this typically involves the use of a suite of software tools—or a single integrated SaaS or on-premises tool—to view and diagnose an application’s speed, reliability, and other performance metrics in order to maintain an optimal level of service."
AppDynamics,47754925,47754856,0,"2017/12/11, 16:20:02",True,"2017/12/11, 16:20:02",5644.0,9908141.0,1,"Here is
 Wiki description."
AppDynamics,43064247,42076632,0,"2017/03/28, 11:21:22",False,"2017/03/28, 11:21:22",324.0,7220665.0,0,You don't need a machine agent extension (and additional custom metrics) to capture if a JVM goes down.
AppDynamics,43064247,42076632,0,"2017/03/28, 11:21:22",False,"2017/03/28, 11:21:22",324.0,7220665.0,0,The machine agent delivers these kind of information out of the box.
AppDynamics,43064247,42076632,0,"2017/03/28, 11:21:22",False,"2017/03/28, 11:21:22",324.0,7220665.0,0,"You have to start the java process with the app server agent and associate the application, that is instrumented with the machine agent."
AppDynamics,43064247,42076632,0,"2017/03/28, 11:21:22",False,"2017/03/28, 11:21:22",324.0,7220665.0,0,Now you can go to the  application dashboard -&gt; Events tab  and check for the  JVM Crash Event .
AppDynamics,43064247,42076632,0,"2017/03/28, 11:21:22",False,"2017/03/28, 11:21:22",324.0,7220665.0,0,See also  here  and  here .
AppDynamics,36874297,36871506,0,"2016/04/26, 22:41:42",False,"2016/04/26, 22:41:42",14666.0,2067527.0,1,I'm not familiar with the AppDynamics output.
AppDynamics,36874297,36871506,0,"2016/04/26, 22:41:42",False,"2016/04/26, 22:41:42",14666.0,2067527.0,1,I assume that's a cumulative view of Threads and their sleep times.
AppDynamics,36874297,36871506,0,"2016/04/26, 22:41:42",False,"2016/04/26, 22:41:42",14666.0,2067527.0,1,So Threads get reused and so the sleep times add up.
AppDynamics,36874297,36871506,0,"2016/04/26, 22:41:42",False,"2016/04/26, 22:41:42",14666.0,2067527.0,1,"In some cases, a Thread gets a connection directly, without any waiting and in another call the Thread has to wait until the connection can be provided."
AppDynamics,36874297,36871506,0,"2016/04/26, 22:41:42",False,"2016/04/26, 22:41:42",14666.0,2067527.0,1,"The wait duration depends on when a connection becomes available, or the wait limit is hit."
AppDynamics,36874297,36871506,0,"2016/04/26, 22:41:42",False,"2016/04/26, 22:41:42",14666.0,2067527.0,1,"Your screenshot shows a Thread, which waited  172ms ."
AppDynamics,36874297,36871506,0,"2016/04/26, 22:41:42",False,"2016/04/26, 22:41:42",14666.0,2067527.0,1,"Assuming the  sleep  is only called within the Jedis/Redis invocation path, the Thread waited  172ms  in total to get a connection."
AppDynamics,36874297,36871506,0,"2016/04/26, 22:41:42",False,"2016/04/26, 22:41:42",14666.0,2067527.0,1,"Another Thread, which waited  530ms  looks to me as if the first attempt to get a connection wasn't successful (which explains the first  500ms ) and on a second attempt, it had to wait for  30ms ."
AppDynamics,36874297,36871506,0,"2016/04/26, 22:41:42",False,"2016/04/26, 22:41:42",14666.0,2067527.0,1,It could also be that it waited 2x for  265ms .
AppDynamics,36874297,36871506,0,"2016/04/26, 22:41:42",False,"2016/04/26, 22:41:42",14666.0,2067527.0,1,Sidenote:
AppDynamics,36874297,36871506,0,"2016/04/26, 22:41:42",False,"2016/04/26, 22:41:42",14666.0,2067527.0,1,1000+ connections could severely limit scalability.
AppDynamics,36874297,36871506,0,"2016/04/26, 22:41:42",False,"2016/04/26, 22:41:42",14666.0,2067527.0,1,Spring Data Redis also supports other drivers which don't require pooling but work with fewer connections (see  Spring Data Redis Connectors  and  here ).
AppDynamics,36460104,36459842,7,"2016/04/06, 22:01:39",False,"2016/04/06, 22:01:39",6806.0,807193.0,0,You are getting this error as you have not provided the  values required by app to work.
AppDynamics,36460104,36459842,7,"2016/04/06, 22:01:39",False,"2016/04/06, 22:01:39",6806.0,807193.0,0,You need to add the values in  PreferenceConstants  .
AppDynamics,38212864,36459842,0,"2016/07/06, 00:16:07",False,"2016/07/06, 01:16:50",4169.0,6146338.0,0,Did you get it working?
AppDynamics,38212864,36459842,0,"2016/07/06, 00:16:07",False,"2016/07/06, 01:16:50",4169.0,6146338.0,0,"Actually, you know."
AppDynamics,38212864,36459842,0,"2016/07/06, 00:16:07",False,"2016/07/06, 01:16:50",4169.0,6146338.0,0,You are trying to connect to server and app key is the one generated at server and has to be implemented at end application.
AppDynamics,38212864,36459842,0,"2016/07/06, 00:16:07",False,"2016/07/06, 01:16:50",4169.0,6146338.0,0,"Since this is a template, there is no server allocated by the code writer to test and so you don't have app key."
AppDynamics,38212864,36459842,0,"2016/07/06, 00:16:07",False,"2016/07/06, 01:16:50",4169.0,6146338.0,0,"Try exploring IoT frameworks by IBM any other or even you can try open source, Kaa."
AppDynamics,38212864,36459842,0,"2016/07/06, 00:16:07",False,"2016/07/06, 01:16:50",4169.0,6146338.0,0,It will be worth if want explore this kind of cloud dependent apps.
AppDynamics,34131925,34103927,2,"2015/12/07, 12:58:09",False,"2015/12/07, 12:58:09",11383.0,18591.0,0,This sounds like you open a transaction and get a database connection always and only after check cache content.
AppDynamics,34131925,34103927,2,"2015/12/07, 12:58:09",False,"2015/12/07, 12:58:09",11383.0,18591.0,0,"But since you have opened the transactional context, it will be closed, issuing the commit, as no exceptions happened."
AppDynamics,34131925,34103927,2,"2015/12/07, 12:58:09",False,"2015/12/07, 12:58:09",11383.0,18591.0,0,You most likely need to move the checking of the cache outside of the transactional context.
AppDynamics,34131925,34103927,2,"2015/12/07, 12:58:09",False,"2015/12/07, 12:58:09",11383.0,18591.0,0,"Of course, confirming this depends on your application code not included in the question."
AppDynamics,34137555,34103927,0,"2015/12/07, 17:44:30",True,"2015/12/07, 17:44:30",1220.0,526781.0,1,The transaction demarcation related invokes on the  Connection  will happen nevertheless.
AppDynamics,34137555,34103927,0,"2015/12/07, 17:44:30",True,"2015/12/07, 17:44:30",1220.0,526781.0,1,You could use something like Spring's  LazyConnectionDataSourceProxy  (doc'ed  here ) to avoid having these sent when they are not required.
AppDynamics,28040996,28027105,4,"2015/01/20, 10:48:34",False,"2015/01/20, 10:48:34",13083.0,1880810.0,0,The  asadmin  manual page says the following:
AppDynamics,28040996,28027105,4,"2015/01/20, 10:48:34",False,"2015/01/20, 10:48:34",13083.0,1880810.0,0,"For the Windows operating system in single mode, a backslash is
  required to escape the colon and the backslash characters."
AppDynamics,28040996,28027105,4,"2015/01/20, 10:48:34",False,"2015/01/20, 10:48:34",13083.0,1880810.0,0,So try the following:
AppDynamics,28040996,28027105,4,"2015/01/20, 10:48:34",False,"2015/01/20, 10:48:34",13083.0,1880810.0,0,See also:
AppDynamics,61510109,61509776,0,"2020/04/29, 22:29:43",False,"2020/04/29, 22:29:43",6328.0,10726850.0,1,"In theory, yes: if Resource Manager has been enabled it could be the case that different Resource Manager plans have such an impact but experience shows that this feature is seldom used."
AppDynamics,61510109,61509776,0,"2020/04/29, 22:29:43",False,"2020/04/29, 22:29:43",6328.0,10726850.0,1,In practive this kind of difference can have many cause:-
AppDynamics,61510109,61509776,0,"2020/04/29, 22:29:43",False,"2020/04/29, 22:29:43",6328.0,10726850.0,1,The first thing to look at database level is something similar to Statspack report (or AWR if licensing allows) to compare database configuration and activity.
AppDynamics,61510109,61509776,0,"2020/04/29, 22:29:43",False,"2020/04/29, 22:29:43",6328.0,10726850.0,1,"And don't forget that application performance is not only database performance it depends also on application server, network and front-end."
AppDynamics,37659288,37658175,0,"2016/06/06, 16:52:28",False,"2016/06/06, 17:01:30",5857.0,3641043.0,0,I think this should do what you want.
AppDynamics,37659288,37658175,0,"2016/06/06, 16:52:28",False,"2016/06/06, 17:01:30",5857.0,3641043.0,0,"I'm afraid it's not tested, but the principle is this:"
AppDynamics,37659288,37658175,0,"2016/06/06, 16:52:28",False,"2016/06/06, 17:01:30",5857.0,3641043.0,0,"1) if the Set-Cookie header starts with ADRUM, then set the env variable ADRUM_cookie."
AppDynamics,37659288,37658175,0,"2016/06/06, 16:52:28",False,"2016/06/06, 17:01:30",5857.0,3641043.0,0,"2) if the ADRUM_cookie env variable is /not/ set, edit the Set-Cookie header."
AppDynamics,32768918,32768813,10,"2015/09/24, 22:00:00",True,"2016/02/03, 19:13:19",3381.0,868947.0,1,"If we are talking about ""run tool-get result"" the best option -  Java Mission Control ."
AppDynamics,32768918,32768813,10,"2015/09/24, 22:00:00",True,"2016/02/03, 19:13:19",3381.0,868947.0,1,It's free in test environment.
AppDynamics,32768918,32768813,10,"2015/09/24, 22:00:00",True,"2016/02/03, 19:13:19",3381.0,868947.0,1,You need to pay only for some features in production.
AppDynamics,32768918,32768813,10,"2015/09/24, 22:00:00",True,"2016/02/03, 19:13:19",3381.0,868947.0,1,It's much better than old VisualVM.
AppDynamics,32768918,32768813,10,"2015/09/24, 22:00:00",True,"2016/02/03, 19:13:19",3381.0,868947.0,1,You can write a data to file using  Flight Recorder .
AppDynamics,32768918,32768813,10,"2015/09/24, 22:00:00",True,"2016/02/03, 19:13:19",3381.0,868947.0,1,You can setup start point and duration.
AppDynamics,32768918,32768813,10,"2015/09/24, 22:00:00",True,"2016/02/03, 19:13:19",3381.0,868947.0,1,You just need to start your application like this:
AppDynamics,32768918,32768813,10,"2015/09/24, 22:00:00",True,"2016/02/03, 19:13:19",3381.0,868947.0,1,"-XX:+UnlockCommercialFeatures -XX:+FlightRecorder -XX:StartFlightRecording=duration=60s,filename=myrecording.jfr"
AppDynamics,67107284,66992681,0,"2021/04/15, 14:03:49",False,"2021/04/15, 14:03:49",3170.0,1237595.0,0,I think what you are wanting is info around instrumentation of the front end of an Angular SPA.
AppDynamics,67107284,66992681,0,"2021/04/15, 14:03:49",False,"2021/04/15, 14:03:49",3170.0,1237595.0,0,Please see documentation here:  https://docs.appdynamics.com/display/PRO21/Monitor+Single-Page+Applications  - Angular is mentioned as being available using SPA2.
AppDynamics,67107284,66992681,0,"2021/04/15, 14:03:49",False,"2021/04/15, 14:03:49",3170.0,1237595.0,0,"(Note for the front end we use Javascript BRUM for monitoring, general docs are here:  https://docs.appdynamics.com/display/PRO21/Browser+Monitoring  - this is a seperate part of the product than the APM used to monitor backends)"
AppDynamics,50694003,50682773,2,"2018/06/05, 10:03:15",False,"2018/06/05, 10:03:15",30936.0,6587650.0,0,"Since some packages are not working with different OS configurations, you need to setup a new build agent."
AppDynamics,50694003,50682773,2,"2018/06/05, 10:03:15",False,"2018/06/05, 10:03:15",30936.0,6587650.0,0,Deploy an agent on Windows
Datadog,52609589,49671175,1,"2018/10/02, 16:40:17",False,"2018/11/16, 12:34:17",107.0,4772912.0,2,It's possible to solve this by to changing the query by using zero interpolation.
Datadog,52609589,49671175,1,"2018/10/02, 16:40:17",False,"2018/11/16, 12:34:17",107.0,4772912.0,2,"You can put "".fill(zero)"" behind your query in the json or choose the option from the UI."
Datadog,52609589,49671175,1,"2018/10/02, 16:40:17",False,"2018/11/16, 12:34:17",107.0,4772912.0,2,EDIT:
Datadog,52609589,49671175,1,"2018/10/02, 16:40:17",False,"2018/11/16, 12:34:17",107.0,4772912.0,2,"You're right, the interpolation is not working when no data is available."
Datadog,52609589,49671175,1,"2018/10/02, 16:40:17",False,"2018/11/16, 12:34:17",107.0,4772912.0,2,I had the same problem in the end.
Datadog,52609589,49671175,1,"2018/10/02, 16:40:17",False,"2018/11/16, 12:34:17",107.0,4772912.0,2,Support of Datadog said it isn't possible to show zero when there is no data for a metric.
Datadog,52609589,49671175,1,"2018/10/02, 16:40:17",False,"2018/11/16, 12:34:17",107.0,4772912.0,2,Now there is a feature request made for it.
Datadog,52609589,49671175,1,"2018/10/02, 16:40:17",False,"2018/11/16, 12:34:17",107.0,4772912.0,2,"It would be nice if more people will request for this feature, so it will be prioritized."
Datadog,52609589,49671175,1,"2018/10/02, 16:40:17",False,"2018/11/16, 12:34:17",107.0,4772912.0,2,"Nonetheless I have tried to create a workaround by adding a second metric that always has data as a second query and added a formula ((b - b) + a) that negates the second query, but when there is data in the intended query it's show in the graph."
Datadog,52609589,49671175,1,"2018/10/02, 16:40:17",False,"2018/11/16, 12:34:17",107.0,4772912.0,2,This will result in a zero line when there is no data available.
Datadog,52609589,49671175,1,"2018/10/02, 16:40:17",False,"2018/11/16, 12:34:17",107.0,4772912.0,2,Scenario without data:
Datadog,52609589,49671175,1,"2018/10/02, 16:40:17",False,"2018/11/16, 12:34:17",107.0,4772912.0,2,"The only problem is that when you have a data in the intended query, it looks ugly and the zero line is gone."
Datadog,52609589,49671175,1,"2018/10/02, 16:40:17",False,"2018/11/16, 12:34:17",107.0,4772912.0,2,As you see in the following screenshot.
Datadog,52609589,49671175,1,"2018/10/02, 16:40:17",False,"2018/11/16, 12:34:17",107.0,4772912.0,2,Scenario with data:
Datadog,52609589,49671175,1,"2018/10/02, 16:40:17",False,"2018/11/16, 12:34:17",107.0,4772912.0,2,"Conclusion: 
The workaround is not perfect, but it will work for some situation."
Datadog,52609589,49671175,1,"2018/10/02, 16:40:17",False,"2018/11/16, 12:34:17",107.0,4772912.0,2,"For example, filling up query values with zero instead of (no data)."
Datadog,52609589,49671175,1,"2018/10/02, 16:40:17",False,"2018/11/16, 12:34:17",107.0,4772912.0,2,I hope this is a bit better answer to the problem.
Datadog,53345774,49671175,0,"2018/11/16, 23:38:15",False,"2018/11/16, 23:38:15",1371.0,5540166.0,21,"how about  the ""default"" function ?"
Datadog,53345774,49671175,0,"2018/11/16, 23:38:15",False,"2018/11/16, 23:38:15",1371.0,5540166.0,21,"so  default(sum:foo.bar{hello:world} by {baz}, 0)  or some such?"
Datadog,58276947,49671175,0,"2019/10/07, 23:49:14",False,"2020/08/28, 05:49:42",452.0,1655072.0,6,There is now a  default_zero()  function that can be used in Datadog by modifying through JSON directly.
Datadog,61641189,49671175,0,"2020/05/06, 20:15:31",False,"2020/05/06, 20:15:31",47308.0,3141234.0,7,The  default_zero()  function does what you're looking for.
Datadog,61641189,49671175,0,"2020/05/06, 20:15:31",False,"2020/05/06, 20:15:31",47308.0,3141234.0,7,"You can type it in manually, as  stephenlechner suggests ."
Datadog,61641189,49671175,0,"2020/05/06, 20:15:31",False,"2020/05/06, 20:15:31",47308.0,3141234.0,7,There's another way I found:
Datadog,61641189,49671175,0,"2020/05/06, 20:15:31",False,"2020/05/06, 20:15:31",47308.0,3141234.0,7,"When you save the graph and reopen, you'll see that things have been reshuffled a little, and you'll see a ""default 0"" section tagged onto the end of the metric's definition."
Datadog,66855333,55848522,0,"2021/03/29, 16:43:11",False,"2021/03/29, 16:43:11",336.0,7243426.0,0,Metrics queries now support wildcards.
Datadog,66855333,55848522,0,"2021/03/29, 16:43:11",False,"2021/03/29, 16:43:11",336.0,7243426.0,0,"Example 1: Getting all the requests with a status tag starting with  2 :
 http.server.requests.count{status:2*}"
Datadog,66855333,55848522,0,"2021/03/29, 16:43:11",False,"2021/03/29, 16:43:11",336.0,7243426.0,0,"Example 1: Getting all the requests with a service tag ending with  mongo :
 http.server.requests.count{service:*mongo}"
Datadog,66855333,55848522,0,"2021/03/29, 16:43:11",False,"2021/03/29, 16:43:11",336.0,7243426.0,0,"Example 3 (advanced): Getting all the requests with a service tag starting with  blob  and ending with  postgres :
 http.server.requests.count{service:blob*,service:*postgres} 
 (this will match  service:blob-foo-postgres  and  service:blob_bar_postgres  but not  service:my_name_postgres )"
Datadog,34400454,34398692,2,"2015/12/21, 18:41:39",True,"2015/12/21, 18:41:39",7984.0,750117.0,11,I've finally found a dropwizzard module that integrates this library with datadog:  metrics-datadog
Datadog,34400454,34398692,2,"2015/12/21, 18:41:39",True,"2015/12/21, 18:41:39",7984.0,750117.0,11,I've created a Spring configuration class that creates and initializes this Reporter using properties of my YAML.
Datadog,34400454,34398692,2,"2015/12/21, 18:41:39",True,"2015/12/21, 18:41:39",7984.0,750117.0,11,Just insert this dependency in your pom:
Datadog,34400454,34398692,2,"2015/12/21, 18:41:39",True,"2015/12/21, 18:41:39",7984.0,750117.0,11,Add this configuration to your YAML:
Datadog,34400454,34398692,2,"2015/12/21, 18:41:39",True,"2015/12/21, 18:41:39",7984.0,750117.0,11,and add this configuration class to your project:
Datadog,34400755,34398692,3,"2015/12/21, 18:57:48",False,"2015/12/21, 18:57:48",2073.0,3239981.0,2,"If JMX is an option for you, you may use the  JMX dropwizrd reporter  combined with  java datalog integration"
Datadog,49444331,34398692,3,"2018/03/23, 09:18:59",False,"2018/03/23, 12:50:05",3355.0,607038.0,6,It seems that Spring Boot 2.x added several monitoring system into its metrics.
Datadog,49444331,34398692,3,"2018/03/23, 09:18:59",False,"2018/03/23, 12:50:05",3355.0,607038.0,6,DataDog is one of them supported by  micrometer.io .
Datadog,49444331,34398692,3,"2018/03/23, 09:18:59",False,"2018/03/23, 12:50:05",3355.0,607038.0,6,See reference documentation:  https://docs.spring.io/spring-boot/docs/current/reference/html/production-ready-metrics.html#production-ready-metrics-export-newrelic
Datadog,49444331,34398692,3,"2018/03/23, 09:18:59",False,"2018/03/23, 12:50:05",3355.0,607038.0,6,"For Spring Boot 1.x, you can use back-ported package:"
Datadog,49444331,34398692,3,"2018/03/23, 09:18:59",False,"2018/03/23, 12:50:05",3355.0,607038.0,6,compile 'io.micrometer:micrometer-spring-legacy:latest.release'
Datadog,37013010,37010163,1,"2016/05/03, 22:41:22",True,"2016/05/03, 22:41:22",12342.0,496289.0,10,Confirmed on IRC (#datadog on freenode) that:
Datadog,37013010,37010163,1,"2016/05/03, 22:41:22",True,"2016/05/03, 22:41:22",12342.0,496289.0,10,Datadog doesn't support multiple Y-axis at this time.
Datadog,41817123,37010163,0,"2017/01/24, 00:52:07",False,"2017/01/24, 00:52:07",15978.0,770425.0,0,"If the two axis have the same units but different degrees (10 vs 10 million), then using a non-linear scale such as  log  might provide what you need:"
Datadog,41817123,37010163,0,"2017/01/24, 00:52:07",False,"2017/01/24, 00:52:07",15978.0,770425.0,0,https://help.datadoghq.com/hc/en-us/articles/203038729-Is-it-possible-to-adjust-the-y-axis-for-my-graphs-
Datadog,64150659,37010163,2,"2020/10/01, 10:13:05",False,"2020/10/01, 10:13:05",399.0,1619555.0,3,They do allow dual y-axis now
Datadog,64150659,37010163,2,"2020/10/01, 10:13:05",False,"2020/10/01, 10:13:05",399.0,1619555.0,3,https://docs.datadoghq.com/dashboards/widgets/timeseries/#y-axis-controls
Datadog,64150659,37010163,2,"2020/10/01, 10:13:05",False,"2020/10/01, 10:13:05",399.0,1619555.0,3,Introducing dual Y-axis for time series widgets.
Datadog,64150659,37010163,2,"2020/10/01, 10:13:05",False,"2020/10/01, 10:13:05",399.0,1619555.0,3,"The time series widget on dashboards now support dual y-axis, making it easier than ever to compare two sets of data on a single graph."
Datadog,64150659,37010163,2,"2020/10/01, 10:13:05",False,"2020/10/01, 10:13:05",399.0,1619555.0,3,"By removing the need to create separate graphs, your dashboards can show even more valuable information viewable at a glance."
Datadog,35113402,35111147,2,"2016/01/31, 13:39:25",False,"2016/01/31, 13:39:25",4968.0,19892.0,3,If  dd-agent  listens on  localhost  it can receive data only from localhost (127.0.0.1).
Datadog,35113402,35111147,2,"2016/01/31, 13:39:25",False,"2016/01/31, 13:39:25",4968.0,19892.0,3,Try to change the  dd-agent  host to  0.0.0.0  instead of  localhost .
Datadog,35113402,35111147,2,"2016/01/31, 13:39:25",False,"2016/01/31, 13:39:25",4968.0,19892.0,3,We are using  docker-dd-agent  and it works OOTB.
Datadog,35133077,35111147,3,"2016/02/01, 16:31:05",False,"2016/02/01, 16:31:05",111.0,5868112.0,11,You will need to set  non_local_traffic: yes  in your  /etc/dd-agent/datadog.conf  file.
Datadog,35133077,35111147,3,"2016/02/01, 16:31:05",False,"2016/02/01, 16:31:05",111.0,5868112.0,11,Otherwise the agent will reject metrics from containers.
Datadog,35133077,35111147,3,"2016/02/01, 16:31:05",False,"2016/02/01, 16:31:05",111.0,5868112.0,11,"After setting, you will need to restart the agent for the change to take effect:  sudo /etc/init.d/datadog-agent restart  or  sudo service datadog-agent restart"
Datadog,35133077,35111147,3,"2016/02/01, 16:31:05",False,"2016/02/01, 16:31:05",111.0,5868112.0,11,The  docker-dd-agent  image enables  non_local_traffic: yes  by default.
Datadog,39639402,35111147,0,"2016/09/22, 15:43:05",False,"2016/09/22, 15:43:05",296.0,166816.0,0,You don't actually want to use the IP of the host in this case.
Datadog,39639402,35111147,0,"2016/09/22, 15:43:05",False,"2016/09/22, 15:43:05",296.0,166816.0,0,"If you're running the docker dd-agent, there are two environment variables you can tap into:"
Datadog,39639402,35111147,0,"2016/09/22, 15:43:05",False,"2016/09/22, 15:43:05",296.0,166816.0,0,"statsd.connect(DOGSTATSD_PORT_8125_UDP_ADDR, DOGSTATSD_PORT_8125_UDP_PORT)"
Datadog,39639402,35111147,0,"2016/09/22, 15:43:05",False,"2016/09/22, 15:43:05",296.0,166816.0,0,That should do the trick.
Datadog,39639402,35111147,0,"2016/09/22, 15:43:05",False,"2016/09/22, 15:43:05",296.0,166816.0,0,"If not, you should be able to find the relevant info to your problem in  this section of the Datadog docs ."
Datadog,39639402,35111147,0,"2016/09/22, 15:43:05",False,"2016/09/22, 15:43:05",296.0,166816.0,0,"Also, I should point out that the only Python library that Datadog shows in their docs is  datadogpy ."
Datadog,59791963,57918254,0,"2020/01/17, 19:12:16",True,"2020/01/17, 19:12:16",10672.0,75801.0,10,"I asked DataDog support, and apparently as of January 2020 this is not possible, but is a feature request in their backlog."
Datadog,59791963,57918254,0,"2020/01/17, 19:12:16",True,"2020/01/17, 19:12:16",10672.0,75801.0,10,"I know this is not a great answer to the question but if I hear that this changes, I will update my answer."
Datadog,64519899,35608127,0,"2020/10/25, 04:57:48",False,"2020/10/25, 04:57:48",493.0,244037.0,0,There doesn't appear to be a Crashalytics direct integration yet.
Datadog,64519899,35608127,0,"2020/10/25, 04:57:48",False,"2020/10/25, 04:57:48",493.0,244037.0,0,Datadog maintains mobile SDKs that can be included in mobile clients to produce metrics and logs to the platform.
Datadog,64519899,35608127,0,"2020/10/25, 04:57:48",False,"2020/10/25, 04:57:48",493.0,244037.0,0,"For iOS Logs, see here:  https://docs.datadoghq.com/logs/log_collection/ios/"
Datadog,64519899,35608127,0,"2020/10/25, 04:57:48",False,"2020/10/25, 04:57:48",493.0,244037.0,0,For Android Logs:  https://docs.datadoghq.com/logs/log_collection/android/
Datadog,64519899,35608127,0,"2020/10/25, 04:57:48",False,"2020/10/25, 04:57:48",493.0,244037.0,0,"There's also a public Android SDK for Real User Monitoring, which can be read here:  https://docs.datadoghq.com/real_user_monitoring/android/"
Datadog,64519899,35608127,0,"2020/10/25, 04:57:48",False,"2020/10/25, 04:57:48",493.0,244037.0,0,"And the announcement, with a link for the private beta for iOS signup here:  https://www.datadoghq.com/blog/datadog-mobile-rum/"
Datadog,42023919,40741803,0,"2017/02/03, 14:05:43",True,"2017/02/03, 14:05:43",1199.0,2525626.0,4,"After speaking to the support team at DataDog, I managed to find out the following information relating to what the no_pod pods were."
Datadog,42023919,40741803,0,"2017/02/03, 14:05:43",True,"2017/02/03, 14:05:43",1199.0,2525626.0,4,"Our Kubernetes check is getting the list of containers from the Kubernetes API, which exposes aggregated data."
Datadog,42023919,40741803,0,"2017/02/03, 14:05:43",True,"2017/02/03, 14:05:43",1199.0,2525626.0,4,"In the metric explorer configuration here, you can see a couple of containers named /docker and / that are getting picked up along with the other containers."
Datadog,42023919,40741803,0,"2017/02/03, 14:05:43",True,"2017/02/03, 14:05:43",1199.0,2525626.0,4,Metrics with pod_name:no_pod that come from container_name:/ and container_name:/docker  are just metrics aggregated across multiple containers.
Datadog,42023919,40741803,0,"2017/02/03, 14:05:43",True,"2017/02/03, 14:05:43",1199.0,2525626.0,4,(So it makes sense that these are the highest values in your graphs.)
Datadog,42023919,40741803,0,"2017/02/03, 14:05:43",True,"2017/02/03, 14:05:43",1199.0,2525626.0,4,"If you don't want your graphs to show these aggregated container metrics though, you can clone the dashboard and then exclude these pods from the query."
Datadog,42023919,40741803,0,"2017/02/03, 14:05:43",True,"2017/02/03, 14:05:43",1199.0,2525626.0,4,"To do so, on the cloned dashboard, just edit the query in the JSON tab, and in the tag scope, add !pod_name:no_pod."
Datadog,42023919,40741803,0,"2017/02/03, 14:05:43",True,"2017/02/03, 14:05:43",1199.0,2525626.0,4,So it appears that these pods are the docker and root level containers running outside of the cluster and will always display unless you want to filter them out specifically which I now do.
Datadog,42023919,40741803,0,"2017/02/03, 14:05:43",True,"2017/02/03, 14:05:43",1199.0,2525626.0,4,Many thanks to the support guys at DataDog for looking into the issue for me and giving me a great explanation as to what the pods were and essentially confirming that I can just safely filter these out and not worry about them.
Datadog,42935965,42927552,0,"2017/03/21, 20:59:47",False,"2017/03/21, 20:59:47",2676.0,1230594.0,0,The free text editor you have in the screenshot is for metric queries.
Datadog,42935965,42927552,0,"2017/03/21, 20:59:47",False,"2017/03/21, 20:59:47",2676.0,1230594.0,0,Events in graphs are added as overlay to show when events happened over time.
Datadog,42935965,42927552,0,"2017/03/21, 20:59:47",False,"2017/03/21, 20:59:47",2676.0,1230594.0,0,"There is no widget, as of now, that shows a single value for the number of times an event occurred."
Datadog,42935965,42927552,0,"2017/03/21, 20:59:47",False,"2017/03/21, 20:59:47",2676.0,1230594.0,0,"But you can use the event timeline widget, which will show a timeline of events grouped by status and bucketed over a defined period of time."
Datadog,42935965,42927552,0,"2017/03/21, 20:59:47",False,"2017/03/21, 20:59:47",2676.0,1230594.0,0,See below:
Datadog,42999464,42927552,0,"2017/03/24, 14:26:09",False,"2017/03/24, 14:26:09",1929.0,2599875.0,3,"Well I realized that the  query value  only works with metrics, so to create a counter we can emit metrics with  value: 1  and then count them with the  rollup(sum, 60)  function."
Datadog,42999464,42927552,0,"2017/03/24, 14:26:09",False,"2017/03/24, 14:26:09",1929.0,2599875.0,3,"dog.emit_point('some.event.name', 1)"
Datadog,42999464,42927552,0,"2017/03/24, 14:26:09",False,"2017/03/24, 14:26:09",1929.0,2599875.0,3,"sum:some.event.name{*}.rollup(sum, 60)"
Datadog,42999464,42927552,0,"2017/03/24, 14:26:09",False,"2017/03/24, 14:26:09",1929.0,2599875.0,3,The main thing to understand here is that DataDog does not retrieve all the points for a given timeframe.
Datadog,42999464,42927552,0,"2017/03/24, 14:26:09",False,"2017/03/24, 14:26:09",1929.0,2599875.0,3,"Actually as  McCloud  says,  for a given time range we do not return more than 350 points , which is very important to have in mind when you create a counter."
Datadog,42999464,42927552,0,"2017/03/24, 14:26:09",False,"2017/03/24, 14:26:09",1929.0,2599875.0,3,"When you query value from a metric in a timeframe, DataDog return a group of points that represents the real stored points, not all the points; the level of how those points are represented (as I understand) is called granurallity, and what you do with this  rollup  function is to define how those points are going to represent the real points, which in this case is going to be using the  sum  function."
Datadog,42999464,42927552,0,"2017/03/24, 14:26:09",False,"2017/03/24, 14:26:09",1929.0,2599875.0,3,"I hope this helps somebody, I'm still learning about it."
Datadog,42999464,42927552,0,"2017/03/24, 14:26:09",False,"2017/03/24, 14:26:09",1929.0,2599875.0,3,Regards
Datadog,65565807,42927552,0,"2021/01/04, 18:05:02",False,"2021/01/04, 18:05:02",66.0,2625807.0,0,"In Ruby on the client, I use:"
Datadog,65565807,42927552,0,"2021/01/04, 18:05:02",False,"2021/01/04, 18:05:02",66.0,2625807.0,0,"I then have a dashboard with a
 Query Value 
widget, set to &quot;take the [Sum] value from the displayed timeframe&quot;."
Datadog,65565807,42927552,0,"2021/01/04, 18:05:02",False,"2021/01/04, 18:05:02",66.0,2625807.0,0,I tested this and it seems to give the right numbers.
Datadog,65565807,42927552,0,"2021/01/04, 18:05:02",False,"2021/01/04, 18:05:02",66.0,2625807.0,0,"The
 .as_count() 
seems key."
Datadog,58493964,45506314,0,"2019/10/21, 23:51:33",True,"2019/10/21, 23:51:33",20407.0,1017941.0,1,Answering just in case someone will spot this question via Google.
Datadog,58493964,45506314,0,"2019/10/21, 23:51:33",True,"2019/10/21, 23:51:33",20407.0,1017941.0,1,You cannot.
Datadog,58493964,45506314,0,"2019/10/21, 23:51:33",True,"2019/10/21, 23:51:33",20407.0,1017941.0,1,"StatsD protocol do not define tags or comments at all, so there is no possibility for that."
Datadog,58493964,45506314,0,"2019/10/21, 23:51:33",True,"2019/10/21, 23:51:33",20407.0,1017941.0,1,You need to use different library like  Statix  for that.
Datadog,44207761,42588899,1,"2017/05/26, 21:08:43",False,"2017/05/26, 21:08:43",1.0,1976300.0,-1,Yes it is possible to emit metrics to DataDog from a AWS Lambda function.
Datadog,44207761,42588899,1,"2017/05/26, 21:08:43",False,"2017/05/26, 21:08:43",1.0,1976300.0,-1,If you were using node.js you could use  https://www.npmjs.com/package/datadog-metrics  to emit metrics to the API.
Datadog,44207761,42588899,1,"2017/05/26, 21:08:43",False,"2017/05/26, 21:08:43",1.0,1976300.0,-1,"It supports counters, gauges and histograms."
Datadog,44207761,42588899,1,"2017/05/26, 21:08:43",False,"2017/05/26, 21:08:43",1.0,1976300.0,-1,You just need to pass in your app/api key as environment variables.
Datadog,44207761,42588899,1,"2017/05/26, 21:08:43",False,"2017/05/26, 21:08:43",1.0,1976300.0,-1,Matt
Datadog,54256464,42588899,0,"2019/01/18, 16:57:13",False,"2019/01/18, 16:57:13",809.0,1647226.0,0,The easier way is using this library:  https://github.com/marceloboeira/aws-lambda-datadog
Datadog,54256464,42588899,0,"2019/01/18, 16:57:13",False,"2019/01/18, 16:57:13",809.0,1647226.0,0,"It has runtime no dependencies, doesn't require authentication and reports everything to cloud-watch too."
Datadog,54256464,42588899,0,"2019/01/18, 16:57:13",False,"2019/01/18, 16:57:13",809.0,1647226.0,0,You can read more about it here:  https://www.datadoghq.com/blog/how-to-monitor-lambda-functions/
Datadog,51941286,42722353,0,"2018/08/21, 06:48:18",False,"2018/08/21, 06:57:14",1474.0,10109833.0,0,"In most cases, the datadog agent retrieves metrics from an integration by connecting to a URL endpoint."
Datadog,51941286,42722353,0,"2018/08/21, 06:48:18",False,"2018/08/21, 06:57:14",1474.0,10109833.0,0,"This would be the case for services such as nginx, mysql etc."
Datadog,51941286,42722353,0,"2018/08/21, 06:48:18",False,"2018/08/21, 06:57:14",1474.0,10109833.0,0,"This means that you can run just one datadog agent on the host, and configure it to listen to URL endpoints of services exposed from each container."
Datadog,51941286,42722353,0,"2018/08/21, 06:48:18",False,"2018/08/21, 06:57:14",1474.0,10109833.0,0,"For example, assuming a mysql docker container is run with the following command:"
Datadog,51941286,42722353,0,"2018/08/21, 06:48:18",False,"2018/08/21, 06:57:14",1474.0,10109833.0,0,You can instruct the agent running on the host to connect to the container IP in the  mysql.yaml  agent configuration:
Datadog,51941286,42722353,0,"2018/08/21, 06:48:18",False,"2018/08/21, 06:57:14",1474.0,10109833.0,0,Varnish is slightly different as the agent retrieves metrics using the  varnishstat  binary.
Datadog,51941286,42722353,0,"2018/08/21, 06:48:18",False,"2018/08/21, 06:57:14",1474.0,10109833.0,0,According to the example template:
Datadog,51941286,42722353,0,"2018/08/21, 06:48:18",False,"2018/08/21, 06:57:14",1474.0,10109833.0,0,In order to support monitoring a Varnish instance which is running as a Docker container we need to wrap commands (varnishstat) with scripts which perform a docker exec on the running container.
Datadog,51941286,42722353,0,"2018/08/21, 06:48:18",False,"2018/08/21, 06:57:14",1474.0,10109833.0,0,"To do this, on the host, create a wrapper script for the container:"
Datadog,51941286,42722353,0,"2018/08/21, 06:48:18",False,"2018/08/21, 06:57:14",1474.0,10109833.0,0,Then specify the script location in the  varnish.yaml  agent configuration:
Datadog,47167529,45188547,0,"2017/11/07, 23:12:04",False,"2017/11/07, 23:12:04",6992.0,902415.0,2,There are 2 relevant options in  /etc/dd-agent/conf.d/docker_daemon.yaml :
Datadog,47167529,45188547,0,"2017/11/07, 23:12:04",False,"2017/11/07, 23:12:04",6992.0,902415.0,2,"collect_disk_stats 
If you use devicemapper-backed storage (which is default in ECS but not in vanilla Docker or Kubernetes), docker.data."
Datadog,47167529,45188547,0,"2017/11/07, 23:12:04",False,"2017/11/07, 23:12:04",6992.0,902415.0,2,* and docker.metadata.
Datadog,47167529,45188547,0,"2017/11/07, 23:12:04",False,"2017/11/07, 23:12:04",6992.0,902415.0,2,* statistics should do what you are looking for.
Datadog,47167529,45188547,0,"2017/11/07, 23:12:04",False,"2017/11/07, 23:12:04",6992.0,902415.0,2,"collect_container_size 
A generic way, using the docker API but virtually running df in every container."
Datadog,47167529,45188547,0,"2017/11/07, 23:12:04",False,"2017/11/07, 23:12:04",6992.0,902415.0,2,This enables the docker.container.
Datadog,47167529,45188547,0,"2017/11/07, 23:12:04",False,"2017/11/07, 23:12:04",6992.0,902415.0,2,* metrics.
Datadog,47167529,45188547,0,"2017/11/07, 23:12:04",False,"2017/11/07, 23:12:04",6992.0,902415.0,2,"See more here:
 https://help.datadoghq.com/hc/en-us/articles/115001786703-How-to-report-host-disk-metrics-when-dd-agent-runs-in-a-docker-container-"
Datadog,47167529,45188547,0,"2017/11/07, 23:12:04",False,"2017/11/07, 23:12:04",6992.0,902415.0,2,"and here:
 https://github.com/DataDog/docker-dd-agent/blob/master/conf.d/docker_daemon.yaml#L46"
Datadog,55916769,45954769,0,"2019/04/30, 11:27:17",False,"2019/04/30, 11:27:17",11.0,11431260.0,1,"In your question, I looked for your purpose in terms of using the port 8125 or 8126 ports."
Datadog,55916769,45954769,0,"2019/04/30, 11:27:17",False,"2019/04/30, 11:27:17",11.0,11431260.0,1,"8125 port is used for stasd metrics, and 8126 is used for APM (trace) data."
Datadog,55916769,45954769,0,"2019/04/30, 11:27:17",False,"2019/04/30, 11:27:17",11.0,11431260.0,1,So if you want to use 8125 the important thing to do is having  non_local_traffic : yes .
Datadog,55916769,45954769,0,"2019/04/30, 11:27:17",False,"2019/04/30, 11:27:17",11.0,11431260.0,1,So there must be another problem which I don't know yet.
Datadog,55916769,45954769,0,"2019/04/30, 11:27:17",False,"2019/04/30, 11:27:17",11.0,11431260.0,1,But if your purpose is using APM/trace port: 8126 is only bound to localhost by default.
Datadog,55916769,45954769,0,"2019/04/30, 11:27:17",False,"2019/04/30, 11:27:17",11.0,11431260.0,1,You should make it listen to any network interface by the  bind_host: 0.0.0.0  configuration.
Datadog,55916769,45954769,0,"2019/04/30, 11:27:17",False,"2019/04/30, 11:27:17",11.0,11431260.0,1,"Currently, it will refuse the requests from your containers since they are not coming from localhost."
Datadog,55916769,45954769,0,"2019/04/30, 11:27:17",False,"2019/04/30, 11:27:17",11.0,11431260.0,1,I had a similar problem and this page helped me:  https://github.com/DataDog/ansible-datadog/issues/149
Datadog,53449880,53435248,2,"2018/11/23, 18:09:47",True,"2019/07/29, 16:36:02",1371.0,5540166.0,5,UPDATED ANSWER:
Datadog,53449880,53435248,2,"2018/11/23, 18:09:47",True,"2019/07/29, 16:36:02",1371.0,5540166.0,5,Still yes.
Datadog,53449880,53435248,2,"2018/11/23, 18:09:47",True,"2019/07/29, 16:36:02",1371.0,5540166.0,5,Docs for new Dashboard endpoint  here .
Datadog,53449880,53435248,2,"2018/11/23, 18:09:47",True,"2019/07/29, 16:36:02",1371.0,5540166.0,5,ORIGINAL ANSWER:
Datadog,53449880,53435248,2,"2018/11/23, 18:09:47",True,"2019/07/29, 16:36:02",1371.0,5540166.0,5,Yes.
Datadog,53449880,53435248,2,"2018/11/23, 18:09:47",True,"2019/07/29, 16:36:02",1371.0,5540166.0,5,Docs for screenboards  here .
Datadog,53449880,53435248,2,"2018/11/23, 18:09:47",True,"2019/07/29, 16:36:02",1371.0,5540166.0,5,Docs for timeboards  here .
Datadog,54194264,53880368,2,"2019/01/15, 09:19:00",True,"2019/01/15, 09:19:00",747.0,2461574.0,1,"I doubt the Datadog agent will ever work on App Services web app as you do not have access to the running host, it was designed for VMs."
Datadog,54194264,53880368,2,"2019/01/15, 09:19:00",True,"2019/01/15, 09:19:00",747.0,2461574.0,1,Have you tried this  https://www.datadoghq.com/blog/azure-monitoring-enhancements/  ?
Datadog,54194264,53880368,2,"2019/01/15, 09:19:00",True,"2019/01/15, 09:19:00",747.0,2461574.0,1,They say they support AppServices
Datadog,55897902,53880368,2,"2019/04/29, 09:16:35",False,"2019/04/29, 09:16:35",27.0,5404159.0,0,"To respond to your comment on wanting custom metrics, this is still possible without the agent at the same location."
Datadog,55897902,53880368,2,"2019/04/29, 09:16:35",False,"2019/04/29, 09:16:35",27.0,5404159.0,0,After installing the nuget package of datadog called statsdclient you can then configure it to send the custom metrics to an agent located elsewhere.
Datadog,55897902,53880368,2,"2019/04/29, 09:16:35",False,"2019/04/29, 09:16:35",27.0,5404159.0,0,Example below:
Datadog,56799311,53880368,0,"2019/06/28, 01:59:38",False,"2019/06/28, 01:59:38",55.0,11661354.0,0,I have written a app service extension for sending Datadog APM metrics with .NET core and provided instructions for how to set it up here:  https://github.com/payscale/datadog-app-service-extension
Datadog,56799311,53880368,0,"2019/06/28, 01:59:38",False,"2019/06/28, 01:59:38",55.0,11661354.0,0,Let me know if you have any questions or if this doesn't apply to your situation.
Datadog,56808294,53880368,0,"2019/06/28, 16:56:58",False,"2019/06/28, 16:56:58",11.0,11713986.0,1,Logs from App Services can also be sent to Blob storage and forwarded from there via an Azure Function.
Datadog,56808294,53880368,0,"2019/06/28, 16:56:58",False,"2019/06/28, 16:56:58",11.0,11713986.0,1,"Unlike traces and custom metrics from App Services, this does not require a VM running the agent."
Datadog,56808294,53880368,0,"2019/06/28, 16:56:58",False,"2019/06/28, 16:56:58",11.0,11713986.0,1,Docs and code for the Function are available here:
Datadog,56808294,53880368,0,"2019/06/28, 16:56:58",False,"2019/06/28, 16:56:58",11.0,11713986.0,1,https://github.com/DataDog/datadog-serverless-functions/tree/master/azure/blobs_logs_monitoring
Datadog,58550213,53880368,0,"2019/10/25, 02:03:41",False,"2019/10/25, 02:03:41",1630.0,1246590.0,1,If you want to use DataDog for logging from Azure Function of App Service you can use Serilog and DataDog Sink to the log files:
Datadog,58550213,53880368,0,"2019/10/25, 02:03:41",False,"2019/10/25, 02:03:41",1630.0,1246590.0,1,Full source code and required NuGet packages  are here:
Datadog,61118441,59500017,0,"2020/04/09, 12:53:48",False,"2020/04/09, 12:53:48",385.0,1068531.0,0,You can deploy the Datadog agent in a container / instance that you manage and the configure it according to  these instructions  to gather metrics from the remote ElasticSearch cluster that is hosted on Elastic Cloud.
Datadog,61118441,59500017,0,"2020/04/09, 12:53:48",False,"2020/04/09, 12:53:48",385.0,1068531.0,0,"You need to create a  conf.yaml  file in the  elastic.d/  directory and provide the required information (Elasticsearch endpoint/URL, username, password, port, etc) for the agent to be able to connect to the cluster."
Datadog,61118441,59500017,0,"2020/04/09, 12:53:48",False,"2020/04/09, 12:53:48",385.0,1068531.0,0,You may find a sample configuration file  here .
Datadog,61276030,59500017,0,"2020/04/17, 19:13:14",True,"2020/04/17, 19:13:14",2287.0,466390.0,0,"As George Tseres mentioned above, the way I had to get this working was to set up collection on a separate instance (through docker) and then to configure it to read the specific Elastic Cloud instances."
Datadog,61276030,59500017,0,"2020/04/17, 19:13:14",True,"2020/04/17, 19:13:14",2287.0,466390.0,0,"I ended up making this:  https://github.com/crwang/datadog-elasticsearch , building that docker image, and then pushing it up to AWS ECR."
Datadog,61276030,59500017,0,"2020/04/17, 19:13:14",True,"2020/04/17, 19:13:14",2287.0,466390.0,0,"Then, I spun up a Fargate service / task to run the container."
Datadog,61276030,59500017,0,"2020/04/17, 19:13:14",True,"2020/04/17, 19:13:14",2287.0,466390.0,0,I also set it to run locally with  docker-compose  as a test.
Datadog,35509278,35224366,1,"2016/02/19, 17:47:27",True,"2016/02/19, 17:47:27",455.0,4676932.0,10,These system.io metrics are reported from a  system agent check  that uses  iostat  under the hood.
Datadog,35509278,35224366,1,"2016/02/19, 17:47:27",True,"2016/02/19, 17:47:27",455.0,4676932.0,10,According to the  iostat manpage  one of the metrics  %util  (reported as  system.io.util  within Datadog) seems to do the job:
Datadog,35509278,35224366,1,"2016/02/19, 17:47:27",True,"2016/02/19, 17:47:27",455.0,4676932.0,10,%util: Percentage of CPU time during which I/O requests were issued to the device (bandwidth utilization for the device).
Datadog,35509278,35224366,1,"2016/02/19, 17:47:27",True,"2016/02/19, 17:47:27",455.0,4676932.0,10,Device saturation occurs when this value is close to 100%.
Datadog,35509278,35224366,1,"2016/02/19, 17:47:27",True,"2016/02/19, 17:47:27",455.0,4676932.0,10,"You can create a monitor, as a multi alert on host/device, when this metric is over 90 on the last 30 minutes on average, here is a current screenshot of such an example:"
Datadog,35509278,35224366,1,"2016/02/19, 17:47:27",True,"2016/02/19, 17:47:27",455.0,4676932.0,10,Of course one can also monitor other iostat metrics to identify other I/O performance failure modes.
Datadog,35560762,35554988,2,"2016/02/22, 20:06:38",True,"2016/02/22, 20:06:38",261.0,4172512.0,10,This can be achieved in two ways.
Datadog,35560762,35554988,2,"2016/02/22, 20:06:38",True,"2016/02/22, 20:06:38",261.0,4172512.0,10,"If you only want this logic to applied on this one graph, you can divide metric either using the UI editor and clicking advanced or using the JSON editor:"
Datadog,35560762,35554988,2,"2016/02/22, 20:06:38",True,"2016/02/22, 20:06:38",261.0,4172512.0,10,UI Editor:  http://cl.ly/1c0K2O3P1E2K
Datadog,35560762,35554988,2,"2016/02/22, 20:06:38",True,"2016/02/22, 20:06:38",261.0,4172512.0,10,Or JSON editor:  https://help.datadoghq.com/hc/en-us/articles/203764925-How-do-I-use-arithmetic-for-my-time-series-data-
Datadog,35560762,35554988,2,"2016/02/22, 20:06:38",True,"2016/02/22, 20:06:38",261.0,4172512.0,10,"Alternatively, you can use the Metric Summary page to edit this metric's metadata and alter this metric's unit throughout the application as seen here:  http://cl.ly/2x0Z290w2I3V"
Datadog,35560762,35554988,2,"2016/02/22, 20:06:38",True,"2016/02/22, 20:06:38",261.0,4172512.0,10,https://app.datadoghq.com/metric/summary
Datadog,35560762,35554988,2,"2016/02/22, 20:06:38",True,"2016/02/22, 20:06:38",261.0,4172512.0,10,Hope this helps.
Datadog,35560762,35554988,2,"2016/02/22, 20:06:38",True,"2016/02/22, 20:06:38",261.0,4172512.0,10,"Also, you can reach out to support@datadoghq.com if you run into any other issues in the future."
Datadog,42481675,42441120,3,"2017/02/27, 11:09:46",True,"2017/02/27, 11:09:46",6657.0,86.0,1,"So, while trying to debug this I deleted the deployment + dameonset and service and recreated it."
Datadog,42481675,42441120,3,"2017/02/27, 11:09:46",True,"2017/02/27, 11:09:46",6657.0,86.0,1,Afterwards it worked....
Datadog,44038527,42441120,0,"2017/05/18, 07:21:01",False,"2017/05/18, 07:21:01",9.0,7901388.0,0,Have you seen the  Discovering Services  docs?
Datadog,44038527,42441120,0,"2017/05/18, 07:21:01",False,"2017/05/18, 07:21:01",9.0,7901388.0,0,I'd recommend using DNS for service discovery rather than environment variables since environment variables require services to come up in a particular order.
Datadog,43669352,42538664,1,"2017/04/28, 02:11:48",False,"2017/04/28, 02:11:48",162.0,2254902.0,2,There are two error in your code:
Datadog,43669352,42538664,1,"2017/04/28, 02:11:48",False,"2017/04/28, 02:11:48",162.0,2254902.0,2,"Once done, your code will run flawlessly."
Datadog,44193701,42958937,4,"2017/05/26, 07:35:57",True,"2017/05/26, 07:35:57",1371.0,5540166.0,5,"So from that log line, it appears as though  this  try  is excepting  in the library's  hostname.py ."
Datadog,44193701,42958937,4,"2017/05/26, 07:35:57",True,"2017/05/26, 07:35:57",1371.0,5540166.0,5,So either...
Datadog,44193701,42958937,4,"2017/05/26, 07:35:57",True,"2017/05/26, 07:35:57",1371.0,5540166.0,5,"(A) The  hostname line  is where it's excepting, and (weirdly) the
library requires that a  hostname  option be set in your
 datadog.conf  file."
Datadog,44193701,42958937,4,"2017/05/26, 07:35:57",True,"2017/05/26, 07:35:57",1371.0,5540166.0,5,"Maybe worth setting a hostname there if you
haven't already."
Datadog,44193701,42958937,4,"2017/05/26, 07:35:57",True,"2017/05/26, 07:35:57",1371.0,5540166.0,5,"Or,"
Datadog,44193701,42958937,4,"2017/05/26, 07:35:57",True,"2017/05/26, 07:35:57",1371.0,5540166.0,5,"(B) The  get_config() line  is where it's excepting, and so the
library isn't able to correctly identify the configuration file
location (or access it, possibly related to permissions)."
Datadog,44193701,42958937,4,"2017/05/26, 07:35:57",True,"2017/05/26, 07:35:57",1371.0,5540166.0,5,"Based on
the directory structure in your question, I think you're working on
an OSX / mac environment, which means the library will be using the
function  _mac_config_path()  in  config.py  to try to identify the
configuration path, which from  this line in the function  would
make it  seem  as though the library were looking for the
configuration file in  ~/.datadog-agent/agent/datadog.conf  instead
of the appropriate  ~/.datadog-agent/datadog.conf ."
Datadog,44193701,42958937,4,"2017/05/26, 07:35:57",True,"2017/05/26, 07:35:57",1371.0,5540166.0,5,"Which might be a
legitimate bug..."
Datadog,44193701,42958937,4,"2017/05/26, 07:35:57",True,"2017/05/26, 07:35:57",1371.0,5540166.0,5,"So if I were you, and if all this seemed right, I'd try adding a  hostname in the  datadog.conf  to see if that helped, and if it didn't, then I'd try making a  ~/.datadog-agent/agent/  directory and copying your  datadog.conf  file there as well, just to see if that got things working."
Datadog,44193701,42958937,4,"2017/05/26, 07:35:57",True,"2017/05/26, 07:35:57",1371.0,5540166.0,5,"This answer assumes you are working in an OSX / mac environment, and will likely not be correct otherwise."
Datadog,44193701,42958937,4,"2017/05/26, 07:35:57",True,"2017/05/26, 07:35:57",1371.0,5540166.0,5,"If either (A) or (B) are the case, then that's a problem with the library and should be updated--it would be kind of you to open an issue on  the library itself  to bring this up so the Datadog team that supports that library can be made aware."
Datadog,44193701,42958937,4,"2017/05/26, 07:35:57",True,"2017/05/26, 07:35:57",1371.0,5540166.0,5,"I suspect not many people end up this library in OSX / mac environments to begin with, so that could explain all this."
Datadog,56620105,52390804,0,"2019/06/16, 18:14:27",False,"2019/06/16, 18:14:27",301.0,2708541.0,0,"You installed the Datadog agent &amp; trace agent on a Mac, listening on localhost."
Datadog,56620105,52390804,0,"2019/06/16, 18:14:27",False,"2019/06/16, 18:14:27",301.0,2708541.0,0,You installed the flask application and ddtrace library in a docker container on a linux vm sending traffic to localhost.
Datadog,56620105,52390804,0,"2019/06/16, 18:14:27",False,"2019/06/16, 18:14:27",301.0,2708541.0,0,Those two localhosts are describing two different machines.
Datadog,56620105,52390804,0,"2019/06/16, 18:14:27",False,"2019/06/16, 18:14:27",301.0,2708541.0,0,"The easiest option is going to be running both the Agent &amp; flask app on the Mac, or running both in docker."
Datadog,56620105,52390804,0,"2019/06/16, 18:14:27",False,"2019/06/16, 18:14:27",301.0,2708541.0,0,The latter is most similar to an eventual production deployment.
Datadog,56620105,52390804,0,"2019/06/16, 18:14:27",False,"2019/06/16, 18:14:27",301.0,2708541.0,0,Do that.
Datadog,58681226,58476988,0,"2019/11/03, 16:41:58",True,"2019/11/03, 16:41:58",2206.0,712543.0,3,"According to the documentation, this can be achieved using following properties in telegraf.conf:"
Datadog,58681226,58476988,0,"2019/11/03, 16:41:58",True,"2019/11/03, 16:41:58",2206.0,712543.0,3,https://docs.influxdata.com/telegraf/v1.12/administration/configuration/#measurement-filtering
Datadog,58681226,58476988,0,"2019/11/03, 16:41:58",True,"2019/11/03, 16:41:58",2206.0,712543.0,3,where  namepass  defines pattern list of points which will be emitted.
Datadog,60213083,60064427,1,"2020/02/13, 19:02:26",True,"2020/02/14, 09:17:50",27576.0,1839482.0,3,Datadog has two agents.
Datadog,60213083,60064427,1,"2020/02/13, 19:02:26",True,"2020/02/14, 09:17:50",27576.0,1839482.0,3,And yes for DogStatsD the node agents need to be deployed as Daemonset.
Datadog,60213083,60064427,1,"2020/02/13, 19:02:26",True,"2020/02/14, 09:17:50",27576.0,1839482.0,3,Here is the the deployment manifest for  cluster agent  and  node agent .
Datadog,64168376,60290897,0,"2020/10/02, 11:24:58",True,"2020/10/02, 11:24:58",347.0,1485995.0,0,"Yes, the following should work:"
Datadog,64168376,60290897,0,"2020/10/02, 11:24:58",True,"2020/10/02, 11:24:58",347.0,1485995.0,0,tag_one:(A OR B)
Datadog,64168376,60290897,0,"2020/10/02, 11:24:58",True,"2020/10/02, 11:24:58",347.0,1485995.0,0,"Unfortunately the query syntax is slightly different in different contexts, I find, so I don't know if that will solve your particular problem."
Datadog,60667516,60662893,0,"2020/03/13, 11:01:40",False,"2020/03/13, 11:01:40",336.0,7243426.0,1,"If you want to remove the warning, you can try adding  none  and  shm  to the  excluded_filesystems  in disk.yaml."
Datadog,60667516,60662893,0,"2020/03/13, 11:01:40",False,"2020/03/13, 11:01:40",336.0,7243426.0,1,This file should exist or be created in the Agent's conf.d directory.
Datadog,60667516,60662893,0,"2020/03/13, 11:01:40",False,"2020/03/13, 11:01:40",336.0,7243426.0,1,"Otherwise, you'll find more options  here ."
Datadog,60667516,60662893,0,"2020/03/13, 11:01:40",False,"2020/03/13, 11:01:40",336.0,7243426.0,1,If you are looking to exclude the logs from the agent within the platform you can look at excluding the agent ( doc )
Datadog,60677166,60662893,0,"2020/03/13, 22:56:50",True,"2020/03/13, 22:56:50",787.0,1363715.0,2,Found the answer here:  https://github.com/DataDog/datadog-agent/issues/3329
Datadog,60677166,60662893,0,"2020/03/13, 22:56:50",True,"2020/03/13, 22:56:50",787.0,1363715.0,2,The field is  mount_point_blacklist
Datadog,61972253,61900949,0,"2020/05/23, 15:46:33",False,"2020/05/23, 15:46:33",493.0,244037.0,4,"In Datadog Logs, there's a difference between the Tags associated with the execution environment, and Attributes set on Log entry content."
Datadog,61972253,61900949,0,"2020/05/23, 15:46:33",False,"2020/05/23, 15:46:33",493.0,244037.0,4,From  this section in the docs :
Datadog,61972253,61900949,0,"2020/05/23, 15:46:33",False,"2020/05/23, 15:46:33",493.0,244037.0,4,Context  refers to the infrastructure and application context in which the log has been generated.
Datadog,61972253,61900949,0,"2020/05/23, 15:46:33",False,"2020/05/23, 15:46:33",493.0,244037.0,4,"Information is gathered from tags—whether automatically attached (host name, container name, log file name, serverless function name, etc."
Datadog,61972253,61900949,0,"2020/05/23, 15:46:33",False,"2020/05/23, 15:46:33",493.0,244037.0,4,")—or added through custom tags (team in charge, environment, application version, etc.)"
Datadog,61972253,61900949,0,"2020/05/23, 15:46:33",False,"2020/05/23, 15:46:33",493.0,244037.0,4,on the log by the Datadog Agent or Log Forwarder.
Datadog,61972253,61900949,0,"2020/05/23, 15:46:33",False,"2020/05/23, 15:46:33",493.0,244037.0,4,"And looking into the  source for the browser SDK , we can see:"
Datadog,61972253,61900949,0,"2020/05/23, 15:46:33",False,"2020/05/23, 15:46:33",493.0,244037.0,4,"This shows us that the  tags  query string parameter being submitted is being calculated based on configuration, and only provides a small amount of user-configurable entries, like  env ,  service  - these were released very recently in version 1.11.5 -  here's the change  introducing them."
Datadog,61972253,61900949,0,"2020/05/23, 15:46:33",False,"2020/05/23, 15:46:33",493.0,244037.0,4,"So you may not be able to set  tags  for a specific log entry - rather you can set  attributes  per log entry, like in the example you shared, which is setting  Attributes  for the logger instance as a whole."
Datadog,61972253,61900949,0,"2020/05/23, 15:46:33",False,"2020/05/23, 15:46:33",493.0,244037.0,4,Attributes are part of the log  Content  - which will be viewable in the body of the log entry.
Datadog,61972253,61900949,0,"2020/05/23, 15:46:33",False,"2020/05/23, 15:46:33",493.0,244037.0,4,"Yes, this is confusing since the function used is named  addContext / setContext  - and these don't set the same thing as the documentation's ""Context"" - rather they modify the Attributes that are associated with the log entry."
Datadog,61972253,61900949,0,"2020/05/23, 15:46:33",False,"2020/05/23, 15:46:33",493.0,244037.0,4,"In that event, you may want to have either custom logger instances that provide specific attributes for that logger, or add context inline to the log entry, like this:"
Datadog,61972253,61900949,0,"2020/05/23, 15:46:33",False,"2020/05/23, 15:46:33",493.0,244037.0,4,Here's the docs  on this approach which show what other default attributes are being set per log entry.
Datadog,31682034,29697464,0,"2015/07/28, 19:24:53",False,"2015/07/28, 19:24:53",455.0,4676932.0,1,"The Monitor section of Datadog now includes a ""Monitor status"" page for each of the monitor you define (for example the URL monitoring you already have)."
Datadog,31682034,29697464,0,"2015/07/28, 19:24:53",False,"2015/07/28, 19:24:53",455.0,4676932.0,1,"On this page, you can see by group/scope the monitor history and it shows you the uptime for that monitor."
Datadog,31682034,29697464,0,"2015/07/28, 19:24:53",False,"2015/07/28, 19:24:53",455.0,4676932.0,1,More to read about this new feature  here
Datadog,31682034,29697464,0,"2015/07/28, 19:24:53",False,"2015/07/28, 19:24:53",455.0,4676932.0,1,"It's not yet available as a ""report"" but that's a good idea!"
Datadog,29994613,29949571,0,"2015/05/01, 23:26:19",False,"2015/05/01, 23:26:19",455.0,4676932.0,2,The exception you are seeing is related to the IO system metrics collection and has nothing to do with your custom dogstream parser.
Datadog,29994613,29949571,0,"2015/05/01, 23:26:19",False,"2015/05/01, 23:26:19",455.0,4676932.0,2,If you look at the stack trace it says that it wasn't able to apply the  _parse_linux2  function.
Datadog,29994613,29949571,0,"2015/05/01, 23:26:19",False,"2015/05/01, 23:26:19",455.0,4676932.0,2,To troubleshoot that further you should take a look at the output of
Datadog,29994613,29949571,0,"2015/05/01, 23:26:19",False,"2015/05/01, 23:26:19",455.0,4676932.0,2,which is the command launched by the agent.
Datadog,29994613,29949571,0,"2015/05/01, 23:26:19",False,"2015/05/01, 23:26:19",455.0,4676932.0,2,Feel free to open a bug on the agent GitHub repository.
Datadog,29994613,29949571,0,"2015/05/01, 23:26:19",False,"2015/05/01, 23:26:19",455.0,4676932.0,2,References:
Datadog,34909299,34893203,2,"2016/01/20, 21:54:55",False,"2016/01/20, 21:54:55",261.0,4172512.0,4,Just a few items to note to get this working:
Datadog,34909299,34893203,2,"2016/01/20, 21:54:55",False,"2016/01/20, 21:54:55",261.0,4172512.0,4,"
dogstatsd = Statsd.new('MY_API_KEY')"
Datadog,34909299,34893203,2,"2016/01/20, 21:54:55",False,"2016/01/20, 21:54:55",261.0,4172512.0,4,"This line of code is trying to use your API key to establish a statsD connection, but this should actually be trying to establish the statsD connection via the statsD port currently configured on your Agent as seen here:"
Datadog,34909299,34893203,2,"2016/01/20, 21:54:55",False,"2016/01/20, 21:54:55",261.0,4172512.0,4,"
Create a stats instance."
Datadog,34909299,34893203,2,"2016/01/20, 21:54:55",False,"2016/01/20, 21:54:55",261.0,4172512.0,4,"statsd = Statsd.new('localhost', 8125)"
Datadog,34909299,34893203,2,"2016/01/20, 21:54:55",False,"2016/01/20, 21:54:55",261.0,4172512.0,4,"The easiest way to get your custom metrics into Datadog is to send them to DogStatsD, a metrics aggregation server bundled with the Datadog Agent (in versions 3.0 and above)."
Datadog,34909299,34893203,2,"2016/01/20, 21:54:55",False,"2016/01/20, 21:54:55",261.0,4172512.0,4,"DogStatsD implements the StatsD protocol, along with a few extensions for special Datadog features."
Datadog,34909299,34893203,2,"2016/01/20, 21:54:55",False,"2016/01/20, 21:54:55",261.0,4172512.0,4,http://docs.datadoghq.com/guides/dogstatsd/
Datadog,34909299,34893203,2,"2016/01/20, 21:54:55",False,"2016/01/20, 21:54:55",261.0,4172512.0,4,"If you would not like to deploy an Agent on the host running the RoR application, you can utilize DogAPI gem:"
Datadog,34909299,34893203,2,"2016/01/20, 21:54:55",False,"2016/01/20, 21:54:55",261.0,4172512.0,4,https://github.com/DataDog/dogapi-rb
Datadog,34909299,34893203,2,"2016/01/20, 21:54:55",False,"2016/01/20, 21:54:55",261.0,4172512.0,4,Which has additional documentation to get this custom metrics submitted:
Datadog,34909299,34893203,2,"2016/01/20, 21:54:55",False,"2016/01/20, 21:54:55",261.0,4172512.0,4,"If you have additional questions, please reach out to support@datadoghq.com"
Datadog,41144060,41142456,0,"2016/12/14, 15:38:50",True,"2016/12/14, 15:38:50",101.0,6197827.0,2,This Datadog blog should guide you on how to build a monitor.
Datadog,41144060,41142456,0,"2016/12/14, 15:38:50",True,"2016/12/14, 15:38:50",101.0,6197827.0,2,https://www.datadoghq.com/blog/monitoring-cassandra-with-datadog/
Datadog,41149100,41142456,0,"2016/12/14, 19:56:34",False,"2016/12/14, 19:56:34",41.0,7297928.0,4,There is indeed a  template Cassandra dashboard  in Datadog (where I work) that should appear as soon as you enable the integration.
Datadog,41149100,41142456,0,"2016/12/14, 19:56:34",False,"2016/12/14, 19:56:34",41.0,7297928.0,4,This dash has a mix of Cassandra-specific metrics (e.g.
Datadog,41149100,41142456,0,"2016/12/14, 19:56:34",False,"2016/12/14, 19:56:34",41.0,7297928.0,4,"cache hit rates), plus metrics from the host (e.g."
Datadog,41149100,41142456,0,"2016/12/14, 19:56:34",False,"2016/12/14, 19:56:34",41.0,7297928.0,4,CPU).
Datadog,41149100,41142456,0,"2016/12/14, 19:56:34",False,"2016/12/14, 19:56:34",41.0,7297928.0,4,"You can select a particular host or subset of hosts to make those host-level metrics more meaningful by  changing the scope of the dashboard , and the graphs will re-render on the fly."
Datadog,41149100,41142456,0,"2016/12/14, 19:56:34",False,"2016/12/14, 19:56:34",41.0,7297928.0,4,You can also clone and modify the dashboard as you wish by clicking the gear icon in the upper right.
Datadog,41149100,41142456,0,"2016/12/14, 19:56:34",False,"2016/12/14, 19:56:34",41.0,7297928.0,4,"This dash should provide a good starting point for monitoring Cassandra, but we have an even better template dashboard in the works."
Datadog,41149100,41142456,0,"2016/12/14, 19:56:34",False,"2016/12/14, 19:56:34",41.0,7297928.0,4,I'll update this answer as soon as it's released.
Datadog,41149100,41142456,0,"2016/12/14, 19:56:34",False,"2016/12/14, 19:56:34",41.0,7297928.0,4,"In the meantime, the blog post shared by John KVS should help you to identify key metrics that you might want to monitor."
Datadog,41995839,41992681,1,"2017/02/02, 08:48:29",True,"2017/02/02, 08:48:29",1981.0,2473382.0,4,Duplicate the definition of  event.sent  in  event.failed .
Datadog,41995839,41992681,1,"2017/02/02, 08:48:29",True,"2017/02/02, 08:48:29",1981.0,2473382.0,4,"As soon as you restart the agent, any  sent  event will be seen as  sent   and   failed ."
Datadog,41995839,41992681,1,"2017/02/02, 08:48:29",True,"2017/02/02, 08:48:29",1981.0,2473382.0,4,"After a minute or so you revert the definition of  failed  to the proper definition, but you now have a few (admittedly bogus) failed event in datadog, allowing you to use the metric."
Datadog,41995839,41992681,1,"2017/02/02, 08:48:29",True,"2017/02/02, 08:48:29",1981.0,2473382.0,4,"On your datadog dashboard, edit the relevant metric, and instead of using the graphical interface (tab  edit ) got to the  JSON  tab, where you can manually enter your metric name (probably a slightly updated cut &amp; paste of your  sent  metric), no matter if an event exists or not."
Datadog,43699066,43146756,0,"2017/04/29, 21:21:56",False,"2017/04/29, 21:21:56",1371.0,5540166.0,0,"Datadog monitors evaluate every minute, I think."
Datadog,43699066,43146756,0,"2017/04/29, 21:21:56",False,"2017/04/29, 21:21:56",1371.0,5540166.0,0,"So in your  sum(last_30m){X}  example, every minute, the monitor would sum the values of  {X}  over the last 30 minutes, and if that value was above whatever threshold you set, then it would trigger an alert."
Datadog,43699066,43146756,0,"2017/04/29, 21:21:56",False,"2017/04/29, 21:21:56",1371.0,5540166.0,0,"Same thing for  sum(last_1h){X} , but every minute it would evaluate the sum over the last 1 hour."
Datadog,43698915,43224591,0,"2017/04/29, 21:05:40",False,"2017/04/29, 21:05:40",1371.0,5540166.0,1,"The first step will be to make sure you have the  datadog agent running , and that the APM component of it is running and ready to receive trace data from your applications ( this option in your datadog.conf , which must be set to ""true"")."
Datadog,43698915,43224591,0,"2017/04/29, 21:05:40",False,"2017/04/29, 21:05:40",1371.0,5540166.0,1,"Second, you'll want to install the appropriate library(ies) for the languages your applications are written in."
Datadog,43698915,43224591,0,"2017/04/29, 21:05:40",False,"2017/04/29, 21:05:40",1371.0,5540166.0,1,You can find them all listed in your datadog account on this page:  https://app.datadoghq.com/apm/docs
Datadog,43698915,43224591,0,"2017/04/29, 21:05:40",False,"2017/04/29, 21:05:40",1371.0,5540166.0,1,"Third, once the trace libraries are installed, you'll want to add trace integrations for the tools you're interested in collecting APM data on, and the instructions for those will be found in each library's docs."
Datadog,43698915,43224591,0,"2017/04/29, 21:05:40",False,"2017/04/29, 21:05:40",1371.0,5540166.0,1,"(E.g,  Python ,  Ruby , and  Go )"
Datadog,43698915,43224591,0,"2017/04/29, 21:05:40",False,"2017/04/29, 21:05:40",1371.0,5540166.0,1,"The integraitons will be a fairly quick way to get pretty granular spans on where your applications have higher latency, errors, etc."
Datadog,43698915,43224591,0,"2017/04/29, 21:05:40",False,"2017/04/29, 21:05:40",1371.0,5540166.0,1,"If from there you'd like to go even further, each library's docs also have instructions on how you can write your own custom tracing functions to expose more info on your custom applications--that's a little more work, but is fairly straight-forward."
Datadog,43698915,43224591,0,"2017/04/29, 21:05:40",False,"2017/04/29, 21:05:40",1371.0,5540166.0,1,You'll probably want to add those bit-by-bit as you go.
Datadog,43698915,43224591,0,"2017/04/29, 21:05:40",False,"2017/04/29, 21:05:40",1371.0,5540166.0,1,"Then you'd be all set, I think."
Datadog,43698915,43224591,0,"2017/04/29, 21:05:40",False,"2017/04/29, 21:05:40",1371.0,5540166.0,1,"You'll be tracing services, resources to get the latency, request-count, and error-count of your application requests, and you can drill down to the flame-graphs to further understand what requests spend the most time where in your applications."
Datadog,43698915,43224591,0,"2017/04/29, 21:05:40",False,"2017/04/29, 21:05:40",1371.0,5540166.0,1,"Looking back now, seems like they made some recent changes to the setup process that makes it even easier to get the web framework and database integrations added if you're using Python."
Datadog,43698915,43224591,0,"2017/04/29, 21:05:40",False,"2017/04/29, 21:05:40",1371.0,5540166.0,1,They've even got a command line tool in their  get-started section now .
Datadog,43698915,43224591,0,"2017/04/29, 21:05:40",False,"2017/04/29, 21:05:40",1371.0,5540166.0,1,Hope this helps!
Datadog,43698915,43224591,0,"2017/04/29, 21:05:40",False,"2017/04/29, 21:05:40",1371.0,5540166.0,1,And reach out to their support team (support@datadoghq.com) if you run into issues along the way--they're always happy to lend a hand.
Datadog,51896300,47577288,2,"2018/08/17, 16:18:03",False,"2018/08/18, 20:06:12",11.0,7120456.0,0,For now I see only two possibilities:
Datadog,51896300,47577288,2,"2018/08/17, 16:18:03",False,"2018/08/18, 20:06:12",11.0,7120456.0,0,"Use GCP custom metrics
 https://cloud.google.com/monitoring/custom-metrics/creating-metrics 
and datadog integration with GCP
 https://www.datadoghq.com/product/integrations/#cat-google-cloud"
Datadog,51896300,47577288,2,"2018/08/17, 16:18:03",False,"2018/08/18, 20:06:12",11.0,7120456.0,0,"Use datadog statsd client, java one -
 https://github.com/DataDog/java-dogstatsd-client  so you can deploy
datadog agent on GCP and connect through it."
Datadog,51896300,47577288,2,"2018/08/17, 16:18:03",False,"2018/08/18, 20:06:12",11.0,7120456.0,0,Sample with kubernetes.
Datadog,51896300,47577288,2,"2018/08/17, 16:18:03",False,"2018/08/18, 20:06:12",11.0,7120456.0,0,https://docs.datadoghq.com/tracing/setup/kubernetes/#deploy-agent-daemonset
Datadog,51896300,47577288,2,"2018/08/17, 16:18:03",False,"2018/08/18, 20:06:12",11.0,7120456.0,0,datadog deployment.yaml for kubernetes
Datadog,51896300,47577288,2,"2018/08/17, 16:18:03",False,"2018/08/18, 20:06:12",11.0,7120456.0,0,"Currently I'm investigating this so I'm not sure how to do this, yet."
Datadog,49867829,49690040,0,"2018/04/17, 02:53:52",True,"2018/04/17, 02:53:52",356.0,2750290.0,1,It has support to CURL means you can make REST API calls.
Datadog,49867829,49690040,0,"2018/04/17, 02:53:52",True,"2018/04/17, 02:53:52",356.0,2750290.0,1,Try using some Http libraries like  HttpURLConnection  in java to make those POST requests.
Datadog,49867829,49690040,0,"2018/04/17, 02:53:52",True,"2018/04/17, 02:53:52",356.0,2750290.0,1,I don't think we need a java based SDK for that as you can frame your own SDK on top of REST api's.
Datadog,49706349,49699969,1,"2018/04/07, 13:26:10",False,"2018/04/07, 13:26:10",17842.0,7862821.0,0,There are two ways to access  dd-trace agent  on host from a container:
Datadog,49706349,49699969,1,"2018/04/07, 13:26:10",False,"2018/04/07, 13:26:10",17842.0,7862821.0,0,1.
Datadog,49706349,49699969,1,"2018/04/07, 13:26:10",False,"2018/04/07, 13:26:10",17842.0,7862821.0,0,"Only on  &lt;HOST_IP&gt;:8126 , if docker container is started in a bridge network:"
Datadog,49706349,49699969,1,"2018/04/07, 13:26:10",False,"2018/04/07, 13:26:10",17842.0,7862821.0,0,dd-trace agent  should be bound to  &lt;HOST_IP&gt;  or  0.0.0.0  (which includes  &lt;HOST_IP&gt; ).
Datadog,49706349,49699969,1,"2018/04/07, 13:26:10",False,"2018/04/07, 13:26:10",17842.0,7862821.0,0,2.
Datadog,49706349,49699969,1,"2018/04/07, 13:26:10",False,"2018/04/07, 13:26:10",17842.0,7862821.0,0,"On  &lt;HOST_IP&gt;:8126  (if  dd-trace agent  is bound to  &lt;HOST_IP&gt;  or  0.0.0.0 ) and  localhost:8126 , if docker container is started in the host network:"
Datadog,49706349,49699969,1,"2018/04/07, 13:26:10",False,"2018/04/07, 13:26:10",17842.0,7862821.0,0,"As you already try to reach  dd-trace agent  on  localhost:8126 , so the second way is the best solution."
Datadog,51125051,51124961,2,"2018/07/01, 19:25:56",False,"2018/07/02, 02:14:02",10159.0,4907630.0,2,First step is to install the DataDog agent on the server in which you are running your application:
Datadog,51125051,51124961,2,"2018/07/01, 19:25:56",False,"2018/07/02, 02:14:02",10159.0,4907630.0,2,https://docs.datadoghq.com/agent/
Datadog,51125051,51124961,2,"2018/07/01, 19:25:56",False,"2018/07/02, 02:14:02",10159.0,4907630.0,2,You then need to enable the  DogStatsD  service in the DataDog agent:
Datadog,51125051,51124961,2,"2018/07/01, 19:25:56",False,"2018/07/02, 02:14:02",10159.0,4907630.0,2,https://docs.datadoghq.com/developers/dogstatsd/
Datadog,51125051,51124961,2,"2018/07/01, 19:25:56",False,"2018/07/02, 02:14:02",10159.0,4907630.0,2,"After that, you can send metrics to the  statsd  agent using any Go library that connects to  statsd ."
Datadog,51125051,51124961,2,"2018/07/01, 19:25:56",False,"2018/07/02, 02:14:02",10159.0,4907630.0,2,For example:
Datadog,51125051,51124961,2,"2018/07/01, 19:25:56",False,"2018/07/02, 02:14:02",10159.0,4907630.0,2,https://github.com/DataDog/datadog-go
Datadog,51125051,51124961,2,"2018/07/01, 19:25:56",False,"2018/07/02, 02:14:02",10159.0,4907630.0,2,https://github.com/go-kit/kit/tree/master/metrics/statsd
Datadog,51125051,51124961,2,"2018/07/01, 19:25:56",False,"2018/07/02, 02:14:02",10159.0,4907630.0,2,Here's an example program sending some counts using the first library:
Datadog,51145348,51124961,0,"2018/07/03, 04:00:56",False,"2018/07/03, 04:00:56",3174.0,4639336.0,0,Here is a convenience wrapper for DD.
Datadog,51145348,51124961,0,"2018/07/03, 04:00:56",False,"2018/07/03, 04:00:56",3174.0,4639336.0,0,"It uses ENV vars to configure it, but it's a nice utility to package out common DD calls once you have the agent running in the background."
Datadog,52615071,51975736,0,"2018/10/02, 22:23:00",True,"2018/10/04, 04:20:38",899.0,4386440.0,1,I figured out how to do this using the datadog api  https://docs.datadoghq.com/api/?lang=python#post-timeseries-points .
Datadog,52615071,51975736,0,"2018/10/02, 22:23:00",True,"2018/10/04, 04:20:38",899.0,4386440.0,1,"The following python script takes in the jtl file (jmeter results) and posts the transaction name, response time, and status (pass/fail) to datadog."
Datadog,52828393,52828258,1,"2018/10/16, 08:20:06",False,"2018/10/16, 08:44:48",21.0,10510929.0,2,You can actually just use:
Datadog,52828393,52828258,1,"2018/10/16, 08:20:06",False,"2018/10/16, 08:44:48",21.0,10510929.0,2,Datadog's monitor templating feature will fill in the blank and forward to the relevant channel as long as you whitelisted it in the integrations tile for Slack.
Datadog,61282340,52828258,0,"2020/04/18, 02:32:29",False,"2020/04/18, 02:32:29",5547.0,296829.0,1,"Starts to get messy, but you could nest two ""does not"" conditional variables, like this:"
Datadog,60719420,54508636,0,"2020/03/17, 11:20:42",False,"2020/03/17, 11:20:42",23.0,6286219.0,0,AFAIK it is not possible at the Moment to use micrometer to send events to datadog.
Datadog,60719420,54508636,0,"2020/03/17, 11:20:42",False,"2020/03/17, 11:20:42",23.0,6286219.0,0,"Micrometer states that  ""Micrometer is not a distributed tracing system or an event logger. """
Datadog,60719420,54508636,0,"2020/03/17, 11:20:42",False,"2020/03/17, 11:20:42",23.0,6286219.0,0,"on its section ""1."
Datadog,60719420,54508636,0,"2020/03/17, 11:20:42",False,"2020/03/17, 11:20:42",23.0,6286219.0,0,"Purpose"" on its  concepts page ."
Datadog,55661412,55588976,0,"2019/04/13, 05:13:40",True,"2019/04/13, 05:13:40",1371.0,5540166.0,1,This doc will show you a comprehensive list  of all integrations that involve log collection.
Datadog,55661412,55588976,0,"2019/04/13, 05:13:40",True,"2019/04/13, 05:13:40",1371.0,5540166.0,1,"Some of these include other common log shippers, which can also be used to forward logs to Datadog."
Datadog,55661412,55588976,0,"2019/04/13, 05:13:40",True,"2019/04/13, 05:13:40",1371.0,5540166.0,1,Among these you'd find...
Datadog,55661412,55588976,0,"2019/04/13, 05:13:40",True,"2019/04/13, 05:13:40",1371.0,5540166.0,1,"That said, you  can still just use the Datadog agent to collect logs only  (they want you to collect everything with their agent, that's why they warn you against collecting just their logs)."
Datadog,55661412,55588976,0,"2019/04/13, 05:13:40",True,"2019/04/13, 05:13:40",1371.0,5540166.0,1,"If you want to collect logs from docker containers, the Datadog agent is an easy way to do that, and it has the benefit of adding lots of relevant docker-metadata as tags to your logs."
Datadog,55661412,55588976,0,"2019/04/13, 05:13:40",True,"2019/04/13, 05:13:40",1371.0,5540166.0,1,( Docker log collection instructions here .)
Datadog,55661412,55588976,0,"2019/04/13, 05:13:40",True,"2019/04/13, 05:13:40",1371.0,5540166.0,1,"If you don't want to do that, I'd look at Fluentd first on the list above -- it has a good reputation for containerized log collection, promotes JSON log formatting (for easier processing), and scales reasonably well."
Datadog,58421772,58414654,1,"2019/10/17, 00:11:59",True,"2019/10/17, 00:11:59",6375.0,1803990.0,2,You need to tell Datadog to pull custom metric.
Datadog,58421772,58414654,1,"2019/10/17, 00:11:59",True,"2019/10/17, 00:11:59",6375.0,1803990.0,2,Go to AWS integrations configuration page (Integrations side menu -  Integrations -  Amazon Web Services).
Datadog,58421772,58414654,1,"2019/10/17, 00:11:59",True,"2019/10/17, 00:11:59",6375.0,1803990.0,2,"You will see a list of services to integrate with, custom metrics is the last option on list."
Datadog,58421772,58414654,1,"2019/10/17, 00:11:59",True,"2019/10/17, 00:11:59",6375.0,1803990.0,2,Make sure it's ticked.
Datadog,58421772,58414654,1,"2019/10/17, 00:11:59",True,"2019/10/17, 00:11:59",6375.0,1803990.0,2,Takes a while for Datadog to actually start pulling the metric.
Datadog,58548229,58505214,1,"2019/10/24, 22:47:27",False,"2019/10/24, 22:47:27",336.0,7243426.0,1,Do you have the ability to add some parameters in the logs sent.
Datadog,58548229,58505214,1,"2019/10/24, 22:47:27",False,"2019/10/24, 22:47:27",336.0,7243426.0,1,From  the documentation  you should be able to inject the trace id into your logs in a way that Datadog will interpret them.
Datadog,58548229,58505214,1,"2019/10/24, 22:47:27",False,"2019/10/24, 22:47:27",336.0,7243426.0,1,You can also look at a parser to extract the trace id and span id from the raw log.
Datadog,58548229,58505214,1,"2019/10/24, 22:47:27",False,"2019/10/24, 22:47:27",336.0,7243426.0,1,This documentation  should help you out on that.
Datadog,61929834,58505214,1,"2020/05/21, 10:55:37",False,"2020/05/21, 10:55:37",8464.0,85348.0,1,"From the documentation, if you don't have JSON logs, you need to include  dd.trace_id  and  dd.span_id  in your formatter:"
Datadog,61929834,58505214,1,"2020/05/21, 10:55:37",False,"2020/05/21, 10:55:37",8464.0,85348.0,1,"If your logs are raw formatted, update your formatter to include
   dd.trace_id  and  dd.span_id  in your logger configuration:"
Datadog,61929834,58505214,1,"2020/05/21, 10:55:37",False,"2020/05/21, 10:55:37",8464.0,85348.0,1,"So if you add  %X{dd.trace_id:-0} %X{ dd.span_id:-0} , it should work."
Datadog,61783745,61769846,0,"2020/05/13, 22:59:10",True,"2020/05/13, 23:10:16",2172.0,3514300.0,1,"A better way would be to use a sidecar container with a logging agent, it won't increase the load on the API server."
Datadog,61783745,61769846,0,"2020/05/13, 22:59:10",True,"2020/05/13, 23:10:16",2172.0,3514300.0,1,Reference:  https://kubernetes.io/docs/concepts/cluster-administration/logging/#sidecar-container-with-a-logging-agent
Datadog,61783745,61769846,0,"2020/05/13, 22:59:10",True,"2020/05/13, 23:10:16",2172.0,3514300.0,1,Datadog agent looks like doesn't support /suggest running as a sidecar ( https://github.com/DataDog/datadog-agent/issues/2203#issuecomment-416180642 )
Datadog,61783745,61769846,0,"2020/05/13, 22:59:10",True,"2020/05/13, 23:10:16",2172.0,3514300.0,1,I suggest looking at using other logging agent and pointing the backend to datadog.
Datadog,61783745,61769846,0,"2020/05/13, 22:59:10",True,"2020/05/13, 23:10:16",2172.0,3514300.0,1,Some options are:
Datadog,61783745,61769846,0,"2020/05/13, 22:59:10",True,"2020/05/13, 23:10:16",2172.0,3514300.0,1,Datadog supports them
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,Is that sample text formatted properly?
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,The final entity object is missing a  ]  from the end.
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,"entity=[HttpEntity.Strict application/json {""type"":""text"",""extract"": ""text"", ""field2"":""text2"",""duration"": 451 }"
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,should be
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,"entity=[HttpEntity.Strict application/json {""type"":""text"",""extract"": ""text"", ""field2"":""text2"",""duration"": 451 }]"
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,I'm going to continue these instructions assuming that was a typo and the entity field actually ends with  ] .
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,"If it doesn't, I think you need to fix the underlying log to be formatted properly and close out the bracket."
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,"Instead of just skipping the entire log and only parsing out that json bit, I decided to parse the entire thing and show what would look good as a final result."
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,So the first thing we need to do is pull out that set of key/value pairs after the request object:
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,"Example Input:  thread-191555 app.main - [cid: 2cacd6f9-546d-41ew-a7ce-d5d41b39eb8f, uid: e6ffc3b0-2f39-44f7-85b6-1abf5f9ad970] Request: protocol=[HTTP/1.0] method=[POST] path=[/metrics] headers=[Timeout-Access: &lt;function1&gt;, Remote-Address: 192.168.0.1:37936, Host: app:5000, Connection: close, X-Real-Ip: 192.168.1.1, X-Forwarded-For: 192.168.1.1, Authorization: ***, Accept: application/json, text/plain, */*, Referer: https://google.com, Accept-Language: cs-CZ, Accept-Encoding: gzip, deflate, User-Agent: Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko, Cache-Control: no-cache] entity=[HttpEntity.Strict application/json {""type"":""text"",""extract"": ""text"", ""field2"":""text2"",""duration"": 451 }]"
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,"Grok parser rule:  app_log thread-%{integer:thread} %{notSpace:file} - \[%{data::keyvalue("": "")}\] Request: %{data:request:keyvalue(""="","""",""[]"")}"
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,Result:
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,"Notice how we use the keyvalue parser with a quoting string of  [] , that allows us to easily pull out everything from the request object."
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,Now the goal is to pull out the details from that entity field inside of the request object.
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,With Grok parsers you can specify a specific attribute to parse further.
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,"So in that same pipeline we'll add another grok parser processor, right after our first"
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,"And then configure the advanced options section to run on  request.entity , since that is what we called the attribute"
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,"Example Input:  HttpEntity.Strict application/json {""type"":""text"",""extract"": ""text"", ""field2"":""text2"",""duration"": 451 }"
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,Grok Parser Rule:  entity_rule %{notSpace:request.entity.class} %{notSpace:request.entity.media_type} %{data:request.entity.json:json}
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,Result:
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,Now when we look at the final parsed log it has everything we need broken out:
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,"Also just because it was really simple, I threw in a third grok processor for the headers chunk as well (the advanced settings are set to parse from  request.headers ):"
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,"Example Input:  Timeout-Access: &lt;function1&gt;, Remote-Address: 192.168.0.1:37936, Host: app:5000, Connection: close, X-Real-Ip: 192.168.1.1, X-Forwarded-For: 192.168.1.1, Authorization: ***, Accept: application/json, text/plain, */*, Referer: https://google.com, Accept-Language: cs-CZ, Accept-Encoding: gzip, deflate, User-Agent: Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko, Cache-Control: no-cache"
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,"Grok Parser Rule:  headers_rule %{data:request.headers:keyvalue("": "", ""/)(; :"")}"
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,Result:
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,The only tricky bit here is that I had to define a characterWhiteList of  /)(; : .
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,Mostly to handle all those special characters are in the  User-Agent  field.
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,References :
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,Just the documentation and some guess &amp; checking in my personal Datadog account.
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,https://docs.datadoghq.com/logs/processing/parsing/?tab=matcher#key-value-or-logfmt
Datadog,62551284,62549173,1,"2020/06/24, 11:56:43",False,"2020/06/24, 11:56:43",8138.0,2385808.0,2,"When installing Datadog in your K8s Cluster, you install a  Node Logging Agent  as a Daemonset with various volume mounts on the hosting nodes."
Datadog,62551284,62549173,1,"2020/06/24, 11:56:43",False,"2020/06/24, 11:56:43",8138.0,2385808.0,2,"Among other things, this gives Datadog access to the Pod logs at /var/log/pods and the container logs at /var/lib/docker/containers."
Datadog,62551284,62549173,1,"2020/06/24, 11:56:43",False,"2020/06/24, 11:56:43",8138.0,2385808.0,2,Kubernetes and the underlying Docker engine will only include output from stdout and stderror in those two locations (see  here  for more information).
Datadog,62551284,62549173,1,"2020/06/24, 11:56:43",False,"2020/06/24, 11:56:43",8138.0,2385808.0,2,"Everything that is written by containers to log files residing inside the containers, will be invisible to K8s, unless more configuration is applied to extract that data, e.g."
Datadog,62551284,62549173,1,"2020/06/24, 11:56:43",False,"2020/06/24, 11:56:43",8138.0,2385808.0,2,by applying the  side care container pattern .
Datadog,62551284,62549173,1,"2020/06/24, 11:56:43",False,"2020/06/24, 11:56:43",8138.0,2385808.0,2,"So, to get things working in your setup,  configure logback to log to stdout rather than /var/app/logs/myapp.log"
Datadog,62551284,62549173,1,"2020/06/24, 11:56:43",False,"2020/06/24, 11:56:43",8138.0,2385808.0,2,"Also, if you don't use APM there is no need to instrument your code with the datadog.jar and do all that tracing setup (setting up ports etc)."
Datadog,63974474,63650568,0,"2020/09/20, 03:18:54",False,"2020/09/20, 03:18:54",3852.0,1601506.0,2,You need to tell Datadog that you're interested in that content by creating a facet from the field.
Datadog,63974474,63650568,0,"2020/09/20, 03:18:54",False,"2020/09/20, 03:18:54",3852.0,1601506.0,2,"Click a log message, mouse over the attribute name, click the gear on the left, then Create facet for @..."
Datadog,63974474,63650568,0,"2020/09/20, 03:18:54",False,"2020/09/20, 03:18:54",3852.0,1601506.0,2,"For logs indexed after you create the facet, you can search with  @fieldName:text* , where  fieldName  is the name of your field."
Datadog,63974474,63650568,0,"2020/09/20, 03:18:54",False,"2020/09/20, 03:18:54",3852.0,1601506.0,2,You'll need to re-hydrate (reprocess) earlier logs to make them searchable.
Datadog,63974474,63650568,0,"2020/09/20, 03:18:54",False,"2020/09/20, 03:18:54",3852.0,1601506.0,2,You won't need to create a facet if you use fields from the  standard attributes list .
Datadog,66879609,63650568,0,"2021/03/31, 02:09:46",False,"2021/04/01, 21:13:00",11.0,6062224.0,1,The error message itself is not a good fit to be defined as a facet.
Datadog,66879609,63650568,0,"2021/03/31, 02:09:46",False,"2021/04/01, 21:13:00",11.0,6062224.0,1,If you are using JSON and want the main message (say from a  msg  json field) to be searchable in the Datadog  content  field.
Datadog,66879609,63650568,0,"2021/03/31, 02:09:46",False,"2021/04/01, 21:13:00",11.0,6062224.0,1,"Instead of making
facet for  msg , you can define a &quot;Message Remapper&quot; in the log configuration to map it to the  Content ."
Datadog,66879609,63650568,0,"2021/03/31, 02:09:46",False,"2021/04/01, 21:13:00",11.0,6062224.0,1,And then you can do wildcard searches.
Datadog,66879609,63650568,0,"2021/03/31, 02:09:46",False,"2021/04/01, 21:13:00",11.0,6062224.0,1,log config screenshot
Datadog,66307900,64126426,0,"2021/02/22, 00:39:04",False,"2021/02/22, 00:44:46",1272.0,1563480.0,0,"After reading this documentation,  https://docs.datadoghq.com/tracing/setup_overview/setup/ruby/#resque , did you try making the options a hash with curly braces surrounding?"
Datadog,66307900,64126426,0,"2021/02/22, 00:39:04",False,"2021/02/22, 00:44:46",1272.0,1563480.0,0,Options is specified as being a hash.
Datadog,66307900,64126426,0,"2021/02/22, 00:39:04",False,"2021/02/22, 00:44:46",1272.0,1563480.0,0,"So everything after  c.use :resque,   should be a  hash ."
Datadog,37795216,37188932,0,"2016/06/13, 19:53:12",False,"2016/06/13, 19:53:12",261.0,4172512.0,2,You can use the Agent's info command to see if the check is reporting correctly:
Datadog,37795216,37188932,0,"2016/06/13, 19:53:12",False,"2016/06/13, 19:53:12",261.0,4172512.0,2,https://help.datadoghq.com/hc/en-us/articles/203764635
Datadog,37795216,37188932,0,"2016/06/13, 19:53:12",False,"2016/06/13, 19:53:12",261.0,4172512.0,2,If the Agent Status shows the check is reporting correctly but your metrics are still not reporting you can contact support@datadoghq.com and they can troubleshoot this issue further.
Datadog,37795216,37188932,0,"2016/06/13, 19:53:12",False,"2016/06/13, 19:53:12",261.0,4172512.0,2,Hope this helps!
Datadog,38752963,38746266,1,"2016/08/03, 23:23:06",False,"2016/08/03, 23:23:06",15334.0,3348604.0,1,This is not correct graph to detect correct resource limit.
Datadog,38752963,38746266,1,"2016/08/03, 23:23:06",False,"2016/08/03, 23:23:06",15334.0,3348604.0,1,"You graph shows CPU usage of your app in the cluster, but resource limit is per pod (container)."
Datadog,38752963,38746266,1,"2016/08/03, 23:23:06",False,"2016/08/03, 23:23:06",15334.0,3348604.0,1,We (and you as well) don't know from the graph how many containers were up and running.
Datadog,38752963,38746266,1,"2016/08/03, 23:23:06",False,"2016/08/03, 23:23:06",15334.0,3348604.0,1,You can determinate right CPU limit from the container CPU usage graph(s).
Datadog,38752963,38746266,1,"2016/08/03, 23:23:06",False,"2016/08/03, 23:23:06",15334.0,3348604.0,1,You will need Datadog-Docker integration:
Datadog,38752963,38746266,1,"2016/08/03, 23:23:06",False,"2016/08/03, 23:23:06",15334.0,3348604.0,1,"Please be aware that Kubernetes relies on Heapster to report metrics,
  rather than the cgroup file directly."
Datadog,38752963,38746266,1,"2016/08/03, 23:23:06",False,"2016/08/03, 23:23:06",15334.0,3348604.0,1,"The collection interval for
  Heapster is unknown which can lead to innacurate time-related data,
  such as CPU usage."
Datadog,38752963,38746266,1,"2016/08/03, 23:23:06",False,"2016/08/03, 23:23:06",15334.0,3348604.0,1,"If you require more precise metrics, we recommend
  using the Datadog-Docker Integration."
Datadog,38752963,38746266,1,"2016/08/03, 23:23:06",False,"2016/08/03, 23:23:06",15334.0,3348604.0,1,Then it will depends how Datadog measure CPU utilization per container.
Datadog,38752963,38746266,1,"2016/08/03, 23:23:06",False,"2016/08/03, 23:23:06",15334.0,3348604.0,1,"If container CPU utilization has max 100%, then 100% CPU container utilization ~ 1000m ~ 1."
Datadog,38752963,38746266,1,"2016/08/03, 23:23:06",False,"2016/08/03, 23:23:06",15334.0,3348604.0,1,I recommend you to read how and when cgroup limits CPU -  https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Resource_Management_Guide/sec-cpu.html
Datadog,38752963,38746266,1,"2016/08/03, 23:23:06",False,"2016/08/03, 23:23:06",15334.0,3348604.0,1,You will need a deep knowledge to set proper CPU limits.
Datadog,38752963,38746266,1,"2016/08/03, 23:23:06",False,"2016/08/03, 23:23:06",15334.0,3348604.0,1,"If you don't need to prioritize any container, then IMHO the best practice is to set 1 ( resources.requests.cpu ) for all your containers - they will have always equal CPU times."
Datadog,39751062,39750699,0,"2016/09/28, 17:55:57",False,"2016/09/28, 18:05:43",4213.0,5802417.0,-1,"I am not familiar with the products and libraries that you are using, but there is an open source library  MgntUtils  that can extract full or filtered stacktrace from exception as a String."
Datadog,39751062,39750699,0,"2016/09/28, 17:55:57",False,"2016/09/28, 18:05:43",4213.0,5802417.0,-1,Since you mentioned that you can pass the text (i.e.
Datadog,39751062,39750699,0,"2016/09/28, 17:55:57",False,"2016/09/28, 18:05:43",4213.0,5802417.0,-1,String) this library may help you.
Datadog,39751062,39750699,0,"2016/09/28, 17:55:57",False,"2016/09/28, 18:05:43",4213.0,5802417.0,-1,Here are the links to  MgntUtils  library:
Datadog,39751062,39750699,0,"2016/09/28, 17:55:57",False,"2016/09/28, 18:05:43",4213.0,5802417.0,-1,I hope this helps
Datadog,41968804,41964906,0,"2017/02/01, 00:39:07",True,"2017/02/01, 00:39:07",26.0,4313641.0,1,"You need to pass  Content-Type  as a header with the request, as shown  in the docs"
Datadog,41968804,41964906,0,"2017/02/01, 00:39:07",True,"2017/02/01, 00:39:07",26.0,4313641.0,1,Response:
Datadog,41968804,41964906,0,"2017/02/01, 00:39:07",True,"2017/02/01, 00:39:07",26.0,4313641.0,1,"Your data is also not formatted according to the docs (there should be no  dash  field at the top level, for starters)."
Datadog,42839492,42815252,1,"2017/03/16, 18:18:52",True,"2017/03/16, 18:35:39",455.0,4676932.0,3,The format you're using to send the data does not comply with the  documentation  and your call fails to complete.
Datadog,42839492,42815252,1,"2017/03/16, 18:18:52",True,"2017/03/16, 18:35:39",455.0,4676932.0,3,The call would work if you change your  $data  to:
Datadog,42839492,42815252,1,"2017/03/16, 18:18:52",True,"2017/03/16, 18:35:39",455.0,4676932.0,3,"Agreed that the error returned by the API is not really helpful, filed an issue in Datadog to correct that  behavior and return the proper error code and not a 500 (it's actually a 500 and you can see it by printing  curl_getinfo($ch)  after you executed your curl session)."
Datadog,44193846,44155361,6,"2017/05/26, 07:50:45",True,"2017/05/26, 07:50:45",1371.0,5540166.0,2,Why not use dogstatsd instead of threadstats?
Datadog,44193846,44155361,6,"2017/05/26, 07:50:45",True,"2017/05/26, 07:50:45",1371.0,5540166.0,2,"If you're already running the dd-agent on your node in a way that's reachable by your containers, you can use the  datadog.statsd.increment()  method instead to send the metric over statsd to the agent, and from there it would get forwarded to your datadog account."
Datadog,44193846,44155361,6,"2017/05/26, 07:50:45",True,"2017/05/26, 07:50:45",1371.0,5540166.0,2,"Dogstatsd has the benefits of being more straight-forward and of being somewhat easier to troublehsoot, at least with debug-level logging."
Datadog,44193846,44155361,6,"2017/05/26, 07:50:45",True,"2017/05/26, 07:50:45",1371.0,5540166.0,2,"Threadstats sometimes has the benefit of not requiring a dd-agent, but it does very little (if any) error logging, so makes it difficult to troubleshoot cases like these."
Datadog,44193846,44155361,6,"2017/05/26, 07:50:45",True,"2017/05/26, 07:50:45",1371.0,5540166.0,2,"If you went the dogstatsd route, you'd use the following code:"
Datadog,44193846,44155361,6,"2017/05/26, 07:50:45",True,"2017/05/26, 07:50:45",1371.0,5540166.0,2,"And from there you'd find your metric metadata with the ""rate"" type and with an interval of ""10"", and you could use the ""as_count"" function to translate the values to counts."
Datadog,44406163,44155361,1,"2017/06/07, 10:26:03",False,"2017/06/07, 10:26:03",2644.0,4279006.0,0,In the python script I was initializing with an api key:
Datadog,44406163,44155361,1,"2017/06/07, 10:26:03",False,"2017/06/07, 10:26:03",2644.0,4279006.0,0,And sending some events
Datadog,44406163,44155361,1,"2017/06/07, 10:26:03",False,"2017/06/07, 10:26:03",2644.0,4279006.0,0,"When I changed it to initialize like this, it started working with the dd-agent:"
Datadog,44406163,44155361,1,"2017/06/07, 10:26:03",False,"2017/06/07, 10:26:03",2644.0,4279006.0,0,I didn't need the link command (--link dogstatsd:dogstastd).
Datadog,44406163,44155361,1,"2017/06/07, 10:26:03",False,"2017/06/07, 10:26:03",2644.0,4279006.0,0,"With that setup it now works in the staging environment, but not in production."
Datadog,44406163,44155361,1,"2017/06/07, 10:26:03",False,"2017/06/07, 10:26:03",2644.0,4279006.0,0,:/
Datadog,45014280,44996259,0,"2017/07/10, 17:09:28",False,"2017/07/10, 17:09:28",1371.0,5540166.0,0,Maybe the  Datadog events-post api endpoint ?
Datadog,45014280,44996259,0,"2017/07/10, 17:09:28",False,"2017/07/10, 17:09:28",1371.0,5540166.0,0,Or their  metrics-post endpoint ?
Datadog,45014280,44996259,0,"2017/07/10, 17:09:28",False,"2017/07/10, 17:09:28",1371.0,5540166.0,0,"Although, it's not altogether clear to me what is meant by ""send this record only to Datadog""."
Datadog,45014280,44996259,0,"2017/07/10, 17:09:28",False,"2017/07/10, 17:09:28",1371.0,5540166.0,0,"If you use the log setup for Lambda log parsing that is described in  Datadog's AWS Lambda integration tile , that should be another perfectly good way to get the data collected into Datadog, but I think it may only collect those logs as metrics instead of events."
Datadog,46122641,46122135,0,"2017/09/08, 21:34:13",True,"2017/09/08, 22:02:50",1269.0,3294286.0,1,I found the answer thanks to  @tqr_aupa_atleti  and the support team from Datadog.
Datadog,46122641,46122135,0,"2017/09/08, 21:34:13",True,"2017/09/08, 22:02:50",1269.0,3294286.0,1,"On the Datadog dashboard panel, I had to click Metrics -  Summary and look for my metric."
Datadog,46122641,46122135,0,"2017/09/08, 21:34:13",True,"2017/09/08, 22:02:50",1269.0,3294286.0,1,I looked at the tags and I could figure out it was a custom metric form my company that uses data from Amplitude.
Datadog,49818303,46300932,0,"2018/04/13, 16:34:59",True,"2018/04/13, 16:56:36",939.0,2506172.0,2,"However if you look at this metric carefully it appears to be
  calculating these percentiles on a short range (not sure what) and for
  each tuple of the tags that exist."
Datadog,49818303,46300932,0,"2018/04/13, 16:34:59",True,"2018/04/13, 16:56:36",939.0,2506172.0,2,The short range that you have noticed is actually the flush interval which defaults to 10 seconds.
Datadog,49818303,46300932,0,"2018/04/13, 16:34:59",True,"2018/04/13, 16:56:36",939.0,2506172.0,2,"As per  this  article on histogram metric by datadog,"
Datadog,49818303,46300932,0,"2018/04/13, 16:34:59",True,"2018/04/13, 16:56:36",939.0,2506172.0,2,"It aggregates the values that are sent during the flush interval
  (usually defaults to 10 seconds)."
Datadog,49818303,46300932,0,"2018/04/13, 16:34:59",True,"2018/04/13, 16:56:36",939.0,2506172.0,2,"So if you send 20 values for a
  metric during the flush interval, it'll give you the aggregation of
  those values for the flush interval"
Datadog,49818303,46300932,0,"2018/04/13, 16:34:59",True,"2018/04/13, 16:56:36",939.0,2506172.0,2,For your query -
Datadog,49818303,46300932,0,"2018/04/13, 16:34:59",True,"2018/04/13, 16:56:36",939.0,2506172.0,2,"Ideally what I'd like to get is a 95th percentile of the ResponseTime
  metric over all the tags (maybe I filter it down by 1 or 2 and have a
  couple of different graphs) but over the last week or so."
Datadog,49818303,46300932,0,"2018/04/13, 16:34:59",True,"2018/04/13, 16:56:36",939.0,2506172.0,2,"Is there an
  easy way to do this?"
Datadog,49818303,46300932,0,"2018/04/13, 16:34:59",True,"2018/04/13, 16:56:36",939.0,2506172.0,2,"as per my reading of the datadog docs, there isn't a way to get this done at the moment."
Datadog,49818303,46300932,0,"2018/04/13, 16:34:59",True,"2018/04/13, 16:56:36",939.0,2506172.0,2,It might be a good idea to check with datadog support regarding this.
Datadog,49818303,46300932,0,"2018/04/13, 16:34:59",True,"2018/04/13, 16:56:36",939.0,2506172.0,2,More details  here .
Datadog,46827281,46826873,0,"2017/10/19, 12:56:44",True,"2017/10/19, 14:14:51",1371.0,5540166.0,3,"I think what you want is to add the  &quot;exact_match: false&quot;  option, like so:"
Datadog,46827281,46826873,0,"2017/10/19, 12:56:44",True,"2017/10/19, 14:14:51",1371.0,5540166.0,3,This should match on any process whose path+name  include  the search string you provide.
Datadog,46827281,46826873,0,"2017/10/19, 12:56:44",True,"2017/10/19, 14:14:51",1371.0,5540166.0,3,"Alternatively, if you only want it to match on the name of the process, you'll want to set the search_string to be the exact name of the process that's running (so whatever is given as the name when you run a  ps | grep &quot;ecommerce-order&quot; , which in your case seems to be  ecommerce-order-0.0.1-SNAPSHOT.jar )"
Datadog,47394149,47142664,0,"2017/11/20, 16:17:28",True,"2017/11/20, 16:17:28",1629.0,1970882.0,1,I am on the same boat.
Datadog,47394149,47142664,0,"2017/11/20, 16:17:28",True,"2017/11/20, 16:17:28",1629.0,1970882.0,1,I found this link:  datadog instrumentation .
Datadog,47394149,47142664,0,"2017/11/20, 16:17:28",True,"2017/11/20, 16:17:28",1629.0,1970882.0,1,"So, currently (20.11.2017) there are not agents for C#."
Datadog,47394149,47142664,0,"2017/11/20, 16:17:28",True,"2017/11/20, 16:17:28",1629.0,1970882.0,1,"Only Go, python and ruby are available."
Datadog,54639737,47142664,0,"2019/02/11, 23:52:44",False,"2019/03/01, 18:33:14",16566.0,24231.0,0,(Disclosure: I'm a software developer at Datadog.)
Datadog,54639737,47142664,0,"2019/02/11, 23:52:44",False,"2019/03/01, 18:33:14",16566.0,24231.0,0,Datadog's open-source .NET Tracer is currently (2019-02-11) in open beta.
Datadog,54639737,47142664,0,"2019/02/11, 23:52:44",False,"2019/03/01, 18:33:14",16566.0,24231.0,0,It supports manual and automatic instrumentation and  OpenTracing .
Datadog,54639737,47142664,0,"2019/02/11, 23:52:44",False,"2019/03/01, 18:33:14",16566.0,24231.0,0,"For a list of currently supported languages/frameworks, see the  updated documentation ."
Datadog,54639737,47142664,0,"2019/02/11, 23:52:44",False,"2019/03/01, 18:33:14",16566.0,24231.0,0,Happy tracing!
Datadog,47769427,47739117,1,"2017/12/12, 11:41:56",True,"2017/12/12, 11:41:56",2084.0,5156990.0,2,Looks like I found the problem -  https://github.com/DataDog/jenkins-datadog-plugin/issues/101
Datadog,47769427,47739117,1,"2017/12/12, 11:41:56",True,"2017/12/12, 11:41:56",2084.0,5156990.0,2,"current Datadog version 0.6.1. has a bug , after change the Jenkins main config ( any change , not related to Datadog configuration) it stop works."
Datadog,47769427,47739117,1,"2017/12/12, 11:41:56",True,"2017/12/12, 11:41:56",2084.0,5156990.0,2,I downgrade it to 0.5.7 and it works OK
Datadog,50768198,50768038,0,"2018/06/08, 23:48:11",False,"2018/06/08, 23:48:11",1371.0,5540166.0,0,"Not possible today, but that is in Datadog's plans for development."
Datadog,50768198,50768038,0,"2018/06/08, 23:48:11",False,"2018/06/08, 23:48:11",1371.0,5540166.0,0,"What you can do as a workaround, though, is add a link to your logs explorer with the query that triggered the monitor alert, so you can get a quick reference to what were the logs that triggered it."
Datadog,50768198,50768038,0,"2018/06/08, 23:48:11",False,"2018/06/08, 23:48:11",1371.0,5540166.0,0,"This link, for example, would quickly scope you to the error logs over the last 15 minutes:  https://app.datadoghq.com/logs?live=true&amp;query=status%3Aerror&amp;stream_sort=desc"
Datadog,50768198,50768038,0,"2018/06/08, 23:48:11",False,"2018/06/08, 23:48:11",1371.0,5540166.0,0,"And markdown is supported, so you can keep your monitor messages prettier without long links in the messages."
Datadog,50768198,50768038,0,"2018/06/08, 23:48:11",False,"2018/06/08, 23:48:11",1371.0,5540166.0,0,"Like so:
 [Check the error logs here](https://app.datadoghq.com/logs?live=true&amp;query=status%3Aerror&amp;stream_sort=desc)"
Datadog,51045835,51033962,0,"2018/06/26, 17:52:41",False,"2018/06/26, 17:52:41",23418.0,4379151.0,0,It looks to me that these are Datadog specific configuration parameters.
Datadog,51045835,51033962,0,"2018/06/26, 17:52:41",False,"2018/06/26, 17:52:41",23418.0,4379151.0,0,"So you first need to install the Datadog app to your Slack workspace, which you find on the Slack App Directory."
Datadog,51045835,51033962,0,"2018/06/26, 17:52:41",False,"2018/06/26, 17:52:41",23418.0,4379151.0,0,Here is how the process is described in the official documentation:
Datadog,51045835,51033962,0,"2018/06/26, 17:52:41",False,"2018/06/26, 17:52:41",23418.0,4379151.0,0,Installation
Datadog,51045835,51033962,0,"2018/06/26, 17:52:41",False,"2018/06/26, 17:52:41",23418.0,4379151.0,0,"The Slack integration is installed via its integration tile in the
  Datadog application."
Datadog,51045835,51033962,0,"2018/06/26, 17:52:41",False,"2018/06/26, 17:52:41",23418.0,4379151.0,0,Configuration
Datadog,51045835,51033962,0,"2018/06/26, 17:52:41",False,"2018/06/26, 17:52:41",23418.0,4379151.0,0,Source:  official documentation  from Datadog
Datadog,53290808,52891518,0,"2018/11/14, 01:03:15",False,"2018/11/14, 01:03:15",51.0,6111276.0,1,This might be a problem that  other people are running into too .
Datadog,53290808,52891518,0,"2018/11/14, 01:03:15",False,"2018/11/14, 01:03:15",51.0,6111276.0,1,"kubelet is no longer listening on the ReadOnlyPort in newer Kubernetes versions, and the port is being deprecated."
Datadog,53290808,52891518,0,"2018/11/14, 01:03:15",False,"2018/11/14, 01:03:15",51.0,6111276.0,1,Samuel Cormier-Iijima reports that the issue can be solved by adding adding  KUBELET_EXTRA_ARGS=--read-only-port=10255  in  /etc/default/kubelet  on the node host.
Datadog,53029359,52989371,0,"2018/10/28, 09:39:29",False,"2018/10/28, 09:39:29",113.0,4742614.0,1,It is in fact possible to send an alert if a metric shows the same value for a fix period of time.
Datadog,53029359,52989371,0,"2018/10/28, 09:39:29",False,"2018/10/28, 09:39:29",113.0,4742614.0,1,You can do this by using the diff() function to your query to produce delta values from consecutive delta points and then apply the abs() function to take absolute values of these deltas.
Datadog,53029359,52989371,0,"2018/10/28, 09:39:29",False,"2018/10/28, 09:39:29",113.0,4742614.0,1,"To do this we use the Arithmetic functions available, which can be applied using the '+' button to your query in UI."
Datadog,53029359,52989371,0,"2018/10/28, 09:39:29",False,"2018/10/28, 09:39:29",113.0,4742614.0,1,"For alert conditions in the metric monitor itself, configure as follows:
Select threshold alert
Set the “Trigger when the metric is…” dropdown selector to below or equal to
Set the “Alert Threshold” field to 0 (zero)"
Datadog,53029359,52989371,0,"2018/10/28, 09:39:29",False,"2018/10/28, 09:39:29",113.0,4742614.0,1,This configuration will trigger an alert event when no change in value has been registered over the selected timeframe.
Datadog,53029359,52989371,0,"2018/10/28, 09:39:29",False,"2018/10/28, 09:39:29",113.0,4742614.0,1,Here is a link to datadog article:  https://docs.datadoghq.com/monitors/faq/how-can-i-configure-a-metric-monitor-to-alert-on-no-change-in-value/
Datadog,54813734,54811591,0,"2019/02/21, 20:18:42",False,"2019/02/21, 20:18:42",790.0,1958107.0,1,By introducing space in datadog.json.j2 template definition .i.e.
Datadog,54813734,54811591,0,"2019/02/21, 20:18:42",False,"2019/02/21, 20:18:42",790.0,1958107.0,1,and running deployment again I got the working config as below
Datadog,54813734,54811591,0,"2019/02/21, 20:18:42",False,"2019/02/21, 20:18:42",790.0,1958107.0,1,However I am not able to understand the behaviour if anyone could help me understand it
Datadog,55152101,55087322,0,"2019/03/14, 00:27:39",False,"2019/03/14, 00:27:39",1371.0,5540166.0,0,it appears to me like you're missing a couple environment vars in your docker-compose  datadog  service configuration.
Datadog,55152101,55087322,0,"2019/03/14, 00:27:39",False,"2019/03/14, 00:27:39",1371.0,5540166.0,0,And also the volume that adds the registry for tailing the logs from the docker socket.
Datadog,55152101,55087322,0,"2019/03/14, 00:27:39",False,"2019/03/14, 00:27:39",1371.0,5540166.0,0,Maybe try something like this if you haven't?
Datadog,55152101,55087322,0,"2019/03/14, 00:27:39",False,"2019/03/14, 00:27:39",1371.0,5540166.0,0,"from there, if you still end up with trouble, you may want to reach out to support@datadoghq.com to ask for help."
Datadog,55152101,55087322,0,"2019/03/14, 00:27:39",False,"2019/03/14, 00:27:39",1371.0,5540166.0,0,They're pretty quick to reply.
Datadog,55806500,55632833,0,"2019/04/23, 10:46:35",True,"2019/04/23, 10:46:35",410.0,5254815.0,0,"Installed it as ""Run with Admin rights"" and it fixed the issue."
Datadog,57805506,55632833,0,"2019/09/05, 15:28:28",False,"2019/09/05, 15:28:28",971.0,3271599.0,0,"For me, I had to manually give the  ddagentuser  account read access to the file"
Datadog,57805506,55632833,0,"2019/09/05, 15:28:28",False,"2019/09/05, 15:28:28",971.0,3271599.0,0,C:\ProgramData\Datadog\auth_token
Datadog,59403759,58599000,0,"2019/12/19, 07:30:11",False,"2019/12/19, 09:15:25",1.0,12559838.0,0,Have you tried specifying dependencies with  go mod  yet?
Datadog,59403759,58599000,0,"2019/12/19, 07:30:11",False,"2019/12/19, 09:15:25",1.0,12559838.0,0,"I faced the same issue and finally can solved by generating  go.mod  file with these command,"
Datadog,59403759,58599000,0,"2019/12/19, 07:30:11",False,"2019/12/19, 09:15:25",1.0,12559838.0,0,Here  is the details explanation.
Datadog,61108577,61091978,0,"2020/04/08, 22:31:05",False,"2020/04/08, 22:31:05",61.0,6722990.0,2,"I think the disconnect here is that the pattern needs to be in the Jboss log manager, and then they're encoded to JSON."
Datadog,61108577,61091978,0,"2020/04/08, 22:31:05",False,"2020/04/08, 22:31:05",61.0,6722990.0,2,Have you tried puttinng  %X{dd.trace_id:-0} %X{dd.span_id:-0}  into your Jboss logging pattern?
Datadog,61108577,61091978,0,"2020/04/08, 22:31:05",False,"2020/04/08, 22:31:05",61.0,6722990.0,2,"If not, I'd also recommend opening a ticket at support@datadoghq.com and we can work this through with you."
Datadog,66962237,61092487,0,"2021/04/06, 06:12:46",False,"2021/04/06, 06:12:46",733.0,1219379.0,0,"I’m not sure if it is a recent addition, but the Datadog public API supports configuring Log Archives:  https://docs.datadoghq.com/api/latest/logs-archives/"
Datadog,66962237,61092487,0,"2021/04/06, 06:12:46",False,"2021/04/06, 06:12:46",733.0,1219379.0,0,You can also use tools like Terraform to configure them:  https://registry.terraform.io/providers/DataDog/datadog/latest/docs/resources/logs_archive  (it uses the Datadog API internally)
Datadog,61247522,61148607,1,"2020/04/16, 13:00:50",True,"2020/04/16, 13:00:50",408.0,3754710.0,1,"If you are trying to connect to an HTTPS URL for Datadog ( https://app.datadoghq.com  in your example), then you will need to set the  https.proxyHost  system property for it to have effect -  http.proxyHost  is for HTTP URLs[1]."
Datadog,61247522,61148607,1,"2020/04/16, 13:00:50",True,"2020/04/16, 13:00:50",408.0,3754710.0,1,These are system-wide settings that will be used by the default  HttpSender  ( HttpUrlConnectionSender ) if a  Proxy  is not passed to its constructor.
Datadog,61247522,61148607,1,"2020/04/16, 13:00:50",True,"2020/04/16, 13:00:50",408.0,3754710.0,1,The micrometer doc says
Datadog,61247522,61148607,1,"2020/04/16, 13:00:50",True,"2020/04/16, 13:00:50",408.0,3754710.0,1,But I dont understand what it means?
Datadog,61247522,61148607,1,"2020/04/16, 13:00:50",True,"2020/04/16, 13:00:50",408.0,3754710.0,1,"should I replace this url with my
  proxy url or is there any specific uri pattern with the proxy?"
Datadog,61247522,61148607,1,"2020/04/16, 13:00:50",True,"2020/04/16, 13:00:50",408.0,3754710.0,1,"This is referring to a different kind of proxy that you would configure to receive your Datadog traffic on your internal network, and it would forward it to Datadog outside of your network."
Datadog,61247522,61148607,1,"2020/04/16, 13:00:50",True,"2020/04/16, 13:00:50",408.0,3754710.0,1,If you are using an HTTP proxy then you should use the system properties or an  HttpSender  with your HTTP proxy configured (e.g.
Datadog,61247522,61148607,1,"2020/04/16, 13:00:50",True,"2020/04/16, 13:00:50",408.0,3754710.0,1,an  HttpUrlConnectionSender  and passing a  Proxy  to its constructor).
Datadog,61247522,61148607,1,"2020/04/16, 13:00:50",True,"2020/04/16, 13:00:50",408.0,3754710.0,1,You can configure a custom  HttpSender  with a  DatadogMeterRegistry  using its  Builder .
Datadog,61247522,61148607,1,"2020/04/16, 13:00:50",True,"2020/04/16, 13:00:50",408.0,3754710.0,1,"If you expose this as a  Bean  in a  @Configuration  class, Spring Boot will use it in its auto-configuration."
Datadog,61247522,61148607,1,"2020/04/16, 13:00:50",True,"2020/04/16, 13:00:50",408.0,3754710.0,1,For example:
Datadog,61247522,61148607,1,"2020/04/16, 13:00:50",True,"2020/04/16, 13:00:50",408.0,3754710.0,1,[1]  https://docs.oracle.com/javase/8/docs/technotes/guides/net/proxies.html
Datadog,62798229,62787931,3,"2020/07/08, 18:23:57",True,"2020/07/08, 18:23:57",1371.0,5540166.0,2,That's all handled in the  message  attribute with  conditional logic variables .
Datadog,62798229,62787931,3,"2020/07/08, 18:23:57",True,"2020/07/08, 18:23:57",1371.0,5540166.0,2,"If, for example, you define your  message  value to be this..."
Datadog,62798229,62787931,3,"2020/07/08, 18:23:57",True,"2020/07/08, 18:23:57",1371.0,5540166.0,2,... then ...
Datadog,63907036,63859370,0,"2020/09/15, 20:27:53",False,"2020/09/16, 01:09:03",31.0,12517930.0,0,got this resolved with below
Datadog,63907036,63859370,0,"2020/09/15, 20:27:53",False,"2020/09/16, 01:09:03",31.0,12517930.0,0,"rule1  %{ipv4:network.client.ip}\s+-\s+%{word:user}\s+\[%{date(&quot;dd/MMM/yyyy:HH:mm:ss Z&quot;):date}\]\s+\&quot;%{word:http.module}\s+\/v1\/resources\/+%{word:onemds.module}\?+%{data:onemds.params:keyvalue(&quot;=&quot;,&quot;/:&quot;,&quot;&quot;,&quot;:&amp;&quot;)}"
Datadog,63907036,63859370,0,"2020/09/15, 20:27:53",False,"2020/09/16, 01:09:03",31.0,12517930.0,0,was never expecting the delimiter that can take 2 characters above  :&amp;
Datadog,63907036,63859370,0,"2020/09/15, 20:27:53",False,"2020/09/16, 01:09:03",31.0,12517930.0,0,this helped to remove  rs:  for all except the first one.
Datadog,63907036,63859370,0,"2020/09/15, 20:27:53",False,"2020/09/16, 01:09:03",31.0,12517930.0,0,not an elegant approach but it worked for my use case.
Datadog,64447994,64395922,0,"2020/10/20, 18:05:44",False,"2020/10/20, 18:05:44",23.0,9053059.0,1,Found the answer:
Datadog,64447994,64395922,0,"2020/10/20, 18:05:44",False,"2020/10/20, 18:05:44",23.0,9053059.0,1,The metric  kubernetes.kubelet.volume.stats.used_bytes  will allow you to get the disk usage on PersistentVolumes.
Datadog,64447994,64395922,0,"2020/10/20, 18:05:44",False,"2020/10/20, 18:05:44",23.0,9053059.0,1,This can be achieved using the tag  persistentvolumeclaim  on this metric.
Datadog,64447994,64395922,0,"2020/10/20, 18:05:44",False,"2020/10/20, 18:05:44",23.0,9053059.0,1,You can view this metric and tag combination in your account here:  https://app.datadoghq.com/metric/summary?filter=kubernetes.kubelet.volume.stats.used_bytes&amp;metric=kubernetes.kubelet.volume.stats.used_bytes
Datadog,64447994,64395922,0,"2020/10/20, 18:05:44",False,"2020/10/20, 18:05:44",23.0,9053059.0,1,Documentation on this metric can be found here:  https://docs.datadoghq.com/agent/kubernetes/data_collected/#kubelet
Datadog,64484497,64482767,4,"2020/10/22, 17:32:13",False,"2020/10/22, 19:17:02",156.0,2676108.0,1,"This answer doesn't solve the problem, because postgres is not running in the cluster, it's running in Azure."
Datadog,64484497,64482767,4,"2020/10/22, 17:32:13",False,"2020/10/22, 19:17:02",156.0,2676108.0,1,"I'll leave it up since it might be interesting, but I posted another answer for the actually environment setup."
Datadog,64484497,64482767,4,"2020/10/22, 17:32:13",False,"2020/10/22, 19:17:02",156.0,2676108.0,1,"For containerized setups, it's not usually recommended to set up a configmap or try giving the agent a yaml file."
Datadog,64484497,64482767,4,"2020/10/22, 17:32:13",False,"2020/10/22, 19:17:02",156.0,2676108.0,1,Instead the recommended configuration is to put annotations on the postgres pod:  https://docs.datadoghq.com/integrations/postgres/?tab=containerized#containerized .
Datadog,64484497,64482767,4,"2020/10/22, 17:32:13",False,"2020/10/22, 19:17:02",156.0,2676108.0,1,"This concept of placing the config on the application pod, not with the datadog agent, is called autodiscovery."
Datadog,64484497,64482767,4,"2020/10/22, 17:32:13",False,"2020/10/22, 19:17:02",156.0,2676108.0,1,This blog post does a good job explaining the benefits of this solution:  https://www.datadoghq.com/blog/monitoring-kubernetes-with-datadog/#autodiscovery
Datadog,64484497,64482767,4,"2020/10/22, 17:32:13",False,"2020/10/22, 19:17:02",156.0,2676108.0,1,Here is a picture diagram showing how the agent goes out to the pods on the same node and would pull the configuration from them:
Datadog,64484497,64482767,4,"2020/10/22, 17:32:13",False,"2020/10/22, 19:17:02",156.0,2676108.0,1,"To configure this, you'd take each of the sections of the yaml config, convert them to json, and set them as annotations on the postgres manifest."
Datadog,64484497,64482767,4,"2020/10/22, 17:32:13",False,"2020/10/22, 19:17:02",156.0,2676108.0,1,"An example of how to set up pod annotations is provided for redis, apache, and http here:  https://docs.datadoghq.com/agent/kubernetes/integrations/?tab=kubernetes#examples"
Datadog,64484497,64482767,4,"2020/10/22, 17:32:13",False,"2020/10/22, 19:17:02",156.0,2676108.0,1,For your scenario I would do something like:
Datadog,64484497,64482767,4,"2020/10/22, 17:32:13",False,"2020/10/22, 19:17:02",156.0,2676108.0,1,"notice how the folder name  postgres.d/conf.yaml  maps to the  check_names  annotation, the  init_configs  section maps to  init_configs  annotation, etc."
Datadog,64484497,64482767,4,"2020/10/22, 17:32:13",False,"2020/10/22, 19:17:02",156.0,2676108.0,1,"For the section on custom metrics, since I personally am more familiar with the yaml config, and it's easier to just fill out, I'll usually go to a yaml to json converter, and copy the json from there"
Datadog,64484497,64482767,4,"2020/10/22, 17:32:13",False,"2020/10/22, 19:17:02",156.0,2676108.0,1,A key thing to notice for all those configs is that I never set the hostname.
Datadog,64484497,64482767,4,"2020/10/22, 17:32:13",False,"2020/10/22, 19:17:02",156.0,2676108.0,1,That is automatically discovered by the agent as it scans through containers.
Datadog,64484497,64482767,4,"2020/10/22, 17:32:13",False,"2020/10/22, 19:17:02",156.0,2676108.0,1,"However you may have set  my-postgres-host.com  because this postgres instance is not actually running in your kubernetes cluster, and is instead living on its own, and not in a container."
Datadog,64484497,64482767,4,"2020/10/22, 17:32:13",False,"2020/10/22, 19:17:02",156.0,2676108.0,1,"If that is the case, I would recommend trying to just put the agent on the postgres node directly, all the yaml stuff you've done would work just fine, if that db and the agent are both directly on the vm."
Datadog,64486131,64482767,0,"2020/10/22, 19:02:54",False,"2020/10/22, 19:11:25",156.0,2676108.0,1,It looks like the question has been updated to say that this postgres db you are trying to monitor is not actually running the the cluster.
Datadog,64486131,64482767,0,"2020/10/22, 19:02:54",False,"2020/10/22, 19:11:25",156.0,2676108.0,1,"And you are not able to put an agent directly on the postgres server since it's a managed service in Azure, so you don't have access to the underlying host."
Datadog,64486131,64482767,0,"2020/10/22, 19:02:54",False,"2020/10/22, 19:11:25",156.0,2676108.0,1,"In those situations it is common to have a random datadog agent on some other host set up the postgres integration anyway, but instead of having  host: localhost  in the yaml config, put the hostname you would put to access the db externally."
Datadog,64486131,64482767,0,"2020/10/22, 19:02:54",False,"2020/10/22, 19:11:25",156.0,2676108.0,1,In your example it was  host: my-postgres-host.com .
Datadog,64486131,64482767,0,"2020/10/22, 19:02:54",False,"2020/10/22, 19:11:25",156.0,2676108.0,1,This provides all the same benefits of the normal integration (except you won't have cpu/disk/resource metrics available obviously)
Datadog,64486131,64482767,0,"2020/10/22, 19:02:54",False,"2020/10/22, 19:11:25",156.0,2676108.0,1,"This is all fine and makes sense, but what if all of the agents you have installed are the agents in the kubernetes daemonset you created?"
Datadog,64486131,64482767,0,"2020/10/22, 19:02:54",False,"2020/10/22, 19:11:25",156.0,2676108.0,1,You don't have any hosts directly on VMs to run this check.
Datadog,64486131,64482767,0,"2020/10/22, 19:02:54",False,"2020/10/22, 19:11:25",156.0,2676108.0,1,But we definitely don't recommend configuring the daemonset to run this check directly.
Datadog,64486131,64482767,0,"2020/10/22, 19:02:54",False,"2020/10/22, 19:11:25",156.0,2676108.0,1,"If you did, that would mean you are collecting duplicate metrics from that one postgres db in every single node in your cluster."
Datadog,64486131,64482767,0,"2020/10/22, 19:02:54",False,"2020/10/22, 19:11:25",156.0,2676108.0,1,"Since every agent is a copy, they'd each be running the same check on the same db you define."
Datadog,64486131,64482767,0,"2020/10/22, 19:02:54",False,"2020/10/22, 19:11:25",156.0,2676108.0,1,Luckily I notice that you are running the Datadog Cluster Agent.
Datadog,64486131,64482767,0,"2020/10/22, 19:02:54",False,"2020/10/22, 19:11:25",156.0,2676108.0,1,"This is a separate Datadog tool that is deployed as a single service once per cluster, instead of a daemonset running once per node."
Datadog,64486131,64482767,0,"2020/10/22, 19:02:54",False,"2020/10/22, 19:11:25",156.0,2676108.0,1,It is possible to have the cluster agent configured to run 'cluster level' checks.
Datadog,64486131,64482767,0,"2020/10/22, 19:02:54",False,"2020/10/22, 19:11:25",156.0,2676108.0,1,"Perfect for things like databases, message queues, or http checks."
Datadog,64486131,64482767,0,"2020/10/22, 19:02:54",False,"2020/10/22, 19:11:25",156.0,2676108.0,1,The basic idea is that (in addition to it's other jobs) the cluster agent will also schedule checks.
Datadog,64486131,64482767,0,"2020/10/22, 19:02:54",False,"2020/10/22, 19:11:25",156.0,2676108.0,1,"the DCA (datadog cluster agent) will choose one agent from the daemonset to run the check, and if that node agent pod dies, the DCA will find a new one to run the cluster check."
Datadog,64486131,64482767,0,"2020/10/22, 19:02:54",False,"2020/10/22, 19:11:25",156.0,2676108.0,1,Here are the docs on how to set up the DCA to run cluster checks:  https://docs.datadoghq.com/agent/cluster_agent/clusterchecks/#how-it-works
Datadog,64486131,64482767,0,"2020/10/22, 19:02:54",False,"2020/10/22, 19:11:25",156.0,2676108.0,1,"To configure it you would enable some flags, and give the DCA the yaml file you created with a config map, or just mounting the file directly."
Datadog,64486131,64482767,0,"2020/10/22, 19:02:54",False,"2020/10/22, 19:11:25",156.0,2676108.0,1,The DCA will pass along that config to whichever node agent it chooses to run the check.
Datadog,65583978,65580606,0,"2021/01/05, 19:47:25",False,"2021/01/05, 19:47:25",45878.0,180368.0,1,Answering my own question.
Datadog,65583978,65580606,0,"2021/01/05, 19:47:25",False,"2021/01/05, 19:47:25",45878.0,180368.0,1,The DataDog logging page has a Configuration section.
Datadog,65583978,65580606,0,"2021/01/05, 19:47:25",False,"2021/01/05, 19:47:25",45878.0,180368.0,1,On that page the &quot;Pre processing for JSON logs&quot; section allows you to specify alternate property names for a few of the major log message properties.
Datadog,65583978,65580606,0,"2021/01/05, 19:47:25",False,"2021/01/05, 19:47:25",45878.0,180368.0,1,If you add @m to the Message attributes section and @l to the Status attributes section you will correctly ingest JSON messages from the  RenderedCompactJsonFormatter  formatter.
Datadog,65583978,65580606,0,"2021/01/05, 19:47:25",False,"2021/01/05, 19:47:25",45878.0,180368.0,1,If you add RenderedMessage and Level respectively you will correctly ingest  JsonFormatter(renderMessage: true)  formatter.
Datadog,65583978,65580606,0,"2021/01/05, 19:47:25",False,"2021/01/05, 19:47:25",45878.0,180368.0,1,"You can specify multiple attributes in each section, so you can simultaneously support both formats."
Datadog,66284080,66113635,0,"2021/02/19, 21:45:06",False,"2021/02/19, 21:45:06",383.0,2032722.0,1,"After few days of research and follow up with datadog support team, I am able to get the APM logs on datadog portal."
Datadog,66284080,66113635,0,"2021/02/19, 21:45:06",False,"2021/02/19, 21:45:06",383.0,2032722.0,1,"Below is my  docker-compose.yml  file configuration,  I believe it helps someone in future"
Datadog,66284080,66113635,0,"2021/02/19, 21:45:06",False,"2021/02/19, 21:45:06",383.0,2032722.0,1,The  Dockerfile  for my python long running application
Datadog,66284080,66113635,0,"2021/02/19, 21:45:06",False,"2021/02/19, 21:45:06",383.0,2032722.0,1,"Please note, on the requirements.txt file I have   ddtrace  package listed"
Datadog,66329196,66172172,1,"2021/02/23, 10:00:35",False,"2021/02/23, 10:00:35",871.0,7779815.0,0,"In the  instructions  it refers to http://ccloudexporter_ccloud_exporter_1:2112/metrics in the open metrics file, but in my setup docker-compose gave the ccloud exporter container the name ccloud_ccloud_exporter_1."
Datadog,66329196,66172172,1,"2021/02/23, 10:00:35",False,"2021/02/23, 10:00:35",871.0,7779815.0,0,To prevent this I added &quot;container_name: ccloud_ccloud_exporter_1&quot; in the docker compose file and used &quot;http://ccloud_ccloud_exporter_1:2112/metrics&quot; as prometheus url in the openmetrics file.
Datadog,64973799,64929616,0,"2020/11/23, 20:02:47",True,"2020/11/23, 20:08:08",251.0,5477963.0,0,Tracing for requests coming from the browser are handled by RUM and not APM.
Datadog,64973799,64929616,0,"2020/11/23, 20:02:47",True,"2020/11/23, 20:08:08",251.0,5477963.0,0,I can't find the documentation for it but there is a configuration option on the  browser SDK  to allow tracing to specific endpoints.
Datadog,64973799,64929616,0,"2020/11/23, 20:02:47",True,"2020/11/23, 20:08:08",251.0,5477963.0,0,"The tracing context will automatically be propagated to APM if enabled on the server side, and both the browser and server spans will appear in the same trace."
Datadog,63815085,63814864,11,"2020/09/09, 18:41:28",False,"2020/09/09, 18:41:28",30563.0,605153.0,2,Usually metrics exposed to  /actuator/metrics  are sent to the metrics system like datadog.
Datadog,63815085,63814864,11,"2020/09/09, 18:41:28",False,"2020/09/09, 18:41:28",30563.0,605153.0,2,You can try to check what exactly gets sent to datadog by examining the source code of  DatadogMeterRegistry
Datadog,63815085,63814864,11,"2020/09/09, 18:41:28",False,"2020/09/09, 18:41:28",30563.0,605153.0,2,"Put a breakpoint in the publish method and see what gets sent, or, alternatively set the logger of the class to &quot;trace&quot; so that it will print the information that gets sent to the datadog (line 131 in the linked source code)."
Datadog,63815085,63814864,11,"2020/09/09, 18:41:28",False,"2020/09/09, 18:41:28",30563.0,605153.0,2,Another possible direction to check is usage of filters (see  MeterFilter ) that can filter out some metrics.
Datadog,63835825,63814864,0,"2020/09/10, 21:39:35",True,"2020/09/10, 22:25:47",205.0,8534030.0,2,This did the trick : Thanks to @MarkBramnik
Datadog,65920537,63314162,2,"2021/01/27, 15:50:52",True,"2021/01/27, 15:50:52",48.0,8161041.0,1,Try this
Datadog,62545412,62545185,0,"2020/06/24, 02:43:20",False,"2020/06/24, 02:43:20",1371.0,5540166.0,1,You might be able to get this if you...
Datadog,62545412,62545185,0,"2020/06/24, 02:43:20",False,"2020/06/24, 02:43:20",1371.0,5540166.0,1,"If you start having trouble as you start setting it up, you may want to reach out to support@datadoghq.com to get their assistance."
Datadog,61524549,61521576,0,"2020/04/30, 16:28:56",False,"2020/04/30, 16:28:56",1824.0,944768.0,3,"this seem to be working:
 -@userId:*?"
Datadog,61524549,61521576,0,"2020/04/30, 16:28:56",False,"2020/04/30, 16:28:56",1824.0,944768.0,3,*  do not forget the minus at the start.
Datadog,61128911,61108009,2,"2020/04/09, 22:36:58",False,"2020/04/15, 21:52:15",1371.0,5540166.0,1,Either of the  count_not_null()  or  count_nonzero()  functions should get you where you want.
Datadog,61128911,61108009,2,"2020/04/09, 22:36:58",False,"2020/04/15, 21:52:15",1371.0,5540166.0,1,"If graph your metric, grouped by your tag, and then apply one of those functions, it should return the count of unique tag values under that tag key."
Datadog,61128911,61108009,2,"2020/04/09, 22:36:58",False,"2020/04/15, 21:52:15",1371.0,5540166.0,1,So in your case:
Datadog,61128911,61108009,2,"2020/04/09, 22:36:58",False,"2020/04/15, 21:52:15",1371.0,5540166.0,1,count_not_null(sum:your.metric.name{*} by {file_name})
Datadog,61128911,61108009,2,"2020/04/09, 22:36:58",False,"2020/04/15, 21:52:15",1371.0,5540166.0,1,"And it works with multiple group-by tags too, so if you had separate tags for  file_name  and  directory  then you could use this same approach to graph the count of unique  combinations  of these tag values, or the count of unique combinations of  directory + file_name :"
Datadog,61128911,61108009,2,"2020/04/09, 22:36:58",False,"2020/04/15, 21:52:15",1371.0,5540166.0,1,"count_not_null(your.metric.name{*} by {file_name,directory})"
Datadog,60171576,60170609,3,"2020/02/11, 16:50:50",False,"2020/02/11, 16:50:50",1371.0,5540166.0,0,"Yes, it is possible."
Datadog,60171576,60170609,3,"2020/02/11, 16:50:50",False,"2020/02/11, 16:50:50",1371.0,5540166.0,0,"You can do that in a  processing pipeline  with a grok parser, but you'll want to configure which attribute the grok parser applies to in the advanced settings ( docs here )."
Datadog,60171576,60170609,3,"2020/02/11, 16:50:50",False,"2020/02/11, 16:50:50",1371.0,5540166.0,0,"(By default grok parsers apply to the ""message"" attribute, but you can configure them to parse any attribute.)"
Datadog,60171576,60170609,3,"2020/02/11, 16:50:50",False,"2020/02/11, 16:50:50",1371.0,5540166.0,0,"In this case, you'd set the  Extract From  field to  requestUri ."
Datadog,60171576,60170609,3,"2020/02/11, 16:50:50",False,"2020/02/11, 16:50:50",1371.0,5540166.0,0,The  Helper Rules  section is not necessary for this.
Datadog,60171576,60170609,3,"2020/02/11, 16:50:50",False,"2020/02/11, 16:50:50",1371.0,5540166.0,0,"And then in the main  Define Parsing Rules  section, you'll plug in a rule similar to this:"
Datadog,60171576,60170609,3,"2020/02/11, 16:50:50",False,"2020/02/11, 16:50:50",1371.0,5540166.0,0,or even further
Datadog,59625224,59622491,2,"2020/01/07, 11:08:08",True,"2020/01/08, 11:23:00",336.0,7243426.0,1,What about using the template variables  doc ?
Datadog,59625224,59622491,2,"2020/01/07, 11:08:08",True,"2020/01/08, 11:23:00",336.0,7243426.0,1,You could select:
Datadog,59625224,59622491,2,"2020/01/07, 11:08:08",True,"2020/01/08, 11:23:00",336.0,7243426.0,1,Then you'll be able to replace your  {name:$flavor-db-master}  with  {$Name}
Datadog,59625224,59622491,2,"2020/01/07, 11:08:08",True,"2020/01/08, 11:23:00",336.0,7243426.0,1,"Otherwise, if you actually wants the value of the template variable you have to use  $flavor.value ."
Datadog,59625224,59622491,2,"2020/01/07, 11:08:08",True,"2020/01/08, 11:23:00",336.0,7243426.0,1,I advise to use a not widget to check the actual behavior.
Datadog,59625224,59622491,2,"2020/01/07, 11:08:08",True,"2020/01/08, 11:23:00",336.0,7243426.0,1,EDIT:
Datadog,59625224,59622491,2,"2020/01/07, 11:08:08",True,"2020/01/08, 11:23:00",336.0,7243426.0,1,This kind of setup is not the recommended.
Datadog,59625224,59622491,2,"2020/01/07, 11:08:08",True,"2020/01/08, 11:23:00",336.0,7243426.0,1,It would be better to set two tags on your database:
Datadog,59625224,59622491,2,"2020/01/07, 11:08:08",True,"2020/01/08, 11:23:00",336.0,7243426.0,1,"You would then have a unique selection of tags,  env:dev,dbname:db1-master ."
Datadog,59625224,59622491,2,"2020/01/07, 11:08:08",True,"2020/01/08, 11:23:00",336.0,7243426.0,1,It would then be easy to have a query such as:
Datadog,59359866,59357130,0,"2019/12/16, 17:42:43",True,"2019/12/16, 17:42:43",1371.0,5540166.0,2,You can do this in a processing pipeline with 2 steps:
Datadog,59359866,59357130,0,"2019/12/16, 17:42:43",True,"2019/12/16, 17:42:43",1371.0,5540166.0,2,"If there are other queries/patterns you want to use to determine the log level/status, you can add multiple rules to the  Category Processor  in (1), and you can map the  level  value to  info/warn/error  and any other relevant status value."
Datadog,58760072,58725596,0,"2019/11/08, 05:46:30",False,"2019/11/08, 05:46:30",156.0,2676108.0,1,You mostly have to wait for it all to fill in over time.
Datadog,58760072,58725596,0,"2019/11/08, 05:46:30",False,"2019/11/08, 05:46:30",156.0,2676108.0,1,Metric timestamps cannot be more than 10 minutes in the future or more than 1 hour in the past.
Datadog,58760072,58725596,0,"2019/11/08, 05:46:30",False,"2019/11/08, 05:46:30",156.0,2676108.0,1,https://docs.datadoghq.com/developers/metrics/
Datadog,58963300,58564144,0,"2019/11/20, 23:00:45",False,"2019/11/20, 23:00:45",107.0,10913713.0,1,I got the answer to the second question.
Datadog,58963300,58564144,0,"2019/11/20, 23:00:45",False,"2019/11/20, 23:00:45",107.0,10913713.0,1,"Now, I can get all tables from one database that I specified."
Datadog,58963300,58564144,0,"2019/11/20, 23:00:45",False,"2019/11/20, 23:00:45",107.0,10913713.0,1,All I needed to do; relation_regex: '.
Datadog,58963300,58564144,0,"2019/11/20, 23:00:45",False,"2019/11/20, 23:00:45",107.0,10913713.0,1,*' and disabled relation_name.
Datadog,58963300,58564144,0,"2019/11/20, 23:00:45",False,"2019/11/20, 23:00:45",107.0,10913713.0,1,Answer to the first question I got from datadog is that there is no way to monitor all the DBs without listing them individually.
Datadog,58963300,58564144,0,"2019/11/20, 23:00:45",False,"2019/11/20, 23:00:45",107.0,10913713.0,1,"They may change this in future, but for now we have to add blocks for each and every database that we want to monitor"
Datadog,61927265,56382266,0,"2020/05/21, 07:25:17",False,"2020/05/21, 07:25:17",728.0,2218580.0,0,This works for me:
Datadog,55850182,55794663,0,"2019/04/25, 16:15:12",False,"2019/04/25, 16:15:12",4059.0,8763847.0,4,"What you are doing is correct only, however, the common mistake is not following the below."
Datadog,55850182,55794663,0,"2019/04/25, 16:15:12",False,"2019/04/25, 16:15:12",4059.0,8763847.0,4,"This library MUST be imported and initialized before any instrumented
  module."
Datadog,55850182,55794663,0,"2019/04/25, 16:15:12",False,"2019/04/25, 16:15:12",4059.0,8763847.0,4,"When using a transpiler, you MUST import and initialize the
  tracer library in an external file and then import that file as a
  whole when building your application."
Datadog,55850182,55794663,0,"2019/04/25, 16:15:12",False,"2019/04/25, 16:15:12",4059.0,8763847.0,4,"This prevents hoisting and
  ensures that the tracer library gets imported and initialized before
  importing any other instrumented module."
Datadog,55850182,55794663,0,"2019/04/25, 16:15:12",False,"2019/04/25, 16:15:12",4059.0,8763847.0,4,"Basically, you cannot have  require(any instrumented lib)  (e.g."
Datadog,55850182,55794663,0,"2019/04/25, 16:15:12",False,"2019/04/25, 16:15:12",4059.0,8763847.0,4,"http, express, etc) before calling init() tracing function."
Datadog,55850182,55794663,0,"2019/04/25, 16:15:12",False,"2019/04/25, 16:15:12",4059.0,8763847.0,4,https://docs.datadoghq.com/tracing/setup/nodejs/
Datadog,53585633,52176297,0,"2018/12/03, 01:28:11",False,"2018/12/03, 01:28:11",41.0,1970447.0,4,"It's probably too late, but it may be useful to others."
Datadog,53585633,52176297,0,"2018/12/03, 01:28:11",False,"2018/12/03, 01:28:11",41.0,1970447.0,4,You can set tags by using the environment variables of the application container (not the agent container) by using DD_TAGS.
Datadog,51933718,51866333,0,"2018/08/20, 18:11:48",True,"2018/08/20, 18:11:48",3212.0,282172.0,2,Apparently there is a datadog/agent:latest-jmx that should be used that contains the java image...
Datadog,51933718,51866333,0,"2018/08/20, 18:11:48",True,"2018/08/20, 18:11:48",3212.0,282172.0,2,I just missed it in the docs.
Datadog,51451079,51443070,0,"2018/07/21, 01:20:42",True,"2018/07/21, 01:20:42",46.0,9872725.0,3,"I discussed this with Datadog support, and they confirmed that the  awslogs  logging driver prevents the Datadog agent container from accessing a container's logs."
Datadog,51451079,51443070,0,"2018/07/21, 01:20:42",True,"2018/07/21, 01:20:42",46.0,9872725.0,3,"Since  awslogs  is currently the only logging driver available to tasks using the Fargate launch type, getting logs into Datadog will require another method."
Datadog,51451079,51443070,0,"2018/07/21, 01:20:42",True,"2018/07/21, 01:20:42",46.0,9872725.0,3,"Since the  awslogs  logging driver emits logs to CloudWatch, one method that I have used is to create a subscription to stream those log groups to Datadog's Lambda function as configured  here ."
Datadog,51451079,51443070,0,"2018/07/21, 01:20:42",True,"2018/07/21, 01:20:42",46.0,9872725.0,3,"You can do that from the  Lambda side  using CloudWatch logs as the trigger, or from the CloudWatch Logs side, by clicking  Actions    Stream to AWS Lambda ."
Datadog,51451079,51443070,0,"2018/07/21, 01:20:42",True,"2018/07/21, 01:20:42",46.0,9872725.0,3,I chose the Lambda option because it was quick and easy and required no code changes to our applications (since we are still in the evaluation stage).
Datadog,51451079,51443070,0,"2018/07/21, 01:20:42",True,"2018/07/21, 01:20:42",46.0,9872725.0,3,Datadog support advised me that it was necessary to modify the Lambda function in order to attribute logs to the corresponding service:
Datadog,51451079,51443070,0,"2018/07/21, 01:20:42",True,"2018/07/21, 01:20:42",46.0,9872725.0,3,"In  this block , modify it to something like:"
Datadog,51451079,51443070,0,"2018/07/21, 01:20:42",True,"2018/07/21, 01:20:42",46.0,9872725.0,3,According to Datadog support:
Datadog,51451079,51443070,0,"2018/07/21, 01:20:42",True,"2018/07/21, 01:20:42",46.0,9872725.0,3,I had to make further modifications to set the value of the  syslog.
Datadog,51451079,51443070,0,"2018/07/21, 01:20:42",True,"2018/07/21, 01:20:42",46.0,9872725.0,3,"*  keys in a way that made sense for our applications, but it works great."
Datadog,51324294,51323878,0,"2018/07/13, 14:39:40",False,"2018/07/13, 14:39:40",61.0,9763778.0,1,I found the solution: the user  datadog  didnt have permission to read connections that wasnt form him.
Datadog,51324294,51323878,0,"2018/07/13, 14:39:40",False,"2018/07/13, 14:39:40",61.0,9763778.0,1,So it was just getting a single row.
Datadog,51324294,51323878,0,"2018/07/13, 14:39:40",False,"2018/07/13, 14:39:40",61.0,9763778.0,1,I gave permissions for that user to read  pg_stat_activity
Datadog,50793070,50792558,0,"2018/06/11, 11:09:13",True,"2018/06/11, 11:09:13",7647.0,1958151.0,2,"It looks like you created  some  policy, but not the policy of required type."
Datadog,50793070,50792558,0,"2018/06/11, 11:09:13",True,"2018/06/11, 11:09:13",7647.0,1958151.0,2,"When you create the role for Datadog, you have to choose a very specific role type:"
Datadog,50793070,50792558,0,"2018/06/11, 11:09:13",True,"2018/06/11, 11:09:13",7647.0,1958151.0,2,Select Another AWS account for the Role Type.
Datadog,50793070,50792558,0,"2018/06/11, 11:09:13",True,"2018/06/11, 11:09:13",7647.0,1958151.0,2,and then create a policy for that role.
Datadog,50793070,50792558,0,"2018/06/11, 11:09:13",True,"2018/06/11, 11:09:13",7647.0,1958151.0,2,"Also, don't forget to"
Datadog,50793070,50792558,0,"2018/06/11, 11:09:13",True,"2018/06/11, 11:09:13",7647.0,1958151.0,2,Check off Require external ID
Datadog,50793070,50792558,0,"2018/06/11, 11:09:13",True,"2018/06/11, 11:09:13",7647.0,1958151.0,2,You shouldn't have any problems as long as you follow the guideline step by step:  https://docs.datadoghq.com/integrations/amazon_web_services/
Datadog,61044468,50792558,1,"2020/04/05, 17:51:15",False,"2020/04/05, 17:51:15",1.0,12211192.0,0,"I had this problem, when I tried to both use the role-assumption role as an assumption role on the  assume_role_policy , as well as trying to attach it."
Datadog,61044468,50792558,1,"2020/04/05, 17:51:15",False,"2020/04/05, 17:51:15",1.0,12211192.0,0,"Once I got rid of the aws_iam_policy that I created with the role-assumption policy doc as well as the role-policy attachment, it worked."
Datadog,61044468,50792558,1,"2020/04/05, 17:51:15",False,"2020/04/05, 17:51:15",1.0,12211192.0,0,Hope this helps.
Datadog,55032271,49022133,0,"2019/03/06, 23:16:09",False,"2019/03/06, 23:16:09",141.0,8721010.0,1,One of Dockers main features is portability and it makes sense to bind datadog into that environment.
Datadog,55032271,49022133,0,"2019/03/06, 23:16:09",False,"2019/03/06, 23:16:09",141.0,8721010.0,1,That way they are packaged and deployed together and you don't have the overhead of installing datadog manually everywhere you choose to deploy.
Datadog,55032271,49022133,0,"2019/03/06, 23:16:09",False,"2019/03/06, 23:16:09",141.0,8721010.0,1,"What they are also implying is that you should use  docker-compose  and turn your application / docker container into an multi-container Docker application, running your image(s) alongside the docker agent."
Datadog,55032271,49022133,0,"2019/03/06, 23:16:09",False,"2019/03/06, 23:16:09",141.0,8721010.0,1,"Thus you will not need to write/build/run/manage a container via Dockerfile, but rather add the agent image to your  docker-compose.yml  along with its configuration."
Datadog,55032271,49022133,0,"2019/03/06, 23:16:09",False,"2019/03/06, 23:16:09",141.0,8721010.0,1,Starting your multi-container application will still be easy via:
Datadog,55032271,49022133,0,"2019/03/06, 23:16:09",False,"2019/03/06, 23:16:09",141.0,8721010.0,1,Its really convenient and gives you additional features like their  autodiscovery  service.
Datadog,47085123,47069283,1,"2017/11/02, 23:45:28",False,"2017/11/02, 23:45:28",5158.0,7147666.0,2,Just use a find me Twimlet.
Datadog,47085123,47069283,1,"2017/11/02, 23:45:28",False,"2017/11/02, 23:45:28",5158.0,7147666.0,2,Enter up to 10 numbers and a timeout between moving on to the next number.
Datadog,47085123,47069283,1,"2017/11/02, 23:45:28",False,"2017/11/02, 23:45:28",5158.0,7147666.0,2,Twilio will do the rest.
Datadog,47085123,47069283,1,"2017/11/02, 23:45:28",False,"2017/11/02, 23:45:28",5158.0,7147666.0,2,https://www.twilio.com/labs/twimlets/findme
Datadog,47730118,47069283,0,"2017/12/09, 17:29:38",False,"2017/12/09, 17:29:38",1150.0,1394755.0,1,If you are looking for a more full featured paid solution I'd recommend PagerDuty.
Datadog,47730118,47069283,0,"2017/12/09, 17:29:38",False,"2017/12/09, 17:29:38",1150.0,1394755.0,1,DataDog has an integration for PagerDuty.
Datadog,47730118,47069283,0,"2017/12/09, 17:29:38",False,"2017/12/09, 17:29:38",1150.0,1394755.0,1,Any monitor that gets triggered that mentions  @pagerduty-myteamname (as example) in the monitor message will cause PagerDuty to page the on call person.
Datadog,47730118,47069283,0,"2017/12/09, 17:29:38",False,"2017/12/09, 17:29:38",1150.0,1394755.0,1,If that person does not acknowledge the page you can configure it to go through list of people to contact next until it is acknowledged by someone.
Datadog,46202190,45695083,0,"2017/09/13, 19:05:55",True,"2017/09/13, 19:05:55",421.0,2016045.0,4,The role arn:aws:iam::xxxxxxxxxx:role/DatadogAWSIntegrationRole also has to have permission to assume the role on the other account.
Datadog,46202190,45695083,0,"2017/09/13, 19:05:55",True,"2017/09/13, 19:05:55",421.0,2016045.0,4,You'll have to update the DatadogAWSIntegrationRole on the primary account to include:
Datadog,45438610,45438114,9,"2017/08/01, 16:00:52",True,"2017/08/01, 16:00:52",36.0,3185251.0,2,I suspect you're probably looking to query the event stream which is where all alerts from monitors can be found.
Datadog,45438610,45438114,9,"2017/08/01, 16:00:52",True,"2017/08/01, 16:00:52",36.0,3185251.0,2,The docs at  https://docs.datadoghq.com/api/#events-get-all  are a pretty good starting place.
Datadog,45438610,45438114,9,"2017/08/01, 16:00:52",True,"2017/08/01, 16:00:52",36.0,3185251.0,2,"You'll want to query this endpoint with the proper source and tags, but this should be a starting point."
Datadog,45438610,45438114,9,"2017/08/01, 16:00:52",True,"2017/08/01, 16:00:52",36.0,3185251.0,2,"If this doesn't quite work, I'd recommend looking at pulling the details from the monitor as shown here:   https://docs.datadoghq.com/api/#monitor-get-details ."
Datadog,45438610,45438114,9,"2017/08/01, 16:00:52",True,"2017/08/01, 16:00:52",36.0,3185251.0,2,This may be a second option if you're unable to get the information you're looking for from the event stream.
Datadog,45124573,45104434,1,"2017/07/16, 06:03:15",True,"2017/07/16, 06:03:15",1171.0,6826691.0,2,"The ""ReadTimeout: HTTPConnectionPool"" error can be corrected by adding a timeout parameter under instances in the elasticsearch.yaml"
Datadog,41008234,41007369,0,"2016/12/07, 04:05:10",False,"2016/12/07, 04:05:10",261.0,4172512.0,1,"Frank,"
Datadog,41008234,41007369,0,"2016/12/07, 04:05:10",False,"2016/12/07, 04:05:10",261.0,4172512.0,1,"Your use case follows the standard ""custom metric"" submission that is common within Datadog."
Datadog,41008234,41007369,0,"2016/12/07, 04:05:10",False,"2016/12/07, 04:05:10",261.0,4172512.0,1,Using one of the supported libraries:
Datadog,41008234,41007369,0,"2016/12/07, 04:05:10",False,"2016/12/07, 04:05:10",261.0,4172512.0,1,http://docs.datadoghq.com/libraries/#java
Datadog,41008234,41007369,0,"2016/12/07, 04:05:10",False,"2016/12/07, 04:05:10",261.0,4172512.0,1,You can leverage the statsD port of an Agent running on your host to submit these custom metrics:
Datadog,41008234,41007369,0,"2016/12/07, 04:05:10",False,"2016/12/07, 04:05:10",261.0,4172512.0,1,http://docs.datadoghq.com/guides/dogstatsd/
Datadog,41008234,41007369,0,"2016/12/07, 04:05:10",False,"2016/12/07, 04:05:10",261.0,4172512.0,1,You will want to install the Agent on either the host running this function or point your statsD connection towards an accepting host:
Datadog,41008234,41007369,0,"2016/12/07, 04:05:10",False,"2016/12/07, 04:05:10",261.0,4172512.0,1,http://docs.datadoghq.com/guides/basic_agent_usage/
Datadog,41008234,41007369,0,"2016/12/07, 04:05:10",False,"2016/12/07, 04:05:10",261.0,4172512.0,1,There are additional docs found here that should help you understand how custom metrics work in Datadog:
Datadog,41008234,41007369,0,"2016/12/07, 04:05:10",False,"2016/12/07, 04:05:10",261.0,4172512.0,1,https://help.datadoghq.com/hc/en-us/articles/204271775-What-is-a-custom-metric-and-what-is-the-limit-on-the-number-of-custom-metrics-I-can-have-
Datadog,41008234,41007369,0,"2016/12/07, 04:05:10",False,"2016/12/07, 04:05:10",261.0,4172512.0,1,https://help.datadoghq.com/hc/en-us/articles/203765485-How-do-I-submit-custom-metrics-What-s-their-overhead-
Datadog,41008234,41007369,0,"2016/12/07, 04:05:10",False,"2016/12/07, 04:05:10",261.0,4172512.0,1,"Usually when troubleshooting custom metric submissions, we try to implement some form of local printing/logging to ensure the statsD connection is being made and that the custom function is being called and submitted."
Datadog,41008234,41007369,0,"2016/12/07, 04:05:10",False,"2016/12/07, 04:05:10",261.0,4172512.0,1,"Once you can confirm the metric is being sent to the Agent, use the Metric Summary page to check for the custom metric:"
Datadog,41008234,41007369,0,"2016/12/07, 04:05:10",False,"2016/12/07, 04:05:10",261.0,4172512.0,1,https://app.datadoghq.com/metric/summary
Datadog,41008234,41007369,0,"2016/12/07, 04:05:10",False,"2016/12/07, 04:05:10",261.0,4172512.0,1,"If all else fails, reach out to Datadog at support@datadoghq.com"
Datadog,41220782,40933155,0,"2016/12/19, 12:42:07",True,"2016/12/19, 12:42:07",1981.0,2473382.0,0,"It is very much possible, just use the alias property of the  attribute  filter:"
Datadog,39235805,39235163,0,"2016/08/30, 22:38:33",True,"2016/08/30, 22:40:17",496.0,2050873.0,6,Verify if the datadog package is installed in your environment.
Datadog,39235805,39235163,0,"2016/08/30, 22:38:33",True,"2016/08/30, 22:40:17",496.0,2050873.0,6,You can do this with this command:
Datadog,39235805,39235163,0,"2016/08/30, 22:38:33",True,"2016/08/30, 22:40:17",496.0,2050873.0,6,"If it's not installed, you can install it with this command:"
Datadog,66955358,66943560,0,"2021/04/05, 18:20:48",False,"2021/04/05, 18:20:48",156.0,2676108.0,1,You could tag your metrics with the name or ID of the agent it is collecting metrics from (if you aren't already).
Datadog,66955358,66943560,0,"2021/04/05, 18:20:48",False,"2021/04/05, 18:20:48",156.0,2676108.0,1,Then in Datadog you could write a query that groups by the agent ID and applies a count_not_null function:  https://docs.datadoghq.com/dashboards/functions/count/
Datadog,66955358,66943560,0,"2021/04/05, 18:20:48",False,"2021/04/05, 18:20:48",156.0,2676108.0,1,This basically hijacks a random metric to extract the unique count of agents reporting that metric to assume the total count of agents.
Datadog,66955358,66943560,0,"2021/04/05, 18:20:48",False,"2021/04/05, 18:20:48",156.0,2676108.0,1,"You wouldn't be able to easily group by queue though, so idk if it would be a good solution to your use case."
Datadog,66955358,66943560,0,"2021/04/05, 18:20:48",False,"2021/04/05, 18:20:48",156.0,2676108.0,1,Your idea around using gauges sounds good to me.
Datadog,66955358,66943560,0,"2021/04/05, 18:20:48",False,"2021/04/05, 18:20:48",156.0,2676108.0,1,You can send a new metric called something like  myagent.running  which sends a value of 1 for each of your agents and does a sum of all gauges in order to get a count.
Datadog,66955358,66943560,0,"2021/04/05, 18:20:48",False,"2021/04/05, 18:20:48",156.0,2676108.0,1,That is actually how the metric  datadog.agent.running  is implemented:  https://docs.datadoghq.com/integrations/agent_metrics/#metrics
Datadog,66632382,66606637,0,"2021/03/15, 06:26:59",True,"2021/03/15, 06:26:59",902.0,10709519.0,0,After testing different queries I found that running this query groups the results by query statements and will return the count of each.
Datadog,66632382,66606637,0,"2021/03/15, 06:26:59",True,"2021/03/15, 06:26:59",902.0,10709519.0,0,In Datadogs mysql configuration I added tags for the query statement and database name.
Datadog,66632382,66606637,0,"2021/03/15, 06:26:59",True,"2021/03/15, 06:26:59",902.0,10709519.0,0,"Now since they are grouped, I can see information per different statement in datadog."
Datadog,66453250,66443203,2,"2021/03/03, 10:27:46",False,"2021/03/04, 10:26:16",1249.0,13336642.0,0,Datadog’s IIS integration queries the Web Service performance counters automatically and sends the results to Datadog.
Datadog,66453250,66443203,2,"2021/03/03, 10:27:46",False,"2021/03/04, 10:26:16",1249.0,13336642.0,0,The Web Service performance counter class collects information from the World Wide Web Publishing Service.
Datadog,66453250,66443203,2,"2021/03/03, 10:27:46",False,"2021/03/04, 10:26:16",1249.0,13336642.0,0,You can enable the IIS integration by creating a configuration file either manually or through the Datadog Agent GUI.
Datadog,66453250,66443203,2,"2021/03/03, 10:27:46",False,"2021/03/04, 10:26:16",1249.0,13336642.0,0,"To create a configuration file through the GUI, navigate to the “Checks” tab, choose “Manage Checks,” and select the iis check from the “Add a Check” menu."
Datadog,66453250,66443203,2,"2021/03/03, 10:27:46",False,"2021/03/04, 10:26:16",1249.0,13336642.0,0,You can also manually create a conf.yaml file in C:\ProgramData\Datadog\conf.d\iis.d.
Datadog,66453250,66443203,2,"2021/03/03, 10:27:46",False,"2021/03/04, 10:26:16",1249.0,13336642.0,0,There is a sites attribute in the conf.yaml file.
Datadog,66453250,66443203,2,"2021/03/03, 10:27:46",False,"2021/03/04, 10:26:16",1249.0,13336642.0,0,This attribute represents the IIS site you want to monitor.
Datadog,66453250,66443203,2,"2021/03/03, 10:27:46",False,"2021/03/04, 10:26:16",1249.0,13336642.0,0,You only need to delete the sites you want to exclude.
Datadog,66453250,66443203,2,"2021/03/03, 10:27:46",False,"2021/03/04, 10:26:16",1249.0,13336642.0,0,More information you can refer to this link:  IIS monitoring with Datadog .
Datadog,66642805,66443203,0,"2021/03/15, 19:33:06",True,"2021/03/15, 19:33:06",16566.0,24231.0,0,"If you are using Datadog's .NET Tracer, you can set  DD_TRACE_ENABLED=false  in the  appSettings  section of the  web.config  file ( docs )."
Datadog,66642805,66443203,0,"2021/03/15, 19:33:06",True,"2021/03/15, 19:33:06",16566.0,24231.0,0,For example:
Datadog,66642805,66443203,0,"2021/03/15, 19:33:06",True,"2021/03/15, 19:33:06",16566.0,24231.0,0,Another option is to deploy a  datadog.json  file ( docs ) in the root of your app that contains:
Datadog,66642805,66443203,0,"2021/03/15, 19:33:06",True,"2021/03/15, 19:33:06",16566.0,24231.0,0,(Disclaimer: I work at Datadog)
Datadog,66357392,66316616,2,"2021/02/24, 21:12:06",False,"2021/02/25, 00:49:28",156.0,2676108.0,0,Update: I found this bit of code in the tracing client repo:
Datadog,66357392,66316616,2,"2021/02/24, 21:12:06",False,"2021/02/25, 00:49:28",156.0,2676108.0,0,https://github.com/DataDog/dd-trace-js/issues/1249
Datadog,66357392,66316616,2,"2021/02/24, 21:12:06",False,"2021/02/25, 00:49:28",156.0,2676108.0,0,maybe it would help
Datadog,66357392,66316616,2,"2021/02/24, 21:12:06",False,"2021/02/25, 00:49:28",156.0,2676108.0,0,Old message:
Datadog,66357392,66316616,2,"2021/02/24, 21:12:06",False,"2021/02/25, 00:49:28",156.0,2676108.0,0,Never mind.
Datadog,66357392,66316616,2,"2021/02/24, 21:12:06",False,"2021/02/25, 00:49:28",156.0,2676108.0,0,"seems like my solution is only for express, graphql doesn't support that property"
Datadog,66357392,66316616,2,"2021/02/24, 21:12:06",False,"2021/02/25, 00:49:28",156.0,2676108.0,0,You probably want to just modify the validateStatus property in the http module:
Datadog,66357392,66316616,2,"2021/02/24, 21:12:06",False,"2021/02/25, 00:49:28",156.0,2676108.0,0,Callback function to determine if there was an error.
Datadog,66357392,66316616,2,"2021/02/24, 21:12:06",False,"2021/02/25, 00:49:28",156.0,2676108.0,0,It should take a status code as its only parameter and return true for success or false for errors
Datadog,66357392,66316616,2,"2021/02/24, 21:12:06",False,"2021/02/25, 00:49:28",156.0,2676108.0,0,https://datadoghq.dev/dd-trace-js/interfaces/plugins.http.html#validatestatus
Datadog,66357392,66316616,2,"2021/02/24, 21:12:06",False,"2021/02/25, 00:49:28",156.0,2676108.0,0,As an example you should be able to mark 403s as not be errors with something like this:
Datadog,66299116,66282520,0,"2021/02/21, 06:18:23",False,"2021/02/21, 06:18:23",493.0,244037.0,0,"Unfortunately, that doesn't seem to be a setting you can directly control at this time."
Datadog,66299116,66282520,0,"2021/02/21, 06:18:23",False,"2021/02/21, 06:18:23",493.0,244037.0,0,"The reasoning is that a given timeseries could be split by tag, so setting a single color for a timeseries split by tag would amount to multiple entires of the same color, and that wouldn't make sense."
Datadog,66299116,66282520,0,"2021/02/21, 06:18:23",False,"2021/02/21, 06:18:23",493.0,244037.0,0,"To support the semantic meaning, I've often used the following settings:"
Datadog,66299116,66282520,0,"2021/02/21, 06:18:23",False,"2021/02/21, 06:18:23",493.0,244037.0,0,"I'm not sure I know what distinction there is of Error vs Critical in your definition, but using these palettes has proven useful for my team."
Datadog,66299116,66282520,0,"2021/02/21, 06:18:23",False,"2021/02/21, 06:18:23",493.0,244037.0,0,"If you're looking for a specific widget to change color based on value - so if the number exceeds a threshold - take a look at the  Query Value Widget , as that can be customized to change color based on the current value."
Datadog,66299116,66282520,0,"2021/02/21, 06:18:23",False,"2021/02/21, 06:18:23",493.0,244037.0,0,"Alternately, if you have a Monitor already set for the timeseries, use the  Alert Value Widget  to show the current status, with less configuration, since the thresholds are managed in the Monitor's definition."
Datadog,66356118,66282520,0,"2021/02/24, 19:43:52",False,"2021/02/24, 19:43:52",156.0,2676108.0,0,"You can't set a color per line, but you can set the color per query."
Datadog,66356118,66282520,0,"2021/02/24, 19:43:52",False,"2021/02/24, 19:43:52",156.0,2676108.0,0,"If you edit the graph using json, that field  requests.style.palette  is exposed and you can just try typing in whatever color you want there."
Datadog,66356118,66282520,0,"2021/02/24, 19:43:52",False,"2021/02/24, 19:43:52",156.0,2676108.0,0,https://imgur.com/VrZZl72
Datadog,66356118,66282520,0,"2021/02/24, 19:43:52",False,"2021/02/24, 19:43:52",156.0,2676108.0,0,"If you want to have one time series that is green for hits, and one that is red for errors, you just make two metric queries, and then color one green and one red."
Datadog,66356118,66282520,0,"2021/02/24, 19:43:52",False,"2021/02/24, 19:43:52",156.0,2676108.0,0,https://imgur.com/AHGi1Hk
Datadog,66355760,66271979,0,"2021/02/24, 19:20:38",False,"2021/02/24, 19:20:38",156.0,2676108.0,0,"That endpoint is used to send events to Datadog, if your one account is sending them to the logs product, it is most likely because that account has a special flag that converts all your incoming events into logs."
Datadog,66355760,66271979,0,"2021/02/24, 19:20:38",False,"2021/02/24, 19:20:38",156.0,2676108.0,0,This is common for users that use the Security Monitoring product.
Datadog,66355760,66271979,0,"2021/02/24, 19:20:38",False,"2021/02/24, 19:20:38",156.0,2676108.0,0,I would recommend reaching out to support@datadoghq.com to see if this flag is enabled in that one account.
Datadog,66355760,66271979,0,"2021/02/24, 19:20:38",False,"2021/02/24, 19:20:38",156.0,2676108.0,0,"And if you want to replicate that behavior in another account, you can ask them to enable that feature."
Datadog,64847039,64847038,3,"2020/11/15, 18:43:14",False,"2020/11/15, 18:43:14",1072.0,252340.0,2,"In Datadog, create an API key in Integrations, APIs."
Datadog,64847039,64847038,3,"2020/11/15, 18:43:14",False,"2020/11/15, 18:43:14",1072.0,252340.0,2,Give the API key a name.
Datadog,64847039,64847038,3,"2020/11/15, 18:43:14",False,"2020/11/15, 18:43:14",1072.0,252340.0,2,In NLog.config create a target.
Datadog,64847039,64847038,3,"2020/11/15, 18:43:14",False,"2020/11/15, 18:43:14",1072.0,252340.0,2,The url is either datadoghq.com or datadoghq.eu (for europe).
Datadog,64847039,64847038,3,"2020/11/15, 18:43:14",False,"2020/11/15, 18:43:14",1072.0,252340.0,2,Now create a rule to write to the target and you are done!
Datadog,64847039,64847038,3,"2020/11/15, 18:43:14",False,"2020/11/15, 18:43:14",1072.0,252340.0,2,"All of the parameters can be configured to become columns in Datadog, and/or facets to select on."
Datadog,64847039,64847038,3,"2020/11/15, 18:43:14",False,"2020/11/15, 18:43:14",1072.0,252340.0,2,"I am using a date parameter so that the date matches other logs, rather than displaying the built in date."
Datadog,64484665,64476688,0,"2020/10/22, 17:40:57",True,"2020/10/22, 17:40:57",156.0,2676108.0,0,The datadog agent you deployed has no power to run scripts or take action.
Datadog,64484665,64476688,0,"2020/10/22, 17:40:57",True,"2020/10/22, 17:40:57",156.0,2676108.0,0,It is purely a monitoring/data collection tool.
Datadog,64484665,64476688,0,"2020/10/22, 17:40:57",True,"2020/10/22, 17:40:57",156.0,2676108.0,0,However one of the things your monitors in the Datadog application can do is trigger events when they go into an alert state.
Datadog,64484665,64476688,0,"2020/10/22, 17:40:57",True,"2020/10/22, 17:40:57",156.0,2676108.0,0,"There are  lots of integrations : creating a ticket in Jira, posting a message to Slack, triggering an SNS topic."
Datadog,64484665,64476688,0,"2020/10/22, 17:40:57",True,"2020/10/22, 17:40:57",156.0,2676108.0,0,"What I recommend you try to do, is to create some kind of job or script that can be triggered externally, like a lambda function, or a jenkins job, or anything really."
Datadog,64484665,64476688,0,"2020/10/22, 17:40:57",True,"2020/10/22, 17:40:57",156.0,2676108.0,0,When the monitor goes off you can use a  webhook  to trigger that script to do whatever you define.
Datadog,64484665,64476688,0,"2020/10/22, 17:40:57",True,"2020/10/22, 17:40:57",156.0,2676108.0,0,Here is a blog post showing  how twilio sent out a text message by connecting their api to a webhook .
Datadog,64407940,64108531,1,"2020/10/18, 01:24:48",True,"2020/10/18, 01:24:48",493.0,244037.0,0,"In these kinds of checks, the response order matters, since the columns returned from the DB are going to be mapped back to the names specified in the YAML."
Datadog,64407940,64108531,1,"2020/10/18, 01:24:48",True,"2020/10/18, 01:24:48",493.0,244037.0,0,Reading the error message:
Datadog,64407940,64108531,1,"2020/10/18, 01:24:48",True,"2020/10/18, 01:24:48",493.0,244037.0,0,Error: postgres:953578488181a512 | (postgres.py:398) | non-numeric value  cldtx  for metric column  active_connections  of metric_prefix  postgresql
Datadog,64407940,64108531,1,"2020/10/18, 01:24:48",True,"2020/10/18, 01:24:48",493.0,244037.0,0,"We can see that the value of  cldtx  is being returned for the  active_connections  column, which in the YAML is declared as a gauge, and this is a string."
Datadog,64407940,64108531,1,"2020/10/18, 01:24:48",True,"2020/10/18, 01:24:48",493.0,244037.0,0,"The fix should be straightforward, by reordering the YAML, like so:"
Datadog,64407940,64108531,1,"2020/10/18, 01:24:48",True,"2020/10/18, 01:24:48",493.0,244037.0,0,"Alternately, if you want to keep the YAML ordered, change the query to:"
Datadog,63912117,63888511,0,"2020/09/16, 05:05:14",False,"2020/09/16, 05:05:14",2509.0,1190203.0,1,So I ended up just looking at the tests in the datadog terraform provider and noticing the query format they are testing.
Datadog,63912117,63888511,0,"2020/09/16, 05:05:14",False,"2020/09/16, 05:05:14",2509.0,1190203.0,1,It seems you need to specify a time range and also add in a comparison threshold that matches your critical alert threshold.
Datadog,63912117,63888511,0,"2020/09/16, 05:05:14",False,"2020/09/16, 05:05:14",2509.0,1190203.0,1,That was what was missing.
Datadog,63779195,63779194,0,"2020/09/07, 17:06:52",True,"2020/09/07, 17:06:52",95.0,285601.0,0,It is not possible no.
Datadog,63779195,63779194,0,"2020/09/07, 17:06:52",True,"2020/09/07, 17:06:52",95.0,285601.0,0,Confirmed with DD support.
Datadog,64995876,63779194,0,"2020/11/25, 00:36:55",False,"2020/12/02, 18:50:18",906.0,3758005.0,1,"Actually it is possible, but you need to put every json log into quotes (some prefix before each log will also work), so that Datadog agent will consider this as a 'text'."
Datadog,64995876,63779194,0,"2020/11/25, 00:36:55",False,"2020/12/02, 18:50:18",906.0,3758005.0,1,I.e.
Datadog,64995876,63779194,0,"2020/11/25, 00:36:55",False,"2020/12/02, 18:50:18",906.0,3758005.0,1,log.json  file should contain quoted logs:
Datadog,64995876,63779194,0,"2020/11/25, 00:36:55",False,"2020/12/02, 18:50:18",906.0,3758005.0,1,"After that, in Datadog Logs Configuration you need to add a pipeline with Grok parser filter  json  (see filter tab in  Matcher and Filter ):"
Datadog,64995876,63779194,0,"2020/11/25, 00:36:55",False,"2020/12/02, 18:50:18",906.0,3758005.0,1,This allowed me to perform full text search thru all fields in my json logs and automatically parse all json fields as attributes.
Datadog,64995876,63779194,0,"2020/11/25, 00:36:55",False,"2020/12/02, 18:50:18",906.0,3758005.0,1,P.S.
Datadog,64995876,63779194,0,"2020/11/25, 00:36:55",False,"2020/12/02, 18:50:18",906.0,3758005.0,1,This solution was provided by Datadog support 2 years ago.
Datadog,64995876,63779194,0,"2020/11/25, 00:36:55",False,"2020/12/02, 18:50:18",906.0,3758005.0,1,And seems they are working on solution to allow full text search for JSON logs.
Datadog,62747357,62739543,0,"2020/07/06, 02:24:28",True,"2020/07/06, 02:24:28",251.0,5477963.0,1,"It looks from the above snippets that the  combined  Morgan format is sent directly sent to Winston, and then parsed within a log pipeline in Datadog."
Datadog,62747357,62739543,0,"2020/07/06, 02:24:28",True,"2020/07/06, 02:24:28",251.0,5477963.0,1,"Since the  combined  format doesn't include the body and there is no built-in token for it, you would have to use a custom format with your own tokens and then update your pipeline accordingly."
Datadog,62747357,62739543,0,"2020/07/06, 02:24:28",True,"2020/07/06, 02:24:28",251.0,5477963.0,1,"For example, to create a custom format in Morgan that includes the status code and the body:"
Datadog,62747357,62739543,0,"2020/07/06, 02:24:28",True,"2020/07/06, 02:24:28",251.0,5477963.0,1,You can also create a token to achieve the same result with a simpler format definition:
Datadog,62747357,62739543,0,"2020/07/06, 02:24:28",True,"2020/07/06, 02:24:28",251.0,5477963.0,1,"You can find the documentation for custom Morgan formats  here , creating tokens  here , and Datadog log pipeline parsing  here ."
Datadog,62747357,62739543,0,"2020/07/06, 02:24:28",True,"2020/07/06, 02:24:28",251.0,5477963.0,1,Hope this helps!
Datadog,62228894,62094698,0,"2020/06/06, 10:54:35",False,"2020/06/06, 10:54:35",21.0,7314273.0,0,"Maybe you can ask them to add it by opening a feature request:
 https://github.com/DataDog/documentation/issues/new/choose"
Datadog,62008387,61971889,0,"2020/05/25, 21:21:06",True,"2020/05/25, 21:21:06",2771.0,842302.0,0,but it looks like I'm missing an important part here.
Datadog,62008387,61971889,0,"2020/05/25, 21:21:06",True,"2020/05/25, 21:21:06",2771.0,842302.0,0,That was the point.
Datadog,62008387,61971889,0,"2020/05/25, 21:21:06",True,"2020/05/25, 21:21:06",2771.0,842302.0,0,The lambda itself has not much todo with particular  statusCodes .
Datadog,62008387,61971889,0,"2020/05/25, 21:21:06",True,"2020/05/25, 21:21:06",2771.0,842302.0,0,So I either may log each status code and let datadog parse it accordingly.
Datadog,62008387,61971889,0,"2020/05/25, 21:21:06",True,"2020/05/25, 21:21:06",2771.0,842302.0,0,"Or, that's the solution I went for, I can leverage API-Gateway for monitoring status codes per lambda."
Datadog,61822822,61817555,1,"2020/05/15, 18:27:58",False,"2020/05/15, 18:27:58",1371.0,5540166.0,2,"You can use most any of the common open source log shippers to send server logs to Datadog without using the Datadog agent, for example  fluentd ."
Datadog,61822822,61817555,1,"2020/05/15, 18:27:58",False,"2020/05/15, 18:27:58",1371.0,5540166.0,2,"But there can be several benefits to using the Datadog agent to collect server logs, such as:"
Datadog,61822822,61817555,1,"2020/05/15, 18:27:58",False,"2020/05/15, 18:27:58",1371.0,5540166.0,2,"There are other ways to collect logs in Datadog, among those is the  HTTP API ."
Datadog,61822822,61817555,1,"2020/05/15, 18:27:58",False,"2020/05/15, 18:27:58",1371.0,5540166.0,2,"Since this API uses a POST method, I bet you could configure Datadog's  webhook integration  to generate log events from Datadog events and alerts."
Datadog,61822822,61817555,1,"2020/05/15, 18:27:58",False,"2020/05/15, 18:27:58",1371.0,5540166.0,2,"That said, before you go through the trouble of doing this, if you have a use-case or reason you're interested in doing this, you may want to reach out to Datadog support to see if they have some features coming / in beta that would get you what you want without the extra work on your end."
Datadog,61822822,61817555,1,"2020/05/15, 18:27:58",False,"2020/05/15, 18:27:58",1371.0,5540166.0,2,(What  is  your use-case?
Datadog,61822822,61817555,1,"2020/05/15, 18:27:58",False,"2020/05/15, 18:27:58",1371.0,5540166.0,2,I'm curious)
Datadog,61513379,61511271,1,"2020/04/30, 02:25:23",True,"2020/04/30, 02:25:23",1371.0,5540166.0,1,"Sounds like some of your  http.server.requests.count  metric values do not have any  status  tag, so when you group by the  status  tag, those are being aggregaed with a value of  n/a ."
Datadog,61513379,61511271,1,"2020/04/30, 02:25:23",True,"2020/04/30, 02:25:23",1371.0,5540166.0,1,"If it is intentional/expected that this metric would have values without the  status  tag and you just want to ignored those metric values, then you can use the  exclude_null()  function to remove that tag grouping from your graph (docs  here )."
Datadog,61513379,61511271,1,"2020/04/30, 02:25:23",True,"2020/04/30, 02:25:23",1371.0,5540166.0,1,"If it is not intentional/expected that this metric would have values without the  status  tag, then you probably want to reach out to support@datadoghq.com to get that looked into."
Datadog,62812732,61405563,0,"2020/07/09, 13:23:09",False,"2020/07/09, 13:23:09",877.0,1570636.0,0,Just add this in the startup.cs.
Datadog,65504306,61405563,0,"2020/12/30, 10:52:27",False,"2020/12/30, 10:52:27",365.0,5666309.0,0,I'm very sorry for this late answer.
Datadog,65504306,61405563,0,"2020/12/30, 10:52:27",False,"2020/12/30, 10:52:27",365.0,5666309.0,0,"After exchanging a bunch of emails with the Datadog support guys, it turned out that the solution is straightforward:"
Datadog,65504306,61405563,0,"2020/12/30, 10:52:27",False,"2020/12/30, 10:52:27",365.0,5666309.0,0,The  Dispose()  method force the sinks to gracefully close up and send the logs stored in cache.
Datadog,65504306,61405563,0,"2020/12/30, 10:52:27",False,"2020/12/30, 10:52:27",365.0,5666309.0,0,Please note that the logs do not appear instantly in the Datadog console: allow a few seconds (to some minutes) for their systems to process the logs you send.
Datadog,61225485,61191225,0,"2020/04/15, 12:32:12",False,"2020/04/15, 12:32:12",21.0,7314273.0,1,"You might want to use datadog service-check which is also can be sent by the StatsDClient, and then add it to your monitor/dashboard page."
Datadog,61225485,61191225,0,"2020/04/15, 12:32:12",False,"2020/04/15, 12:32:12",21.0,7314273.0,1,https://docs.datadoghq.com/developers/service_checks/
Datadog,61232317,61191225,0,"2020/04/15, 18:18:06",False,"2020/04/15, 18:18:06",1371.0,5540166.0,0,"Another option is to use  Datadog's Java / JMX integration  to capture the health data that's exposed over JMX -- this can probably give you up/down health data, and can certainly give you a lot more granular health metrics too."
Datadog,60829558,60828990,1,"2020/03/24, 12:51:21",True,"2020/03/24, 18:07:20",3151.0,4409319.0,3,"Well, apparently you can  -@facet:*"
Datadog,60829558,60828990,1,"2020/03/24, 12:51:21",True,"2020/03/24, 18:07:20",3151.0,4409319.0,3,"Didn't specify it in my question because it was not important, but what I really needed was a way to either  filter by a specific facet value, or get logs without said facet"
Datadog,60829558,60828990,1,"2020/03/24, 12:51:21",True,"2020/03/24, 18:07:20",3151.0,4409319.0,3,The following works for me:
Datadog,60754931,60715172,4,"2020/03/19, 12:10:42",False,"2020/03/19, 12:10:42",37971.0,1237575.0,0,Spring is scanning your classpath that seems incomplete.
Datadog,60754931,60715172,4,"2020/03/19, 12:10:42",False,"2020/03/19, 12:10:42",37971.0,1237575.0,0,Maybe it is related to Spring's class loading mechanisms.
Datadog,60754931,60715172,4,"2020/03/19, 12:10:42",False,"2020/03/19, 12:10:42",37971.0,1237575.0,0,The  class in question  exists and seems to be part of the agent.
Datadog,60754931,60715172,4,"2020/03/19, 12:10:42",False,"2020/03/19, 12:10:42",37971.0,1237575.0,0,"Possibly, you are using an outdated version of the agent."
Datadog,60660835,60615996,1,"2020/03/12, 21:36:18",False,"2020/03/12, 21:36:18",28832.0,281848.0,1,I'm not particularly familiar with this Datadog JSON format but the general pattern I would propose here has multiple steps:
Datadog,60660835,60615996,1,"2020/03/12, 21:36:18",False,"2020/03/12, 21:36:18",28832.0,281848.0,1,Here's a basic example of that to show the general principle.
Datadog,60660835,60615996,1,"2020/03/12, 21:36:18",False,"2020/03/12, 21:36:18",28832.0,281848.0,1,It will need more work to capture all of the details you included in your initial example.
Datadog,60660835,60615996,1,"2020/03/12, 21:36:18",False,"2020/03/12, 21:36:18",28832.0,281848.0,1,"The separate normalization step to build  local.screenboard  here isn't strictly necessary: you could instead put the same sort of normalization expressions (using  try  to set defaults for things that aren't set) directly inside the  resource ""datadog_screenboard""  block arguments if you wanted."
Datadog,60660835,60615996,1,"2020/03/12, 21:36:18",False,"2020/03/12, 21:36:18",28832.0,281848.0,1,"I prefer to treat normalization as a separate step because then this leaves a clear definition in the configuration for what we're expecting to find in the JSON and what default values we'll use for optional items, separate from defining how that result is then mapped onto the physical  datadog_screenboard  resource."
Datadog,60660835,60615996,1,"2020/03/12, 21:36:18",False,"2020/03/12, 21:36:18",28832.0,281848.0,1,I wasn't able to test the example above because I don't have a Datadog account.
Datadog,60660835,60615996,1,"2020/03/12, 21:36:18",False,"2020/03/12, 21:36:18",28832.0,281848.0,1,I'm sorry if there are minor typos/mistakes in it that lead to errors.
Datadog,60660835,60615996,1,"2020/03/12, 21:36:18",False,"2020/03/12, 21:36:18",28832.0,281848.0,1,"My hope was to show the general principle of mapping from a serialized data file to a resource rather than to give a ready-to-use solution, so I hope the above includes enough examples of different situations that you can see how to extend it for the remaining Datadog JSON features you want to support in this module."
Datadog,60660835,60615996,1,"2020/03/12, 21:36:18",False,"2020/03/12, 21:36:18",28832.0,281848.0,1,"If this JSON format is a interchange format formally documented by Datadog, it could make sense for Terraform's Datadog provider to have the option of accepting a single JSON string in this format as configuration, for easier exporting."
Datadog,60660835,60615996,1,"2020/03/12, 21:36:18",False,"2020/03/12, 21:36:18",28832.0,281848.0,1,"That may require changes to the Datadog provider itself, which is beyond what I can answer here but might be worth raising in the GitHub issues for that provider to streamline this use-case."
Datadog,64437131,60326963,0,"2020/10/20, 04:12:43",False,"2020/10/20, 04:12:43",11.0,7000092.0,0,I solved this by adding 'squashfs' to the list of filesystem types to be ignored by the datadog agent.
Datadog,64437131,60326963,0,"2020/10/20, 04:12:43",False,"2020/10/20, 04:12:43",11.0,7000092.0,0,Create a file  /etc/datadog-agent/conf.d/disk.d/conf.yaml :
Datadog,64437131,60326963,0,"2020/10/20, 04:12:43",False,"2020/10/20, 04:12:43",11.0,7000092.0,0,Restart datadog agent ( systemctl restart datadog-agent ).
Datadog,59757720,59757258,2,"2020/01/15, 20:38:32",False,"2020/01/15, 20:38:32",336.0,7243426.0,1,"If you have an ever increasing counter, you can use the a function called  rate ."
Datadog,59757720,59757258,2,"2020/01/15, 20:38:32",False,"2020/01/15, 20:38:32",336.0,7243426.0,1,You'll be able to select it with the  +  on the query line.
Datadog,59757720,59757258,2,"2020/01/15, 20:38:32",False,"2020/01/15, 20:38:32",336.0,7243426.0,1,"With that you'll be able to have a rate of increase per seconds, minutes or hours."
Datadog,59757720,59757258,2,"2020/01/15, 20:38:32",False,"2020/01/15, 20:38:32",336.0,7243426.0,1,"If you are looking to get a difference between the same metric but at another point in the past, you have a function called  timeshift  that could also help."
Datadog,59757720,59757258,2,"2020/01/15, 20:38:32",False,"2020/01/15, 20:38:32",336.0,7243426.0,1,This is also accessible with the small  +  on the right of the query line.
Datadog,59757720,59757258,2,"2020/01/15, 20:38:32",False,"2020/01/15, 20:38:32",336.0,7243426.0,1,"Finally, if you are looking at comparing two different metrics, you  have a button called  Advanced  that will enable you to write more complex queries such as a difference between two metrics."
Datadog,59177979,59177043,0,"2019/12/04, 16:10:33",False,"2019/12/04, 16:10:33",807.0,1506396.0,2,"I believe you are looking for  clusterName :
 https://github.com/helm/charts/blob/master/stable/datadog/values.yaml#L75"
Datadog,59177979,59177043,0,"2019/12/04, 16:10:33",False,"2019/12/04, 16:10:33",807.0,1506396.0,2,You can add it in your  values.yaml  under the  datadog  section like this:
Datadog,59178230,59177043,0,"2019/12/04, 16:24:18",False,"2019/12/04, 16:24:18",9281.0,3270785.0,2,refer below command
Datadog,59747217,59088171,0,"2020/01/15, 10:05:27",False,"2020/01/15, 10:05:27",111.0,5054074.0,0,"I am not a fan of helm, but you can accomplish this in 2 ways:"
Datadog,59747217,59088171,0,"2020/01/15, 10:05:27",False,"2020/01/15, 10:05:27",111.0,5054074.0,0,via env vars: make use of  DD_AC_EXCLUDE  variable to exclude the Redis containers: eg  DD_AC_EXCLUDE=name:prefix-redis
Datadog,59747217,59088171,0,"2020/01/15, 10:05:27",False,"2020/01/15, 10:05:27",111.0,5054074.0,0,"via a config map: mount an empty config map in  /etc/datadog-agent/conf.d/redisdb.d/ , below is an example where I renamed the  auto_conf.yaml  to  auto_conf.yaml.example ."
Datadog,59747217,59088171,0,"2020/01/15, 10:05:27",False,"2020/01/15, 10:05:27",111.0,5054074.0,0,alter the daemonset/deployment object:
Datadog,58695216,58694140,1,"2019/11/04, 16:13:25",False,"2019/11/04, 16:13:25",336.0,7243426.0,1,"On the bottom of the infrastructure list, you should see a link called ""JSON API permalink""."
Datadog,58695216,58694140,1,"2019/11/04, 16:13:25",False,"2019/11/04, 16:13:25",336.0,7243426.0,1,"If you  query it , this should give you a JSON of all your hosts with their agent version."
Datadog,58695216,58694140,1,"2019/11/04, 16:13:25",False,"2019/11/04, 16:13:25",336.0,7243426.0,1,You can then query it with a quick Python script.
Datadog,58647078,58641893,0,"2019/10/31, 17:55:11",False,"2019/10/31, 17:55:11",14987.0,90580.0,0,"Your metrics should have a common prefix like  myapp.metric1 ,  myapp.metric2 , etc."
Datadog,58647078,58641893,0,"2019/10/31, 17:55:11",False,"2019/10/31, 17:55:11",14987.0,90580.0,0,Then you can disable all metrics and enable explicitly all  myapp.
Datadog,58647078,58641893,0,"2019/10/31, 17:55:11",False,"2019/10/31, 17:55:11",14987.0,90580.0,0,*  metrics like so:
Datadog,58647078,58641893,0,"2019/10/31, 17:55:11",False,"2019/10/31, 17:55:11",14987.0,90580.0,0,application.properties:
Datadog,58647078,58641893,0,"2019/10/31, 17:55:11",False,"2019/10/31, 17:55:11",14987.0,90580.0,0,the  management.metrics.enable.&lt;your_custom_prefix&gt;  will enable all  &lt;your_custome_prefix&gt;.
Datadog,58647078,58641893,0,"2019/10/31, 17:55:11",False,"2019/10/31, 17:55:11",14987.0,90580.0,0,*  metrics.
Datadog,58647078,58641893,0,"2019/10/31, 17:55:11",False,"2019/10/31, 17:55:11",14987.0,90580.0,0,"If you want to enable some of the built-in core metrics again, for example reenabling  jvm."
Datadog,58647078,58641893,0,"2019/10/31, 17:55:11",False,"2019/10/31, 17:55:11",14987.0,90580.0,0,"* , you can do:"
Datadog,58647078,58641893,0,"2019/10/31, 17:55:11",False,"2019/10/31, 17:55:11",14987.0,90580.0,0,"I've created a sample project  in github  that disables core metrics, enables custom metrics, and  jvm."
Datadog,58647078,58641893,0,"2019/10/31, 17:55:11",False,"2019/10/31, 17:55:11",14987.0,90580.0,0,*  metrics and sends to Datadog.
Datadog,58689932,58607248,0,"2019/11/04, 10:54:36",False,"2019/11/04, 10:54:36",336.0,7243426.0,0,Not sure I fully grasp the issue.
Datadog,58689932,58607248,0,"2019/11/04, 10:54:36",False,"2019/11/04, 10:54:36",336.0,7243426.0,0,Here are some steps to collect your traces:
Datadog,58689932,58607248,0,"2019/11/04, 10:54:36",False,"2019/11/04, 10:54:36",336.0,7243426.0,0,"Just in case, more info on Open Tracing  here"
Datadog,58708338,58579323,0,"2019/11/05, 11:33:57",False,"2021/02/05, 00:01:35",4059.0,8763847.0,1,Seems like there is a typo in your command.
Datadog,58708338,58579323,0,"2019/11/05, 11:33:57",False,"2021/02/05, 00:01:35",4059.0,8763847.0,1,DD_DOGSTATD_NON_LOCAL_TRAFFIC   is used instead of  DD_DOGSTATSD_NON_LOCAL_TRAFFIC
Datadog,58708338,58579323,0,"2019/11/05, 11:33:57",False,"2021/02/05, 00:01:35",4059.0,8763847.0,1,I usually used the below command for testing with Datadog:
Datadog,58547485,58502004,6,"2019/10/24, 21:47:16",True,"2019/10/24, 21:47:16",336.0,7243426.0,1,Have you tried  composite monitors ?
Datadog,58547485,58502004,6,"2019/10/24, 21:47:16",True,"2019/10/24, 21:47:16",336.0,7243426.0,1,You should be able to combine your low CPU Credit monitor with another monitor that looks at events from RDS.
Datadog,58547485,58502004,6,"2019/10/24, 21:47:16",True,"2019/10/24, 21:47:16",336.0,7243426.0,1,Two monitors such as:
Datadog,58547485,58502004,6,"2019/10/24, 21:47:16",True,"2019/10/24, 21:47:16",336.0,7243426.0,1,A composite monitor: A &amp;&amp; !B
Datadog,58547485,58502004,6,"2019/10/24, 21:47:16",True,"2019/10/24, 21:47:16",336.0,7243426.0,1,(I hope my example makes sense)
Datadog,53935650,53222715,0,"2018/12/26, 20:21:57",True,"2018/12/26, 20:21:57",347.0,2650254.0,0,"so, I had to install a Datadog agent on ec2 instance and configure it to be able to access all mysql dbs and collect the mysql performance schema metrics."
Datadog,53935650,53222715,0,"2018/12/26, 20:21:57",True,"2018/12/26, 20:21:57",347.0,2650254.0,0,And surely with correct security groups for the ec2.
Datadog,53935650,53222715,0,"2018/12/26, 20:21:57",True,"2018/12/26, 20:21:57",347.0,2650254.0,0,I followed this  docs  and  this  and contacted the support.
Datadog,51871692,51707255,1,"2018/08/16, 10:24:11",True,"2018/08/16, 10:24:11",76.0,4481158.0,0,"If the Windows OS is D drive, the setting is installed in  D:\ProgramData\Datadog ."
Datadog,51871692,51707255,1,"2018/08/16, 10:24:11",True,"2018/08/16, 10:24:11",76.0,4481158.0,0,"Copying it to  C:\ProgramData\Datadog  will work, but I submitted an improvement request to Datadog Support."
Datadog,50785165,47436429,0,"2018/06/10, 18:22:21",True,"2018/06/10, 18:22:21",518.0,3605831.0,0,Yes it does have this functionality in the Audit Association entity.
Datadog,50785165,47436429,0,"2018/06/10, 18:22:21",True,"2018/06/10, 18:22:21",518.0,3605831.0,0,The entity stored with the blame_id in the Audit Log entity contains information regarding the user.
Datadog,50785165,47436429,0,"2018/06/10, 18:22:21",True,"2018/06/10, 18:22:21",518.0,3605831.0,0,The one with source_id contains information regarding the entity itself and thus the ID of the entity in the  fk  field.
Datadog,47294550,41578245,0,"2017/11/14, 22:22:41",False,"2017/11/14, 22:22:41",51.0,2454643.0,0,"Once the datadog is properly installed on your server, you can use the custom metric feature to let datadog read your query result into a custom metric and then use that metric to create a dashboard."
Datadog,47294550,41578245,0,"2017/11/14, 22:22:41",False,"2017/11/14, 22:22:41",51.0,2454643.0,0,You can find more on custom metric on datadog  here
Datadog,47294550,41578245,0,"2017/11/14, 22:22:41",False,"2017/11/14, 22:22:41",51.0,2454643.0,0,They work with yaml file so be cautious with the formatting of the yaml file that will hold your custom metric.
Datadog,66880008,66875990,0,"2021/03/31, 03:15:15",False,"2021/03/31, 03:15:15",3.0,1467883.0,0,It looks like Datadog uses zstd compression in order to compress its data before sending it:  https://github.com/DataDog/datadog-agent/blob/972c4caf3e6bc7fa877c4a761122aef88e748b48/pkg/util/compression/zlib.go
Datadog,66345892,66326300,0,"2021/02/24, 08:42:37",False,"2021/02/24, 08:55:54",3000.0,1335245.0,0,This is what I've got so far.
Datadog,66345892,66326300,0,"2021/02/24, 08:42:37",False,"2021/02/24, 08:55:54",3000.0,1335245.0,0,Everything but the  source .
Datadog,65834472,65829133,0,"2021/01/21, 21:29:00",False,"2021/01/21, 21:29:00",17806.0,119790.0,0,Need to use category-processor  https://docs.datadoghq.com/logs/processing/processors/?tab=ui#category-processor
Datadog,65834472,65829133,0,"2021/01/21, 21:29:00",False,"2021/01/21, 21:29:00",17806.0,119790.0,0,Example:
Datadog,65834472,65829133,0,"2021/01/21, 21:29:00",False,"2021/01/21, 21:29:00",17806.0,119790.0,0,"I didn't do enough research before posting this question, but the answer for anyone else looking."
Datadog,65770731,65766741,0,"2021/01/18, 10:05:03",True,"2021/01/18, 10:05:03",756.0,9629802.0,0,"You are using the DataDog configuration for the commercial  k6 Cloud service  ( k6 cloud ), not locally run k6 tests ( k6 run )."
Datadog,65770731,65766741,0,"2021/01/18, 10:05:03",True,"2021/01/18, 10:05:03",756.0,9629802.0,0,"test_run_id  is a concept in the cloud service, though it's also easy to emulate locally as a way to distinguish between test runs."
Datadog,65770731,65766741,0,"2021/01/18, 10:05:03",True,"2021/01/18, 10:05:03",756.0,9629802.0,0,"For local tests, you should enable the DataDog output by running k6 with  k6 run --out datadog script.js ."
Datadog,65770731,65766741,0,"2021/01/18, 10:05:03",True,"2021/01/18, 10:05:03",756.0,9629802.0,0,"I assume you did that, otherwise you wouldn't see any metrics in DataDog."
Datadog,65770731,65766741,0,"2021/01/18, 10:05:03",True,"2021/01/18, 10:05:03",756.0,9629802.0,0,"Then, you can use the  tags  option  to inject a unique extra tag for all metrics generated by a particular k6 run, so you can differentiate them in DataDog."
Datadog,65770731,65766741,0,"2021/01/18, 10:05:03",True,"2021/01/18, 10:05:03",756.0,9629802.0,0,For example:
Datadog,65770731,65766741,0,"2021/01/18, 10:05:03",True,"2021/01/18, 10:05:03",756.0,9629802.0,0,"Of course, you can choose any  key=value  combination, you are not restricted to  test_run_id ."
Datadog,66783806,65717686,0,"2021/03/24, 17:02:55",True,"2021/03/24, 17:02:55",56.0,10960667.0,1,Nothing is wrong there.
Datadog,66783806,65717686,0,"2021/03/24, 17:02:55",True,"2021/03/24, 17:02:55",56.0,10960667.0,1,DataDog conceals that the Kafka integration uses Dogstatsd under the hood.
Datadog,66783806,65717686,0,"2021/03/24, 17:02:55",True,"2021/03/24, 17:02:55",56.0,10960667.0,1,"When  use_dogstatsd: 'true  within /etc/datadog-agent/datadog.yaml is set, metrics do appear in DataDog webUI."
Datadog,66783806,65717686,0,"2021/03/24, 17:02:55",True,"2021/03/24, 17:02:55",56.0,10960667.0,1,If that option is not set the default Broker data is available via JMXFetch using  sudo -u dd-agent datadog-agent status  as also via  sudo -u dd-agent datadog-agent check kafka  but not in the webUI.
Datadog,65631462,65631278,1,"2021/01/08, 17:04:29",False,"2021/01/08, 17:04:29",1530.0,11866104.0,0,Based on the  doc  you can decide which one is good for your use case.
Datadog,65631462,65631278,1,"2021/01/08, 17:04:29",False,"2021/01/08, 17:04:29",1530.0,11866104.0,0,"Stackdriver is detailed as &quot;Monitoring, logging, and diagnostics for applications on Google Cloud Platform and AWS&quot;."
Datadog,65631462,65631278,1,"2021/01/08, 17:04:29",False,"2021/01/08, 17:04:29",1530.0,11866104.0,0,"Google Stackdriver provides powerful monitoring, logging, and diagnostics."
Datadog,65631462,65631278,1,"2021/01/08, 17:04:29",False,"2021/01/08, 17:04:29",1530.0,11866104.0,0,"It equips you with insight into the health, performance, and availability of cloud-powered applications, enabling you to find and fix issues faster."
Datadog,65631462,65631278,1,"2021/01/08, 17:04:29",False,"2021/01/08, 17:04:29",1530.0,11866104.0,0,"Grafana can be classified as a tool in the &quot;Monitoring Tools&quot; category, while Stackdriver is grouped under &quot;Cloud Monitoring&quot;."
Datadog,65631654,65629898,0,"2021/01/08, 17:15:24",False,"2021/01/08, 17:15:24",678.0,11584728.0,0,Answering my own question this might be helpful for others
Datadog,65631654,65629898,0,"2021/01/08, 17:15:24",False,"2021/01/08, 17:15:24",678.0,11584728.0,0,I had to set
Datadog,65631654,65629898,0,"2021/01/08, 17:15:24",False,"2021/01/08, 17:15:24",678.0,11584728.0,0,inside  /etc/datadog-agent/datadog.yaml  then I've created  python.d/conf.yaml  with the following configs
Datadog,65631654,65629898,0,"2021/01/08, 17:15:24",False,"2021/01/08, 17:15:24",678.0,11584728.0,0,Restart the agent with
Datadog,65631654,65629898,0,"2021/01/08, 17:15:24",False,"2021/01/08, 17:15:24",678.0,11584728.0,0,You can see your logs in the dashboard logs panel
Datadog,65403420,65320127,0,"2020/12/22, 06:52:25",False,"2020/12/22, 06:52:25",1.0,14869489.0,0,Window option available... Go to integration and  click on agents..
Datadog,65403420,65320127,0,"2020/12/22, 06:52:25",False,"2020/12/22, 06:52:25",1.0,14869489.0,0,There is an option available windows ( left side ).. Click on windows then u get link..... Just copy the link and paste it on ur windows Server.... Then datadog starts monitoring...
Datadog,65403420,65320127,0,"2020/12/22, 06:52:25",False,"2020/12/22, 06:52:25",1.0,14869489.0,0,It takes 5 minutes to monitor
Datadog,64776832,64772196,0,"2020/11/10, 23:11:22",False,"2020/11/10, 23:11:22",11561.0,970308.0,0,The MeterRegistry already has implemented how to send custom metrics (posting to DataDog) See the code  https://github.com/micrometer-metrics/micrometer/blob/master/implementations/micrometer-registry-datadog/src/main/java/io/micrometer/datadog/DatadogMeterRegistry.java#L133
Datadog,64776832,64772196,0,"2020/11/10, 23:11:22",False,"2020/11/10, 23:11:22",11561.0,970308.0,0,It appears like you are trying to add common tags to each metric.
Datadog,64776832,64772196,0,"2020/11/10, 23:11:22",False,"2020/11/10, 23:11:22",11561.0,970308.0,0,"Perhaps you shouldn't implement your own DataDog registry, and instead use the provided one to send the metrics and set the common tags via config:"
Datadog,64542859,64539546,0,"2020/10/26, 20:21:25",True,"2020/10/26, 20:21:25",156.0,2676108.0,1,I don't believe there is any way to graph the historical behavior of the SLI from an SLO.
Datadog,64542859,64539546,0,"2020/10/26, 20:21:25",True,"2020/10/26, 20:21:25",156.0,2676108.0,1,"The closest you could get would be to measure the underlying metric, so if you had  good events / bad events  you could display that percentage."
Datadog,64542859,64539546,0,"2020/10/26, 20:21:25",True,"2020/10/26, 20:21:25",156.0,2676108.0,1,But the calculation of how often that percentage is above or below a certain threshold would not be possible.
Datadog,64542859,64539546,0,"2020/10/26, 20:21:25",True,"2020/10/26, 20:21:25",156.0,2676108.0,1,I recommend reaching out to support@datadoghq.com to let them know it's a feature you're interested in.
Datadog,64542859,64539546,0,"2020/10/26, 20:21:25",True,"2020/10/26, 20:21:25",156.0,2676108.0,1,They might be able to provide some updates.
Datadog,64907275,64453730,0,"2020/11/19, 09:38:41",False,"2020/11/19, 09:38:41",21.0,7314273.0,0,Not sure if this will work but you can give it a try..:
Datadog,64907275,64453730,0,"2020/11/19, 09:38:41",False,"2020/11/19, 09:38:41",21.0,7314273.0,0,{{^is_exact_match a.value b.value }}
Datadog,64907275,64453730,0,"2020/11/19, 09:38:41",False,"2020/11/19, 09:38:41",21.0,7314273.0,0,"@my@mail.com
Alert 2 hosts has passed the threshold"
Datadog,64907275,64453730,0,"2020/11/19, 09:38:41",False,"2020/11/19, 09:38:41",21.0,7314273.0,0,{{/is_exact_match}}
Datadog,64907275,64453730,0,"2020/11/19, 09:38:41",False,"2020/11/19, 09:38:41",21.0,7314273.0,0,same value - ignore - do nothing
Datadog,64907275,64453730,0,"2020/11/19, 09:38:41",False,"2020/11/19, 09:38:41",21.0,7314273.0,0,The problem is that you probably might get 2 alerts at the same time...
Datadog,64396016,64345103,0,"2020/10/16, 23:28:57",True,"2020/10/16, 23:28:57",318.0,1253272.0,0,It turns out that trace id can be set via HTTP endpoint  https://docs.datadoghq.com/api/v1/tracing/#send-traces .
Datadog,64396016,64345103,0,"2020/10/16, 23:28:57",True,"2020/10/16, 23:28:57",318.0,1253272.0,0,There doesn't seem to be an option for sending traces to the agent directly.
Datadog,64396016,64345103,0,"2020/10/16, 23:28:57",True,"2020/10/16, 23:28:57",318.0,1253272.0,0,"This can still be useful if the performance penalty of making HTTP calls is not a concern, i.e., if you are not working on a real-time system."
Datadog,63971375,63909996,0,"2020/09/19, 20:06:40",True,"2020/09/19, 20:06:40",3852.0,1601506.0,1,"Datadog keeps the logs for a period of time according to the billing plan you've selected:  https://www.datadoghq.com/pricing/#section-log  If you choose the 7 day plan, logs will be dropped from Datadog after 7 days."
Datadog,63971375,63909996,0,"2020/09/19, 20:06:40",True,"2020/09/19, 20:06:40",3852.0,1601506.0,1,"The default plan seems to be 15 days, but there are other options between 3-60 days."
Datadog,63668170,63603260,0,"2020/08/31, 12:25:38",True,"2020/08/31, 12:25:38",2771.0,842302.0,0,I've solved it now by verifying the status via code and by adding tags to the metrics:
Datadog,63668170,63603260,0,"2020/08/31, 12:25:38",True,"2020/08/31, 12:25:38",2771.0,842302.0,0,This way I can filter in my dashboard for  occurrence:first  only.
Datadog,63682403,63603260,0,"2020/09/01, 09:21:17",False,"2020/09/01, 09:21:17",336.0,7243426.0,0,"To make sure things are clear, you have a metric called  myService.errorType  with a tag  entity ."
Datadog,63682403,63603260,0,"2020/09/01, 09:21:17",False,"2020/09/01, 09:21:17",336.0,7243426.0,0,This metric is a counter that will increase every time an entity is in error.
Datadog,63682403,63603260,0,"2020/09/01, 09:21:17",False,"2020/09/01, 09:21:17",336.0,7243426.0,0,You will then use this metric query:
Datadog,63682403,63603260,0,"2020/09/01, 09:21:17",False,"2020/09/01, 09:21:17",336.0,7243426.0,0,"When you speak about UUID, it seems that the cardinality is small (here you show 3)."
Datadog,63682403,63603260,0,"2020/09/01, 09:21:17",False,"2020/09/01, 09:21:17",336.0,7243426.0,0,Which means that every hour you will have small amount of UUID available.
Datadog,63682403,63603260,0,"2020/09/01, 09:21:17",False,"2020/09/01, 09:21:17",336.0,7243426.0,0,"In that case, adding UUID to the metric tags is not as critical as user ID, timestamp, etc."
Datadog,63682403,63603260,0,"2020/09/01, 09:21:17",False,"2020/09/01, 09:21:17",336.0,7243426.0,0,which have a limitless number of options.
Datadog,63682403,63603260,0,"2020/09/01, 09:21:17",False,"2020/09/01, 09:21:17",336.0,7243426.0,0,"I would invite you to add this uuid tag, and check the cardinality in the  metric summary page  to ensure it works."
Datadog,63682403,63603260,0,"2020/09/01, 09:21:17",False,"2020/09/01, 09:21:17",336.0,7243426.0,0,"Then to get the number of UUID concerned by errors, you can use something like:"
Datadog,63682403,63603260,0,"2020/09/01, 09:21:17",False,"2020/09/01, 09:21:17",336.0,7243426.0,0,"Finally, as an alternative, if the cardinality of UUID can go through the roof, I would invite you to work with logs or work with Christopher's solution which seems to limit the cardinality increase as well."
Datadog,64256562,63599025,0,"2020/10/08, 08:55:02",False,"2020/10/08, 08:55:02",537.0,3134333.0,0,You may need to set the environment variable  DD_APM_NON_LOCAL_TRAFFIC=true  in your datadog agent container.
Datadog,64256562,63599025,0,"2020/10/08, 08:55:02",False,"2020/10/08, 08:55:02",537.0,3134333.0,0,Ref:  https://docs.datadoghq.com/agent/docker/apm/?tab=linux#docker-apm-agent-environment-variables
Datadog,63606465,63326724,0,"2020/08/27, 01:15:40",True,"2020/08/27, 01:15:40",797.0,2135.0,0,Allowing 'triggered' (but not 'recovery') monitor notifications is not a configurable option for the integration on either Opsgenie or Datadog.
Datadog,63606465,63326724,0,"2020/08/27, 01:15:40",True,"2020/08/27, 01:15:40",797.0,2135.0,0,"You  can , however, control this within the Datadog Monitor message body where you reference opsgenie"
Datadog,63606465,63326724,0,"2020/08/27, 01:15:40",True,"2020/08/27, 01:15:40",797.0,2135.0,0,Lorem ipsum dolor sit amet @opsgenie-oncall @slack-somechannel
Datadog,63606465,63326724,0,"2020/08/27, 01:15:40",True,"2020/08/27, 01:15:40",797.0,2135.0,0,"You can wrap the opsgenie reference within the message body with conditional tags (datadog actually calls them variables) documented here:
 https://docs.datadoghq.com/monitors/notifications/?tab=is_alert#conditional-variables 
Then the message body might look something like this:"
Datadog,63606465,63326724,0,"2020/08/27, 01:15:40",True,"2020/08/27, 01:15:40",797.0,2135.0,0,Lorem ipsum dolor sit amet {{#is_alert}}@opsgenie-oncall{{/is_alert}} @slack-somechannel
Datadog,63606465,63326724,0,"2020/08/27, 01:15:40",True,"2020/08/27, 01:15:40",797.0,2135.0,0,Now the alert to opsgenie will only occur on the 'trigger' of the monitor but not on 'recovery'
Datadog,62698566,62667694,0,"2020/07/02, 17:24:10",False,"2020/07/02, 17:24:10",251.0,5477963.0,1,"If you are sending metrics to an actual StatsD server, then tags are not supported by the protocol."
Datadog,62698566,62667694,0,"2020/07/02, 17:24:10",False,"2020/07/02, 17:24:10",251.0,5477963.0,1,You would need to instead send the metrics to the Datadog agent's DogStatsD endpoint which extends StatsD with additional features such as tags.
Datadog,62698566,62667694,0,"2020/07/02, 17:24:10",False,"2020/07/02, 17:24:10",251.0,5477963.0,1,You can find more information about DogStatsD  here .
Datadog,62698566,62667694,0,"2020/07/02, 17:24:10",False,"2020/07/02, 17:24:10",251.0,5477963.0,1,"If you are already using the DogStatsD endpoint, then I would suspect an incompatibility with the  node-statsd  library."
Datadog,62698566,62667694,0,"2020/07/02, 17:24:10",False,"2020/07/02, 17:24:10",251.0,5477963.0,1,"The library has not been updated for 6 years and it's possible that something changed since then, causing it to no longer work."
Datadog,62698566,62667694,0,"2020/07/02, 17:24:10",False,"2020/07/02, 17:24:10",251.0,5477963.0,1,In that case I would recommend switching to a more recent DogStatsD client that is still maintained such as  hot-shots .
Datadog,62698566,62667694,0,"2020/07/02, 17:24:10",False,"2020/07/02, 17:24:10",251.0,5477963.0,1,Hope this helps!
Datadog,62476112,62474317,0,"2020/06/19, 21:03:23",False,"2020/06/19, 21:03:23",1371.0,5540166.0,0,There's a 2-click path from Slack that should already do this for you out-of-the-box.
Datadog,62476112,62474317,0,"2020/06/19, 21:03:23",False,"2020/06/19, 21:03:23",1371.0,5540166.0,0,"The slack notification gives you a link to the alert event in your Datadog account (click-1), and from the alert event, towards the bottom you'll find a series of links to other relevant places, one of those is ""Related Logs"" (click-2)."
Datadog,62476112,62474317,0,"2020/06/19, 21:03:23",False,"2020/06/19, 21:03:23",1371.0,5540166.0,0,"That brings you to the Log Explorer scoped to the relevant time period of the alert, and scoped to the tags of whatever it was that was alerted on (so presumably the logs you're looking for)."
Datadog,62476112,62474317,0,"2020/06/19, 21:03:23",False,"2020/06/19, 21:03:23",1371.0,5540166.0,0,"If you want to add a link of this sort as something you can configure in the alert message, that sounds like something you should reach out to support@datadoghq.com for to ask Datadog to implement it."
Datadog,62672749,62474317,0,"2020/07/01, 11:29:39",False,"2020/07/01, 11:29:39",1.0,3327131.0,0,In the end we solved the problem dynamically building the url for the logs:
Datadog,62301304,62300720,2,"2020/06/10, 13:27:51",False,"2020/06/10, 13:27:51",12179.0,1032890.0,0,There's a Prometheus endpoint for Tibco EMS:
Datadog,62301304,62300720,2,"2020/06/10, 13:27:51",False,"2020/06/10, 13:27:51",12179.0,1032890.0,0,https://community.tibco.com/wiki/statistics-logger-tibco-enterprise-message-service#toc-15
Datadog,62301304,62300720,2,"2020/06/10, 13:27:51",False,"2020/06/10, 13:27:51",12179.0,1032890.0,0,"I think you can then add the prometheus integration to your datadog agent to send the data do datadog, and build your own dashboard for that:"
Datadog,62301304,62300720,2,"2020/06/10, 13:27:51",False,"2020/06/10, 13:27:51",12179.0,1032890.0,0,https://docs.datadoghq.com/integrations/prometheus/#data-collected
Datadog,62228982,62221212,3,"2020/06/06, 11:04:38",False,"2020/06/06, 11:04:38",21.0,7314273.0,1,"I had something similar issue and chose the first option, but i won't say it is from terraform perspective (since i also had lack in experience in terraform)."
Datadog,62228982,62221212,3,"2020/06/06, 11:04:38",False,"2020/06/06, 11:04:38",21.0,7314273.0,1,"The first hierarchy was more reasonable in segregation aspects, plus would be easier to add/remove/update organizations by demand."
Datadog,62016448,61888648,0,"2020/05/26, 10:07:33",False,"2020/05/26, 10:07:33",744.0,10300113.0,0,Answer from Datadog Support to this:
Datadog,62016448,61888648,0,"2020/05/26, 10:07:33",False,"2020/05/26, 10:07:33",744.0,10300113.0,0,Thanks again for reaching out to Datadog!
Datadog,62016448,61888648,0,"2020/05/26, 10:07:33",False,"2020/05/26, 10:07:33",744.0,10300113.0,0,"From looking further into this, there does not seem to be a way we can package the JDBC  driver with the Datadog Agent."
Datadog,62016448,61888648,0,"2020/05/26, 10:07:33",False,"2020/05/26, 10:07:33",744.0,10300113.0,0,I understand that this is not desirable as you would prefer to use a standard image but I believe the best way to have these bundled together would be to have a custom image for your deployment.
Datadog,62016448,61888648,0,"2020/05/26, 10:07:33",False,"2020/05/26, 10:07:33",744.0,10300113.0,0,Apologies for any inconveniences that this may cause.
Datadog,61015284,61010490,2,"2020/04/03, 18:30:55",False,"2020/04/03, 18:30:55",1371.0,5540166.0,3,"Yes, you can configure widgets to exclude results by tags."
Datadog,61015284,61010490,2,"2020/04/03, 18:30:55",False,"2020/04/03, 18:30:55",1371.0,5540166.0,3,You can do this by applying a tag prepended with a  !
Datadog,61015284,61010490,2,"2020/04/03, 18:30:55",False,"2020/04/03, 18:30:55",1371.0,5540166.0,3,"to signify ""not""."
Datadog,61015284,61010490,2,"2020/04/03, 18:30:55",False,"2020/04/03, 18:30:55",1371.0,5540166.0,3,"So in your case, you can set up your widget scoped over  importance:ignore  and then hit the little  &lt;/&gt;  button on the right to expose the underlying query, and sneak a  !"
Datadog,61015284,61010490,2,"2020/04/03, 18:30:55",False,"2020/04/03, 18:30:55",1371.0,5540166.0,3,in front to make it  !importance:ignore .
Datadog,61015284,61010490,2,"2020/04/03, 18:30:55",False,"2020/04/03, 18:30:55",1371.0,5540166.0,3,"This doc has a nice example  (although it's for notebooks, it works the same in dashboards as well)."
Datadog,61019569,60940393,0,"2020/04/03, 23:00:56",False,"2020/04/03, 23:00:56",1447.0,554481.0,0,"After talking with Datadog support, it seems like this is a known issue."
Datadog,61019569,60940393,0,"2020/04/03, 23:00:56",False,"2020/04/03, 23:00:56",1447.0,554481.0,0,Thanks for your patience while we looked into this issue.
Datadog,61019569,60940393,0,"2020/04/03, 23:00:56",False,"2020/04/03, 23:00:56",1447.0,554481.0,0,We we're currently investigating this along with PoolExecutors and will reach out with updates.
Datadog,61019569,60940393,0,"2020/04/03, 23:00:56",False,"2020/04/03, 23:00:56",1447.0,554481.0,0,"Right now it looks like those child spans within the async call lose context, so they appear disconnected."
Datadog,61019569,60940393,0,"2020/04/03, 23:00:56",False,"2020/04/03, 23:00:56",1447.0,554481.0,0,The workaround for now is to pass in the parent's context.
Datadog,61019569,60940393,0,"2020/04/03, 23:00:56",False,"2020/04/03, 23:00:56",1447.0,554481.0,0,Add this line just before calling the thread pool executor.
Datadog,61019569,60940393,0,"2020/04/03, 23:00:56",False,"2020/04/03, 23:00:56",1447.0,554481.0,0,Then pass that context to the function that gets run in the threadpool:
Datadog,61019569,60940393,0,"2020/04/03, 23:00:56",False,"2020/04/03, 23:00:56",1447.0,554481.0,0,And use it to create a span inside the function like this:
Datadog,61019569,60940393,0,"2020/04/03, 23:00:56",False,"2020/04/03, 23:00:56",1447.0,554481.0,0,The complete example looks like this:
Datadog,61019569,60940393,0,"2020/04/03, 23:00:56",False,"2020/04/03, 23:00:56",1447.0,554481.0,0,This will produce a result that looks like this:
Datadog,60381277,60369412,3,"2020/02/24, 19:41:28",False,"2020/02/24, 19:41:28",1371.0,5540166.0,0,First  you'll want to make sure your logs are well structured (which you can control in  Datadog's processing pipelines ).
Datadog,60381277,60369412,3,"2020/02/24, 19:41:28",False,"2020/02/24, 19:41:28",1371.0,5540166.0,0,"Effectively you'll want to parse out the ""code"" values into some ""error code"" attribute."
Datadog,60381277,60369412,3,"2020/02/24, 19:41:28",False,"2020/02/24, 19:41:28",1371.0,5540166.0,0,If your log events are in this format...
Datadog,60381277,60369412,3,"2020/02/24, 19:41:28",False,"2020/02/24, 19:41:28",1371.0,5540166.0,0,"...Then all you need is a fairly simple grok parser rule, thanks to the ""json"" filter function."
Datadog,60381277,60369412,3,"2020/02/24, 19:41:28",False,"2020/02/24, 19:41:28",1371.0,5540166.0,0,"Something like this would get you where you want (note the  %{data::json}  part, that's what parses the in-log JSON)."
Datadog,60381277,60369412,3,"2020/02/24, 19:41:28",False,"2020/02/24, 19:41:28",1371.0,5540166.0,0,"Once you've configured this, your logs will also have an attribute called ""error.code"" with a value of  2001  or  1001  or whatever."
Datadog,60381277,60369412,3,"2020/02/24, 19:41:28",False,"2020/02/24, 19:41:28",1371.0,5540166.0,0,Second  you'll want to  create a facet  for that new  error.code  attribute so that you can  make toplist / timeseries / etc.
Datadog,60381277,60369412,3,"2020/02/24, 19:41:28",False,"2020/02/24, 19:41:28",1371.0,5540166.0,0,"graphs grouped out by your ""error code"" facet ."
Datadog,60307539,60216953,0,"2020/02/19, 21:07:06",False,"2020/02/19, 21:07:06",571.0,2019978.0,0,"No, you cannot install the Datadog agent on a Snowflake host."
Datadog,60307539,60216953,0,"2020/02/19, 21:07:06",False,"2020/02/19, 21:07:06",571.0,2019978.0,0,We use our separate job scheduling system to monitor Snowflake by running queries (e.g.
Datadog,60307539,60216953,0,"2020/02/19, 21:07:06",False,"2020/02/19, 21:07:06",571.0,2019978.0,0,"checks on SYSTEM$CLUSTERING_DEPTH, aggregate queries against the QUERY_HISTORY for timing, etc) via the JDBC connector then relaying the results to our monitoring stack (similar to how Datadog agent would work.)"
Datadog,59096255,59092413,0,"2019/11/28, 23:19:23",True,"2019/11/28, 23:19:23",91617.0,354577.0,1,Point both of those at a service like  httpbin  to see how they differ.
Datadog,59096255,59092413,0,"2019/11/28, 23:19:23",True,"2019/11/28, 23:19:23",91617.0,354577.0,1,"Requests'  data  option for POST requests  generates form-encoded data  by default, while  curl  passes the JSON string through directly."
Datadog,59096255,59092413,0,"2019/11/28, 23:19:23",True,"2019/11/28, 23:19:23",91617.0,354577.0,1,You can manually encode your payload as a JSON string:
Datadog,59096255,59092413,0,"2019/11/28, 23:19:23",True,"2019/11/28, 23:19:23",91617.0,354577.0,1,or if you have Requests version 2.4.2 or later you can use the  json  parameter to have your  dict  converted to JSON automatically:
Datadog,58877543,58873873,2,"2019/11/15, 14:56:44",False,"2019/11/19, 14:49:29",336.0,7243426.0,0,One solution would be to setup in  logs &gt; configuration &gt; pipelines  a  category processor  to add a new attribute that could be made searchable.
Datadog,58877543,58873873,2,"2019/11/15, 14:56:44",False,"2019/11/19, 14:49:29",336.0,7243426.0,0,edit: 19th Nov 2019
Datadog,58877543,58873873,2,"2019/11/15, 14:56:44",False,"2019/11/19, 14:49:29",336.0,7243426.0,0,Step 1:
Datadog,58877543,58873873,2,"2019/11/15, 14:56:44",False,"2019/11/19, 14:49:29",336.0,7243426.0,0,Add grok parser to extract sign:
Datadog,58877543,58873873,2,"2019/11/15, 14:56:44",False,"2019/11/19, 14:49:29",336.0,7243426.0,0,"rule:  detect_dollar .*%{regex(""[$]+""):dollarSign}."
Datadog,58877543,58873873,2,"2019/11/15, 14:56:44",False,"2019/11/19, 14:49:29",336.0,7243426.0,0,*
Datadog,58877543,58873873,2,"2019/11/15, 14:56:44",False,"2019/11/19, 14:49:29",336.0,7243426.0,0,Step 2:
Datadog,58877543,58873873,2,"2019/11/15, 14:56:44",False,"2019/11/19, 14:49:29",336.0,7243426.0,0,Then you can setup a category processor as indicated above.
Datadog,58877543,58873873,2,"2019/11/15, 14:56:44",False,"2019/11/19, 14:49:29",336.0,7243426.0,0,This could look for the attribute  @dollarSign:$  and set the attribute  hasDollarSign  to True and set it to false otherwise.
Datadog,58877543,58873873,2,"2019/11/15, 14:56:44",False,"2019/11/19, 14:49:29",336.0,7243426.0,0,Step 3:
Datadog,58877543,58873873,2,"2019/11/15, 14:56:44",False,"2019/11/19, 14:49:29",336.0,7243426.0,0,Create a facet on  dollarSign  attribute.
Datadog,58877543,58873873,2,"2019/11/15, 14:56:44",False,"2019/11/19, 14:49:29",336.0,7243426.0,0,Following logs can then be searched for.
Datadog,58877543,58873873,2,"2019/11/15, 14:56:44",False,"2019/11/19, 14:49:29",336.0,7243426.0,0,For logs with no  $
Datadog,58877543,58873873,2,"2019/11/15, 14:56:44",False,"2019/11/19, 14:49:29",336.0,7243426.0,0,For logs with  $
Datadog,58877543,58873873,2,"2019/11/15, 14:56:44",False,"2019/11/19, 14:49:29",336.0,7243426.0,0,You can do the same with the  hasDollarSign  attribute and set it as a facet.
Datadog,58771357,58765810,1,"2019/11/08, 19:36:03",True,"2019/11/08, 19:36:03",336.0,7243426.0,1,I've tried with this query which is similar to yours:
Datadog,58771357,58765810,1,"2019/11/08, 19:36:03",True,"2019/11/08, 19:36:03",336.0,7243426.0,1,And this seems to give me a count by  dbinstanceidentifier  results.
Datadog,58771357,58765810,1,"2019/11/08, 19:36:03",True,"2019/11/08, 19:36:03",336.0,7243426.0,1,Do you have more information to provide?
Datadog,58771357,58765810,1,"2019/11/08, 19:36:03",True,"2019/11/08, 19:36:03",336.0,7243426.0,1,Maybe an event list and a monitor result screenshot?
Datadog,58689757,58630932,1,"2019/11/04, 10:41:42",True,"2019/11/04, 11:05:42",336.0,7243426.0,0,Side note : I also use this for Kafka (just as a reference) but it should not be required in your case:
Datadog,58268784,58267953,0,"2019/10/07, 14:34:45",False,"2019/10/07, 14:34:45",251.0,5477963.0,2,The main issue is that you would have to mount the volume in both the app container and the agent container in order to make it available.
Datadog,58268784,58267953,0,"2019/10/07, 14:34:45",False,"2019/10/07, 14:34:45",251.0,5477963.0,2,It also means you have to find a place to store the log file before it gets picked up by the agent.
Datadog,58268784,58267953,0,"2019/10/07, 14:34:45",False,"2019/10/07, 14:34:45",251.0,5477963.0,2,Doing this for every container could become difficult to maintain and time consuming.
Datadog,58268784,58267953,0,"2019/10/07, 14:34:45",False,"2019/10/07, 14:34:45",251.0,5477963.0,2,An alternative approach would be to instead send the logs to  stdout  and let the agent collect them with the Docker integration.
Datadog,58268784,58267953,0,"2019/10/07, 14:34:45",False,"2019/10/07, 14:34:45",251.0,5477963.0,2,"Since you configured  logsConfigContainerCollectAll  to  true , the agent is already configured to collect the logs from every container output, so configuring Winston to output to  stdout  should just work."
Datadog,58268784,58267953,0,"2019/10/07, 14:34:45",False,"2019/10/07, 14:34:45",251.0,5477963.0,2,See:  https://docs.datadoghq.com/agent/docker/log/
Datadog,58270801,58267953,1,"2019/10/07, 16:41:01",False,"2019/10/07, 16:41:01",336.0,7243426.0,2,"To support rochdev comment, here are a few code snippets to help out (if you do not opt in for the STDOUT method which should be simpler)."
Datadog,58270801,58267953,1,"2019/10/07, 16:41:01",False,"2019/10/07, 16:41:01",336.0,7243426.0,2,This is only to mount the right volume inside the container agent.
Datadog,58270801,58267953,1,"2019/10/07, 16:41:01",False,"2019/10/07, 16:41:01",336.0,7243426.0,2,"On your app deployment, add:"
Datadog,58270801,58267953,1,"2019/10/07, 16:41:01",False,"2019/10/07, 16:41:01",336.0,7243426.0,2,And on your agent daemonset:
Datadog,58270568,58266541,0,"2019/10/07, 16:28:46",False,"2019/10/07, 16:28:46",336.0,7243426.0,0,"At first glance, this seems to be a use case more suitable for the APM part of Datadog which would measure the execution time and could be  instrumented  to measure the execution time of the smaller functions (if it does not picked up this data automatically)."
Datadog,58270568,58266541,0,"2019/10/07, 16:28:46",False,"2019/10/07, 16:28:46",336.0,7243426.0,0,You can then create some nice charts with  Trace Search and Analytics .
Datadog,58270568,58266541,0,"2019/10/07, 16:28:46",False,"2019/10/07, 16:28:46",336.0,7243426.0,0,You could also use a custom metric with tags such as  opsize:large  and  opsize:small  which would represent the execution time (a gauge).
Datadog,58270568,58266541,0,"2019/10/07, 16:28:46",False,"2019/10/07, 16:28:46",336.0,7243426.0,0,You can find more details  here .
Datadog,58270568,58266541,0,"2019/10/07, 16:28:46",False,"2019/10/07, 16:28:46",336.0,7243426.0,0,"At the moment, the log module of Datadog does not seem to support the calculation you expect to see."
Datadog,58270568,58266541,0,"2019/10/07, 16:28:46",False,"2019/10/07, 16:28:46",336.0,7243426.0,0,"However, the two solutions above and the related logs can be made visible in a dashboard side by side."
Datadog,55956522,55953321,1,"2019/05/02, 19:18:34",False,"2019/05/02, 19:18:34",2574.0,4162641.0,0,Have you tried using the  start  and  end  tags with a 24 hour window?
Datadog,54938650,54920625,0,"2019/03/01, 07:49:07",True,"2019/03/01, 07:49:07",9.0,11043786.0,0,"As I posted the issue on GitHub, they give an answer the issue was in source code for dd-trace-php and they will fix and new release."
Datadog,54938650,54920625,0,"2019/03/01, 07:49:07",True,"2019/03/01, 07:49:07",9.0,11043786.0,0,https://github.com/DataDog/dd-trace-php/issues/334
Datadog,54938650,54920625,0,"2019/03/01, 07:49:07",True,"2019/03/01, 07:49:07",9.0,11043786.0,0,Below response of DatDog in github:
Datadog,54938650,54920625,0,"2019/03/01, 07:49:07",True,"2019/03/01, 07:49:07",9.0,11043786.0,0,"Ah now this is much more clear, thanks for sharing."
Datadog,54938650,54920625,0,"2019/03/01, 07:49:07",True,"2019/03/01, 07:49:07",9.0,11043786.0,0,This is a known problem that we are currently and actively working on.
Datadog,54938650,54920625,0,"2019/03/01, 07:49:07",True,"2019/03/01, 07:49:07",9.0,11043786.0,0,"As I cannot commit to that, the fix will be probably come out with the next release."
Datadog,54938650,54920625,0,"2019/03/01, 07:49:07",True,"2019/03/01, 07:49:07",9.0,11043786.0,0,"At an higher level, the cause is an issue we have in some specific cases while invoking private/protected methods and parent::* invocations."
Datadog,54938650,54920625,0,"2019/03/01, 07:49:07",True,"2019/03/01, 07:49:07",9.0,11043786.0,0,"In the meantime, if you are still interested in testing/using the other integrations, the only thing I can recommend is to disable the pdo integration:  fastcgi_param DD_INTEGRATIONS_DISABLED pdo ."
Datadog,54938650,54920625,0,"2019/03/01, 07:49:07",True,"2019/03/01, 07:49:07",9.0,11043786.0,0,"Again, the fix to this is currently in development and will be released very soon."
Datadog,53624609,53459133,0,"2018/12/05, 05:10:35",True,"2018/12/05, 05:10:35",3223.0,43973.0,1,Use the Query Value widget.
Datadog,53624609,53459133,0,"2018/12/05, 05:10:35",True,"2018/12/05, 05:10:35",3223.0,43973.0,1,"It can only show a single value, which is the average for the current time window that has been chosen."
Datadog,53383195,53382882,0,"2018/11/19, 23:55:24",False,"2018/11/19, 23:55:24",1371.0,5540166.0,1,Maybe you could mod the process check to also tag the process number metric by PID ( this is probly where you'd change that ).
Datadog,53383195,53382882,0,"2018/11/19, 23:55:24",False,"2018/11/19, 23:55:24",1371.0,5540166.0,1,That way you could group your monitor by your pid tag and the no-data alerts would tell you when the pid switched.
Datadog,53383195,53382882,0,"2018/11/19, 23:55:24",False,"2018/11/19, 23:55:24",1371.0,5540166.0,1,"But this would also alert on expected pid changes, so maybe you'd have to schedule downtimes too aggressively for this to be a good idea?"
Datadog,53383195,53382882,0,"2018/11/19, 23:55:24",False,"2018/11/19, 23:55:24",1371.0,5540166.0,1,Maybe monitoring some crash logs with  their Log Management tool  would be a better approach?
Datadog,53232465,53231545,0,"2018/11/09, 21:54:22",False,"2018/11/09, 21:54:22",1710.0,2779323.0,2,Lambda is serverless.
Datadog,53232465,53231545,0,"2018/11/09, 21:54:22",False,"2018/11/09, 21:54:22",1710.0,2779323.0,2,Datadog agent is for the host.
Datadog,53232465,53231545,0,"2018/11/09, 21:54:22",False,"2018/11/09, 21:54:22",1710.0,2779323.0,2,While running lambda you have absolutely no control over the host as you are not managing it.
Datadog,53232465,53231545,0,"2018/11/09, 21:54:22",False,"2018/11/09, 21:54:22",1710.0,2779323.0,2,"Hence, You can monitor application running on lambda using datadog integration of lambda for the different application."
Datadog,53232465,53231545,0,"2018/11/09, 21:54:22",False,"2018/11/09, 21:54:22",1710.0,2779323.0,2,You may follow below link for AWS Integration of datadog.
Datadog,53232465,53231545,0,"2018/11/09, 21:54:22",False,"2018/11/09, 21:54:22",1710.0,2779323.0,2,Ref:  https://docs.datadoghq.com/integrations/amazon_lambda/
Datadog,53400457,53226600,0,"2018/11/20, 21:44:45",False,"2018/11/20, 21:44:45",156.0,2676108.0,0,You can monitor a database from a different host as long as the host the agent is running on has access.
Datadog,53400457,53226600,0,"2018/11/20, 21:44:45",False,"2018/11/20, 21:44:45",156.0,2676108.0,0,So for this section in the config file:
Datadog,53400457,53226600,0,"2018/11/20, 21:44:45",False,"2018/11/20, 21:44:45",156.0,2676108.0,0,https://github.com/DataDog/integrations-core/blob/master/postgres/datadog_checks/postgres/data/conf.yaml.example#L4
Datadog,53400457,53226600,0,"2018/11/20, 21:44:45",False,"2018/11/20, 21:44:45",156.0,2676108.0,0,instead of using  localhost  you can set the IP.
Datadog,53400457,53226600,0,"2018/11/20, 21:44:45",False,"2018/11/20, 21:44:45",156.0,2676108.0,0,This will allow you to monitor your database without adding a new agent.
Datadog,53400457,53226600,0,"2018/11/20, 21:44:45",False,"2018/11/20, 21:44:45",156.0,2676108.0,0,Then you would just follow the normal  postrgres setup  and you will have access to all the metrics and  service checks  including  postgres.can_connect  which is probably what you care about.
Datadog,53114821,52311463,0,"2018/11/02, 10:10:01",False,"2018/11/02, 10:10:01",1976.0,43842.0,1,"It seems to be an intentional ""feature""  https://github.com/kamon-io/kamon-datadog/issues/19  introduced in 1.x."
Datadog,53114821,52311463,0,"2018/11/02, 10:10:01",False,"2018/11/02, 10:10:01",1976.0,43842.0,1,They have chosen approach to put service name in tag instead.
Datadog,51841340,51840681,1,"2018/08/14, 15:22:29",False,"2018/08/14, 15:22:29",1978.0,7217896.0,0,"Even though datadog is being run from the same machine, it is setting up a separate server on your machine."
Datadog,51841340,51840681,1,"2018/08/14, 15:22:29",False,"2018/08/14, 15:22:29",1978.0,7217896.0,0,"Because of that, it sounds like the datadog agent doesn't have access to your z:/ driver."
Datadog,51841340,51840681,1,"2018/08/14, 15:22:29",False,"2018/08/14, 15:22:29",1978.0,7217896.0,0,"Try to put the ""TaskResults"" folder in your root directory (when running from datadog - where the mycheck.yaml file is) and change the path accordingly."
Datadog,51841340,51840681,1,"2018/08/14, 15:22:29",False,"2018/08/14, 15:22:29",1978.0,7217896.0,0,"If this works and you still want to have a common drive to be able to share files from your computer to datadog's agent, you have to find a way to mount a drive\folder to the agent."
Datadog,51841340,51840681,1,"2018/08/14, 15:22:29",False,"2018/08/14, 15:22:29",1978.0,7217896.0,0,They probably have a way to do that in the  documentation
Datadog,51894917,51840681,0,"2018/08/17, 14:54:34",True,"2018/08/17, 14:54:34",410.0,831608.0,0,The solution to this is to create a file share on the network drive and use that path instead of the full network drive path.
Datadog,51894917,51840681,0,"2018/08/17, 14:54:34",True,"2018/08/17, 14:54:34",410.0,831608.0,0,May be obvious to some but it didn't occur to me right away since the normal Python code worked without any issue outside of Datadog.
Datadog,51894917,51840681,0,"2018/08/17, 14:54:34",True,"2018/08/17, 14:54:34",410.0,831608.0,0,So instead of:
Datadog,51894917,51840681,0,"2018/08/17, 14:54:34",True,"2018/08/17, 14:54:34",410.0,831608.0,0,use
Datadog,51603701,51377442,0,"2018/07/31, 02:52:59",False,"2018/07/31, 02:52:59",1474.0,10109833.0,0,Have a look at the documentation for  Dogstream .
Datadog,51603701,51377442,0,"2018/07/31, 02:52:59",False,"2018/07/31, 02:52:59",1474.0,10109833.0,0,It allows you to send metrics to datadog from log files (including summarised metrics).
Datadog,51603701,51377442,0,"2018/07/31, 02:52:59",False,"2018/07/31, 02:52:59",1474.0,10109833.0,0,You may need to write a custom parser for any data that is not in the datadog canonical format in order for datadog to recognize the data.
Datadog,51603701,51377442,0,"2018/07/31, 02:52:59",False,"2018/07/31, 02:52:59",1474.0,10109833.0,0,Checkout the example  here .
Datadog,50155727,50148430,3,"2018/05/03, 15:59:44",False,"2018/05/04, 19:46:07",11561.0,970308.0,0,This sounds like a bug.
Datadog,50155727,50148430,3,"2018/05/03, 15:59:44",False,"2018/05/04, 19:46:07",11561.0,970308.0,0,It is possible the Datadog exporter is running in a non-daemon thread.
Datadog,50155727,50148430,3,"2018/05/03, 15:59:44",False,"2018/05/04, 19:46:07",11561.0,970308.0,0,The JVM views non-daemon threads as application critical work.
Datadog,50155727,50148430,3,"2018/05/03, 15:59:44",False,"2018/05/04, 19:46:07",11561.0,970308.0,0,So essentially the JVM thinks it shouldn't shutdown until the non-daemon thread finishes.
Datadog,50155727,50148430,3,"2018/05/03, 15:59:44",False,"2018/05/04, 19:46:07",11561.0,970308.0,0,"In the case of the Datadog exporter thread, that probably won't happen."
Datadog,50155727,50148430,3,"2018/05/03, 15:59:44",False,"2018/05/04, 19:46:07",11561.0,970308.0,0,"To verify there are non-daemon threads, use  jstack  to generate a thread dump."
Datadog,50155727,50148430,3,"2018/05/03, 15:59:44",False,"2018/05/04, 19:46:07",11561.0,970308.0,0,(command:  jstack &lt;pid&gt; ) or dump all threads in your  close  method:
Datadog,50155727,50148430,3,"2018/05/03, 15:59:44",False,"2018/05/04, 19:46:07",11561.0,970308.0,0,An example thread dump output is below.
Datadog,50155727,50148430,3,"2018/05/03, 15:59:44",False,"2018/05/04, 19:46:07",11561.0,970308.0,0,Notice the word 'daemon' on the first line:
Datadog,49818706,49381672,0,"2018/04/13, 16:56:49",False,"2018/04/13, 16:56:49",939.0,2506172.0,3,Gauge metric  types will do the job here given that your query does not run more than once within 10 seconds.
Datadog,49818706,49381672,0,"2018/04/13, 16:56:49",False,"2018/04/13, 16:56:49",939.0,2506172.0,3,"If that is not the case, go for  count metric"
Datadog,49818706,49381672,0,"2018/04/13, 16:56:49",False,"2018/04/13, 16:56:49",939.0,2506172.0,3,"The flush interval in datadog by default is 10 seconds, if you use a  gauge metric  and the metric is reported more than once in a flush interval, datadog agent only sends the last value ignoring the previous ones."
Datadog,49818706,49381672,0,"2018/04/13, 16:56:49",False,"2018/04/13, 16:56:49",939.0,2506172.0,3,"For  count metric  in contrast, the agent sums up all the values reported in the flush interval."
Datadog,49818706,49381672,0,"2018/04/13, 16:56:49",False,"2018/04/13, 16:56:49",939.0,2506172.0,3,More details about flush interval  here .
Datadog,65533013,49381672,0,"2021/01/01, 21:35:13",False,"2021/01/01, 21:35:13",1718.0,555329.0,0,The best metric type would be a  histogram  metric.
Datadog,65533013,49381672,0,"2021/01/01, 21:35:13",False,"2021/01/01, 21:35:13",1718.0,555329.0,0,"This will take multiple values, and pre-aggregate them within a flush window, so you will be able to get things like min/max/sum/avg and various percentiles."
Datadog,65533013,49381672,0,"2021/01/01, 21:35:13",False,"2021/01/01, 21:35:13",1718.0,555329.0,0,If you run multiple times within a flush window:
Datadog,48946203,48922641,0,"2018/02/23, 12:55:16",True,"2018/02/23, 12:55:16",1365.0,4894399.0,0,"As I have mentioned in the comment, you are affected by  two pass model ."
Datadog,48946203,48922641,0,"2018/02/23, 12:55:16",True,"2018/02/23, 12:55:16",1365.0,4894399.0,0,You should remove the keys in the resource added to the end of the chef run or triggered by the DD cookbook resources invoked as the last one in the run.
Datadog,48946203,48922641,0,"2018/02/23, 12:55:16",True,"2018/02/23, 12:55:16",1365.0,4894399.0,0,"However, it may not work with all versions of DD cookbook."
Datadog,48946203,48922641,0,"2018/02/23, 12:55:16",True,"2018/02/23, 12:55:16",1365.0,4894399.0,0,"From few DD cookbook versions, it is possible to store keys in node's run state which is not written to the Chef server."
Datadog,48946203,48922641,0,"2018/02/23, 12:55:16",True,"2018/02/23, 12:55:16",1365.0,4894399.0,0,The above example is preferred solution to your issue.
Datadog,47003630,47003531,0,"2017/10/29, 20:17:50",True,"2017/10/29, 20:17:50",2010.0,6020610.0,0,I noticed that the API key of Datadog changes everytime whenever in close the site and open a new instance.
Datadog,47003630,47003531,0,"2017/10/29, 20:17:50",True,"2017/10/29, 20:17:50",2010.0,6020610.0,0,"so after entering new API key , the issue was solved"
Datadog,46985445,46984902,0,"2017/10/28, 04:36:56",True,"2017/10/28, 04:36:56",1371.0,5540166.0,2,Two approaches that may work:
Datadog,46985445,46984902,0,"2017/10/28, 04:36:56",True,"2017/10/28, 04:36:56",1371.0,5540166.0,2,"It looks like flink has an  HTTP connector  to send metrics to Datadog, which at first glance looks to send over the Datadog metrics API instead of dogstatsd."
Datadog,46985445,46984902,0,"2017/10/28, 04:36:56",True,"2017/10/28, 04:36:56",1371.0,5540166.0,2,"Dogstatsd is not very different from statsd otherwise, so it's often easy to modify statsd libraries to work for dogstatsd."
Datadog,46985445,46984902,0,"2017/10/28, 04:36:56",True,"2017/10/28, 04:36:56",1371.0,5540166.0,2,"This project on GitHub  seems to be such a project, and may come in handy."
Datadog,46572247,46449330,1,"2017/10/04, 21:57:04",True,"2017/10/04, 21:57:04",1371.0,5540166.0,2,"Well, you  could  use the Datadog API to script the creation of 20 unique dashboards that all share the same content, but with different hosts."
Datadog,46572247,46449330,1,"2017/10/04, 21:57:04",True,"2017/10/04, 21:57:04",1371.0,5540166.0,2,This is the part of the API docs that would help (with examples!)
Datadog,46572247,46449330,1,"2017/10/04, 21:57:04",True,"2017/10/04, 21:57:04",1371.0,5540166.0,2,"for Timeboards , and this one  for Screenboards ."
Datadog,46572247,46449330,1,"2017/10/04, 21:57:04",True,"2017/10/04, 21:57:04",1371.0,5540166.0,2,"That said, I'd personally find 20 dashboards a bit cluttered / unwieldy in my own Datadog account."
Datadog,46572247,46449330,1,"2017/10/04, 21:57:04",True,"2017/10/04, 21:57:04",1371.0,5540166.0,2,"Instead, if it was me, I'd try to (A) find clever uses of dashboard template variables (on e.g, cluster tags, host tags, etc."
Datadog,46572247,46449330,1,"2017/10/04, 21:57:04",True,"2017/10/04, 21:57:04",1371.0,5540166.0,2,"), or (B) group out by each host tag and apply  the ""top()"" function  in some way so that I'd be able to see just the most extreme-value hosts."
Datadog,46572247,46449330,1,"2017/10/04, 21:57:04",True,"2017/10/04, 21:57:04",1371.0,5540166.0,2,But that's certainly up to you :)
Datadog,45988104,45974396,0,"2017/08/31, 21:40:40",False,"2017/08/31, 21:40:40",148.0,6815223.0,0,Your answer appears to be there in the text -- you're missing a Python package.
Datadog,45988104,45974396,0,"2017/08/31, 21:40:40",False,"2017/08/31, 21:40:40",148.0,6815223.0,0,"Try running  sudo pip install psutil , then restarting the agent."
Datadog,45988104,45974396,0,"2017/08/31, 21:40:40",False,"2017/08/31, 21:40:40",148.0,6815223.0,0,"Can you add your agent version, OS and version, and how you installed the agent to your text as well?"
Datadog,45988104,45974396,0,"2017/08/31, 21:40:40",False,"2017/08/31, 21:40:40",148.0,6815223.0,0,It looks like you're also using a  very  old version of the agent (it's up to 5.17.
Datadog,45988104,45974396,0,"2017/08/31, 21:40:40",False,"2017/08/31, 21:40:40",148.0,6815223.0,0,* for a number of OS's) so there may be better package bundling or critical updates since v. 4.4.0.
Datadog,45988104,45974396,0,"2017/08/31, 21:40:40",False,"2017/08/31, 21:40:40",148.0,6815223.0,0,Try installing a newer version as well.
Datadog,45996096,45974396,8,"2017/09/01, 11:27:23",False,"2017/09/01, 12:04:48",1.0,8547108.0,-1,Please find the required
Datadog,45239352,45232107,0,"2017/07/21, 16:46:50",True,"2017/07/21, 16:46:50",10159.0,4907630.0,1,In the code you posted:
Datadog,45239352,45232107,0,"2017/07/21, 16:46:50",True,"2017/07/21, 16:46:50",10159.0,4907630.0,1,This seems to be creating an empty client object.
Datadog,45239352,45232107,0,"2017/07/21, 16:46:50",True,"2017/07/21, 16:46:50",10159.0,4907630.0,1,"Shouldn't you be creating a client with your keys using  datadog.NewClient(""..."", ""..."")  as in the first code snippet you posted?"
Datadog,45239352,45232107,0,"2017/07/21, 16:46:50",True,"2017/07/21, 16:46:50",10159.0,4907630.0,1,"Also, you should check the error returned as that will give you more hints to troubleshoot the issue:"
Datadog,45239352,45232107,0,"2017/07/21, 16:46:50",True,"2017/07/21, 16:46:50",10159.0,4907630.0,1,`
Datadog,44937902,44933565,0,"2017/07/06, 03:17:35",False,"2017/07/06, 03:26:50",1.0,6688988.0,0,The solution:
Datadog,44937902,44933565,0,"2017/07/06, 03:17:35",False,"2017/07/06, 03:26:50",1.0,6688988.0,0,docker container 0
Datadog,44937902,44933565,0,"2017/07/06, 03:17:35",False,"2017/07/06, 03:26:50",1.0,6688988.0,0,"#!/bin/sh 
    java -Djava.util.logging.config.file=logging.properties -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.rmi.port=9998 -Dcom.sun.management.jmxremote.port=9998 -Djava.rmi.server.hostname=$HOSTNAME -Dcom.sun.management.jmxremote.host=$HOSTNAME -Dcom.sun.management.jmxremote.local.only=false -jar /app/my-streams.jar"
Datadog,44937902,44933565,0,"2017/07/06, 03:17:35",False,"2017/07/06, 03:26:50",1.0,6688988.0,0,docker container 1
Datadog,44937902,44933565,0,"2017/07/06, 03:17:35",False,"2017/07/06, 03:26:50",1.0,6688988.0,0,way more stuff was done that is available from from stack overflow posts.
Datadog,44937902,44933565,0,"2017/07/06, 03:17:35",False,"2017/07/06, 03:26:50",1.0,6688988.0,0,but the above fixes the metrics finding error from datadog-agent.
Datadog,44937902,44933565,0,"2017/07/06, 03:17:35",False,"2017/07/06, 03:26:50",1.0,6688988.0,0,Here is how to run each component:
Datadog,44937902,44933565,0,"2017/07/06, 03:17:35",False,"2017/07/06, 03:26:50",1.0,6688988.0,0,"docker container 0 
* my-streams 
* spin up dependent services in tab 
** mvn clean package docker:build 
** docker-compose up"
Datadog,44937902,44933565,0,"2017/07/06, 03:17:35",False,"2017/07/06, 03:26:50",1.0,6688988.0,0,"docker container 1 
* docker build -t dd-agent-my-streams ."
Datadog,44937902,44933565,0,"2017/07/06, 03:17:35",False,"2017/07/06, 03:26:50",1.0,6688988.0,0,* docker run -v /var/run/docker.sock:/var/run/docker.sock:ro   -v /proc/:/host/proc/:ro   -v /sys/fs/cgroup/:/host/sys/fs/cgroup:ro -e LOG_LEVEL=DEBUG -e SD_BACKEND=docker --network=mystreams_default
Datadog,44937902,44933565,0,"2017/07/06, 03:17:35",False,"2017/07/06, 03:26:50",1.0,6688988.0,0,"ssh into docker container 1 to verify if metrics work 
* docker ps // to find the name of the container to log into 
* docker exec -it  /bin/bash 
root@904e6561cc97:/# service datadog-agent configcheck 
root@904e6561cc97:/# service datadog-agent jmx list_everything 
root@904e6561cc97:/# service datadog-agent jmx collect"
Datadog,44336405,44335719,4,"2017/06/02, 22:48:19",False,"2017/06/02, 22:48:19",1371.0,5540166.0,1,I think what you actually want is the metrics-query API endpoint?
Datadog,44336405,44335719,4,"2017/06/02, 22:48:19",False,"2017/06/02, 22:48:19",1371.0,5540166.0,1,http://docs.datadoghq.com/api/#metrics-query
Datadog,44336405,44335719,4,"2017/06/02, 22:48:19",False,"2017/06/02, 22:48:19",1371.0,5540166.0,1,There are also a few Node.JS libraries that may be able to handle this kind of metric querying for you:  http://docs.datadoghq.com/libraries/#community-node
Datadog,44034370,44034151,0,"2017/05/17, 23:37:34",True,"2017/05/17, 23:37:34",261.0,4172512.0,2,The recommendation of:
Datadog,44034370,44034151,0,"2017/05/17, 23:37:34",True,"2017/05/17, 23:37:34",261.0,4172512.0,2,"""Please don't include endlessly growing tags in your metrics, like timestamps or user ids."
Datadog,44034370,44034151,0,"2017/05/17, 23:37:34",True,"2017/05/17, 23:37:34",261.0,4172512.0,2,"Please limit each metric to 1000 tags."""
Datadog,44034370,44034151,0,"2017/05/17, 23:37:34",True,"2017/05/17, 23:37:34",261.0,4172512.0,2,Is more of a warning against using infinitely expanding values as they can drastically increase your custom metric usage.
Datadog,44034370,44034151,0,"2017/05/17, 23:37:34",True,"2017/05/17, 23:37:34",261.0,4172512.0,2,As mentioned in the following article:
Datadog,44034370,44034151,0,"2017/05/17, 23:37:34",True,"2017/05/17, 23:37:34",261.0,4172512.0,2,https://help.datadoghq.com/hc/en-us/articles/204271775-What-is-a-custom-metric-and-what-is-the-limit-on-the-number-of-custom-metrics-I-can-have-
Datadog,44034370,44034151,0,"2017/05/17, 23:37:34",True,"2017/05/17, 23:37:34",261.0,4172512.0,2,"""By default customers are allotted 100 custom metrics per host across their entire infrastructure rather than on a per-host basis."
Datadog,44034370,44034151,0,"2017/05/17, 23:37:34",True,"2017/05/17, 23:37:34",261.0,4172512.0,2,"For example if you were licensed for 3 hosts, you would have 300 custom metrics by default - these 300 metrics may be divided equally amongst each individual host, or all 300 metrics could be sent from a single host."""
Datadog,44034370,44034151,0,"2017/05/17, 23:37:34",True,"2017/05/17, 23:37:34",261.0,4172512.0,2,You will want to keep in mind when configuring your metrics/tags of your current allotment of custom metrics and any billing implications that may have.
Datadog,44034370,44034151,0,"2017/05/17, 23:37:34",True,"2017/05/17, 23:37:34",261.0,4172512.0,2,"That said, if having these tags is important to your team please reach out to support@datadoghq.com and we can sync up with the Sales Team to determine what is best for your team and use case."
Datadog,66357269,66354474,1,"2021/02/24, 21:03:12",False,"2021/02/24, 21:03:12",60467.0,86611.0,0,"There is a preview feature that allows you to graph your SNAT port usage and allocation, see:"
Datadog,66357269,66354474,1,"2021/02/24, 21:03:12",False,"2021/02/24, 21:03:12",60467.0,86611.0,0,https://docs.microsoft.com/en-us/azure/load-balancer/load-balancer-standard-diagnostics#how-do-i-check-my-snat-port-usage-and-allocation
Datadog,65396545,65396200,1,"2020/12/21, 18:34:57",False,"2020/12/21, 18:34:57",64.0,6668901.0,1,You can look in to other datadog kube metrics like kubernetes.replicas.available / total to alert if no of available - total &lt; 0.
Datadog,65396545,65396200,1,"2020/12/21, 18:34:57",False,"2020/12/21, 18:34:57",64.0,6668901.0,1,Same can be done or for daemonset pods also there is a specific metric exposed.
Datadog,65396545,65396200,1,"2020/12/21, 18:34:57",False,"2020/12/21, 18:34:57",64.0,6668901.0,1,"[Datadog docs-kube metrics][1]
[1]: https://docs.datadoghq.com/agent/kubernetes/data_collected/"
Datadog,64825884,64720852,0,"2020/11/13, 20:07:25",True,"2020/11/24, 23:46:04",177.0,872145.0,0,"An issue with IE11 is fixed in v1.26.1
See the fix here:  [RUMF-791] prevent IE11 performance entry error #633"
Datadog,66445632,64611252,0,"2021/03/02, 21:08:29",False,"2021/03/02, 21:08:29",2834.0,444794.0,0,Datadog's Ruby library keeps this info on the struct  Datadog.tracer.active_correlation .
Datadog,66445632,64611252,0,"2021/03/02, 21:08:29",False,"2021/03/02, 21:08:29",2834.0,444794.0,0,You can call  Datadog.tracer.active_correlation.trace_id  to grab the trace ID.
Datadog,62074774,62074299,0,"2020/05/29, 00:23:05",False,"2020/05/29, 00:23:05",156.0,2676108.0,0,"You probably want to be using the MySQL integration, and configure the 'custom queries' option:  https://docs.datadoghq.com/integrations/faq/how-to-collect-metrics-from-custom-mysql-queries"
Datadog,62074774,62074299,0,"2020/05/29, 00:23:05",False,"2020/05/29, 00:23:05",156.0,2676108.0,0,You can follow those instructions after you configure the base integration  https://docs.datadoghq.com/integrations/mysql/#pagetitle  (This will give you a lot of use metrics in addition to the custom queries you want to run)
Datadog,62074774,62074299,0,"2020/05/29, 00:23:05",False,"2020/05/29, 00:23:05",156.0,2676108.0,0,"As you mentioned, DogStatsD is a library you can import to whatever script or application in order to submit metrics."
Datadog,62074774,62074299,0,"2020/05/29, 00:23:05",False,"2020/05/29, 00:23:05",156.0,2676108.0,0,But it really isn't a common practice in the slightest to modify the underlying code of your database.
Datadog,62074774,62074299,0,"2020/05/29, 00:23:05",False,"2020/05/29, 00:23:05",156.0,2676108.0,0,"So instead it makes more sense to externally run a query on the database, take those results, and send them to datadog."
Datadog,62074774,62074299,0,"2020/05/29, 00:23:05",False,"2020/05/29, 00:23:05",156.0,2676108.0,0,You could totally write a python script or something to do this.
Datadog,62074774,62074299,0,"2020/05/29, 00:23:05",False,"2020/05/29, 00:23:05",156.0,2676108.0,0,"However the Datadog agent already has this capability built in, so it's probably easier to just use that."
Datadog,62074774,62074299,0,"2020/05/29, 00:23:05",False,"2020/05/29, 00:23:05",156.0,2676108.0,0,"I am also just assuming SQL refers to MySQL, there are other integration for things like SQL Server, and PostgreSQL, and pretty much every implementation of sql."
Datadog,62074774,62074299,0,"2020/05/29, 00:23:05",False,"2020/05/29, 00:23:05",156.0,2676108.0,0,"And the same pattern applies where you would configure the integration, and then add an extra line to the config file where you have the check run your queries."
Datadog,60749201,60748902,0,"2020/03/19, 02:12:25",False,"2020/03/19, 02:12:25",251.0,5477963.0,0,"There is  dogapi  which wraps the entire Datadog API and should be able to do the above use case, probably using a mix of  metric.query ,  infrastructure.search ,  search.query  and  monitor.getAll ."
Datadog,60749201,60748902,0,"2020/03/19, 02:12:25",False,"2020/03/19, 02:12:25",251.0,5477963.0,0,"For example, to get the list of monitors, it would look something like this:"
Datadog,60749201,60748902,0,"2020/03/19, 02:12:25",False,"2020/03/19, 02:12:25",251.0,5477963.0,0,Please keep in mind that I didn't test the above code.
Datadog,60749201,60748902,0,"2020/03/19, 02:12:25",False,"2020/03/19, 02:12:25",251.0,5477963.0,0,"If you need something that is not in the library, it should also be fairly easy to wrap the API directly since it's a simple HTTP endpoint."
Datadog,60749201,60748902,0,"2020/03/19, 02:12:25",False,"2020/03/19, 02:12:25",251.0,5477963.0,0,I hope this helps!
Datadog,55188839,55187016,0,"2019/03/15, 20:36:45",True,"2019/03/15, 20:36:45",46.0,2089382.0,3,"For each web application that you want to configure with a different  Datadog APM service name , you need to set the environment variable  DD_SERVICE_NAME ."
Datadog,55188839,55187016,0,"2019/03/15, 20:36:45",True,"2019/03/15, 20:36:45",46.0,2089382.0,3,"If they're all running under the same IIS process, that's not possible."
Datadog,55188839,55187016,0,"2019/03/15, 20:36:45",True,"2019/03/15, 20:36:45",46.0,2089382.0,3,"In IIS there's a feature named  Application Pool , which can be used to isolate multiple web applications by running them under different processes."
Datadog,55188839,55187016,0,"2019/03/15, 20:36:45",True,"2019/03/15, 20:36:45",46.0,2089382.0,3,The first thing you need to do is to create a separate application pool for each web application.
Datadog,55188839,55187016,0,"2019/03/15, 20:36:45",True,"2019/03/15, 20:36:45",46.0,2089382.0,3,"Once you're done with that, you can set a different  DD_SERVICE_NAME  for each application pool."
Datadog,55188839,55187016,0,"2019/03/15, 20:36:45",True,"2019/03/15, 20:36:45",46.0,2089382.0,3,The  command  to set an environment variable scoped to a specific application pool is
Datadog,55188839,55187016,0,"2019/03/15, 20:36:45",True,"2019/03/15, 20:36:45",46.0,2089382.0,3,"where  MyAppPool  is the name of the application pool, and  my-service  is the service name that you want to use for the Datadog APM."
Datadog,55188839,55187016,0,"2019/03/15, 20:36:45",True,"2019/03/15, 20:36:45",46.0,2089382.0,3,"After running the above command, you have to restart IIS for the changes to take effect:"
Datadog,55543982,55187016,3,"2019/04/06, 01:10:37",False,"2019/04/06, 01:10:37",16566.0,24231.0,2,"Starting with version 1.0 of Datadog's .NET Tracer, you can set most settings in your application's  app.config / web.config  file."
Datadog,55543982,55187016,3,"2019/04/06, 01:10:37",False,"2019/04/06, 01:10:37",16566.0,24231.0,2,"For example, to set  DD_SERVICE_NAME :"
Datadog,55543982,55187016,3,"2019/04/06, 01:10:37",False,"2019/04/06, 01:10:37",16566.0,24231.0,2,[Disclaimer: I am a Datadog employee]
Datadog,52066429,52066057,0,"2018/08/29, 00:28:16",False,"2018/08/29, 00:28:16",1371.0,5540166.0,0,Maybe you could use the Datadog respective dashboard apis to update your metric names in all dashboards?
Datadog,52066429,52066057,0,"2018/08/29, 00:28:16",False,"2018/08/29, 00:28:16",1371.0,5540166.0,0,Should theoretically be not too complicated to script out.
Datadog,52066429,52066057,0,"2018/08/29, 00:28:16",False,"2018/08/29, 00:28:16",1371.0,5540166.0,0,"https://docs.datadoghq.com/api/?lang=python#screenboards 
 https://docs.datadoghq.com/api/?lang=python#timeboards"
Datadog,50827960,50827869,0,"2018/06/13, 04:31:37",False,"2018/06/13, 04:31:37",470.0,951739.0,0,Use the  requests  library its a lot simpler
Datadog,50827960,50827869,0,"2018/06/13, 04:31:37",False,"2018/06/13, 04:31:37",470.0,951739.0,0,Generate a request header like this
Datadog,50827960,50827869,0,"2018/06/13, 04:31:37",False,"2018/06/13, 04:31:37",470.0,951739.0,0,Send the request like this
Datadog,50827969,50827869,0,"2018/06/13, 04:33:31",True,"2018/06/13, 04:33:31",631.0,9933041.0,1,"While searching through this  other issue , I found that all that is needed to fix this issue is to specify the API key and the application key within the URL."
Datadog,50827969,50827869,0,"2018/06/13, 04:33:31",True,"2018/06/13, 04:33:31",631.0,9933041.0,1,Consider the following.
Datadog,50957805,50460608,0,"2018/06/21, 01:13:11",True,"2018/06/21, 01:13:11",3956.0,2417043.0,1,"Yes, kind of."
Datadog,50957805,50460608,0,"2018/06/21, 01:13:11",True,"2018/06/21, 01:13:11",3956.0,2417043.0,1,"It's possible to show single value on a dashboard (just use ""Query Value"" visualization), but it must be based on some metric reported to Datadog."
Datadog,50957805,50460608,0,"2018/06/21, 01:13:11",True,"2018/06/21, 01:13:11",3956.0,2417043.0,1,This is how it looks like:
Datadog,49465784,49443977,0,"2018/03/24, 16:23:41",False,"2018/03/24, 16:23:41",2077.0,2796894.0,-1,It only applies to produce requests.
Datadog,49328630,49328198,2,"2018/03/16, 21:59:02",False,"2018/03/16, 21:59:02",1371.0,5540166.0,1,First you may need to download the MSI file:
Datadog,49328630,49328198,2,"2018/03/16, 21:59:02",False,"2018/03/16, 21:59:02",1371.0,5540166.0,1,The actual powershell command for installation (with extra optional arguments included as arguments):
Datadog,49328630,49328198,2,"2018/03/16, 21:59:02",False,"2018/03/16, 21:59:02",1371.0,5540166.0,1,It's been a while since i've done this (8 months or so?
Datadog,49328630,49328198,2,"2018/03/16, 21:59:02",False,"2018/03/16, 21:59:02",1371.0,5540166.0,1,"), so it could be outdated, but it used to work :)."
Datadog,49328630,49328198,2,"2018/03/16, 21:59:02",False,"2018/03/16, 21:59:02",1371.0,5540166.0,1,"Note, if you're running this from a remote provisioning script, you'll probly have to schedule this to be executed not-remotely so that the installation command can be run with heightened permissions, which i believe is required."
Datadog,49328630,49328198,2,"2018/03/16, 21:59:02",False,"2018/03/16, 21:59:02",1371.0,5540166.0,1,"And you  may  need to make sure the computer is plugged into the power source (i remember hitting some infuriating issue where that was an arbitrary requirement for Windows scheduled tasks to run, and Windows didn't allow me to configure around that)."
Datadog,48325027,48314382,2,"2018/01/18, 17:39:30",False,"2018/01/18, 17:39:30",1371.0,5540166.0,0,is your  activemq_58.yaml  all in one line like that?
Datadog,48325027,48314382,2,"2018/01/18, 17:39:30",False,"2018/01/18, 17:39:30",1371.0,5540166.0,0,You probably want it to be more like this:
Datadog,47014511,47003909,0,"2017/10/30, 13:47:57",True,"2017/10/30, 13:47:57",1035.0,5825844.0,1,There are a variety of issues here.
Datadog,47014511,47003909,0,"2017/10/30, 13:47:57",True,"2017/10/30, 13:47:57",1035.0,5825844.0,1,1.
Datadog,47014511,47003909,0,"2017/10/30, 13:47:57",True,"2017/10/30, 13:47:57",1035.0,5825844.0,1,You've misconfigured the scope formats.
Datadog,47014511,47003909,0,"2017/10/30, 13:47:57",True,"2017/10/30, 13:47:57",1035.0,5825844.0,1,(metrics.scope.operator)
Datadog,47014511,47003909,0,"2017/10/30, 13:47:57",True,"2017/10/30, 13:47:57",1035.0,5825844.0,1,"For one the configuration doesn't make sense since you specify ""metrics.scope.operator"" multiple times; only the last config entry is honored."
Datadog,47014511,47003909,0,"2017/10/30, 13:47:57",True,"2017/10/30, 13:47:57",1035.0,5825844.0,1,"Second, and more importantly, you have misunderstood for scope formats are used for."
Datadog,47014511,47003909,0,"2017/10/30, 13:47:57",True,"2017/10/30, 13:47:57",1035.0,5825844.0,1,Scope formats configure which context information (like the ID of the task) is included in the reported metric's name.
Datadog,47014511,47003909,0,"2017/10/30, 13:47:57",True,"2017/10/30, 13:47:57",1035.0,5825844.0,1,"By setting it to a constant (""latency"") you've told Flink to not include anything."
Datadog,47014511,47003909,0,"2017/10/30, 13:47:57",True,"2017/10/30, 13:47:57",1035.0,5825844.0,1,"As a result, the numRecordsIn metrics for every operator is reported as ""latency.numRecordsIn""."
Datadog,47014511,47003909,0,"2017/10/30, 13:47:57",True,"2017/10/30, 13:47:57",1035.0,5825844.0,1,I suggest to just remove your scope configuration.
Datadog,47014511,47003909,0,"2017/10/30, 13:47:57",True,"2017/10/30, 13:47:57",1035.0,5825844.0,1,2.
Datadog,47014511,47003909,0,"2017/10/30, 13:47:57",True,"2017/10/30, 13:47:57",1035.0,5825844.0,1,You've misconfigured the Datadog Tags
Datadog,47014511,47003909,0,"2017/10/30, 13:47:57",True,"2017/10/30, 13:47:57",1035.0,5825844.0,1,I do not understand what you were trying to do with your tags configuration.
Datadog,47014511,47003909,0,"2017/10/30, 13:47:57",True,"2017/10/30, 13:47:57",1035.0,5825844.0,1,"The tags configuration option can only be used to provide  global  tags, i.e."
Datadog,47014511,47003909,0,"2017/10/30, 13:47:57",True,"2017/10/30, 13:47:57",1035.0,5825844.0,1,"tags that are attached to every single metrics, like ""Flink""."
Datadog,47014511,47003909,0,"2017/10/30, 13:47:57",True,"2017/10/30, 13:47:57",1035.0,5825844.0,1,By  default  every metric that the Datadog reports has tags attached to it for every available scope variable available.
Datadog,47014511,47003909,0,"2017/10/30, 13:47:57",True,"2017/10/30, 13:47:57",1035.0,5825844.0,1,"So, if you have an operator name A, then the numRecordsIn metric will be reported with a tag ""operator_name:A""."
Datadog,47014511,47003909,0,"2017/10/30, 13:47:57",True,"2017/10/30, 13:47:57",1035.0,5825844.0,1,"Again, I would suggest to just remove your configuration."
Datadog,46846772,46846059,0,"2017/10/20, 13:03:32",False,"2017/10/20, 13:03:32",148.0,6815223.0,1,"For the container agent, you'll want to run  sudo docker exec -it dd-agent /etc/init.d/datadog-agent status  from your unix based box."
Datadog,46846772,46846059,0,"2017/10/20, 13:03:32",False,"2017/10/20, 13:03:32",148.0,6815223.0,1,"If, however, you are using the alpine image the command is:  docker exec -it dd-agent /opt/datadog-agent/bin/agent status  (different path)."
Datadog,46846772,46846059,0,"2017/10/20, 13:03:32",False,"2017/10/20, 13:03:32",148.0,6815223.0,1,More here in this KB from Datadog:  https://help.datadoghq.com/hc/en-us/articles/203764635-Agent-Status-and-Information
Datadog,45742428,45370569,1,"2017/08/17, 21:19:43",True,"2017/08/17, 21:19:43",3223.0,43973.0,1,I had issues with it too.
Datadog,45742428,45370569,1,"2017/08/17, 21:19:43",True,"2017/08/17, 21:19:43",3223.0,43973.0,1,"The agent is most likely sending a value for 100 incorrectly as 10000 instead, as that's the highest I've seen it go."
Datadog,45742428,45370569,1,"2017/08/17, 21:19:43",True,"2017/08/17, 21:19:43",3223.0,43973.0,1,"To make it usable, ensure the graph type is 'line', click ""Advanced"" on the metric, and make the equation  a / 100 ."
Datadog,45742428,45370569,1,"2017/08/17, 21:19:43",True,"2017/08/17, 21:19:43",3223.0,43973.0,1,"I didn't find any guidance on this either in my search, but this seems to go along with what I've seen logged locally into a Windows box and watching the performance counters locally."
Datadog,33961640,33960552,0,"2015/11/27, 18:39:07",True,"2015/11/27, 18:39:07",5244.0,1991579.0,1,Just should set hostname for  org.coursera.metrics.datadog.DatadogReporter.Builder :
Datadog,66430636,66416385,1,"2021/03/02, 00:30:18",False,"2021/03/02, 00:30:18",156.0,2676108.0,0,Set is almost never the right custom metric type to use.
Datadog,66430636,66416385,1,"2021/03/02, 00:30:18",False,"2021/03/02, 00:30:18",156.0,2676108.0,0,It will send a count of the number of unique items per a given tag.
Datadog,66430636,66416385,1,"2021/03/02, 00:30:18",False,"2021/03/02, 00:30:18",156.0,2676108.0,0,"The underlying items details will be stripped from the metric, meaning that from one time slice to the next, you will have no idea that actual true number of items over time."
Datadog,66430636,66416385,1,"2021/03/02, 00:30:18",False,"2021/03/02, 00:30:18",156.0,2676108.0,0,For example
Datadog,66430636,66416385,1,"2021/03/02, 00:30:18",False,"2021/03/02, 00:30:18",156.0,2676108.0,0,"Your time series to datadog will report  3 , and then  2 ."
Datadog,66430636,66416385,1,"2021/03/02, 00:30:18",False,"2021/03/02, 00:30:18",156.0,2676108.0,0,But because the underlying device info is stripped you have no idea how to combine that 2 and 3 if you to zoom out in time and roll up the numbers to show 1 data point per minute.
Datadog,66430636,66416385,1,"2021/03/02, 00:30:18",False,"2021/03/02, 00:30:18",156.0,2676108.0,0,"It could be any number from 3 to 5, but the Datadog backend has no idea."
Datadog,66430636,66416385,1,"2021/03/02, 00:30:18",False,"2021/03/02, 00:30:18",156.0,2676108.0,0,(even though we know that across those 30 seconds there were 4 unique values total)
Datadog,66430636,66416385,1,"2021/03/02, 00:30:18",False,"2021/03/02, 00:30:18",156.0,2676108.0,0,"Plus even if it was accurate somehow, you can't create an alert of it or notify anyone, because you won't know which device is having issues if you see a spike of devices in the 60 second bucket."
Datadog,66430636,66416385,1,"2021/03/02, 00:30:18",False,"2021/03/02, 00:30:18",156.0,2676108.0,0,So let's go through other metric options.
Datadog,66430636,66416385,1,"2021/03/02, 00:30:18",False,"2021/03/02, 00:30:18",156.0,2676108.0,0,"The only metric types that are ever worth using are usually  distributions  or  gauges , or [counts]."
Datadog,66430636,66416385,1,"2021/03/02, 00:30:18",False,"2021/03/02, 00:30:18",156.0,2676108.0,0,"A gauge metric is just a measurement of the latency at a point in time, it's usually good for things like CPU or Memory of a computer, or temperature in a room."
Datadog,66430636,66416385,1,"2021/03/02, 00:30:18",False,"2021/03/02, 00:30:18",156.0,2676108.0,0,"Numbers that are impossible to actually collect all dat a points for so you just take measurements every 10 seconds, or every minute, or however often you never to get an idea of the behavior."
Datadog,66430636,66416385,1,"2021/03/02, 00:30:18",False,"2021/03/02, 00:30:18",156.0,2676108.0,0,"A count metric is more exact, it's the number of things that happened."
Datadog,66430636,66416385,1,"2021/03/02, 00:30:18",False,"2021/03/02, 00:30:18",156.0,2676108.0,0,"Usually good for number of requests to a server, or number of files processed."
Datadog,66430636,66416385,1,"2021/03/02, 00:30:18",False,"2021/03/02, 00:30:18",156.0,2676108.0,0,"Even something like the amount of bytes flowing through something, although that usually is treated like a gauge by most people."
Datadog,66430636,66416385,1,"2021/03/02, 00:30:18",False,"2021/03/02, 00:30:18",156.0,2676108.0,0,"Distributions are good for when you want to create a gauge metric, but you need detailed measurements for every single event that happens."
Datadog,66430636,66416385,1,"2021/03/02, 00:30:18",False,"2021/03/02, 00:30:18",156.0,2676108.0,0,For example a web server is handling hundreds of requests per second and we need to know the latency metrics of that server.
Datadog,66430636,66416385,1,"2021/03/02, 00:30:18",False,"2021/03/02, 00:30:18",156.0,2676108.0,0,It's not possible to send a latency metric for every request as a gauge.
Datadog,66430636,66416385,1,"2021/03/02, 00:30:18",False,"2021/03/02, 00:30:18",156.0,2676108.0,0,Gauges have a built in limit of 1 data point per second (in Datadog).
Datadog,66430636,66416385,1,"2021/03/02, 00:30:18",False,"2021/03/02, 00:30:18",156.0,2676108.0,0,Anything more sent in a 1 second interval gets dropped.
Datadog,66430636,66416385,1,"2021/03/02, 00:30:18",False,"2021/03/02, 00:30:18",156.0,2676108.0,0,"But we need stats for every request, so a distribution will summarize the data, it keep a running count, min, max, average, and optionally several percentiles (p50, p75, p99)."
Datadog,66430636,66416385,1,"2021/03/02, 00:30:18",False,"2021/03/02, 00:30:18",156.0,2676108.0,0,I haven't seen many good use cases for metric types outside of those 3.
Datadog,66430636,66416385,1,"2021/03/02, 00:30:18",False,"2021/03/02, 00:30:18",156.0,2676108.0,0,"For your scenario, it seems like you would want to be sending a distribution metric for that device interval."
Datadog,66430636,66416385,1,"2021/03/02, 00:30:18",False,"2021/03/02, 00:30:18",156.0,2676108.0,0,So device 1 sends a value of 10.14 and device 3 sends a value of 2.3 and so on.
Datadog,66430636,66416385,1,"2021/03/02, 00:30:18",False,"2021/03/02, 00:30:18",156.0,2676108.0,0,Then you can use a  distribution widget  in a dashboard to show the number of devices for each interval bucket.
Datadog,66430636,66416385,1,"2021/03/02, 00:30:18",False,"2021/03/02, 00:30:18",156.0,2676108.0,0,Of course make sure you tag each metric by the device that is generating the metric.
Datadog,53458998,53044670,0,"2018/11/24, 16:09:27",True,"2018/11/24, 16:09:27",347.0,2650254.0,0,"I was able to do that by using this api call:  https://docs.datadoghq.com/api/?lang=python#get-a-screenboard 
and then get it as a son file, which can be passed to cloud formation later."
Datadog,50451247,50134871,0,"2018/05/21, 17:41:57",False,"2018/05/21, 17:41:57",2352.0,8870132.0,0,Actually what you need to do is create a generic method.
Datadog,50451247,50134871,0,"2018/05/21, 17:41:57",False,"2018/05/21, 17:41:57",2352.0,8870132.0,0,Now when you are hitting the different different end points just call the updateCounter method will will capture your metric with the specific name of your route.
Datadog,50451247,50134871,0,"2018/05/21, 17:41:57",False,"2018/05/21, 17:41:57",2352.0,8870132.0,0,"For example you have route like add and subtract
Then call the update counter method with metric name add and subtract."
Datadog,41618026,41605761,0,"2017/01/12, 18:15:50",False,"2017/01/12, 18:15:50",261.0,4172512.0,1,You can leverage Datadog's Agent to collect metrics via a JMX connection.
Datadog,41618026,41605761,0,"2017/01/12, 18:15:50",False,"2017/01/12, 18:15:50",261.0,4172512.0,1,There is documentation found here:
Datadog,41618026,41605761,0,"2017/01/12, 18:15:50",False,"2017/01/12, 18:15:50",261.0,4172512.0,1,http://docs.datadoghq.com/integrations/java/
Datadog,41618026,41605761,0,"2017/01/12, 18:15:50",False,"2017/01/12, 18:15:50",261.0,4172512.0,1,https://www.datadoghq.com/blog/monitoring-jmx-metrics-with-datadog/
Datadog,41618026,41605761,0,"2017/01/12, 18:15:50",False,"2017/01/12, 18:15:50",261.0,4172512.0,1,https://help.datadoghq.com/hc/en-us/articles/204501525-Custom-JMX-Integration-s-
Datadog,41618026,41605761,0,"2017/01/12, 18:15:50",False,"2017/01/12, 18:15:50",261.0,4172512.0,1,https://help.datadoghq.com/hc/en-us/articles/207525586-View-jmx-data-in-jConsole-and-set-up-your-jmx-yaml-to-collect-them
Datadog,41618026,41605761,0,"2017/01/12, 18:15:50",False,"2017/01/12, 18:15:50",261.0,4172512.0,1,That should help you get setup and collecting the necessary metrics exposed by your JMX port.
Datadog,41618026,41605761,0,"2017/01/12, 18:15:50",False,"2017/01/12, 18:15:50",261.0,4172512.0,1,"That said, if you encounter any issues reach out to support@datadoghq.com and they can assist you."
Datadog,36459921,36459827,3,"2016/04/06, 21:52:23",False,"2016/04/06, 21:52:23",12279.0,596041.0,2,What you have is a nil pointer dereference.
Datadog,36459921,36459827,3,"2016/04/06, 21:52:23",False,"2016/04/06, 21:52:23",12279.0,596041.0,2,"(Unless you are using package  unsafe , which you probably shouldn't touch, so I'm assuming you're not.)"
Datadog,36459921,36459827,3,"2016/04/06, 21:52:23",False,"2016/04/06, 21:52:23",12279.0,596041.0,2,It looks like the  e  argument to  func (c *Client) Event(e *Event) error  is  nil  when called from  github.com/some/path/server/http.go:86 .
Datadog,36461477,36459827,1,"2016/04/06, 23:17:02",True,"2016/04/06, 23:17:02",1631.0,97094.0,19,"Thanks to a comment from @twotwotwo, I think I figured this out."
Datadog,36461477,36459827,1,"2016/04/06, 23:17:02",True,"2016/04/06, 23:17:02",1631.0,97094.0,19,In this line
Datadog,36461477,36459827,1,"2016/04/06, 23:17:02",True,"2016/04/06, 23:17:02",1631.0,97094.0,19,I wrote the following program to demonstrate to myself how different function signatures appear in a stack trace:
Datadog,36461477,36459827,1,"2016/04/06, 23:17:02",True,"2016/04/06, 23:17:02",1631.0,97094.0,19,"When  bam  is called, which acts on  *Y  but has no arguments or return value, the output contains:"
Datadog,36461477,36459827,1,"2016/04/06, 23:17:02",True,"2016/04/06, 23:17:02",1631.0,97094.0,19,"When  foo  is called, which acts on  *Y  and takes a  *X  as argument, but has no return value, the output contains:"
Datadog,36461477,36459827,1,"2016/04/06, 23:17:02",True,"2016/04/06, 23:17:02",1631.0,97094.0,19,"When  bar  is called, which acts on  *Y , takes a  *X  as argument, and returns a  *Y , the output contains:"
Datadog,36461477,36459827,1,"2016/04/06, 23:17:02",True,"2016/04/06, 23:17:02",1631.0,97094.0,19,"When  baz  is called, which acts on  *Y , takes  *X  as argument, and returns an  error  (which is an interface), the output contains:"
Datadog,45359315,45358188,3,"2017/07/27, 22:20:09",False,"2018/10/29, 09:50:33",6670.0,3625317.0,7,Think differently :)
Datadog,45359315,45358188,3,"2017/07/27, 22:20:09",False,"2018/10/29, 09:50:33",6670.0,3625317.0,7,Do bind a nginx-server (vhost) on port 10080  in addition  - that server does offer the status location and what you need.
Datadog,45359315,45358188,3,"2017/07/27, 22:20:09",False,"2018/10/29, 09:50:33",6670.0,3625317.0,7,Server on 80/443 is also there and ONLY that one is bound/exposed to host ( exposed to the outer world ).
Datadog,45359315,45358188,3,"2017/07/27, 22:20:09",False,"2018/10/29, 09:50:33",6670.0,3625317.0,7,"Since datadog is part of your docker-network / service network, it can still access 10080 in the internal network, but nobody else from the outer network."
Datadog,45359315,45358188,3,"2017/07/27, 22:20:09",False,"2018/10/29, 09:50:33",6670.0,3625317.0,7,"Bulletproof, easy - no strings attached."
Datadog,45359614,45358188,0,"2017/07/27, 22:38:08",True,"2017/07/27, 22:38:08",123680.0,2830850.0,4,Since we are running the service through  docker-compose  and our issue being we don't know the IP of the agent.
Datadog,45359614,45358188,0,"2017/07/27, 22:38:08",True,"2017/07/27, 22:38:08",123680.0,2830850.0,4,So the simple solution is to know the IP before starting.
Datadog,45359614,45358188,0,"2017/07/27, 22:38:08",True,"2017/07/27, 22:38:08",123680.0,2830850.0,4,And that means assigning our agent a specific IP
Datadog,45359614,45358188,0,"2017/07/27, 22:38:08",True,"2017/07/27, 22:38:08",123680.0,2830850.0,4,Here is a update  docker-compose  to do that
Datadog,45359614,45358188,0,"2017/07/27, 22:38:08",True,"2017/07/27, 22:38:08",123680.0,2830850.0,4,Now you can do two possible things
Datadog,45359614,45358188,0,"2017/07/27, 22:38:08",True,"2017/07/27, 22:38:08",123680.0,2830850.0,4,You can listen only on  172.25.0.101  which is accessible only container running on agent network.
Datadog,45359614,45358188,0,"2017/07/27, 22:38:08",True,"2017/07/27, 22:38:08",123680.0,2830850.0,4,Also you can add  allow 172.25.0.100  to only allow the agent container to be able to access this.
Datadog,61277277,45358188,1,"2020/04/17, 20:27:04",False,"2020/04/17, 20:27:04",515.0,2075004.0,3,There are two (easier) ways to go about it.
Datadog,61277277,45358188,1,"2020/04/17, 20:27:04",False,"2020/04/17, 20:27:04",515.0,2075004.0,3,"First one is  docker-compose  but since I already have a setup running since 2 years which doesn't use docker-compose, I went for the 2nd way."
Datadog,61277277,45358188,1,"2020/04/17, 20:27:04",False,"2020/04/17, 20:27:04",515.0,2075004.0,3,Second way is  Allow  Directive with a range of IPs.
Datadog,61277277,45358188,1,"2020/04/17, 20:27:04",False,"2020/04/17, 20:27:04",515.0,2075004.0,3,Eg:
Datadog,61277277,45358188,1,"2020/04/17, 20:27:04",False,"2020/04/17, 20:27:04",515.0,2075004.0,3,"I am not security expert, but mostly  192.168."
Datadog,61277277,45358188,1,"2020/04/17, 20:27:04",False,"2020/04/17, 20:27:04",515.0,2075004.0,3,"*  IP range is for local networks, not sure about  172.18."
Datadog,61277277,45358188,1,"2020/04/17, 20:27:04",False,"2020/04/17, 20:27:04",515.0,2075004.0,3,*  range though.
Datadog,61277277,45358188,1,"2020/04/17, 20:27:04",False,"2020/04/17, 20:27:04",515.0,2075004.0,3,"To get more idea about this IP range thing and CIDR stuff, refer below links
 http://nginx.org/en/docs/http/ngx_http_access_module.html"
Datadog,61277277,45358188,1,"2020/04/17, 20:27:04",False,"2020/04/17, 20:27:04",515.0,2075004.0,3,https://www.ripe.net/about-us/press-centre/understanding-ip-addressing
Datadog,34031282,33929394,0,"2015/12/02, 00:18:15",False,"2015/12/02, 00:18:15",117.0,5379516.0,0,"As the err info said dh key is too small, a larger one might help."
Datadog,34031282,33929394,0,"2015/12/02, 00:18:15",False,"2015/12/02, 00:18:15",117.0,5379516.0,0,Replace the default dh512.pem file with dh4096.pem
Datadog,34031282,33929394,0,"2015/12/02, 00:18:15",False,"2015/12/02, 00:18:15",117.0,5379516.0,0,"sudo wget ""https://git.openssl.org/gitweb/?p=openssl.git;a=blob_plain;f=apps/dh4096.pem"" -O dh4096.pem"
Datadog,34031282,33929394,0,"2015/12/02, 00:18:15",False,"2015/12/02, 00:18:15",117.0,5379516.0,0,Ref:  http://www.alexrhino.net/jekyll/update/2015/07/14/dh-params-test-fail.html
Datadog,60844436,60842629,1,"2020/03/25, 09:24:11",False,"2020/03/25, 09:59:29",71285.0,3558960.0,3,This is actually a lot harder than it seems.
Datadog,60844436,60842629,1,"2020/03/25, 09:24:11",False,"2020/03/25, 09:59:29",71285.0,3558960.0,3,"Representing big integers in JavaScript can be done using the  BigInt  data type (by suffixing the number with  n ), which is fairly widely supported at this point."
Datadog,60844436,60842629,1,"2020/03/25, 09:24:11",False,"2020/03/25, 09:59:29",71285.0,3558960.0,3,This would make your object look like this:
Datadog,60844436,60842629,1,"2020/03/25, 09:24:11",False,"2020/03/25, 09:59:29",71285.0,3558960.0,3,"The problem presents itself in the JSON serialization, as there is currently no support for the serialization of  BigInt  objects."
Datadog,60844436,60842629,1,"2020/03/25, 09:24:11",False,"2020/03/25, 09:59:29",71285.0,3558960.0,3,"And when it comes to JSON serialization, your options for customization are very limited:"
Datadog,60844436,60842629,1,"2020/03/25, 09:24:11",False,"2020/03/25, 09:59:29",71285.0,3558960.0,3,So the only option that I can find is to (at least partially) implement your own JSON serialization mechanism.
Datadog,60844436,60842629,1,"2020/03/25, 09:24:11",False,"2020/03/25, 09:59:29",71285.0,3558960.0,3,"This is a  very  poor man's implementation that calls  toString()  for object properties that are of type  BigInt , and delegates to  JSON.stringify()  otherwise:"
Datadog,60844436,60842629,1,"2020/03/25, 09:24:11",False,"2020/03/25, 09:59:29",71285.0,3558960.0,3,"
 
 const o = {
  ""span_id"": 16956440953342013954n,
  ""trace_id"": 13756071592735822010n
};

const stringify = (o) =&gt; '{'
  + Object.entries(o).reduce((a, [k, v]) =&gt; ([
      ...a, 
      `""${k}"": ${typeof v === 'bigint' ?"
Datadog,60844436,60842629,1,"2020/03/25, 09:24:11",False,"2020/03/25, 09:59:29",71285.0,3558960.0,3,"v.toString() : JSON.stringify(v)}`
    ])).join(', ')
  + '}';

console.log(stringify(o));"
Datadog,60844436,60842629,1,"2020/03/25, 09:24:11",False,"2020/03/25, 09:59:29",71285.0,3558960.0,3,"Note that the above will not work correctly in a number of cases, most prominently nested objects and arrays."
Datadog,60844436,60842629,1,"2020/03/25, 09:24:11",False,"2020/03/25, 09:59:29",71285.0,3558960.0,3,"If I were to do this for real-world usage, I would probably base myself on  Douglas Crockford's JSON implementation ."
Datadog,60844436,60842629,1,"2020/03/25, 09:24:11",False,"2020/03/25, 09:59:29",71285.0,3558960.0,3,It should be sufficient to add an additional case around  this line :
Datadog,59354863,58897099,0,"2019/12/16, 12:36:35",False,"2019/12/16, 12:36:35",394.0,1602433.0,-1,You could try to use  clinic  in order to debug and profile the app.
Datadog,59354863,58897099,0,"2019/12/16, 12:36:35",False,"2019/12/16, 12:36:35",394.0,1602433.0,-1,pretty good tool for nodeJS.
Datadog,59406939,58897099,1,"2019/12/19, 11:41:00",False,"2019/12/19, 11:41:00",388.0,4546641.0,-1,You could user  node-memwatch  to detect where is memory leak.
Datadog,59406939,58897099,1,"2019/12/19, 11:41:00",False,"2019/12/19, 11:41:00",388.0,4546641.0,-1,"It also might be a known issue, here is the  link  with a similar issue."
Datadog,59433027,58897099,0,"2019/12/21, 04:16:20",False,"2019/12/21, 04:16:20",3025.0,3154872.0,0,You are on the right path.
Datadog,59433027,58897099,0,"2019/12/21, 04:16:20",False,"2019/12/21, 04:16:20",3025.0,3154872.0,0,The guide I'm about to link to begins by following a similar approach to the one you've taken.
Datadog,59433027,58897099,0,"2019/12/21, 04:16:20",False,"2019/12/21, 04:16:20",3025.0,3154872.0,0,"I'll link to the section that talks about monitoring memory in real time, which is available when you  Record allocation timeline  in chrome://inspect"
Datadog,59433027,58897099,0,"2019/12/21, 04:16:20",False,"2019/12/21, 04:16:20",3025.0,3154872.0,0,https://marmelab.com/blog/2018/04/03/how-to-track-and-fix-memory-leak-with-nodejs.html#watching-memory-allocation-in-real-time
Datadog,65950961,47299307,0,"2021/01/29, 10:19:44",False,"2021/01/29, 10:19:44",21.0,6351378.0,2,"This question is quite old, however, still might be useful for new users of Google Cloud."
Datadog,65950961,47299307,0,"2021/01/29, 10:19:44",False,"2021/01/29, 10:19:44",21.0,6351378.0,2,In 'Metrics Explorer' in Google Cloud Console there is an option to write a query with MQL (click  Query Editor  button).
Datadog,65950961,47299307,0,"2021/01/29, 10:19:44",False,"2021/01/29, 10:19:44",21.0,6351378.0,2,MQL supports expressions which are described in detail  here .
Datadog,65950961,47299307,0,"2021/01/29, 10:19:44",False,"2021/01/29, 10:19:44",21.0,6351378.0,2,The simplest example for dividing one metric by another would look like this:
Datadog,63340327,63049334,1,"2020/08/10, 15:35:10",True,"2020/08/10, 15:35:10",632.0,5197466.0,2,"It is probably caused by a regression between .NET Core 2.2 and .NET Core 3.0
Apparently it will be fixed in version 3.1.7"
Datadog,63340327,63049334,1,"2020/08/10, 15:35:10",True,"2020/08/10, 15:35:10",632.0,5197466.0,2,"Just starting the process causes the memory leak on linux, because of a non released handle"
Datadog,63340327,63049334,1,"2020/08/10, 15:35:10",True,"2020/08/10, 15:35:10",632.0,5197466.0,2,Issue has been tracked here  https://github.com/dotnet/runtime/issues/36661
Datadog,60253636,60237664,2,"2020/02/17, 00:04:53",True,"2020/02/17, 23:43:26",37971.0,1237575.0,2,The problem is that the advice class will be loaded on the system class loader as a part of the agent whereas the actual application code is loaded on a sub-class loader that is not visible to the system class loader.
Datadog,60253636,60237664,2,"2020/02/17, 00:04:53",True,"2020/02/17, 23:43:26",37971.0,1237575.0,2,This situation does not change if you load your agent on the boot loader either.
Datadog,60253636,60237664,2,"2020/02/17, 00:04:53",True,"2020/02/17, 23:43:26",37971.0,1237575.0,2,"Therefore, the agent cannot load the  HttpServletRequest  class which is part of the uber-jar."
Datadog,60253636,60237664,2,"2020/02/17, 00:04:53",True,"2020/02/17, 23:43:26",37971.0,1237575.0,2,This is a typical problem with agents and Byte Buddy has a standard way to circumvent it by using a  Transformer.ForAdvice  instance instead of using the  Advice  class directly.
Datadog,60253636,60237664,2,"2020/02/17, 00:04:53",True,"2020/02/17, 23:43:26",37971.0,1237575.0,2,Byte Buddy then creates a virtual class loader hierarchy that considers classes represented by both class loaders.
Datadog,60253636,60237664,2,"2020/02/17, 00:04:53",True,"2020/02/17, 23:43:26",37971.0,1237575.0,2,Update : The problem is that you are calling down to your interceptor that is defined in the system class loader where the class in question is not available.
Datadog,60253636,60237664,2,"2020/02/17, 00:04:53",True,"2020/02/17, 23:43:26",37971.0,1237575.0,2,The annotated code will be inlined but the invoked method will not.
Datadog,60253636,60237664,2,"2020/02/17, 00:04:53",True,"2020/02/17, 23:43:26",37971.0,1237575.0,2,"If you copy-pasted the code into the annotated method, the behavior is as you'd expect it."
Datadog,60253636,60237664,2,"2020/02/17, 00:04:53",True,"2020/02/17, 23:43:26",37971.0,1237575.0,2,Byte Buddy uses the annotated code as template and reuses a lot of information emitted by javac to guarantee a speedy conversion.
Datadog,60253636,60237664,2,"2020/02/17, 00:04:53",True,"2020/02/17, 23:43:26",37971.0,1237575.0,2,"Therefore, the library cannot simply copy the method and should rather feed the entire method body to javac."
Datadog,56645177,54185496,0,"2019/06/18, 11:49:21",True,"2019/06/18, 11:49:21",36.0,11663155.0,2,The reason for this error is because apache is listening to port 80 on IPv4 &amp; IPv6.
Datadog,56645177,54185496,0,"2019/06/18, 11:49:21",True,"2019/06/18, 11:49:21",36.0,11663155.0,2,This will explicitly tell apache to listen to IPv4.
Datadog,56645177,54185496,0,"2019/06/18, 11:49:21",True,"2019/06/18, 11:49:21",36.0,11663155.0,2,In apache config change:
Datadog,56645177,54185496,0,"2019/06/18, 11:49:21",True,"2019/06/18, 11:49:21",36.0,11663155.0,2,Listen 80
Datadog,56645177,54185496,0,"2019/06/18, 11:49:21",True,"2019/06/18, 11:49:21",36.0,11663155.0,2,to
Datadog,56645177,54185496,0,"2019/06/18, 11:49:21",True,"2019/06/18, 11:49:21",36.0,11663155.0,2,Listen 0.0.0.0:80
Datadog,56645177,54185496,0,"2019/06/18, 11:49:21",True,"2019/06/18, 11:49:21",36.0,11663155.0,2,Make sure the file is being copied in to your docker container and being used in apache.
Datadog,56645177,54185496,0,"2019/06/18, 11:49:21",True,"2019/06/18, 11:49:21",36.0,11663155.0,2,Or add an extra step in the Dockerfile:
Datadog,56645177,54185496,0,"2019/06/18, 11:49:21",True,"2019/06/18, 11:49:21",36.0,11663155.0,2,&amp;&amp; sed -i 's/^Listen 80$/Listen 0.0.0.0:80/' /etc/apache2/httpd.conf
Datadog,52392480,52390678,0,"2018/09/18, 21:18:13",True,"2018/09/18, 21:18:13",3537.0,8993347.0,3,"Containers are about isolation so in container ""localhost"" means inside container  so ddtrace-test cannot find ddagent inside his container."
Datadog,52392480,52390678,0,"2018/09/18, 21:18:13",True,"2018/09/18, 21:18:13",3537.0,8993347.0,3,You have 2 ways to fix that:
Datadog,50097560,50096290,2,"2018/04/30, 12:24:42",True,"2018/04/30, 12:52:06",18198.0,1765189.0,6,"I'm guessing you're talking about ""metrics"" instead of matrix!"
Datadog,50097560,50096290,2,"2018/04/30, 12:24:42",True,"2018/04/30, 12:52:06",18198.0,1765189.0,6,"On the Producer, you have  kafka.producer:type=producer-metrics,client-id=""{client-id}"" ."
Datadog,50097560,50096290,2,"2018/04/30, 12:24:42",True,"2018/04/30, 12:52:06",18198.0,1765189.0,6,That metric has 2 interesting attributes:
Datadog,50097560,50096290,2,"2018/04/30, 12:24:42",True,"2018/04/30, 12:52:06",18198.0,1765189.0,6,request-latency-avg: The average request latency in ms
Datadog,50097560,50096290,2,"2018/04/30, 12:24:42",True,"2018/04/30, 12:52:06",18198.0,1765189.0,6,request-latency-max: The maximum request latency in ms
Datadog,50097560,50096290,2,"2018/04/30, 12:24:42",True,"2018/04/30, 12:52:06",18198.0,1765189.0,6,"On the broker side, there are a few metrics you want to check to investigate your issue:"
Datadog,50097560,50096290,2,"2018/04/30, 12:24:42",True,"2018/04/30, 12:52:06",18198.0,1765189.0,6,Request total time: Total time Kafka took to process the request.
Datadog,50097560,50096290,2,"2018/04/30, 12:24:42",True,"2018/04/30, 12:52:06",18198.0,1765189.0,6,"kafka.network:type=RequestMetrics,name=TotalTimeMs,request=Produce"
Datadog,50097560,50096290,2,"2018/04/30, 12:24:42",True,"2018/04/30, 12:52:06",18198.0,1765189.0,6,"In case this is high, you can check the break down metrics:"
Datadog,50097560,50096290,2,"2018/04/30, 12:24:42",True,"2018/04/30, 12:52:06",18198.0,1765189.0,6,These are all listed in the metrics recommended to monitor list in the Kafka documentation:  http://kafka.apache.org/documentation/#monitoring
Datadog,44857071,44855220,2,"2017/07/01, 06:28:14",False,"2017/07/01, 06:28:14",19457.0,684908.0,3,Some preliminary Google searching lands me on  https://github.com/kubernetes/kubernetes/pull/42717  by way of  https://github.com/kubernetes/kubernetes/issues/24657 .
Datadog,44857071,44855220,2,"2017/07/01, 06:28:14",False,"2017/07/01, 06:28:14",19457.0,684908.0,3,It looks like the pull request was merged in time to be in Kubernetes 1.7.
Datadog,44857071,44855220,2,"2017/07/01, 06:28:14",False,"2017/07/01, 06:28:14",19457.0,684908.0,3,This should mean that you can use the Downward API to expose  status.hostIP  as an environment variable ( https://kubernetes.io/docs/tasks/inject-data-application/environment-variable-expose-pod-information/ ) or a file in a volume ( https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/ ).
Datadog,44857071,44855220,2,"2017/07/01, 06:28:14",False,"2017/07/01, 06:28:14",19457.0,684908.0,3,Your application would then need to read the environment variable or file to get the value of the actual host IP address.
Datadog,44867146,44855220,2,"2017/07/02, 06:15:59",False,"2017/07/04, 10:35:08",9153.0,172265.0,0,"If you agent is written by yourself, you can open and listen on a Unix domain socket and let the other pod send data through it."
Datadog,44867146,44855220,2,"2017/07/02, 06:15:59",False,"2017/07/04, 10:35:08",9153.0,172265.0,0,"If not, you can write a small data proxy that listens on a Unix socket for data."
Datadog,44867146,44855220,2,"2017/07/02, 06:15:59",False,"2017/07/04, 10:35:08",9153.0,172265.0,0,"On the other end, by sharing a pod with the daemon, you can easily send data to the local container"
Datadog,44927321,44855220,3,"2017/07/05, 16:14:08",True,"2017/07/05, 16:14:08",1123.0,1595197.0,6,"I use the exact same setup,  dd-agent  running as a DaemonSet in my kubernetes cluster."
Datadog,44927321,44855220,3,"2017/07/05, 16:14:08",True,"2017/07/05, 16:14:08",1123.0,1595197.0,6,"Using the same port mapping you commented  here , you can just send metrics to the hostname of the node an application is running on."
Datadog,44927321,44855220,3,"2017/07/05, 16:14:08",True,"2017/07/05, 16:14:08",1123.0,1595197.0,6,You can add the node name to the pods environment using the downward api in your pod spec:
Datadog,44927321,44855220,3,"2017/07/05, 16:14:08",True,"2017/07/05, 16:14:08",1123.0,1595197.0,6,"Then, you can just open an UDP connection to  ${NODE_NAME}:8125  to connect to the datadog agent."
Datadog,59890601,59881685,0,"2020/01/24, 07:03:49",False,"2020/01/24, 07:03:49",390.0,1467579.0,1,There are a number of different ways you could handle this:
Datadog,59890601,59881685,0,"2020/01/24, 07:03:49",False,"2020/01/24, 07:03:49",390.0,1467579.0,1,These are just some of your options.
Datadog,59890601,59881685,0,"2020/01/24, 07:03:49",False,"2020/01/24, 07:03:49",390.0,1467579.0,1,Since the Airflow webserver is just a Flask app you can really expose metrics in whatever way you see fit.
Datadog,63754701,59881685,0,"2020/09/05, 16:57:34",False,"2020/09/05, 18:28:32",65.0,11246025.0,0,"As I understand, you can monitor running tasks in DAGs using DataDog, refer the integration with Airflow  docs"
Datadog,63754701,59881685,0,"2020/09/05, 16:57:34",False,"2020/09/05, 18:28:32",65.0,11246025.0,0,You may refer metrics via DogStatD  docs .
Datadog,63754701,59881685,0,"2020/09/05, 16:57:34",False,"2020/09/05, 18:28:32",65.0,11246025.0,0,"Also, look at this  page  would be useful to understand what to monitor."
Datadog,63754701,59881685,0,"2020/09/05, 16:57:34",False,"2020/09/05, 18:28:32",65.0,11246025.0,0,"E.g., the metrics as below:"
Datadog,65918401,59879331,0,"2021/01/27, 13:40:25",False,"2021/01/27, 13:40:25",2115.0,2247740.0,0,HikariCP is a connection pool and JDBC is the API for managing a connection.
Datadog,65918401,59879331,0,"2021/01/27, 13:40:25",False,"2021/01/27, 13:40:25",2115.0,2247740.0,0,So it can be thought that Spring thinks about separating connection-pool-manager metrics from connection metrics.
Datadog,59679096,59678757,7,"2020/01/10, 11:44:26",True,"2020/01/10, 12:04:43",39071.0,2291321.0,3,You need to set the  aws_ecs_task_definition 's  network_mode  to  awsvpc  if you are defining the  network_configuration  of the service that uses that task definition.
Datadog,59679096,59678757,7,"2020/01/10, 11:44:26",True,"2020/01/10, 12:04:43",39071.0,2291321.0,3,This is mentioned in the  documentation for the  network_configuration  parameter of the  aws_ecs_service  resource :
Datadog,59679096,59678757,7,"2020/01/10, 11:44:26",True,"2020/01/10, 12:04:43",39071.0,2291321.0,3,"network_configuration  - (Optional) The network configuration for the
  service."
Datadog,59679096,59678757,7,"2020/01/10, 11:44:26",True,"2020/01/10, 12:04:43",39071.0,2291321.0,3,"This parameter is required for task definitions that use the
   awsvpc  network mode to receive their own Elastic Network Interface,
  and it is not supported for other network modes."
Datadog,59679096,59678757,7,"2020/01/10, 11:44:26",True,"2020/01/10, 12:04:43",39071.0,2291321.0,3,In your case you've added the  network_mode  parameter to the  container  definition instead of the  task  definition (a task is a collection of n containers and are grouped together to share some resources).
Datadog,59679096,59678757,7,"2020/01/10, 11:44:26",True,"2020/01/10, 12:04:43",39071.0,2291321.0,3,The  container definition schema  doesn't allow for a  network_mode  parameter.
Datadog,52935718,52934832,1,"2018/10/22, 21:35:34",False,"2018/10/22, 21:35:34",922.0,8622323.0,2,You can use a conditional with count to override if a resource is to be created.
Datadog,52935718,52934832,1,"2018/10/22, 21:35:34",False,"2018/10/22, 21:35:34",922.0,8622323.0,2,The example below will only create the resource when the variable environment is not = production.
Datadog,52935718,52934832,1,"2018/10/22, 21:35:34",False,"2018/10/22, 21:35:34",922.0,8622323.0,2,"If Count = 0 then the resource won't be created,"
Datadog,52935718,52934832,1,"2018/10/22, 21:35:34",False,"2018/10/22, 21:35:34",922.0,8622323.0,2,"Regards,"
Datadog,48836870,48807945,0,"2018/02/17, 02:52:20",False,"2018/02/17, 02:52:20",3131.0,1314028.0,-1,I don't think there is something that does this for you automatically.
Datadog,48836870,48807945,0,"2018/02/17, 02:52:20",False,"2018/02/17, 02:52:20",3131.0,1314028.0,-1,You have to reset the counter yourself at each reporting interval.
Datadog,48836870,48807945,0,"2018/02/17, 02:52:20",False,"2018/02/17, 02:52:20",3131.0,1314028.0,-1,Something like this should work:
Datadog,45324581,45324189,1,"2017/07/26, 13:41:18",False,"2017/07/26, 13:41:18",173.0,8047168.0,6,"Actually, It is quite simple."
Datadog,45324581,45324189,1,"2017/07/26, 13:41:18",False,"2017/07/26, 13:41:18",173.0,8047168.0,6,This is called  Packaging namespace packages .
Datadog,45324581,45324189,1,"2017/07/26, 13:41:18",False,"2017/07/26, 13:41:18",173.0,8047168.0,6,https://packaging.python.org/guides/packaging-namespace-packages/
Datadog,45324581,45324189,1,"2017/07/26, 13:41:18",False,"2017/07/26, 13:41:18",173.0,8047168.0,6,All you need is to separate all packages to sub - packages and after install it with a namespace.
Datadog,42717056,42708258,1,"2017/03/10, 13:15:07",False,"2017/03/10, 13:15:07",136.0,3403240.0,0,2 questions that'll be helpful:
Datadog,42717056,42708258,1,"2017/03/10, 13:15:07",False,"2017/03/10, 13:15:07",136.0,3403240.0,0,Now some clarification on where I think you're going wrong here:
Datadog,42718833,42708258,2,"2017/03/10, 14:48:54",False,"2017/03/10, 14:57:25",761.0,2521248.0,0,I suggest tracing the problematic query to see what cassandra was doing.
Datadog,42718833,42708258,2,"2017/03/10, 14:48:54",False,"2017/03/10, 14:57:25",761.0,2521248.0,0,https://docs.datastax.com/en/cql/3.1/cql/cql_reference/tracing_r.html
Datadog,42718833,42708258,2,"2017/03/10, 14:48:54",False,"2017/03/10, 14:57:25",761.0,2521248.0,0,"Open cql shell, type  TRACING ON  and execute your query."
Datadog,42718833,42708258,2,"2017/03/10, 14:48:54",False,"2017/03/10, 14:57:25",761.0,2521248.0,0,"If everything seems fine, there is a chance that this problem happens occasionally, in which case I'd suggest tracing the queries using nodetool settraceprobablilty for some time, until you manage to catch the problem."
Datadog,42718833,42708258,2,"2017/03/10, 14:48:54",False,"2017/03/10, 14:57:25",761.0,2521248.0,0,You enable it on each node separately using  nodetool settraceprobability &lt;param&gt;  where param is the probability (between 0 and 1) that the query will get traced.
Datadog,42718833,42708258,2,"2017/03/10, 14:48:54",False,"2017/03/10, 14:57:25",761.0,2521248.0,0,"Careful: this WILL cause increased load, so start with a very low number and go up."
Datadog,42718833,42708258,2,"2017/03/10, 14:48:54",False,"2017/03/10, 14:57:25",761.0,2521248.0,0,"If this problem is occasional there is a chance that this might be caused by long garbage collections, in which case you need to analyse the GC logs."
Datadog,42718833,42708258,2,"2017/03/10, 14:48:54",False,"2017/03/10, 14:57:25",761.0,2521248.0,0,Check how long your GC's are.
Datadog,42718833,42708258,2,"2017/03/10, 14:48:54",False,"2017/03/10, 14:57:25",761.0,2521248.0,0,"edit: just to be clear, if this problem is caused by GC's you will NOT see it with tracing."
Datadog,42718833,42708258,2,"2017/03/10, 14:48:54",False,"2017/03/10, 14:57:25",761.0,2521248.0,0,"So first check your GC's, and if its not the problem then move on to tracing."
Datadog,28831358,28436960,0,"2015/03/03, 14:03:18",False,"2015/03/03, 14:03:18",579.0,4531116.0,1,Here you can find how to add a new webhook to your mandrill account:  https://mandrillapp.com/api/docs/webhooks.php.html#method=add
Datadog,28831358,28436960,0,"2015/03/03, 14:03:18",False,"2015/03/03, 14:03:18",579.0,4531116.0,1,"tha main thing here is this:
 $url = 'http://example/webhook-url'; 
this is your webhook URL what will process the data sent by mandrill and forward the information to Datadog."
Datadog,28831358,28436960,0,"2015/03/03, 14:03:18",False,"2015/03/03, 14:03:18",579.0,4531116.0,1,and this is a description about what mandrill will send to your webhook URL:  http://help.mandrill.com/entries/21738186-Introduction-to-Webhooks
Datadog,28835739,28436960,0,"2015/03/03, 17:35:33",False,"2015/03/03, 17:35:33",516.0,1988301.0,1,a listener for webhooks is nothing else then a website/app which triggers an action if a request comes in.
Datadog,28835739,28436960,0,"2015/03/03, 17:35:33",False,"2015/03/03, 17:35:33",516.0,1988301.0,1,Usually you keep it secret or secure it with (http basic) authentication.
Datadog,28835739,28436960,0,"2015/03/03, 17:35:33",False,"2015/03/03, 17:35:33",516.0,1988301.0,1,E.g.
Datadog,28835739,28436960,0,"2015/03/03, 17:35:33",False,"2015/03/03, 17:35:33",516.0,1988301.0,1,create a website called  http://yourdomain.com/hooklistener.php .
Datadog,28835739,28436960,0,"2015/03/03, 17:35:33",False,"2015/03/03, 17:35:33",516.0,1988301.0,1,You can then call it with HTTP POST or GET and pass some data like hooklistener.php?event=triggerDataDog or with POST and send data along with the body.
Datadog,28835739,28436960,0,"2015/03/03, 17:35:33",False,"2015/03/03, 17:35:33",516.0,1988301.0,1,You then run a script or anything you want to process that event.
Datadog,28919358,28436960,0,"2015/03/07, 22:02:50",False,"2015/03/07, 22:02:50",544.0,1428713.0,0,"A ""listener"" is just any URL that you host where you can receive data that is posted to it."
Datadog,28919358,28436960,0,"2015/03/07, 22:02:50",False,"2015/03/07, 22:02:50",544.0,1428713.0,0,"Keep in mind, since you mentioned Zapier, you can set up a trigger that receives the webhook data - in this case the listener URL is provided by Zapier, and you can then send that data into any application (or even post to another webhook)."
Datadog,28919358,28436960,0,"2015/03/07, 22:02:50",False,"2015/03/07, 22:02:50",544.0,1428713.0,0,Using Zapier is nice because it doesn't require you to write the listener code that receives the hook data and does something with it.
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,I believe that Amazon actually offers a service that would accomplish your goal -  CloudWatch   (pricing) .
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,I'm going to take your points one by one.
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,"Note that I haven't actually  used  it before, but the documentation is fairly clear."
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,One server had high CPU for over 5 mins when the alert should be triggered after 1 minute
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,It looks like CloudWatch can be configured to send an alert (which I'll get to) after one minute of a condition being met:
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,"One can actually set conditions for many other metrics as well - this is what I see on one of my instances, and I think that detailed monitoring (I use free), might have even more:"
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,What else is out there that can do the same job and  will also integrate with Pager Duty?
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,I'm assuming you're talking about  this .
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,It turns out the Pager Duty has a  helpful guide  just for integrating CloudWatch.
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,How nice!
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,"Here's the pricing page , as you would probably like to parse it instead of me telling you."
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,"I'll give a brief overview, though:"
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,"You don't want basic monitoring, as it only gives you metrics once per five minutes (which you've indicated is unacceptable.)"
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,"Instead, you want detailed monitoring (once every minute)."
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,"For an EC2 instance, the price for detailed monitoring is $3.50 per instance  per month ."
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,"Additionally, every alarm you make is $0.10 per month."
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,This is actually very cheap if compared to  CopperEgg's pricing  - $70/mo versus  maybe  $30 per month for 9 instances and copious amounts of alarms.
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,"In reality, you'll probably be paying more like $10/mo."
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,"Pager Duty's tutorial suggests you use SNS, which is another cost."
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,The good thing:  it's dirt cheap .
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,$0.60 per million notifications.
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,"If you ever get above a dollar in a year for SNS, you need to perform some serious reliability improvements on your servers."
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,You're not just limited to Amazon's pre-packaged metrics!
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,"You can actually send custom metrics (time it took to complete a cronjob, whatever) to Cloudwatch via a PUT request."
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,Quite handy.
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,Submit Custom Metrics generated by your own applications (or by AWS resources not mentioned above) and have them monitored by Amazon CloudWatch.
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,You can submit these metrics to Amazon CloudWatch via a simple Put API request.
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,(from  here )
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,"So all in all: CloudWatch is quite cheap, can do 1-minute frequency stats, and will integrate with Pager Duty."
Datadog,29150848,21527387,0,"2015/03/19, 19:10:30",False,"2015/03/19, 19:10:30",7448.0,554903.0,3,In short Server Density is a monitoring tool that will monitor all the relevant server metrics.
Datadog,29150848,21527387,0,"2015/03/19, 19:10:30",False,"2015/03/19, 19:10:30",7448.0,554903.0,3,You can take a look at this page  where it’s all described .
Datadog,29150848,21527387,0,"2015/03/19, 19:10:30",False,"2015/03/19, 19:10:30",7448.0,554903.0,3,One server had high CPU for over 5 mins when the alert should be triggered after 1 minute
Datadog,29150848,21527387,0,"2015/03/19, 19:10:30",False,"2015/03/19, 19:10:30",7448.0,554903.0,3,Server Density’s open source agent collects and posts the data to their server every minute and you can decide yourself when that alert should be triggered.
Datadog,29150848,21527387,0,"2015/03/19, 19:10:30",False,"2015/03/19, 19:10:30",7448.0,554903.0,3,In the alert below you can see that the alert will alert 1 person after 1 minute and then repeatedly alert every 5 minutes.
Datadog,29150848,21527387,0,"2015/03/19, 19:10:30",False,"2015/03/19, 19:10:30",7448.0,554903.0,3,There is a lot of other metrics that you can alert on too.
Datadog,29150848,21527387,0,"2015/03/19, 19:10:30",False,"2015/03/19, 19:10:30",7448.0,554903.0,3,What else is out there that can do the same job and will also integrate with Pager Duty?
Datadog,29150848,21527387,0,"2015/03/19, 19:10:30",False,"2015/03/19, 19:10:30",7448.0,554903.0,3,Server Density also integrates with PagerDuty.
Datadog,29150848,21527387,0,"2015/03/19, 19:10:30",False,"2015/03/19, 19:10:30",7448.0,554903.0,3,The only thing you need to do is to  generate an api key at PagerDuty  and then provide that in the settings.
Datadog,29150848,21527387,0,"2015/03/19, 19:10:30",False,"2015/03/19, 19:10:30",7448.0,554903.0,3,Just provide the API key in the settings and you can then in check pagerduty as one of the alert recipients.
Datadog,29150848,21527387,0,"2015/03/19, 19:10:30",False,"2015/03/19, 19:10:30",7448.0,554903.0,3,You can find the  pricing page here .
Datadog,29150848,21527387,0,"2015/03/19, 19:10:30",False,"2015/03/19, 19:10:30",7448.0,554903.0,3,I’ll give you a brief overview of it.
Datadog,29150848,21527387,0,"2015/03/19, 19:10:30",False,"2015/03/19, 19:10:30",7448.0,554903.0,3,The pricing starts at $10 for one server plus one web check and then get’s cheaper per server the more servers you add.
Datadog,29150848,21527387,0,"2015/03/19, 19:10:30",False,"2015/03/19, 19:10:30",7448.0,554903.0,3,"Everything will be monitored once every minute and there is no fees added for the amount of alerts added or triggered, even if that is an SMS to your phone number."
Datadog,29150848,21527387,0,"2015/03/19, 19:10:30",False,"2015/03/19, 19:10:30",7448.0,554903.0,3,"The cost is slightly more expensive than the Cloudwatch example, but the support is good."
Datadog,29150848,21527387,0,"2015/03/19, 19:10:30",False,"2015/03/19, 19:10:30",7448.0,554903.0,3,If you used copperegg before they have a  migration tool  too.
Datadog,29150848,21527387,0,"2015/03/19, 19:10:30",False,"2015/03/19, 19:10:30",7448.0,554903.0,3,Server Density allows you to monitor all the things!
Datadog,29150848,21527387,0,"2015/03/19, 19:10:30",False,"2015/03/19, 19:10:30",7448.0,554903.0,3,Then only thing you need to do is to send us custom metrics which you can do with a plugin written by yourself or by someone else.
Datadog,29150848,21527387,0,"2015/03/19, 19:10:30",False,"2015/03/19, 19:10:30",7448.0,554903.0,3,I have to say that the graphs that Server Density provides is somewhat akin to eye candy too.
Datadog,29150848,21527387,0,"2015/03/19, 19:10:30",False,"2015/03/19, 19:10:30",7448.0,554903.0,3,Most other monitoring solutions I’ve seen out there have quite dull dashboards.
Datadog,29150848,21527387,0,"2015/03/19, 19:10:30",False,"2015/03/19, 19:10:30",7448.0,554903.0,3,It will do the job for you.
Datadog,29150848,21527387,0,"2015/03/19, 19:10:30",False,"2015/03/19, 19:10:30",7448.0,554903.0,3,"Not as cheap as CloudWatch, but doesn’t lock you in into AWS."
Datadog,29150848,21527387,0,"2015/03/19, 19:10:30",False,"2015/03/19, 19:10:30",7448.0,554903.0,3,It’ll give you 1 minute frequency metrics and integrate with pagerduty + a lot more stuff.
Datadog,62818348,62818107,3,"2020/07/09, 18:25:43",True,"2020/07/13, 17:33:38",6777.0,7875623.0,3,Reference
Datadog,62818348,62818107,3,"2020/07/09, 18:25:43",True,"2020/07/13, 17:33:38",6777.0,7875623.0,3,https://people.apache.org/~dkulp/camel/eventnotifier-to-log-details-about-all-sent-exchanges.html
Datadog,62818348,62818107,3,"2020/07/09, 18:25:43",True,"2020/07/13, 17:33:38",6777.0,7875623.0,3,Update
Datadog,62818348,62818107,3,"2020/07/09, 18:25:43",True,"2020/07/13, 17:33:38",6777.0,7875623.0,3,"The  Exchange.CREATED_TIMESTAMP  is no longer stored as exchange property, but you should use the  getCreated  method on Exchange."
Datadog,63516418,62298190,0,"2020/08/21, 07:12:56",False,"2020/08/21, 07:12:56",11.0,4392334.0,1,"If you put in ECS Task Definition (sample from json version, but in UI also possible to setup), you should be able to configure container logs:"
Datadog,62069822,62056153,2,"2020/05/28, 19:31:43",False,"2020/05/28, 19:38:33",1119.0,6490744.0,0,Since your question says  is there a way to inspect inside/after each task completes  - I'm assuming you haven't tried this celery-result-backend stuff.
Datadog,62069822,62056153,2,"2020/05/28, 19:31:43",False,"2020/05/28, 19:38:33",1119.0,6490744.0,0,So you could check out this feature which is provided by Celery itself :  Celery-Result-Backend / Task-result-Backend  .
Datadog,62069822,62056153,2,"2020/05/28, 19:31:43",False,"2020/05/28, 19:38:33",1119.0,6490744.0,0,It is very useful for storing results of your celery tasks.
Datadog,62069822,62056153,2,"2020/05/28, 19:31:43",False,"2020/05/28, 19:38:33",1119.0,6490744.0,0,Read through this =   https://docs.celeryproject.org/en/stable/userguide/configuration.html#task-result-backend-settings
Datadog,62069822,62056153,2,"2020/05/28, 19:31:43",False,"2020/05/28, 19:38:33",1119.0,6490744.0,0,"Once you get an idea of how to setup this result-backend, Search for  result_extended  key (in the same link) to be able to add  queue-names  in your task return values."
Datadog,62069822,62056153,2,"2020/05/28, 19:31:43",False,"2020/05/28, 19:38:33",1119.0,6490744.0,0,Number of options are available - Like you can setup these results to go to any of these :
Datadog,62069822,62056153,2,"2020/05/28, 19:31:43",False,"2020/05/28, 19:38:33",1119.0,6490744.0,0,I have made use of this  Result-Backend  feature with  Elasticsearch  and this how my task results are stored :
Datadog,62069822,62056153,2,"2020/05/28, 19:31:43",False,"2020/05/28, 19:38:33",1119.0,6490744.0,0,It is just a matter of adding few configurations in  settings.py  file as per your requirements.
Datadog,62069822,62056153,2,"2020/05/28, 19:31:43",False,"2020/05/28, 19:38:33",1119.0,6490744.0,0,Worked really well for my application.
Datadog,62069822,62056153,2,"2020/05/28, 19:31:43",False,"2020/05/28, 19:38:33",1119.0,6490744.0,0,And I have a weekly cron that clears only  successful results  of tasks - since we don't need the results anymore - and I can see only  failed results   (like the one in image).
Datadog,62069822,62056153,2,"2020/05/28, 19:31:43",False,"2020/05/28, 19:38:33",1119.0,6490744.0,0,These were main keys for my requirement :  task_track_started  and  task_acks_late  along with  result_backend
Datadog,60517002,60516923,1,"2020/03/04, 01:27:31",True,"2020/03/04, 01:27:31",25945.0,11923999.0,1,Try  recover  to catch all panics and log them.
Datadog,60517002,60516923,1,"2020/03/04, 01:27:31",True,"2020/03/04, 01:27:31",25945.0,11923999.0,1,"Without that, it'll write the panic msg to stderr:"
Datadog,59605166,59549130,2,"2020/01/06, 01:37:29",True,"2020/01/06, 01:37:29",5837.0,9090751.0,3,You can override the default  TracingInstrumentation  with your own implementation.
Datadog,59605166,59549130,2,"2020/01/06, 01:37:29",True,"2020/01/06, 01:37:29",5837.0,9090751.0,3,It will be picked automatically due to the @ConditionalOnMissingBean annotation in the  GraphQLInstrumentationAutoConfiguration  class.
Datadog,59605166,59549130,2,"2020/01/06, 01:37:29",True,"2020/01/06, 01:37:29",5837.0,9090751.0,3,Here is a simple example that adds two custom metrics:  graphql.counter.query.success  and  graphql.counter.query.error :
Datadog,59605166,59549130,2,"2020/01/06, 01:37:29",True,"2020/01/06, 01:37:29",5837.0,9090751.0,3,"My application.yaml, just in case:"
Datadog,59605166,59549130,2,"2020/01/06, 01:37:29",True,"2020/01/06, 01:37:29",5837.0,9090751.0,3,"I'm using spring-boot-starter-parent:2.2.2.RELEASE, graphql-spring-boot-starter:6.0.0"
Datadog,59605166,59549130,2,"2020/01/06, 01:37:29",True,"2020/01/06, 01:37:29",5837.0,9090751.0,3,I hope it helps.
Datadog,66324527,59102943,0,"2021/02/23, 00:44:33",False,"2021/02/23, 00:44:33",1.0,15263108.0,0,Avg CPU usage may not give better view.
Datadog,66324527,59102943,0,"2021/02/23, 00:44:33",False,"2021/02/23, 00:44:33",1.0,15263108.0,0,Check if max CPU utilization is getting around 100%.
Datadog,66324527,59102943,0,"2021/02/23, 00:44:33",False,"2021/02/23, 00:44:33",1.0,15263108.0,0,"If so, you may need to optimize on ES side."
Datadog,55834007,55832300,2,"2019/04/24, 18:48:15",False,"2019/04/24, 18:48:15",5656.0,11374921.0,1,You should understand how cluster autoscaler works.
Datadog,55834007,55832300,2,"2019/04/24, 18:48:15",False,"2019/04/24, 18:48:15",5656.0,11374921.0,1,It is responsible  only  for adding or removing nodes.
Datadog,55834007,55832300,2,"2019/04/24, 18:48:15",False,"2019/04/24, 18:48:15",5656.0,11374921.0,1,It is not responsible for creating or destroying pods.
Datadog,55834007,55832300,2,"2019/04/24, 18:48:15",False,"2019/04/24, 18:48:15",5656.0,11374921.0,1,So in your case cluster autoscaler is not doing anything because it's useless.
Datadog,55834007,55832300,2,"2019/04/24, 18:48:15",False,"2019/04/24, 18:48:15",5656.0,11374921.0,1,Even if you add one more node - there will be still a requirement to run DaemonSet pods on nodes where is not enough CPU.
Datadog,55834007,55832300,2,"2019/04/24, 18:48:15",False,"2019/04/24, 18:48:15",5656.0,11374921.0,1,That's why it is not adding nodes.
Datadog,55834007,55832300,2,"2019/04/24, 18:48:15",False,"2019/04/24, 18:48:15",5656.0,11374921.0,1,What you should do is to manually remove some pods from occupied nodes.
Datadog,55834007,55832300,2,"2019/04/24, 18:48:15",False,"2019/04/24, 18:48:15",5656.0,11374921.0,1,Then it will be able to schedule DaemonSet pods.
Datadog,55834007,55832300,2,"2019/04/24, 18:48:15",False,"2019/04/24, 18:48:15",5656.0,11374921.0,1,"Alternatively you can reduce CPU requests of Datadog to, for example, 100m or 50m."
Datadog,55834007,55832300,2,"2019/04/24, 18:48:15",False,"2019/04/24, 18:48:15",5656.0,11374921.0,1,This should be enough to start those pods.
Datadog,57509622,55832300,3,"2019/08/15, 15:24:55",True,"2019/08/19, 13:40:01",1425.0,1837991.0,4,You can add priorityClassName to point to a high priority PriorityClass to your DaemonSet.
Datadog,57509622,55832300,3,"2019/08/15, 15:24:55",True,"2019/08/19, 13:40:01",1425.0,1837991.0,4,Kubernetes will then remove other pods in order to run the DaemonSet's pods.
Datadog,57509622,55832300,3,"2019/08/15, 15:24:55",True,"2019/08/19, 13:40:01",1425.0,1837991.0,4,"If that results in unschedulable pods, cluster-autoscaler should add a node to schedule them on."
Datadog,57509622,55832300,3,"2019/08/15, 15:24:55",True,"2019/08/19, 13:40:01",1425.0,1837991.0,4,"See  the docs  (Most examples based on that) (For some pre-1.14 versions, the apiVersion is likely a beta (1.11-1.13) or alpha version (1.8 - 1.10) instead)"
Datadog,57509622,55832300,3,"2019/08/15, 15:24:55",True,"2019/08/19, 13:40:01",1425.0,1837991.0,4,Apply it to your workload
Datadog,54660782,54660692,0,"2019/02/13, 02:35:57",True,"2019/02/13, 03:15:24",185548.0,152948.0,1,Here are two ways that work:
Datadog,54660782,54660692,0,"2019/02/13, 02:35:57",True,"2019/02/13, 03:15:24",185548.0,152948.0,1,1.
Datadog,54660782,54660692,0,"2019/02/13, 02:35:57",True,"2019/02/13, 03:15:24",185548.0,152948.0,1,"(Drill down to  .services , remember it as  $services  for later use, get the list of keys, and select the ones such that the corresponding value in  $services  has a  build  key)."
Datadog,54660782,54660692,0,"2019/02/13, 02:35:57",True,"2019/02/13, 03:15:24",185548.0,152948.0,1,2.
Datadog,54660782,54660692,0,"2019/02/13, 02:35:57",True,"2019/02/13, 03:15:24",185548.0,152948.0,1,"(Drill down to  .services , convert to a list of  {""key"": ..., ""value"": ...}  objects, select the ones where the  .value  has a  build  key, and return the  .key  for each)."
Datadog,54660782,54660692,0,"2019/02/13, 02:35:57",True,"2019/02/13, 03:15:24",185548.0,152948.0,1,"The second is probably more idiomatic jq, but the first provides an interesting way to think about the problem as well."
Datadog,54660903,54660692,0,"2019/02/13, 02:51:04",False,"2019/02/13, 08:06:26",70657.0,997358.0,2,"Here's a third approach, notable for being oblivious to the upper reaches:"
Datadog,53328483,53323761,1,"2018/11/15, 23:58:41",False,"2018/11/15, 23:58:41",1283.0,10657880.0,0,Have you tried to mock the  datalog  module inside your function  test ?
Datadog,53328483,53323761,1,"2018/11/15, 23:58:41",False,"2018/11/15, 23:58:41",1283.0,10657880.0,0,"As long as your other scripts are not running concurrently with your test, this may work."
Datadog,53328483,53323761,1,"2018/11/15, 23:58:41",False,"2018/11/15, 23:58:41",1283.0,10657880.0,0,"That way the mock itself will be set only when the function is called, instead of being set in your script scope."
Datadog,53363259,53323761,0,"2018/11/18, 18:50:17",False,"2018/11/18, 18:50:17",1223.0,1939996.0,0,You could use  unittest.mock.patch .
Datadog,53363259,53323761,0,"2018/11/18, 18:50:17",False,"2018/11/18, 18:50:17",1223.0,1939996.0,0,If you are using pytest you can do the same with the  monkeypatch  fixture.
Datadog,52703220,52703024,0,"2018/10/08, 16:16:52",False,"2018/10/08, 16:16:52",9507.0,1312478.0,0,I have used many of solutions you mentioned.
Datadog,52703220,52703024,0,"2018/10/08, 16:16:52",False,"2018/10/08, 16:16:52",9507.0,1312478.0,0,Splunk is good but it becomes really expensive if you have huge amount of data.
Datadog,52703220,52703024,0,"2018/10/08, 16:16:52",False,"2018/10/08, 16:16:52",9507.0,1312478.0,0,You could have always used Cloudwatch Logs but it doesn't give you so much on visual part..
Datadog,52703220,52703024,0,"2018/10/08, 16:16:52",False,"2018/10/08, 16:16:52",9507.0,1312478.0,0,"I will recommend ELK (ElasticSearch, Logstash, Kibana) stack."
Datadog,52703220,52703024,0,"2018/10/08, 16:16:52",False,"2018/10/08, 16:16:52",9507.0,1312478.0,0,It is a very standard solution; in which logs are stored in Elastic Search.
Datadog,52703220,52703024,0,"2018/10/08, 16:16:52",False,"2018/10/08, 16:16:52",9507.0,1312478.0,0,Kibana is used for visualization of logs.
Datadog,52703220,52703024,0,"2018/10/08, 16:16:52",False,"2018/10/08, 16:16:52",9507.0,1312478.0,0,This works in almost real time.
Datadog,52703220,52703024,0,"2018/10/08, 16:16:52",False,"2018/10/08, 16:16:52",9507.0,1312478.0,0,If you have very specific dashboards; then you can always create custom dashboards using some front end technologies like AngularJS etc.
Datadog,52703220,52703024,0,"2018/10/08, 16:16:52",False,"2018/10/08, 16:16:52",9507.0,1312478.0,0,but if visual part is really huge and very flexible then I feel ELK is better.
Datadog,52703399,52703024,0,"2018/10/08, 16:27:05",False,"2018/10/08, 16:27:05",1508.0,1623047.0,0,"ELK (ElasticSearch, Logstash, Kibana) stack is a really good solution for what you are looking for, but in some cases ELK is not going to be able to get some metrics, in this case you have some solutions like create your own  beat  program to get the information or use another program to gather this metrics like Apache NiFi."
Datadog,52703411,52703024,0,"2018/10/08, 16:27:49",False,"2018/10/08, 16:27:49",3533.0,6619626.0,1,"You can use AWS CloudWatch, create a log stream for each of your application or service."
Datadog,52703411,52703024,0,"2018/10/08, 16:27:49",False,"2018/10/08, 16:27:49",3533.0,6619626.0,1,"Define your custom metrics, create a dashboard and alert."
Datadog,52703411,52703024,0,"2018/10/08, 16:27:49",False,"2018/10/08, 16:27:49",3533.0,6619626.0,1,It's not limited to AWS things; you can use CloudWatch log agent for On-premises services or software on your local network.
Datadog,52703411,52703024,0,"2018/10/08, 16:27:49",False,"2018/10/08, 16:27:49",3533.0,6619626.0,1,For more information read the following article by Jeff Barr
Datadog,52703411,52703024,0,"2018/10/08, 16:27:49",False,"2018/10/08, 16:27:49",3533.0,6619626.0,1,https://aws.amazon.com/blogs/aws/cloudwatch-log-service/
Datadog,52703411,52703024,0,"2018/10/08, 16:27:49",False,"2018/10/08, 16:27:49",3533.0,6619626.0,1,and
Datadog,52703411,52703024,0,"2018/10/08, 16:27:49",False,"2018/10/08, 16:27:49",3533.0,6619626.0,1,https://aws.amazon.com/blogs/aws/improvements-to-cloudwatch-logs-dashboards/
Datadog,52703411,52703024,0,"2018/10/08, 16:27:49",False,"2018/10/08, 16:27:49",3533.0,6619626.0,1,"FYI: we already monitor a lot of application and service inside and outside of AWS by CloudWatch, and it works like a charm."
Datadog,52692334,52689144,5,"2018/10/07, 22:57:28",True,"2018/10/08, 13:07:16",3260.0,950762.0,0,It seems normal because  Ansible  does not refer to Python virtual environment in your case:
Datadog,52692334,52689144,5,"2018/10/07, 22:57:28",True,"2018/10/08, 13:07:16",3260.0,950762.0,0,In  virtualenv  non-installed packages are initialized from real system environment.
Datadog,52692334,52689144,5,"2018/10/07, 22:57:28",True,"2018/10/08, 13:07:16",3260.0,950762.0,0,So you can achieve it by setting up  Ansible  within  virtualenv
Datadog,52692334,52689144,5,"2018/10/07, 22:57:28",True,"2018/10/08, 13:07:16",3260.0,950762.0,0,Have a look at this example:
Datadog,52692334,52689144,5,"2018/10/07, 22:57:28",True,"2018/10/08, 13:07:16",3260.0,950762.0,0,After installation of  Ansible  in  virtualenv
Datadog,52692334,52689144,5,"2018/10/07, 22:57:28",True,"2018/10/08, 13:07:16",3260.0,950762.0,0,Ansible  refers to the paths of Python virtual environment:
Datadog,52692334,52689144,5,"2018/10/07, 22:57:28",True,"2018/10/08, 13:07:16",3260.0,950762.0,0,ps: Need to deactivate and activate again the  virtualenv  once to load the  Ansible  from virtual environment after the installation.
Datadog,52834860,51690297,0,"2018/10/16, 14:52:14",False,"2018/10/16, 14:52:14",160.0,9716385.0,2,To get you started: Create a timelion expression:
Datadog,52959391,51690297,1,"2018/10/24, 03:10:30",False,"2018/10/24, 03:10:30",2567.0,7983309.0,4,What you are looking for is achievable using Visual Builder visualization
Datadog,52959391,51690297,1,"2018/10/24, 03:10:30",False,"2018/10/24, 03:10:30",2567.0,7983309.0,4,See  https://www.elastic.co/guide/en/kibana/6.1/time-series-visualizations.html
Datadog,52959391,51690297,1,"2018/10/24, 03:10:30",False,"2018/10/24, 03:10:30",2567.0,7983309.0,4,Aggregate  Max  or  Avg  on  system.cpu.total.pct
Datadog,52959391,51690297,1,"2018/10/24, 03:10:30",False,"2018/10/24, 03:10:30",2567.0,7983309.0,4,Group By  Terms  By  beat.hostname.keyword
Datadog,52959391,51690297,1,"2018/10/24, 03:10:30",False,"2018/10/24, 03:10:30",2567.0,7983309.0,4,The visualization will show CPU usage in % for all hosts sending metrics to your cluster.
Datadog,52959391,51690297,1,"2018/10/24, 03:10:30",False,"2018/10/24, 03:10:30",2567.0,7983309.0,4,If you add more hosts those will show up too!
Datadog,55875339,51690297,0,"2019/04/27, 00:53:24",False,"2019/04/27, 00:53:24",12414.0,33204.0,0,This is just to illustrate @ben5556's answer with an image.
Datadog,55875339,51690297,0,"2019/04/27, 00:53:24",False,"2019/04/27, 00:53:24",12414.0,33204.0,0,"NOTE:  The ""Term"" is  beat.hostname ."
Datadog,53874603,50794695,2,"2018/12/20, 21:06:35",False,"2018/12/20, 21:06:35",101.0,4771526.0,0,sorry for the delay.
Datadog,53874603,50794695,2,"2018/12/20, 21:06:35",False,"2018/12/20, 21:06:35",101.0,4771526.0,0,From your error log I can't see any issues on recovery being thrown but I either don't see any connection attempts.
Datadog,53874603,50794695,2,"2018/12/20, 21:06:35",False,"2018/12/20, 21:06:35",101.0,4771526.0,0,I wonder if you have some issues with data in the group replication relay logs...
Datadog,53874603,50794695,2,"2018/12/20, 21:06:35",False,"2018/12/20, 21:06:35",101.0,4771526.0,0,I suggest you open a bug if the problem still persists.
Datadog,53874603,50794695,2,"2018/12/20, 21:06:35",False,"2018/12/20, 21:06:35",101.0,4771526.0,0,"As a workaround you can try to reset the applier channel before ""START GROUP_REPLICATION"""
Datadog,53874603,50794695,2,"2018/12/20, 21:06:35",False,"2018/12/20, 21:06:35",101.0,4771526.0,0,"RESET SLAVE ALL FOR CHANNEL ""group_replication_applier"";"
Datadog,47014916,47013899,0,"2017/10/30, 14:10:13",False,"2017/10/30, 14:10:13",3148.0,7925197.0,0,The telegraf/influxdb/grafana stack can monitor space left on disc.
Datadog,47014916,47013899,0,"2017/10/30, 14:10:13",False,"2017/10/30, 14:10:13",3148.0,7925197.0,0,Kapacitor can also be added if you want alerts.
Datadog,47014916,47013899,0,"2017/10/30, 14:10:13",False,"2017/10/30, 14:10:13",3148.0,7925197.0,0,"If you want to specify a limit, you have to use a dedicated partition / mount point or a btrfs subvolume with quotas."
Datadog,47016362,47013899,0,"2017/10/30, 15:22:49",False,"2017/10/30, 15:22:49",21.0,7688163.0,0,"Another option is to make cron job to clean up unused docker images, unused docker volume, and exited docker container."
Datadog,47016362,47013899,0,"2017/10/30, 15:22:49",False,"2017/10/30, 15:22:49",21.0,7688163.0,0,I use this method myself.
Datadog,46739062,46716705,0,"2017/10/14, 01:56:40",False,"2017/10/14, 01:56:40",3994.0,505196.0,0,A better approach than using DaemonSets to run your application would be to use a Deployment so that you don't tie your application to the number of nodes in your cluster.
Datadog,46739062,46716705,0,"2017/10/14, 01:56:40",False,"2017/10/14, 01:56:40",3994.0,505196.0,0,You can then deploy the datadog agent image as a DaemonSet with a set  spec.template.spec.affinity  that selects nodes with a pod of your application running.
Datadog,46739062,46716705,0,"2017/10/14, 01:56:40",False,"2017/10/14, 01:56:40",3994.0,505196.0,0,This will make sure you have a datadog agent in every node where your application runs.
Datadog,46739062,46716705,0,"2017/10/14, 01:56:40",False,"2017/10/14, 01:56:40",3994.0,505196.0,0,Another option is to deploy the datadog agent container in the same pod as your application container.
Datadog,46739062,46716705,0,"2017/10/14, 01:56:40",False,"2017/10/14, 01:56:40",3994.0,505196.0,0,"In this case you can reach the agent through localhost and scale together, but might end up with more than an agent per node, hence my preference for a DaemonSet with an affinity."
Datadog,46740957,46716705,0,"2017/10/14, 07:44:41",False,"2017/10/14, 07:44:41",31231.0,242493.0,0,"My team ran it as a daemon set for the purposes of collecting node metrics, but only exposed it as a normal cluster IP service for the purposes of programmatically sending it data from other apps in the cluster."
Datadog,46740957,46716705,0,"2017/10/14, 07:44:41",False,"2017/10/14, 07:44:41",31231.0,242493.0,0,You don't need to expose it on a node port unless you need to access it from outside the cluster and don't have a service-aware load balancer like an ingress controller.
Datadog,46740957,46716705,0,"2017/10/14, 07:44:41",False,"2017/10/14, 07:44:41",31231.0,242493.0,0,"(That would be quite a strange use case, so chances are you don't need to expose it on a node port.)"
Datadog,46411356,46407772,1,"2017/09/25, 20:56:47",True,"2017/09/25, 20:56:47",17772.0,1494519.0,3,is the name of the default queue.
Datadog,46411356,46407772,1,"2017/09/25, 20:56:47",True,"2017/09/25, 20:56:47",17772.0,1494519.0,3,"As you state, it is ""queue:$NAME"" but namespaces (if you use them (please don't)) will also prefix the key."
Datadog,44550799,44550138,0,"2017/06/14, 20:06:35",True,"2017/06/14, 20:06:35",1028.0,8020142.0,0,You can not set a name on the instances of docker that manages amazon.
Datadog,44550799,44550138,0,"2017/06/14, 20:06:35",True,"2017/06/14, 20:06:35",1028.0,8020142.0,0,The namespaces it uses are to be able to handle the scaling of the service.
Datadog,44550799,44550138,0,"2017/06/14, 20:06:35",True,"2017/06/14, 20:06:35",1028.0,8020142.0,0,"Think that if you write the name and then the service you ask for more than one instance of your application, amazon could not instantiate it on the same node."
Datadog,44550799,44550138,0,"2017/06/14, 20:06:35",True,"2017/06/14, 20:06:35",1028.0,8020142.0,0,I hope the explanation has served.
Datadog,44551679,44550138,0,"2017/06/14, 21:02:08",False,"2017/06/14, 21:02:08",3575.0,112821.0,4,"No, there is not a way to control the name used for the container in Amazon ECS."
Datadog,44551679,44550138,0,"2017/06/14, 21:02:08",False,"2017/06/14, 21:02:08",3575.0,112821.0,4,ECS picks a random name designed to avoid conflicts (since names must be unique in Docker; you can't have two containers with the same name) and you can see the code  here .
Datadog,44551679,44550138,0,"2017/06/14, 21:02:08",False,"2017/06/14, 21:02:08",3575.0,112821.0,4,"However, ECS does give you a few things that might be able to help you."
Datadog,44551679,44550138,0,"2017/06/14, 21:02:08",False,"2017/06/14, 21:02:08",3575.0,112821.0,4,"There are automatically-assigned Docker labels for the task ARN, the container name in your task definition, the task definition family, the task definition revision, and the cluster; see  here ."
Datadog,44551679,44550138,0,"2017/06/14, 21:02:08",False,"2017/06/14, 21:02:08",3575.0,112821.0,4,"Additionally, you can assign your own custom Docker labels through the task definition."
Datadog,39333248,39323306,9,"2016/09/05, 17:58:10",True,"2016/09/05, 17:58:10",2479.0,5027078.0,2,I just had some difficulties to determine what is exactly your second separator.
Datadog,39333248,39323306,9,"2016/09/05, 17:58:10",True,"2016/09/05, 17:58:10",2479.0,5027078.0,2,"you text example shows '·', but when I checked what is just after 'Elberg"" and before '2nd...', I found 4 characters : code 32 (space), code 194 (¬), code 183 (∑), code 32 (space)."
Datadog,39333248,39323306,9,"2016/09/05, 17:58:10",True,"2016/09/05, 17:58:10",2479.0,5027078.0,2,"In the script bellow, I have used the code 194. it works when I cut/paste your text example into a file."
Datadog,39333248,39323306,9,"2016/09/05, 17:58:10",True,"2016/09/05, 17:58:10",2479.0,5027078.0,2,Here is the script :
Datadog,39333248,39323306,9,"2016/09/05, 17:58:10",True,"2016/09/05, 17:58:10",2479.0,5027078.0,2,"Note : if the text does not contain ""Job posted by "", then myAuthor is ''."
Datadog,39334390,39323306,0,"2016/09/05, 19:17:04",False,"2016/09/05, 19:17:04",1031.0,5390105.0,0,"You had the right idea to use  AppleScript's text item delimiters , but the way you tried to extract the name was giving you trouble."
Datadog,39334390,39323306,0,"2016/09/05, 19:17:04",False,"2016/09/05, 19:17:04",1031.0,5390105.0,0,"First, though, I'll go through some things you can do to improve your script:"
Datadog,39334390,39323306,0,"2016/09/05, 19:17:04",False,"2016/09/05, 19:17:04",1031.0,5390105.0,0,"There's no need to break the file contents into lines; AppleScript can operate on entire paragraphs or more, if desired."
Datadog,39334390,39323306,0,"2016/09/05, 19:17:04",False,"2016/09/05, 19:17:04",1031.0,5390105.0,0,Removing these unnecessary steps (and adding new ones to make it work on the entire file) shrinks the script considerably:
Datadog,39334390,39323306,0,"2016/09/05, 19:17:04",False,"2016/09/05, 19:17:04",1031.0,5390105.0,0,This right here is what's giving you wrong output:
Datadog,39334390,39323306,0,"2016/09/05, 19:17:04",False,"2016/09/05, 19:17:04",1031.0,5390105.0,0,This is incorrect.
Datadog,39334390,39323306,0,"2016/09/05, 19:17:04",False,"2016/09/05, 19:17:04",1031.0,5390105.0,0,It's not the  last  word you want; that's the last word of the file!
Datadog,39334390,39323306,0,"2016/09/05, 19:17:04",False,"2016/09/05, 19:17:04",1031.0,5390105.0,0,"To extract the poster of the job listing, change it to the following:"
Datadog,39334390,39323306,0,"2016/09/05, 19:17:04",False,"2016/09/05, 19:17:04",1031.0,5390105.0,0,"Due to AppleScript's weird Unicode handling, for whatever reason the dot (·) that separates the name from the other text is converted to ""¬∑"" when run though the script."
Datadog,39334390,39323306,0,"2016/09/05, 19:17:04",False,"2016/09/05, 19:17:04",1031.0,5390105.0,0,"So, we look for ""¬"" instead."
Datadog,39334390,39323306,0,"2016/09/05, 19:17:04",False,"2016/09/05, 19:17:04",1031.0,5390105.0,0,Some last code fixes:
Datadog,39334390,39323306,0,"2016/09/05, 19:17:04",False,"2016/09/05, 19:17:04",1031.0,5390105.0,0,"Some of your variable names use  the_snake_case , while others use  theCamelCase ."
Datadog,39334390,39323306,0,"2016/09/05, 19:17:04",False,"2016/09/05, 19:17:04",1031.0,5390105.0,0,"It's generally a good idea to use one convention or another, so I fixed that, too."
Datadog,39334390,39323306,0,"2016/09/05, 19:17:04",False,"2016/09/05, 19:17:04",1031.0,5390105.0,0,"I assumed you wanted that dollar sign in the output for whatever reason, so I kept it in."
Datadog,39334390,39323306,0,"2016/09/05, 19:17:04",False,"2016/09/05, 19:17:04",1031.0,5390105.0,0,"If you don't want it, just replace  set output to ""$ ""  with  set output to """" ."
Datadog,39334390,39323306,0,"2016/09/05, 19:17:04",False,"2016/09/05, 19:17:04",1031.0,5390105.0,0,"So, your final, working script looks like this:"
Datadog,38986680,38986190,0,"2016/08/17, 04:08:24",False,"2016/08/17, 04:08:24",2571.0,2149974.0,0,"I agree it's hard to find, the closest one I can find is this"
Datadog,38986680,38986190,0,"2016/08/17, 04:08:24",False,"2016/08/17, 04:08:24",2571.0,2149974.0,0,"Return the number of instances that are set for the given module
  version."
Datadog,38986680,38986190,0,"2016/08/17, 04:08:24",False,"2016/08/17, 04:08:24",2571.0,2149974.0,0,"This is only valid for fixed modules, an error will be raised for automatically-scaled modules."
Datadog,38986680,38986190,0,"2016/08/17, 04:08:24",False,"2016/08/17, 04:08:24",2571.0,2149974.0,0,Support for automatically-scaled modules may be supported in the future.
Datadog,38986680,38986190,0,"2016/08/17, 04:08:24",False,"2016/08/17, 04:08:24",2571.0,2149974.0,0,https://cloud.google.com/appengine/docs/python/refdocs/google.appengine.api.modules.modules
Datadog,38986680,38986190,0,"2016/08/17, 04:08:24",False,"2016/08/17, 04:08:24",2571.0,2149974.0,0,Btw you can also have monitoring from the StackDriver which has metric for total instance
Datadog,38999762,38986190,5,"2016/08/17, 17:42:16",False,"2016/08/17, 17:42:16",7229.0,178236.0,4,You should also be able to use the recently GA'd App Engine Admin API to figure this out.
Datadog,38999762,38986190,5,"2016/08/17, 17:42:16",False,"2016/08/17, 17:42:16",7229.0,178236.0,4,"The nice thing about the admin API is that it's going to work for both standard and flexible:
 https://cloud.google.com/appengine/docs/admin-api/"
Datadog,38999762,38986190,5,"2016/08/17, 17:42:16",False,"2016/08/17, 17:42:16",7229.0,178236.0,4,Here's the endpoint that returns all of the instances for a given service/version:
Datadog,38999762,38986190,5,"2016/08/17, 17:42:16",False,"2016/08/17, 17:42:16",7229.0,178236.0,4,https://cloud.google.com/appengine/docs/admin-api/reference/rest/v1/apps.services.versions.instances/list
Datadog,38999762,38986190,5,"2016/08/17, 17:42:16",False,"2016/08/17, 17:42:16",7229.0,178236.0,4,"Depending on the language you're using, there's usually a nice wrapper in the form of a ""Google API client"" + language library."
Datadog,38999762,38986190,5,"2016/08/17, 17:42:16",False,"2016/08/17, 17:42:16",7229.0,178236.0,4,Hope this helps!
Datadog,39003441,38986190,0,"2016/08/17, 21:02:18",False,"2016/08/17, 21:02:18",2646.0,1543502.0,0,"If you're trying to collect stats, you might want to use the  Stackdriver Monitoring API  to collect the timeseries values that Google has already aggregated."
Datadog,39003441,38986190,0,"2016/08/17, 21:02:18",False,"2016/08/17, 21:02:18",2646.0,1543502.0,0,"In particular, the list of  App Engine Metrics is here ."
Datadog,39003441,38986190,0,"2016/08/17, 21:02:18",False,"2016/08/17, 21:02:18",2646.0,1543502.0,0,"For example,  system/instance_count  is the metric indicating the number of instances App Engine is running."
Datadog,39007294,38986190,0,"2016/08/18, 01:36:25",True,"2016/08/19, 08:04:25",10698.0,30997.0,1,"I hate it when SO questions only end up with partial answers, so here's a complete, working example."
Datadog,39007294,38986190,0,"2016/08/18, 01:36:25",True,"2016/08/19, 08:04:25",10698.0,30997.0,1,"If you paste it into your interactive console, it should work for you."
Datadog,39007294,38986190,0,"2016/08/18, 01:36:25",True,"2016/08/19, 08:04:25",10698.0,30997.0,1,(Don't forget to set the  versionsId  to whatever your default app version is.
Datadog,39007294,38986190,0,"2016/08/18, 01:36:25",True,"2016/08/19, 08:04:25",10698.0,30997.0,1,"If you know how I can get it to use the default version, please post a comment."
Datadog,39007294,38986190,0,"2016/08/18, 01:36:25",True,"2016/08/19, 08:04:25",10698.0,30997.0,1,"'default', '*', 'any', etc."
Datadog,39007294,38986190,0,"2016/08/18, 01:36:25",True,"2016/08/19, 08:04:25",10698.0,30997.0,1,all no da workie.)
Datadog,39007294,38986190,0,"2016/08/18, 01:36:25",True,"2016/08/19, 08:04:25",10698.0,30997.0,1,Strictly achieved by trial and error:
Datadog,35230909,35219961,0,"2016/02/05, 20:20:41",True,"2016/02/05, 20:20:41",94.0,2207886.0,2,The easiest way to proceed is to create a custom check.
Datadog,35230909,35219961,0,"2016/02/05, 20:20:41",True,"2016/02/05, 20:20:41",94.0,2207886.0,2,You can read up on this here:  http://docs.datadoghq.com/guides/agent_checks/ .
Datadog,35230909,35219961,0,"2016/02/05, 20:20:41",True,"2016/02/05, 20:20:41",94.0,2207886.0,2,"There isn't a way to take a pre-existing Nagios or Sensu plugin and have it work as is with Datadog, but looking at one of the delayed_job plugins on Github, looks like it should be pretty easy to convert to a Datadog check."
Datadog,35230909,35219961,0,"2016/02/05, 20:20:41",True,"2016/02/05, 20:20:41",94.0,2207886.0,2,"If you have any issues, reach out to support either via email or #datadog on IRC."
Datadog,62680072,62436021,3,"2020/07/01, 18:20:46",True,"2020/07/01, 18:20:46",844.0,11921495.0,3,This was a issue with deployed DataDog daemonset for me:
Datadog,62680072,62436021,3,"2020/07/01, 18:20:46",True,"2020/07/01, 18:20:46",844.0,11921495.0,3,What I did to resolve:
Datadog,62680072,62436021,3,"2020/07/01, 18:20:46",True,"2020/07/01, 18:20:46",844.0,11921495.0,3,Check daemonset if it exists or not:
Datadog,62680072,62436021,3,"2020/07/01, 18:20:46",True,"2020/07/01, 18:20:46",844.0,11921495.0,3,Edit the datadog daemonset:
Datadog,62680072,62436021,3,"2020/07/01, 18:20:46",True,"2020/07/01, 18:20:46",844.0,11921495.0,3,"In the opened yaml, add"
Datadog,62680072,62436021,3,"2020/07/01, 18:20:46",True,"2020/07/01, 18:20:46",844.0,11921495.0,3,Add this in  env:  tag for all places.
Datadog,62680072,62436021,3,"2020/07/01, 18:20:46",True,"2020/07/01, 18:20:46",844.0,11921495.0,3,For me there were 4 places which are having DD tags in the yaml.
Datadog,62680072,62436021,3,"2020/07/01, 18:20:46",True,"2020/07/01, 18:20:46",844.0,11921495.0,3,Save and close it.
Datadog,62680072,62436021,3,"2020/07/01, 18:20:46",True,"2020/07/01, 18:20:46",844.0,11921495.0,3,The daemonset will restart.
Datadog,62680072,62436021,3,"2020/07/01, 18:20:46",True,"2020/07/01, 18:20:46",844.0,11921495.0,3,And the application will start getting traced.
Datadog,67104589,62436021,0,"2021/04/15, 11:08:06",False,"2021/04/15, 11:08:06",1929.0,2179157.0,0,"If you are using the Helm chart, you can overwrite on the values:"
Datadog,64803224,60491872,0,"2020/11/12, 13:39:22",False,"2020/11/12, 13:39:22",129.0,6103623.0,1,hey!
Datadog,64803224,60491872,0,"2020/11/12, 13:39:22",False,"2020/11/12, 13:39:22",129.0,6103623.0,1,"Sorry in advance if my answer isn't correct because  I'm a complete newby  in kuber and helm and I can't make sure that it will help, but maybe it helps."
Datadog,64803224,60491872,0,"2020/11/12, 13:39:22",False,"2020/11/12, 13:39:22",129.0,6103623.0,1,"So, the problem, as I can understand, in the resulting  ConfigMap  configuration."
Datadog,64803224,60491872,0,"2020/11/12, 13:39:22",False,"2020/11/12, 13:39:22",129.0,6103623.0,1,"From my expirience, I faced the same with the following config:"
Datadog,64803224,60491872,0,"2020/11/12, 13:39:22",False,"2020/11/12, 13:39:22",129.0,6103623.0,1,And I could solve it only by surrounding with quotes all the values:
Datadog,59159712,59145932,0,"2019/12/03, 16:50:16",False,"2019/12/03, 16:50:16",1038.0,796064.0,2,"Per Yuri's suggestion, I found the culprit, and this is how (thanks to Google Support for walking me through this):"
Datadog,59159712,59145932,0,"2019/12/03, 16:50:16",False,"2019/12/03, 16:50:16",1038.0,796064.0,2,"This showed me a graph making it clear just about all of my requests were coming from a credential named  datadog-metrics-collection , a service account I'd set up previously to collect GCP metrics and emit to Datadog."
Datadog,59164889,59145932,2,"2019/12/03, 22:32:13",False,"2019/12/03, 22:32:13",136.0,11980517.0,0,"Considering the answer posted and question, If we think we do not need Stackdriver monitoring, we can disable stackdriver monitoring API using bellow steps:"
Datadog,59164889,59145932,2,"2019/12/03, 22:32:13",False,"2019/12/03, 22:32:13",136.0,11980517.0,0,In addition you can view Stackdriver usage by billing account and also can estimate cost using Stackdriver pricing calculator [a] [b].
Datadog,59164889,59145932,2,"2019/12/03, 22:32:13",False,"2019/12/03, 22:32:13",136.0,11980517.0,0,View Stackdriver usage by billing account:
Datadog,59164889,59145932,2,"2019/12/03, 22:32:13",False,"2019/12/03, 22:32:13",136.0,11980517.0,0,4.Select Group By   SKU.
Datadog,59164889,59145932,2,"2019/12/03, 22:32:13",False,"2019/12/03, 22:32:13",136.0,11980517.0,0,"This menu might be hidden; you can access it by clicking Show 
  Filters."
Datadog,59164889,59145932,2,"2019/12/03, 22:32:13",False,"2019/12/03, 22:32:13",136.0,11980517.0,0,You can also select just one or some of these SKUs if you don't want to group your usage data.
Datadog,59164889,59145932,2,"2019/12/03, 22:32:13",False,"2019/12/03, 22:32:13",136.0,11980517.0,0,"Note: If your usage of any of these SKUs is 0, they don't appear in the Group By   SKU pull-down menu."
Datadog,59164889,59145932,2,"2019/12/03, 22:32:13",False,"2019/12/03, 22:32:13",136.0,11980517.0,0,"For example, who use only the Cloud console might never generate API requests, so Monitoring API Requests doesn't appear in the list."
Datadog,59164889,59145932,2,"2019/12/03, 22:32:13",False,"2019/12/03, 22:32:13",136.0,11980517.0,0,Use the Stackdriver pricing calculator [b]:
Datadog,59164889,59145932,2,"2019/12/03, 22:32:13",False,"2019/12/03, 22:32:13",136.0,11980517.0,0,[a]  https://cloud.google.com/stackdriver/estimating-bills#billing-acct-usage
Datadog,59164889,59145932,2,"2019/12/03, 22:32:13",False,"2019/12/03, 22:32:13",136.0,11980517.0,0,[b]  https://cloud.google.com/products/calculator/#tab=google-stackdriver
Datadog,59055123,59046195,0,"2019/11/26, 18:14:07",True,"2019/11/26, 18:14:07",11561.0,970308.0,2,You need to  start  the publishing.
Datadog,59055123,59046195,0,"2019/11/26, 18:14:07",True,"2019/11/26, 18:14:07",11561.0,970308.0,2,Compare with the  LoggingMeterRegistry
Datadog,59055123,59046195,0,"2019/11/26, 18:14:07",True,"2019/11/26, 18:14:07",11561.0,970308.0,2,In your constructor something like:
Datadog,38089941,38088122,0,"2016/06/29, 06:35:15",False,"2016/06/29, 06:35:15",5780.0,6163736.0,3,Why not try?
Datadog,38089941,38088122,0,"2016/06/29, 06:35:15",False,"2016/06/29, 06:35:15",5780.0,6163736.0,3,It will return the docker container's hostname.
Datadog,38089941,38088122,0,"2016/06/29, 06:35:15",False,"2016/06/29, 06:35:15",5780.0,6163736.0,3,"If you haven't set a hostname explicitly, using something like  docker run -h hostname image command  then it will return the docker host's hostname."
Datadog,38089941,38088122,0,"2016/06/29, 06:35:15",False,"2016/06/29, 06:35:15",5780.0,6163736.0,3,"Alternatively, you could do this using a deployment tool like puppet, ansible, etc."
Datadog,38089941,38088122,0,"2016/06/29, 06:35:15",False,"2016/06/29, 06:35:15",5780.0,6163736.0,3,and template the file when you deploy the container.
Datadog,66775560,66775454,2,"2021/03/24, 08:05:50",False,"2021/03/24, 08:05:50",2752.0,2672947.0,2,See  HTTP response status codes
Datadog,66775560,66775454,2,"2021/03/24, 08:05:50",False,"2021/03/24, 08:05:50",2752.0,2672947.0,2,The categories are generally:
Datadog,66775560,66775454,2,"2021/03/24, 08:05:50",False,"2021/03/24, 08:05:50",2752.0,2672947.0,2,"So 4XX errors are errors, but they indicate the client is likely at fault."
Datadog,66775560,66775454,2,"2021/03/24, 08:05:50",False,"2021/03/24, 08:05:50",2752.0,2672947.0,2,E.g.
Datadog,66775560,66775454,2,"2021/03/24, 08:05:50",False,"2021/03/24, 08:05:50",2752.0,2672947.0,2,The user went to a page or user agent made a request to a page that does not exist.
Datadog,66775560,66775454,2,"2021/03/24, 08:05:50",False,"2021/03/24, 08:05:50",2752.0,2672947.0,2,"The server responds with 404 because &quot;Everything on my end is fine, but that page isn't real.&quot;"
Datadog,66775560,66775454,2,"2021/03/24, 08:05:50",False,"2021/03/24, 08:05:50",2752.0,2672947.0,2,Is it an error?
Datadog,66775560,66775454,2,"2021/03/24, 08:05:50",False,"2021/03/24, 08:05:50",2752.0,2672947.0,2,Sure.
Datadog,66775560,66775454,2,"2021/03/24, 08:05:50",False,"2021/03/24, 08:05:50",2752.0,2672947.0,2,Could you potentially identify issues (e.g.
Datadog,66775560,66775454,2,"2021/03/24, 08:05:50",False,"2021/03/24, 08:05:50",2752.0,2672947.0,2,"typos in links, missing pages, misspellings, malformed API requests, etc..) by routing these to your logs?"
Datadog,66775560,66775454,2,"2021/03/24, 08:05:50",False,"2021/03/24, 08:05:50",2752.0,2672947.0,2,Sure.
Datadog,66775560,66775454,2,"2021/03/24, 08:05:50",False,"2021/03/24, 08:05:50",2752.0,2672947.0,2,Are you obligated to take action on it?
Datadog,66775560,66775454,2,"2021/03/24, 08:05:50",False,"2021/03/24, 08:05:50",2752.0,2672947.0,2,Not if you don't want to.
Datadog,66775560,66775454,2,"2021/03/24, 08:05:50",False,"2021/03/24, 08:05:50",2752.0,2672947.0,2,You're probably best to determine why you feel they are not actionable.
Datadog,66775560,66775454,2,"2021/03/24, 08:05:50",False,"2021/03/24, 08:05:50",2752.0,2672947.0,2,Most likely are actionable.
Datadog,65953918,65951118,0,"2021/01/29, 13:51:14",True,"2021/01/29, 13:51:14",84586.0,1384297.0,2,Micrometer uses  MeterFilter s registered with a  MeterRegistry  to modified the meters that are registered.
Datadog,65953918,65951118,0,"2021/01/29, 13:51:14",True,"2021/01/29, 13:51:14",84586.0,1384297.0,2,The modifications include the ability to map a meter's ID to something different.
Datadog,65953918,65951118,0,"2021/01/29, 13:51:14",True,"2021/01/29, 13:51:14",84586.0,1384297.0,2,"In Spring Boot, you can use a  MeterRegistryCustomizer  bean to add a  MeterFilter  to a registry."
Datadog,65953918,65951118,0,"2021/01/29, 13:51:14",True,"2021/01/29, 13:51:14",84586.0,1384297.0,2,"You can use generics to work with a registry of a specific type, for example  MeterRegistryCustomizer&lt;DatadogMeterRegistry&gt;  for a customizer that is only interested in customizing the Datadog registry."
Datadog,65953918,65951118,0,"2021/01/29, 13:51:14",True,"2021/01/29, 13:51:14",84586.0,1384297.0,2,"Putting this together, you can map the ID of the  http.server.request  meter to  i.want.to.be.different  using the following bean:"
Datadog,63631475,63626179,0,"2020/08/28, 12:52:58",True,"2020/08/28, 12:52:58",4922.0,11560878.0,1,There are some options you should consider:
Datadog,63631475,63626179,0,"2020/08/28, 12:52:58",True,"2020/08/28, 12:52:58",4922.0,11560878.0,1,don't update anything and just stick to Kubernetes 1.15 (not recommended as it is 4 main versions behind the latest one)
Datadog,63631475,63626179,0,"2020/08/28, 12:52:58",True,"2020/08/28, 12:52:58",4922.0,11560878.0,1,git clone  your repo and change  apiVersion  to  apps/v1  in all your resources
Datadog,63631475,63626179,0,"2020/08/28, 12:52:58",True,"2020/08/28, 12:52:58",4922.0,11560878.0,1,"use  kubectl convert  in order to change the  apiVersion , for example:  kubectl convert -f deployment.yaml --output-version apps/v1"
Datadog,63631475,63626179,0,"2020/08/28, 12:52:58",True,"2020/08/28, 12:52:58",4922.0,11560878.0,1,It is worth to mention that stuff gets deprecated for a reason and it is strongly not recommended to stick to old ways if they are not supported anymore.
Datadog,63266155,63259337,0,"2020/08/05, 16:22:30",False,"2020/08/05, 16:22:30",434.0,13958041.0,0,"If you are doing the setup in an organisation, datadog or prometheus is probably the way to go."
Datadog,63266155,63259337,0,"2020/08/05, 16:22:30",False,"2020/08/05, 16:22:30",434.0,13958041.0,0,You can capture other Kafka related metrics as well.
Datadog,63266155,63259337,0,"2020/08/05, 16:22:30",False,"2020/08/05, 16:22:30",434.0,13958041.0,0,These agents also have integrations with many other tools beside Kafka and will be a good common choice for monitoring.
Datadog,63266155,63259337,0,"2020/08/05, 16:22:30",False,"2020/08/05, 16:22:30",434.0,13958041.0,0,"If you are just doing it for personal POC type of a project and you just want to  view  the lag, I find CMAK very useful ( https://github.com/yahoo/CMAK )."
Datadog,63266155,63259337,0,"2020/08/05, 16:22:30",False,"2020/08/05, 16:22:30",434.0,13958041.0,0,"This does  not  have historical data, but provides a good  current  visual state of Kafka cluster including lag."
Datadog,63279458,63259337,0,"2020/08/06, 11:15:49",False,"2020/08/06, 11:15:49",106.0,12612778.0,0,For cluster wide metrics you can use kafka_exporter ( https://github.com/danielqsj/kafka_exporter ) which exposes some very useful cluster metrics(including consumer lag) and is easy to integrate with prometheus and visualize using grafana.
Datadog,65170331,63259337,0,"2020/12/06, 18:24:17",False,"2020/12/06, 18:31:54",81.0,5501217.0,0,Burrow is extremely effective and specialised in monitoring consumer lag.Burrow is good at caliberating consumer offset and more importantly validate if the lag is malicious or not.
Datadog,65170331,63259337,0,"2020/12/06, 18:24:17",False,"2020/12/06, 18:31:54",81.0,5501217.0,0,It has integrations with pagerduty so that the alerts are pushed to the necessary parties.
Datadog,65170331,63259337,0,"2020/12/06, 18:24:17",False,"2020/12/06, 18:31:54",81.0,5501217.0,0,https://community.cloudera.com/t5/Community-Articles/Monitoring-Kafka-with-Burrow-Part-1/ta-p/245987
Datadog,65170331,63259337,0,"2020/12/06, 18:24:17",False,"2020/12/06, 18:31:54",81.0,5501217.0,0,What burrow has:
Datadog,65170331,63259337,0,"2020/12/06, 18:24:17",False,"2020/12/06, 18:31:54",81.0,5501217.0,0,If you are looking for quick solution you can deploy burrow followed by the burrow front end  https://github.com/GeneralMills/BurrowUI
Datadog,62246112,62177611,1,"2020/06/07, 16:32:56",False,"2020/06/09, 00:40:01",927.0,2188893.0,0,You could use the Java Admin Kafka API quite easily to expose this over command line or as an HTTP call quite quickly with Spring Boot (could avoid any metrics for each app individually since Admin API could do it for any group).
Datadog,62246112,62177611,1,"2020/06/07, 16:32:56",False,"2020/06/09, 00:40:01",927.0,2188893.0,0,See link for an example:  https://gquintana.github.io/2020/01/16/Retrieving-Kafka-lag.html
Datadog,62246112,62177611,1,"2020/06/07, 16:32:56",False,"2020/06/09, 00:40:01",927.0,2188893.0,0,Code taken from example above.
Datadog,62246112,62177611,1,"2020/06/07, 16:32:56",False,"2020/06/09, 00:40:01",927.0,2188893.0,0,Get all groups:
Datadog,62246112,62177611,1,"2020/06/07, 16:32:56",False,"2020/06/09, 00:40:01",927.0,2188893.0,0,Get highest offset in a set of partitions:
Datadog,62246112,62177611,1,"2020/06/07, 16:32:56",False,"2020/06/09, 00:40:01",927.0,2188893.0,0,High level on connecting them together to get lag:
Datadog,62137876,62123685,0,"2020/06/01, 20:43:26",False,"2020/06/01, 20:43:26",471.0,967088.0,0,"Use the supplied jconsole.sh script in bin, don't try and build up the classpath by hand."
Datadog,62137876,62123685,0,"2020/06/01, 20:43:26",False,"2020/06/01, 20:43:26",471.0,967088.0,0,You also need to use the custom service url.
Datadog,62137876,62123685,0,"2020/06/01, 20:43:26",False,"2020/06/01, 20:43:26",471.0,967088.0,0,See the docs for details
Datadog,63924760,61119886,0,"2020/09/16, 19:57:11",False,"2020/09/16, 19:57:11",460.0,2810489.0,0,You can run datadog tracing on AWS Elastic Beanstalk with Flask by configure tracing manually as defined  here :
Datadog,61063468,61043878,0,"2020/04/06, 18:37:38",False,"2020/04/06, 18:37:38",9394.0,573153.0,2,"Custom metrics are on the roadmap for the APM agent, but we're still working on the exact schedule."
Datadog,61063468,61043878,0,"2020/04/06, 18:37:38",False,"2020/04/06, 18:37:38",9394.0,573153.0,2,In the meantime you could either use the  JMX config options  of the agent with custom JMX key properties.
Datadog,61063468,61043878,0,"2020/04/06, 18:37:38",False,"2020/04/06, 18:37:38",9394.0,573153.0,2,Or use the Elasticsearch output of Micrometer.
Datadog,61063468,61043878,0,"2020/04/06, 18:37:38",False,"2020/04/06, 18:37:38",9394.0,573153.0,2,Maybe just change the Micrometer output as an interim solution and potentially switch to custom APM metrics once they are available?
Datadog,61063468,61043878,0,"2020/04/06, 18:37:38",False,"2020/04/06, 18:37:38",9394.0,573153.0,2,"There's also the option to get metrics with  Metricbeat from JMX / Jolokia , but that sounds like an even bigger change and not really a long-term upside."
Datadog,60050555,59961483,0,"2020/02/04, 06:24:29",False,"2020/02/05, 03:52:39",1201.0,8673695.0,2,One straight forward way I can think of in this case is to use the  .env  file for your docker-compose.
Datadog,60050555,59961483,0,"2020/02/04, 06:24:29",False,"2020/02/05, 03:52:39",1201.0,8673695.0,2,docker-compose.yaml  file will look something like this
Datadog,60050555,59961483,0,"2020/02/04, 06:24:29",False,"2020/02/05, 03:52:39",1201.0,8673695.0,2,.env  file for each stack will look something like this
Datadog,60050555,59961483,0,"2020/02/04, 06:24:29",False,"2020/02/05, 03:52:39",1201.0,8673695.0,2,and
Datadog,60050555,59961483,0,"2020/02/04, 06:24:29",False,"2020/02/05, 03:52:39",1201.0,8673695.0,2,for different projects.
Datadog,60050555,59961483,0,"2020/02/04, 06:24:29",False,"2020/02/05, 03:52:39",1201.0,8673695.0,2,Note:
Datadog,60068032,59961483,1,"2020/02/05, 04:00:43",False,"2020/02/05, 04:00:43",7900.0,2259934.0,0,You're making things harder than they have to be.
Datadog,60068032,59961483,1,"2020/02/05, 04:00:43",False,"2020/02/05, 04:00:43",7900.0,2259934.0,0,Your app is containerized- use a container system.
Datadog,60068032,59961483,1,"2020/02/05, 04:00:43",False,"2020/02/05, 04:00:43",7900.0,2259934.0,0,ECS is  very  easy to get going with.
Datadog,60068032,59961483,1,"2020/02/05, 04:00:43",False,"2020/02/05, 04:00:43",7900.0,2259934.0,0,"It's a json file that defines your deployment- basically analogous to docker-compose (they actually supported compose files at some point, not sure if that feature stayed around)."
Datadog,60068032,59961483,1,"2020/02/05, 04:00:43",False,"2020/02/05, 04:00:43",7900.0,2259934.0,0,You can deploy an arbitrary number of services with different container images.
Datadog,60068032,59961483,1,"2020/02/05, 04:00:43",False,"2020/02/05, 04:00:43",7900.0,2259934.0,0,"We like to use a terraform module with the image tag as a parameter, but easy enough to write a shell script or whatever."
Datadog,60068032,59961483,1,"2020/02/05, 04:00:43",False,"2020/02/05, 04:00:43",7900.0,2259934.0,0,"Since you're trying to save money, create a single application load balancer."
Datadog,60068032,59961483,1,"2020/02/05, 04:00:43",False,"2020/02/05, 04:00:43",7900.0,2259934.0,0,"each app gets a hostname, and each container gets a subpath."
Datadog,60068032,59961483,1,"2020/02/05, 04:00:43",False,"2020/02/05, 04:00:43",7900.0,2259934.0,0,"For short lived feature branch deployments, you can even deploy on Fargate and not have an ongoing server cost."
Datadog,60405918,59961483,0,"2020/02/26, 04:28:04",False,"2020/02/26, 04:28:04",971.0,8379207.0,0,It turns out the solution involved capabilities from docker-compose.
Datadog,60405918,59961483,0,"2020/02/26, 04:28:04",False,"2020/02/26, 04:28:04",971.0,8379207.0,0,In docker docs the concept is called  Multiple Isolated environments on a single host
Datadog,60405918,59961483,0,"2020/02/26, 04:28:04",False,"2020/02/26, 04:28:04",971.0,8379207.0,0,to achieve this:
Datadog,60405918,59961483,0,"2020/02/26, 04:28:04",False,"2020/02/26, 04:28:04",971.0,8379207.0,0,I used an .env file with so many env vars.
Datadog,60405918,59961483,0,"2020/02/26, 04:28:04",False,"2020/02/26, 04:28:04",971.0,8379207.0,0,The main one is  CONTAINER_IMAGE_TAG  that defines the git branch ID to identify the stack.
Datadog,60405918,59961483,0,"2020/02/26, 04:28:04",False,"2020/02/26, 04:28:04",971.0,8379207.0,0,"A separate docker-compose-dev file defines ports, image tags, extra metadata that is dev related"
Datadog,60405918,59961483,0,"2020/02/26, 04:28:04",False,"2020/02/26, 04:28:04",971.0,8379207.0,0,Finally the use of  --project-name  in the docker-compose command allows to have different stacks.
Datadog,60405918,59961483,0,"2020/02/26, 04:28:04",False,"2020/02/26, 04:28:04",971.0,8379207.0,0,an example docker-compose Bash function that uses the docker-compose command
Datadog,60405918,59961483,0,"2020/02/26, 04:28:04",False,"2020/02/26, 04:28:04",971.0,8379207.0,0,"The separation should be done in the image tags, container names, network names, volume names and project name."
Datadog,58576495,58576189,1,"2019/10/27, 05:54:22",False,"2019/10/27, 05:54:22",1190.0,5621569.0,1,The issue is that your library depends on  gcc  to run.
Datadog,58576495,58576189,1,"2019/10/27, 05:54:22",False,"2019/10/27, 05:54:22",1190.0,5621569.0,1,"If you are running in a container, you can try two options:"
Datadog,58576495,58576189,1,"2019/10/27, 05:54:22",False,"2019/10/27, 05:54:22",1190.0,5621569.0,1,"You could also need  musl-dev  package, but you should try without it first."
Datadog,58576495,58576189,1,"2019/10/27, 05:54:22",False,"2019/10/27, 05:54:22",1190.0,5621569.0,1,"Since MacOS and most Linux distros come with GCC, I guess you could be using Windows."
Datadog,58576495,58576189,1,"2019/10/27, 05:54:22",False,"2019/10/27, 05:54:22",1190.0,5621569.0,1,"In this case, you need to install  MinGW ."
Datadog,59060540,58576189,0,"2019/11/27, 00:50:03",False,"2019/11/27, 01:25:23",671.0,445891.0,1,"I know this is old but I ran into this problem too, About Alexey answer, on windows, you should install MinGW and add the path to win environment."
Datadog,59060540,58576189,0,"2019/11/27, 00:50:03",False,"2019/11/27, 01:25:23",671.0,445891.0,1,You should follow  this .
Datadog,59060540,58576189,0,"2019/11/27, 00:50:03",False,"2019/11/27, 01:25:23",671.0,445891.0,1,"In case MinGW did not work, you can install  this  one which worked perfectly for me on windows."
Datadog,59958891,57207906,0,"2020/01/29, 02:05:05",False,"2020/01/29, 02:05:05",1.0,12802667.0,0,I had the same error and installing the .NET Framework 4.6.1 SDK ( https://dotnet.microsoft.com/download/visual-studio-sdks ) and restarting the Datadog Agent solved the problem
Datadog,56117644,56116856,0,"2019/05/13, 21:01:29",False,"2019/05/13, 21:01:29",4287.0,9073130.0,1,Use  context.Request.Path  conditionally if your  routeData  is null.
Datadog,56117644,56116856,0,"2019/05/13, 21:01:29",False,"2019/05/13, 21:01:29",4287.0,9073130.0,1,It is the closest I can think of since Identity Server 4 middleware has internal routing logic for the standard OAuth protocol routes.
Datadog,55054296,55009193,0,"2019/03/08, 01:11:52",True,"2019/03/08, 01:11:52",923.0,1181073.0,1,"After a few more days of research, discovered that:"
Datadog,55159583,54729245,0,"2019/03/14, 12:00:21",False,"2019/03/14, 12:00:21",523.0,145725.0,0,"The errors you are getting are coming from the remote computer, that is, the Heroku dyno."
Datadog,55159583,54729245,0,"2019/03/14, 12:00:21",False,"2019/03/14, 12:00:21",523.0,145725.0,0,You can't follow the instructions in the warning (to update bundler) as you can't run arbitrary instructions on their servers.
Datadog,55159583,54729245,0,"2019/03/14, 12:00:21",False,"2019/03/14, 12:00:21",523.0,145725.0,0,"Heroku only support limited "" carefully curated "" versions of bundler."
Datadog,55159583,54729245,0,"2019/03/14, 12:00:21",False,"2019/03/14, 12:00:21",523.0,145725.0,0,"Normally when the bundler versions don't match it just gives a warning, not an error, so you can  potentially  just ignore it."
Datadog,55159583,54729245,0,"2019/03/14, 12:00:21",False,"2019/03/14, 12:00:21",523.0,145725.0,0,Personally I like to eliminate warnings (or supress them if elimination isn't possible) so that when new warnings pop up I am more likely to notice them and deal with them.
Datadog,55159583,54729245,0,"2019/03/14, 12:00:21",False,"2019/03/14, 12:00:21",523.0,145725.0,0,"That being said, I was not able to ""downgrade"" my Gemfile.lock from 2.0.1 to 1.15.2."
Datadog,55159583,54729245,0,"2019/03/14, 12:00:21",False,"2019/03/14, 12:00:21",523.0,145725.0,0,I had to first delete Gemfile.lock and then recreate it (presumably there are potentially breaking changes across these major versions).
Datadog,55159583,54729245,0,"2019/03/14, 12:00:21",False,"2019/03/14, 12:00:21",523.0,145725.0,0,I suspect this is the second problem you encountered.
Datadog,55159583,54729245,0,"2019/03/14, 12:00:21",False,"2019/03/14, 12:00:21",523.0,145725.0,0,The best way around these warnings/errors is to match your local version of Bundler to Heroku's carefully curated version.
Datadog,55159583,54729245,0,"2019/03/14, 12:00:21",False,"2019/03/14, 12:00:21",523.0,145725.0,0,"That page above links to another page with the currently supported versions: 
 https://devcenter.heroku.com/articles/ruby-support#libraries"
Datadog,55159583,54729245,0,"2019/03/14, 12:00:21",False,"2019/03/14, 12:00:21",523.0,145725.0,0,As of today that's version 2.0.1 for Gemfile.locks bundled with 2.x and 1.15.2 for everything else.
Datadog,56572059,54717464,0,"2019/06/13, 03:54:58",False,"2019/06/13, 03:54:58",2578.0,1235057.0,0,Could be merge conflicts in the Gemfile.lock.
Datadog,56572059,54717464,0,"2019/06/13, 03:54:58",False,"2019/06/13, 03:54:58",2578.0,1235057.0,0,Try running  bundle install  locally and see if it works before committing and pushing to heroku.
Datadog,52758714,52755069,3,"2018/10/11, 14:16:24",False,"2018/10/11, 14:16:24",1.0,7850738.0,-2,add bash script as userparameters in zabbix-agent.
Datadog,52590134,52588302,4,"2018/10/01, 14:29:23",True,"2018/10/01, 14:29:23",941.0,5488567.0,2,"Since it requires admin permissions, we can not give out UAA clients for the firehose."
Datadog,52590134,52588302,4,"2018/10/01, 14:29:23",True,"2018/10/01, 14:29:23",941.0,5488567.0,2,"However, there are different ways to get metrics in context of a user."
Datadog,52590134,52588302,4,"2018/10/01, 14:29:23",True,"2018/10/01, 14:29:23",941.0,5488567.0,2,CF API
Datadog,52590134,52588302,4,"2018/10/01, 14:29:23",True,"2018/10/01, 14:29:23",941.0,5488567.0,2,"You can obtain basic metrics of a specific app by polling the CF API:
 https://apidocs.cloudfoundry.org/5.0.0/apps/get_detailed_stats_for_a_started_app.html"
Datadog,52590134,52588302,4,"2018/10/01, 14:29:23",True,"2018/10/01, 14:29:23",941.0,5488567.0,2,"However, since you have to poll (and for each app), it's not the recommended way."
Datadog,52590134,52588302,4,"2018/10/01, 14:29:23",True,"2018/10/01, 14:29:23",941.0,5488567.0,2,Metrics in syslog drain
Datadog,52590134,52588302,4,"2018/10/01, 14:29:23",True,"2018/10/01, 14:29:23",941.0,5488567.0,2,"CF allows devs to forward their logs to syslog drains; in more recent versions, CF also sends metrics to this syslog drain (see  https://docs.cloudfoundry.org/devguide/deploy-apps/streaming-logs.html#container-metrics )."
Datadog,52590134,52588302,4,"2018/10/01, 14:29:23",True,"2018/10/01, 14:29:23",941.0,5488567.0,2,"For example, you could use Swisscom's Elasticsearch service to store these metrics and then analyze it using Kibana."
Datadog,52590134,52588302,4,"2018/10/01, 14:29:23",True,"2018/10/01, 14:29:23",941.0,5488567.0,2,Metrics using loggregator (firehose)
Datadog,52590134,52588302,4,"2018/10/01, 14:29:23",True,"2018/10/01, 14:29:23",941.0,5488567.0,2,"The firehose allows streaming logs to clients for two types of roles:
Streaming  all  logs to admins (which requires a UAA client with admin permissions) and streaming  app  logs and metrics to devs with permissions in the app's space."
Datadog,52590134,52588302,4,"2018/10/01, 14:29:23",True,"2018/10/01, 14:29:23",941.0,5488567.0,2,This is also what the  cf logs  command uses.
Datadog,52590134,52588302,4,"2018/10/01, 14:29:23",True,"2018/10/01, 14:29:23",941.0,5488567.0,2,cf top  also works this way  (it enumerates all apps and streams the logs of each app).
Datadog,52590134,52588302,4,"2018/10/01, 14:29:23",True,"2018/10/01, 14:29:23",941.0,5488567.0,2,"However, you will find out that most open source tools that leverage the firehose only work in admin mode, since they're written for the platform operator."
Datadog,52590134,52588302,4,"2018/10/01, 14:29:23",True,"2018/10/01, 14:29:23",941.0,5488567.0,2,"Of course you also have the possibility to monitor your app by instrumenting it (white box approach), for example by configuring Spring actuator in a Spring boot app or by including an agent of your favourite APM vendor (Dynatrace, AppDynamics, ...)"
Datadog,52590134,52588302,4,"2018/10/01, 14:29:23",True,"2018/10/01, 14:29:23",941.0,5488567.0,2,I guess this is the most common approach; we've seen a lot of teams having success by instrumenting their applications.
Datadog,52590134,52588302,4,"2018/10/01, 14:29:23",True,"2018/10/01, 14:29:23",941.0,5488567.0,2,Especially since advanced monitoring anyway requires you to create your own metrics as the firehose provided cpu/memory metrics are not that powerful in a microservice world.
Datadog,52590134,52588302,4,"2018/10/01, 14:29:23",True,"2018/10/01, 14:29:23",941.0,5488567.0,2,"However, option 2. would be worth a try as well, especially since the ELK's stack metric support is getting better and better."
Datadog,51209988,51097821,2,"2018/07/06, 14:58:20",False,"2018/07/08, 06:33:21",4349.0,5030709.0,0,How are you spinning up ecs-agent container?
Datadog,51209988,51097821,2,"2018/07/06, 14:58:20",False,"2018/07/08, 06:33:21",4349.0,5030709.0,0,What is docker run command?.
Datadog,51209988,51097821,2,"2018/07/06, 14:58:20",False,"2018/07/08, 06:33:21",4349.0,5030709.0,0,Did you try like below?.
Datadog,50266462,50266331,4,"2018/05/10, 09:02:11",False,"2018/05/10, 09:02:11",162400.0,174777.0,0,"Yes, you can pass a script to the instance that will be executed on the first boot (but not thereafter)."
Datadog,50266462,50266331,4,"2018/05/10, 09:02:11",False,"2018/05/10, 09:02:11",162400.0,174777.0,0,It is often referred to as a  User Data script .
Datadog,50266462,50266331,4,"2018/05/10, 09:02:11",False,"2018/05/10, 09:02:11",162400.0,174777.0,0,See:
Datadog,50266462,50266331,4,"2018/05/10, 09:02:11",False,"2018/05/10, 09:02:11",162400.0,174777.0,0,"If you wish to install  after  the instance has started, use the  AWS Systems Manager Run Command ."
Datadog,54622604,49892339,0,"2019/02/11, 02:38:36",False,"2019/02/11, 02:38:36",385.0,912643.0,1,The simplest way to get access to data in Cloudyn is to configure a report and schedule data to be pushed to a storage account.
Datadog,54622604,49892339,0,"2019/02/11, 02:38:36",False,"2019/02/11, 02:38:36",385.0,912643.0,1,"From there, you can use standard storage account APIs to access the data."
Datadog,54622604,49892339,0,"2019/02/11, 02:38:36",False,"2019/02/11, 02:38:36",385.0,912643.0,1,"Instead of using Cloudyn APIs, however, I would recommend using the  Cost Management Query API  for aggregated cost/usage data or  UsageDetails API  for raw usage."
Datadog,48502902,48475616,0,"2018/01/29, 15:54:07",False,"2018/01/29, 15:54:07",914.0,1679567.0,0,"If you want to secure access to docker socket,  this docker documents  is a good start."
Datadog,48576285,48457894,0,"2018/02/02, 07:34:44",True,"2019/03/29, 04:35:10",253.0,607808.0,1,Spoke with Datadog support.
Datadog,48576285,48457894,0,"2018/02/02, 07:34:44",True,"2019/03/29, 04:35:10",253.0,607808.0,1,Very helpful but the short answer is that there is currently no option to add additional tags to specify the specific proc_name in the individual  gunicorn.yaml  file.
Datadog,48576285,48457894,0,"2018/02/02, 07:34:44",True,"2019/03/29, 04:35:10",253.0,607808.0,1,As a workaround to enable grouping we enabled unique prefixes for each application but the trade-off is that the metrics are no longer sharing the same namespace.
Datadog,48576285,48457894,0,"2018/02/02, 07:34:44",True,"2019/03/29, 04:35:10",253.0,607808.0,1,I've submitted a new feature request on the Github project which will hopefully be considered.
Datadog,46154565,46154416,0,"2017/09/11, 14:24:24",False,"2017/09/11, 14:24:24",31125.0,460802.0,0,Response time is in  time  field.
Datadog,46154565,46154416,0,"2017/09/11, 14:24:24",False,"2017/09/11, 14:24:24",31125.0,460802.0,0,There is an additional metric  latency  which provides time to first byte.
Datadog,46154565,46154416,0,"2017/09/11, 14:24:24",False,"2017/09/11, 14:24:24",31125.0,460802.0,0,See:
Datadog,46154565,46154416,0,"2017/09/11, 14:24:24",False,"2017/09/11, 14:24:24",31125.0,460802.0,0,You might also want to read :
Datadog,46079393,46079056,1,"2017/09/06, 18:42:48",True,"2017/09/06, 18:42:48",2688.0,2718295.0,4,Shared buffers are used for postgres memory cache (at a lower level closer to postgres as compared to OS cache).
Datadog,46079393,46079056,1,"2017/09/06, 18:42:48",True,"2017/09/06, 18:42:48",2688.0,2718295.0,4,Setting it to 7gb means that pg will cache to 7gb of data.
Datadog,46079393,46079056,1,"2017/09/06, 18:42:48",True,"2017/09/06, 18:42:48",2688.0,2718295.0,4,So if you are doing a lot of full table scans or (recursive) CTEs that may improve performance.
Datadog,46079393,46079056,1,"2017/09/06, 18:42:48",True,"2017/09/06, 18:42:48",2688.0,2718295.0,4,"Note that  postgres  master process will allocate this entire amount at database startup, which is why you are seeing your OS use 10GB of ram now."
Datadog,46079393,46079056,1,"2017/09/06, 18:42:48",True,"2017/09/06, 18:42:48",2688.0,2718295.0,4,work_mem  is memory used for sorts and  each  concurrent sort allocates a bucket of this size.
Datadog,46079393,46079056,1,"2017/09/06, 18:42:48",True,"2017/09/06, 18:42:48",2688.0,2718295.0,4,"Therefore this is only bounded by  max_connections  * concurrent sorts, so effectively it is  only  bounded by the sort complexity of your queries, so increasing this poses the most risk to system stability."
Datadog,46079393,46079056,1,"2017/09/06, 18:42:48",True,"2017/09/06, 18:42:48",2688.0,2718295.0,4,"(That is, if you have a single query that the query planner executes with 8 merge sorts, you will use 8* work_mem  every time the query is executed)."
Datadog,46079393,46079056,1,"2017/09/06, 18:42:48",True,"2017/09/06, 18:42:48",2688.0,2718295.0,4,maintenance_work_mem  is the memory used by  VACUUM  and friends (including  ALTER TABLE ADD FOREIGN KEY !
Datadog,46079393,46079056,1,"2017/09/06, 18:42:48",True,"2017/09/06, 18:42:48",2688.0,2718295.0,4,Increasing this may increase VACUUM speed.
Datadog,46079393,46079056,1,"2017/09/06, 18:42:48",True,"2017/09/06, 18:42:48",2688.0,2718295.0,4,"wal_buffers  has no benefit beyond 16MB, which is the largest WAL chunk the server will write at one time."
Datadog,46079393,46079056,1,"2017/09/06, 18:42:48",True,"2017/09/06, 18:42:48",2688.0,2718295.0,4,This can help with slow write i/o.
Datadog,46079393,46079056,1,"2017/09/06, 18:42:48",True,"2017/09/06, 18:42:48",2688.0,2718295.0,4,See also:  https://wiki.postgresql.org/wiki/Tuning_Your_PostgreSQL_Server
Datadog,45147281,45147143,2,"2017/07/17, 17:47:26",False,"2017/07/17, 17:47:26",677.0,6803997.0,1,It depends.
Datadog,45147281,45147143,2,"2017/07/17, 17:47:26",False,"2017/07/17, 17:47:26",677.0,6803997.0,1,"Also, your image doesn't display as of my answer."
Datadog,45147281,45147143,2,"2017/07/17, 17:47:26",False,"2017/07/17, 17:47:26",677.0,6803997.0,1,"If your machines are super memory hungry and you are an individual without unlimited income, I think your approach would be fine."
Datadog,45147281,45147143,2,"2017/07/17, 17:47:26",False,"2017/07/17, 17:47:26",677.0,6803997.0,1,"I would recommend a slightly higher arbitrary percentage to start with, such as 50%, to provide a bit of wiggle room."
Datadog,45147281,45147143,2,"2017/07/17, 17:47:26",False,"2017/07/17, 17:47:26",677.0,6803997.0,1,Continue to analyze the memory usage and adjust your maximum accordingly.
Datadog,45147281,45147143,2,"2017/07/17, 17:47:26",False,"2017/07/17, 17:47:26",677.0,6803997.0,1,I don't see any reason to set memory usage below default.
Datadog,45147281,45147143,2,"2017/07/17, 17:47:26",False,"2017/07/17, 17:47:26",677.0,6803997.0,1,"Otherwise, you can be much more gratuitous and provide 100-200% extra memory, in case your application experiences sudden heavy load."
Datadog,41028352,41025255,0,"2016/12/08, 00:14:48",False,"2016/12/08, 00:14:48",643.0,4214037.0,1,"As you can see  here , Ansible provides role dependecies."
Datadog,41028352,41025255,0,"2016/12/08, 00:14:48",False,"2016/12/08, 00:14:48",643.0,4214037.0,1,You may create in  Datadog.datadog  role new directory named meta with main.yml file.
Datadog,41028352,41025255,0,"2016/12/08, 00:14:48",False,"2016/12/08, 00:14:48",643.0,4214037.0,1,In  meta/main.yml  write
Datadog,41028352,41025255,0,"2016/12/08, 00:14:48",False,"2016/12/08, 00:14:48",643.0,4214037.0,1,"After that, when you call  Datadog.datadog  role, Ansible will run  role1  automatically before  Datadog.datadog  role."
Datadog,41028352,41025255,0,"2016/12/08, 00:14:48",False,"2016/12/08, 00:14:48",643.0,4214037.0,1,"If you create another role named  Datadog.datadog1  with the same  meta/main.yml  file and call roles  Datadog.datadog  and  Datadog.datadog1 , then Ansible will run  role1  only once, before running Datadogs roles."
Datadog,38387725,38359717,0,"2016/07/15, 07:08:42",True,"2016/07/15, 07:08:42",1048.0,3778976.0,1,"For your data model, I would suggest adding   time   as a clustering column:"
Datadog,38387725,38359717,0,"2016/07/15, 07:08:42",True,"2016/07/15, 07:08:42",1048.0,3778976.0,1,Use descending order to keep the latest metrics first.
Datadog,38387725,38359717,0,"2016/07/15, 07:08:42",True,"2016/07/15, 07:08:42",1048.0,3778976.0,1,You can then query using the LIMIT clause to get the most recent hour:
Datadog,38387725,38359717,0,"2016/07/15, 07:08:42",True,"2016/07/15, 07:08:42",1048.0,3778976.0,1,Or day:
Datadog,38387725,38359717,0,"2016/07/15, 07:08:42",True,"2016/07/15, 07:08:42",1048.0,3778976.0,1,"Depending upon how long you plan to keep the data, you may want to add a  column for year, month, or days to the table to limit your partition size."
Datadog,38387725,38359717,0,"2016/07/15, 07:08:42",True,"2016/07/15, 07:08:42",1048.0,3778976.0,1,"For example, if you wish to keep data for 3 months,  a  month  column can be added to partition your keys by id and month:"
Datadog,38387725,38359717,0,"2016/07/15, 07:08:42",True,"2016/07/15, 07:08:42",1048.0,3778976.0,1,"If you keep data for several years, use year + month or a date value."
Datadog,38387725,38359717,0,"2016/07/15, 07:08:42",True,"2016/07/15, 07:08:42",1048.0,3778976.0,1,"Regarding your final question, about separate tables or a single table."
Datadog,38387725,38359717,0,"2016/07/15, 07:08:42",True,"2016/07/15, 07:08:42",1048.0,3778976.0,1,"Cassandra supports sparse columns, so you can make multiple inserts in a common table for each metric without updating any data."
Datadog,38387725,38359717,0,"2016/07/15, 07:08:42",True,"2016/07/15, 07:08:42",1048.0,3778976.0,1,"However, it's always faster to write just once per row."
Datadog,38387725,38359717,0,"2016/07/15, 07:08:42",True,"2016/07/15, 07:08:42",1048.0,3778976.0,1,You may need separate tables if you have to query for different metrics by an alternative key.
Datadog,38387725,38359717,0,"2016/07/15, 07:08:42",True,"2016/07/15, 07:08:42",1048.0,3778976.0,1,"For example, query for disk usage by id and disk name."
Datadog,38387725,38359717,0,"2016/07/15, 07:08:42",True,"2016/07/15, 07:08:42",1048.0,3778976.0,1,You'd need a separate table or a materialized view to support that query pattern.
Datadog,38387725,38359717,0,"2016/07/15, 07:08:42",True,"2016/07/15, 07:08:42",1048.0,3778976.0,1,"Finally, your schema defines an  assetid , but this isn't defined in your primary key so with your current schema you can't query using assetid."
Datadog,36528782,35866315,0,"2016/04/10, 13:51:31",True,"2016/04/10, 13:51:31",56.0,355476.0,3,Try
Datadog,66513856,66005706,0,"2021/03/07, 08:31:28",False,"2021/03/07, 08:31:28",11.0,15128440.0,0,"Finally, found the solution by examining services logs."
Datadog,66513856,66005706,0,"2021/03/07, 08:31:28",False,"2021/03/07, 08:31:28",11.0,15128440.0,0,w32time service was configured as &quot;manual start&quot; instead of delay-auto or auto.
Datadog,66513856,66005706,0,"2021/03/07, 08:31:28",False,"2021/03/07, 08:31:28",11.0,15128440.0,0,"And though &quot;Set time automatically&quot; was &quot;On&quot;, clock synchronization only happened after starting the service and resyncing."
Datadog,66513856,66005706,0,"2021/03/07, 08:31:28",False,"2021/03/07, 08:31:28",11.0,15128440.0,0,So I've changed the Startup type of the Windows time service from Manual to Automatic (Delayed Start) using the following command:
Datadog,66513856,66005706,0,"2021/03/07, 08:31:28",False,"2021/03/07, 08:31:28",11.0,15128440.0,0,&amp;sc.exe config w32time start= delayed-auto
Datadog,66513856,66005706,0,"2021/03/07, 08:31:28",False,"2021/03/07, 08:31:28",11.0,15128440.0,0,Disabled Time Synchronization (service task) in Task Scheduler.
Datadog,66513856,66005706,0,"2021/03/07, 08:31:28",False,"2021/03/07, 08:31:28",11.0,15128440.0,0,Disable-ScheduledTask -TaskName &quot;SynchronizeTime&quot; -TaskPath &quot;\Microsoft\Windows\Time Synchronization&quot;
Datadog,66513856,66005706,0,"2021/03/07, 08:31:28",False,"2021/03/07, 08:31:28",11.0,15128440.0,0,Time Skews are being fixed automatically and occur less often.
Datadog,65786188,65785913,6,"2021/01/19, 07:57:08",True,"2021/01/19, 08:18:40",5019.0,10186808.0,1,Typical work flow will look like this (there are other methods)
Datadog,65786188,65785913,6,"2021/01/19, 07:57:08",True,"2021/01/19, 08:18:40",5019.0,10186808.0,1,"This can be achieved in multiple ways, easiest is to create a template with index pattern , alias and mapping."
Datadog,65786188,65785913,6,"2021/01/19, 07:57:08",True,"2021/01/19, 08:18:40",5019.0,10186808.0,1,Example: Any new index created matching the pattern  staff-*  will be assigned with given mapping and attached to alias  staff   and we can query  staff  instead of individual indexes and setup alerts.
Datadog,65786188,65785913,6,"2021/01/19, 07:57:08",True,"2021/01/19, 08:18:40",5019.0,10186808.0,1,We can use cwl--aws-containerinsights-eks-cluster-for-test-host to run queries.
Datadog,65786188,65785913,6,"2021/01/19, 07:57:08",True,"2021/01/19, 08:18:40",5019.0,10186808.0,1,"Note: If unsure of mapping, we can remove mapping section."
Datadog,66059109,63338416,0,"2021/02/05, 09:17:00",False,"2021/02/05, 09:17:00",1.0,15150258.0,0,FetchFollower acting like &quot;long poll&quot; request.
Datadog,66059109,63338416,0,"2021/02/05, 09:17:00",False,"2021/02/05, 09:17:00",1.0,15150258.0,0,It waits till it gets  replica.fetch.min.bytes  data for replication or  replica.fetch.wait.max.ms  timeout (which is by default 500ms).
Datadog,66059109,63338416,0,"2021/02/05, 09:17:00",False,"2021/02/05, 09:17:00",1.0,15150258.0,0,"So it's basically ok, it's just means that most of FetchFollower requests are waiting for data"
Datadog,58132394,58094024,3,"2019/09/27, 13:06:50",True,"2019/09/27, 13:06:50",26.0,5036870.0,1,This is likely related to this  issue in the portmap plugin .
Datadog,58132394,58094024,3,"2019/09/27, 13:06:50",True,"2019/09/27, 13:06:50",26.0,5036870.0,1,"The current working theory is that a conntrack entry is created when the client pod reaches out for the UDP host port, and that entry becomes stale when the server pod is deleted, but it's not deleted, so clients keep hitting it, essentially blackholing the traffic."
Datadog,58132394,58094024,3,"2019/09/27, 13:06:50",True,"2019/09/27, 13:06:50",26.0,5036870.0,1,You can try removing the conntrack entry with something like  conntrack -D -p udp --dport 8125  on one of the impacted host.
Datadog,58132394,58094024,3,"2019/09/27, 13:06:50",True,"2019/09/27, 13:06:50",26.0,5036870.0,1,If that solves the issue then that was the root cause of your problem.
Datadog,58132394,58094024,3,"2019/09/27, 13:06:50",True,"2019/09/27, 13:06:50",26.0,5036870.0,1,This workaround described in the GitHub issue should mitigate the issue until a fix is merged:
Datadog,58132394,58094024,3,"2019/09/27, 13:06:50",True,"2019/09/27, 13:06:50",26.0,5036870.0,1,You can add an initContainer to the server's pod to run the conntrack command when it starts:
Datadog,51727825,51727504,2,"2018/08/07, 16:19:11",False,"2018/08/07, 16:19:11",554.0,6529322.0,2,Some things I'd consider indicative of the health of the cluster are as follows:
Datadog,51727825,51727504,2,"2018/08/07, 16:19:11",False,"2018/08/07, 16:19:11",554.0,6529322.0,2,Offline/Under Replicated Partitions : This is a good indicator as to whether all the nodes in a cluster are even online.
Datadog,51727825,51727504,2,"2018/08/07, 16:19:11",False,"2018/08/07, 16:19:11",554.0,6529322.0,2,"If one goes offline, you will almost certainly see some under-replication, and if several are offline, you might even see some offline partitions."
Datadog,51727825,51727504,2,"2018/08/07, 16:19:11",False,"2018/08/07, 16:19:11",554.0,6529322.0,2,"Active Controller : If this keeps changing, then it means that the cluster is potentially unstable."
Datadog,51727825,51727504,2,"2018/08/07, 16:19:11",False,"2018/08/07, 16:19:11",554.0,6529322.0,2,"The controller should not change regularly; if it does, then something is wrong with your cluster."
Datadog,51727825,51727504,2,"2018/08/07, 16:19:11",False,"2018/08/07, 16:19:11",554.0,6529322.0,2,Bytes In/Out : These show that your cluster is able to send and receive data.
Datadog,51727825,51727504,2,"2018/08/07, 16:19:11",False,"2018/08/07, 16:19:11",554.0,6529322.0,2,"If these are lower than you'd expect, then it might imply that the cluster is undergoing some sort of network issue which would possibly impact the cluster health."
Datadog,51727825,51727504,2,"2018/08/07, 16:19:11",False,"2018/08/07, 16:19:11",554.0,6529322.0,2,Hope this helps!
Datadog,41252693,41252657,4,"2016/12/21, 01:13:34",False,"2016/12/21, 01:13:34",110.0,6938240.0,-1,.strip()  for removing whitespace characters.
Datadog,41252693,41252657,4,"2016/12/21, 01:13:34",False,"2016/12/21, 01:13:34",110.0,6938240.0,-1,".replace('offset=', '')  for removing that string."
Datadog,41252693,41252657,4,"2016/12/21, 01:13:34",False,"2016/12/21, 01:13:34",110.0,6938240.0,-1,You should be able to chain them too.
Datadog,41252760,41252657,0,"2016/12/21, 01:20:55",False,"2016/12/21, 01:31:54",26687.0,5352399.0,0,How to extract the numeric value appeared after  offset= ?
Datadog,41252760,41252657,0,"2016/12/21, 01:20:55",False,"2016/12/21, 01:31:54",26687.0,5352399.0,0,Why i prefer regular expression?
Datadog,41252760,41252657,0,"2016/12/21, 01:20:55",False,"2016/12/21, 01:31:54",26687.0,5352399.0,0,"Because even if the string contains other keywords, regular expression will extract the numeric value which appeared after the  offset=  expression."
Datadog,41252760,41252657,0,"2016/12/21, 01:20:55",False,"2016/12/21, 01:31:54",26687.0,5352399.0,0,"For example, check for the following cases with my given example."
Datadog,41252760,41252657,0,"2016/12/21, 01:20:55",False,"2016/12/21, 01:31:54",26687.0,5352399.0,0,How to remove leading and trailing whitespace characters?
Datadog,41252760,41252657,0,"2016/12/21, 01:20:55",False,"2016/12/21, 01:31:54",26687.0,5352399.0,0,"will remove all the leading and trailing whitespace characters such as \n, \r, \t, \f, space."
Datadog,41252760,41252657,0,"2016/12/21, 01:20:55",False,"2016/12/21, 01:31:54",26687.0,5352399.0,0,For more flexibility use the following
Datadog,41252760,41252657,0,"2016/12/21, 01:20:55",False,"2016/12/21, 01:31:54",26687.0,5352399.0,0,Reference: see this SO  answer .
Datadog,41252788,41252657,0,"2016/12/21, 01:23:06",False,"2016/12/21, 01:23:06",17322.0,763269.0,1,The straightforward way is:
Datadog,41252788,41252657,0,"2016/12/21, 01:23:06",False,"2016/12/21, 01:23:06",17322.0,763269.0,1,For example:
Datadog,41252788,41252657,0,"2016/12/21, 01:23:06",False,"2016/12/21, 01:23:06",17322.0,763269.0,1,Example:
Datadog,41252788,41252657,0,"2016/12/21, 01:23:06",False,"2016/12/21, 01:23:06",17322.0,763269.0,1,Up to you if you need to convert the output.
Datadog,39060683,39060624,1,"2016/08/21, 06:47:56",True,"2016/08/21, 06:47:56",95948.0,1079354.0,1,"Since it looks like  StatsDClient  is an interface of some kind, it would make your testing effort easier to simply inject this dependency into your object."
Datadog,39060683,39060624,1,"2016/08/21, 06:47:56",True,"2016/08/21, 06:47:56",95948.0,1079354.0,1,"Even if you're not using an IoC container like Spring or Guice, you can still somewhat control this simply by passing an instance of it in through the constructor."
Datadog,39060683,39060624,1,"2016/08/21, 06:47:56",True,"2016/08/21, 06:47:56",95948.0,1079354.0,1,This will make your testing simpler since all you realistically need to do is mock the object passed in during test.
Datadog,39060683,39060624,1,"2016/08/21, 06:47:56",True,"2016/08/21, 06:47:56",95948.0,1079354.0,1,"Right now, the reason it's failing is because you're  new ing up the instance, and Mockito (in this current configuration) isn't equipped to mock the newed instance."
Datadog,39060683,39060624,1,"2016/08/21, 06:47:56",True,"2016/08/21, 06:47:56",95948.0,1079354.0,1,"In all honesty, this set up will make testing simpler to conduct, and you should only need your client configured in one area."
Datadog,39061963,39060624,0,"2016/08/21, 10:42:11",False,"2016/08/21, 10:42:11",126469.0,1531124.0,0,You are getting things wrong here.
Datadog,39061963,39060624,0,"2016/08/21, 10:42:11",False,"2016/08/21, 10:42:11",126469.0,1531124.0,0,"You don't use a  mocking  framework to test your ""class under test""."
Datadog,39061963,39060624,0,"2016/08/21, 10:42:11",False,"2016/08/21, 10:42:11",126469.0,1531124.0,0,"You use the mocking framework to create  mocked  objects; which you then pass to your ""class under test"" within a test case."
Datadog,39061963,39060624,0,"2016/08/21, 10:42:11",False,"2016/08/21, 10:42:11",126469.0,1531124.0,0,"Then your ""code under test"" calls methods on the mocked object; and by controlling returned values (or by verifying what happens to your mock); that is how you write your testcases."
Datadog,39061963,39060624,0,"2016/08/21, 10:42:11",False,"2016/08/21, 10:42:11",126469.0,1531124.0,0,"So, your testcase for a MetricRecorder doesn't mock a MetricRecorder; it should mock the StatsDClient class; and as Makoto suggests; use  dependency  injection to put an object of that class into MetricRecorder."
Datadog,39061963,39060624,0,"2016/08/21, 10:42:11",False,"2016/08/21, 10:42:11",126469.0,1531124.0,0,"Besides: basically writing ""test-able"" code is something that needs to be practiced."
Datadog,39061963,39060624,0,"2016/08/21, 10:42:11",False,"2016/08/21, 10:42:11",126469.0,1531124.0,0,I wholeheartedly recommend you to watch these  videos  if you are serious about getting in this business.
Datadog,39061963,39060624,0,"2016/08/21, 10:42:11",False,"2016/08/21, 10:42:11",126469.0,1531124.0,0,All of them; really (worth each second!
Datadog,39061963,39060624,0,"2016/08/21, 10:42:11",False,"2016/08/21, 10:42:11",126469.0,1531124.0,0,).
Datadog,24537072,24536971,1,"2014/07/02, 20:08:18",True,"2014/07/07, 19:45:33",29.0,3794458.0,0,Kamon was being built for Java 1.7 by default.
Datadog,24537072,24536971,1,"2014/07/02, 20:08:18",True,"2014/07/07, 19:45:33",29.0,3794458.0,0,"Now, it will support 1.6"
Datadog,67184949,67161917,0,"2021/04/20, 22:09:14",False,"2021/04/20, 22:09:14",116.0,9839284.0,0,"As told by @TRW in the comments, using this should do the trick:"
Datadog,66871585,66761816,0,"2021/03/30, 16:09:45",True,"2021/03/30, 16:09:45",329.0,6301287.0,0,I had to open a ticket asking the Heroku CS team to apply the &quot;pg_monitor&quot; role to my user.
Datadog,66871585,66761816,0,"2021/03/30, 16:09:45",True,"2021/03/30, 16:09:45",329.0,6301287.0,0,They've granted the role and now everything is working fine
Datadog,65746430,65744466,0,"2021/01/16, 06:12:41",False,"2021/01/16, 06:12:41",13378.0,1430810.0,1,Maybe it's a complicated idea but I think you can make your own cache store wrapper that decides which cache store to use if I understand your question correctly.
Datadog,65746430,65744466,0,"2021/01/16, 06:12:41",False,"2021/01/16, 06:12:41",13378.0,1430810.0,1,"When calling the cache method, it eventually calls  read_fragment  and  write_fragment  on your controller  https://apidock.com/rails/AbstractController/Caching/Fragments/read_fragment  and those methods call  cache_store.read  and  cache_store.write ."
Datadog,65746430,65744466,0,"2021/01/16, 06:12:41",False,"2021/01/16, 06:12:41",13378.0,1430810.0,1,"Then you could have a custom cache store class with custom  read  and  write  method that, depending on an option, delegates the read and write to real cache stores."
Datadog,65746430,65744466,0,"2021/01/16, 06:12:41",False,"2021/01/16, 06:12:41",13378.0,1430810.0,1,Then you use it like...
Datadog,65746430,65744466,0,"2021/01/16, 06:12:41",False,"2021/01/16, 06:12:41",13378.0,1430810.0,1,I'm not sure if that's what you are asking sorry.
Datadog,65026062,65009092,2,"2020/11/26, 18:39:43",True,"2020/11/26, 18:39:43",782.0,1953079.0,1,Easy solution is to fetch this library directly and do  add_subdirectory .
Datadog,65026062,65009092,2,"2020/11/26, 18:39:43",True,"2020/11/26, 18:39:43",782.0,1953079.0,1,But this requires cmake &gt;= 3.11.
Datadog,65026062,65009092,2,"2020/11/26, 18:39:43",True,"2020/11/26, 18:39:43",782.0,1953079.0,1,Create dir  cmake  and file  cmake/cpp-datadogstatsd.cmake
Datadog,65026062,65009092,2,"2020/11/26, 18:39:43",True,"2020/11/26, 18:39:43",782.0,1953079.0,1,cpp-datadogstatsd.cmake :
Datadog,65026062,65009092,2,"2020/11/26, 18:39:43",True,"2020/11/26, 18:39:43",782.0,1953079.0,1,"Then, include this cmake file, and link  DataDogStatsD_static  to your lib/exe:"
Datadog,62864281,62819086,0,"2020/07/12, 20:27:18",False,"2020/07/12, 20:27:18",449.0,12318748.0,0,"As you have limited requirements, you could achieve this without a bot."
Datadog,62864281,62819086,0,"2020/07/12, 20:27:18",False,"2020/07/12, 20:27:18",449.0,12318748.0,0,MS Teams has income and outgoing webhooks.
Datadog,62864281,62819086,0,"2020/07/12, 20:27:18",False,"2020/07/12, 20:27:18",449.0,12318748.0,0,You could create a  Incoming webhook  inside a Teams channel.
Datadog,62864281,62819086,0,"2020/07/12, 20:27:18",False,"2020/07/12, 20:27:18",449.0,12318748.0,0,It provides an URL which you could use inside the monitoring remote server and POST the message in JSON format to the webhook url.
Datadog,62864281,62819086,0,"2020/07/12, 20:27:18",False,"2020/07/12, 20:27:18",449.0,12318748.0,0,It will be posted in teams channel like below
Datadog,62864281,62819086,0,"2020/07/12, 20:27:18",False,"2020/07/12, 20:27:18",449.0,12318748.0,0,For sending message back to the server you need to configure the  Outgoing webhook  in the channel.
Datadog,62710050,62699424,6,"2020/07/03, 09:57:59",True,"2020/07/03, 09:57:59",3786.0,2509773.0,1,Spring Cloud Data Flow and Skipper servers are  Spring Boot  applications and hence you can configure/customize logging system based on your requirements.
Datadog,62710050,62699424,6,"2020/07/03, 09:57:59",True,"2020/07/03, 09:57:59",3786.0,2509773.0,1,Here are some of the references to configure logging system for a Spring Boot app:
Datadog,62448235,62448098,4,"2020/06/18, 13:49:16",True,"2020/06/18, 13:49:16",64309.0,10008173.0,2,docker logs  and similar just collect the stdout and stderr streams from the main process running inside the container.
Datadog,62448235,62448098,4,"2020/06/18, 13:49:16",True,"2020/06/18, 13:49:16",64309.0,10008173.0,2,"There's not a ""log level"" associated with that, though some systems might treat or highlight the two streams differently."
Datadog,62448235,62448098,4,"2020/06/18, 13:49:16",True,"2020/06/18, 13:49:16",64309.0,10008173.0,2,"As a basic example, you could run"
Datadog,62448235,62448098,4,"2020/06/18, 13:49:16",True,"2020/06/18, 13:49:16",64309.0,10008173.0,2,"The resulting file listing isn't especially ""error"" or ""debug"" level."
Datadog,62448235,62448098,4,"2020/06/18, 13:49:16",True,"2020/06/18, 13:49:16",64309.0,10008173.0,2,"The production-oriented setups I'm used to include the log level in log messages (in a Node context, I've used the  Winston  logging library), and then use a tool like  fluentd  to collect and parse those messages."
Datadog,59973857,59973631,8,"2020/01/29, 21:00:41",False,"2020/01/29, 21:00:41",126283.0,14660.0,0,Make sure  CreatedDate  is indexed.
Datadog,59973857,59973631,8,"2020/01/29, 21:00:41",False,"2020/01/29, 21:00:41",126283.0,14660.0,0,Make sure  CreatedDate  is using the  date  column type .
Datadog,59973857,59973631,8,"2020/01/29, 21:00:41",False,"2020/01/29, 21:00:41",126283.0,14660.0,0,"This will be more efficient on storage (just 4 bytes), performance, and you can use all the built in  date formatting  and  functions ."
Datadog,59973857,59973631,8,"2020/01/29, 21:00:41",False,"2020/01/29, 21:00:41",126283.0,14660.0,0,Avoid  select *  and only select the columns you need.
Datadog,59973857,59973631,8,"2020/01/29, 21:00:41",False,"2020/01/29, 21:00:41",126283.0,14660.0,0,Use  YYYY-MM-DD  ISO 8601 format .
Datadog,59973857,59973631,8,"2020/01/29, 21:00:41",False,"2020/01/29, 21:00:41",126283.0,14660.0,0,"This has nothing to do with performance, but it will avoid a lot of ambiguity."
Datadog,59973857,59973631,8,"2020/01/29, 21:00:41",False,"2020/01/29, 21:00:41",126283.0,14660.0,0,The real problem is likely that you have thousands of tables with which you regularly make unions of hundreds of tables.
Datadog,59973857,59973631,8,"2020/01/29, 21:00:41",False,"2020/01/29, 21:00:41",126283.0,14660.0,0,This indicates a need to redesign your schema to simplify your queries and get better performance.
Datadog,59973857,59973631,8,"2020/01/29, 21:00:41",False,"2020/01/29, 21:00:41",126283.0,14660.0,0,Unions and date change checks suggest a lot of redundancy.
Datadog,59973857,59973631,8,"2020/01/29, 21:00:41",False,"2020/01/29, 21:00:41",126283.0,14660.0,0,Perhaps you've partitioned your tables by date.
Datadog,59973857,59973631,8,"2020/01/29, 21:00:41",False,"2020/01/29, 21:00:41",126283.0,14660.0,0,Postgres has its own built in  table partitioning  which might help.
Datadog,59973857,59973631,8,"2020/01/29, 21:00:41",False,"2020/01/29, 21:00:41",126283.0,14660.0,0,Without more detail that's all I can say.
Datadog,59973857,59973631,8,"2020/01/29, 21:00:41",False,"2020/01/29, 21:00:41",126283.0,14660.0,0,Perhaps ask another question about your schema.
Datadog,59974435,59973631,7,"2020/01/29, 21:40:37",False,"2020/01/29, 21:40:37",18091.0,1721239.0,0,"Without seeing  EXPLAIN (ANALYZE, BUFFERS) , all we can do is speculate."
Datadog,59974435,59973631,7,"2020/01/29, 21:40:37",False,"2020/01/29, 21:40:37",18091.0,1721239.0,0,But we can do some pretty good speculation.
Datadog,59974435,59973631,7,"2020/01/29, 21:40:37",False,"2020/01/29, 21:40:37",18091.0,1721239.0,0,Cluster the tables on the index on CreatedDate.
Datadog,59974435,59973631,7,"2020/01/29, 21:40:37",False,"2020/01/29, 21:40:37",18091.0,1721239.0,0,"This will allow the data to be accessed more sequentially, allowing more read-ahead (but this might not help much for some kinds of storage)."
Datadog,59974435,59973631,7,"2020/01/29, 21:40:37",False,"2020/01/29, 21:40:37",18091.0,1721239.0,0,"If the tables have high write load, they may not stay clustered and so you would have recluster them occasionally."
Datadog,59974435,59973631,7,"2020/01/29, 21:40:37",False,"2020/01/29, 21:40:37",18091.0,1721239.0,0,"If they are static, this could be a one-time event."
Datadog,59974435,59973631,7,"2020/01/29, 21:40:37",False,"2020/01/29, 21:40:37",18091.0,1721239.0,0,Get more RAM.
Datadog,59974435,59973631,7,"2020/01/29, 21:40:37",False,"2020/01/29, 21:40:37",18091.0,1721239.0,0,"If you want to perform as if all the data was in memory, then get all the data into memory."
Datadog,59974435,59973631,7,"2020/01/29, 21:40:37",False,"2020/01/29, 21:40:37",18091.0,1721239.0,0,"Get faster storage, like top-notch SSD."
Datadog,59974435,59973631,7,"2020/01/29, 21:40:37",False,"2020/01/29, 21:40:37",18091.0,1721239.0,0,"It isn't as fast as RAM, but much faster than HDD."
Datadog,58858157,58625365,0,"2019/11/14, 15:40:57",True,"2019/11/14, 15:40:57",14336.0,1251219.0,0,The answer to the question is found in the comments to it.
Datadog,58858157,58625365,0,"2019/11/14, 15:40:57",True,"2019/11/14, 15:40:57",14336.0,1251219.0,0,"Hence, this question should not go unanswered."
Datadog,58858157,58625365,0,"2019/11/14, 15:40:57",True,"2019/11/14, 15:40:57",14336.0,1251219.0,0,"The code from the question works as expected, however, the path where the named pipe resides is a special path and this is the reason why the data that is being sent to it never reaches the script."
Datadog,58858157,58625365,0,"2019/11/14, 15:40:57",True,"2019/11/14, 15:40:57",14336.0,1251219.0,0,The corresponding special casing in Bash for instance can be found in  redir.c .
Datadog,58858157,58625365,0,"2019/11/14, 15:40:57",True,"2019/11/14, 15:40:57",14336.0,1251219.0,0,The solution to the problem is to use a real UDP server on that port:
Datadog,57463587,57423102,0,"2019/08/12, 17:56:44",False,"2019/08/12, 17:56:44",149.0,5059173.0,0,It turns out that someone had turned on a scheduled job that was sending a super expensive query that was supposed to be a singleton onto the query queue every 5 min.
Datadog,57463587,57423102,0,"2019/08/12, 17:56:44",False,"2019/08/12, 17:56:44",149.0,5059173.0,0,"The query takes 20 min to run, so eventually the system bogs down and falls over."
Datadog,57014014,56993743,0,"2019/07/13, 00:03:23",False,"2019/07/13, 00:03:23",8484.0,1224827.0,0,"Apparently this was / is an issue with  rpy2 , which was a dependency of our project."
Datadog,57014014,56993743,0,"2019/07/13, 00:03:23",False,"2019/07/13, 00:03:23",8484.0,1224827.0,0,It was being imported by a utility module that was imported on startup.
Datadog,57014014,56993743,0,"2019/07/13, 00:03:23",False,"2019/07/13, 00:03:23",8484.0,1224827.0,0,This caused it to be called on every single request to our REST API endpoints.
Datadog,57014014,56993743,0,"2019/07/13, 00:03:23",False,"2019/07/13, 00:03:23",8484.0,1224827.0,0,Putting the import inside the actual function that was using it fixed this issue.
Datadog,56173299,56172624,0,"2019/05/16, 19:55:12",True,"2019/05/16, 19:55:12",1371.0,5540166.0,2,"What you want is either the  is_match  or  is_exact_match  conditional variable, which are  documented here  (with examples)."
Datadog,56173299,56172624,0,"2019/05/16, 19:55:12",True,"2019/05/16, 19:55:12",1371.0,5540166.0,2,"The idea is that you can nest your messages  and notifications  in conditional logic arguments so that only when the monitor alerts/warns/resolves, or only when the evaluated tag scope matches certain conditions, will certain messages or notification channels be part of the alert."
Datadog,56173299,56172624,0,"2019/05/16, 19:55:12",True,"2019/05/16, 19:55:12",1371.0,5540166.0,2,So in your case you want your message to include something like this:
Datadog,56173299,56172624,0,"2019/05/16, 19:55:12",True,"2019/05/16, 19:55:12",1371.0,5540166.0,2,"{{#is_exact_match ""environment.name"" ""prod""}}"
Datadog,56173299,56172624,0,"2019/05/16, 19:55:12",True,"2019/05/16, 19:55:12",1371.0,5540166.0,2,Add special prod message here
Datadog,56173299,56172624,0,"2019/05/16, 19:55:12",True,"2019/05/16, 19:55:12",1371.0,5540166.0,2,and @pagerduty or @pagerduty-foo
Datadog,56173299,56172624,0,"2019/05/16, 19:55:12",True,"2019/05/16, 19:55:12",1371.0,5540166.0,2,{{/is_exact_match}}
Datadog,56173299,56172624,0,"2019/05/16, 19:55:12",True,"2019/05/16, 19:55:12",1371.0,5540166.0,2,Add message that should always show up here
Datadog,56173299,56172624,0,"2019/05/16, 19:55:12",True,"2019/05/16, 19:55:12",1371.0,5540166.0,2,and @slack-bar
Datadog,56173299,56172624,0,"2019/05/16, 19:55:12",True,"2019/05/16, 19:55:12",1371.0,5540166.0,2,"In this case, only when the ""environment"" tag's value is ""prod"" will the bracketed content be included (which includes the pagerduty notification)."
Datadog,56173299,56172624,0,"2019/05/16, 19:55:12",True,"2019/05/16, 19:55:12",1371.0,5540166.0,2,The non-bracketed part will always be included (which includes the slack notification).
Datadog,53732835,53732707,0,"2018/12/11, 23:49:57",False,"2018/12/11, 23:49:57",1455.0,2301088.0,0,(?
Datadog,53732835,53732707,0,"2018/12/11, 23:49:57",False,"2018/12/11, 23:49:57",1455.0,2301088.0,0,":\/private\/toolbox\/)(.+)  ought to match your route path, capturing the wildcard as the first group:"
Datadog,53732835,53732707,0,"2018/12/11, 23:49:57",False,"2018/12/11, 23:49:57",1455.0,2301088.0,0,"I cannot speak to that RegExp's performance, however."
Datadog,53624583,53469205,1,"2018/12/05, 05:07:49",False,"2018/12/05, 05:07:49",3223.0,43973.0,0,"I can't speak specifically for the Java implementation, but in the CSharp client, the ability to send this data to Datadog is done to 127.0.0.1 via UDP port 8125."
Datadog,53624583,53469205,1,"2018/12/05, 05:07:49",False,"2018/12/05, 05:07:49",3223.0,43973.0,0,It's on the same thread as your executing code and not asynchronous.
Datadog,53624583,53469205,1,"2018/12/05, 05:07:49",False,"2018/12/05, 05:07:49",3223.0,43973.0,0,The whole effort by your process is finished once the UDP message is sent - it's fired and immediately forgotten.
Datadog,53624583,53469205,1,"2018/12/05, 05:07:49",False,"2018/12/05, 05:07:49",3223.0,43973.0,0,"The thread overhead you mention occurs in the separate Datadog agent process which is listening on the other end of UDP 8125, and has it's own thread pool and ability to buffer some data before sending up to Datadog's servers."
Datadog,53624583,53469205,1,"2018/12/05, 05:07:49",False,"2018/12/05, 05:07:49",3223.0,43973.0,0,Do you have additional information that shows this behavior?
Datadog,53624583,53469205,1,"2018/12/05, 05:07:49",False,"2018/12/05, 05:07:49",3223.0,43973.0,0,"Based on what I know, this doesn't sound like a side effect of the Datadog/StatsD stuff."
Datadog,54224944,53469205,0,"2019/01/16, 22:34:23",True,"2019/01/16, 22:34:23",2723.0,5885013.0,0,"I found the answer on Datadog's help forum:  ""How to graph percentiles in Datadog"" ."
Datadog,54224944,53469205,0,"2019/01/16, 22:34:23",True,"2019/01/16, 22:34:23",2723.0,5885013.0,0,"So the gist is that the latency itself didn't go up, but aggregating over multiple streams (where each stream corresponds to each custom tag) caused the graph to display a different shape."
Datadog,51552249,51524214,0,"2018/07/27, 09:49:35",True,"2018/07/27, 10:22:48",11822.0,193178.0,1,"Just use  NLog.MappedDiagnosticsLogicalContext.Set(""userid"", ""someValue"")  together with  ${mdlc:item=userid}  where needed."
Datadog,51552249,51524214,0,"2018/07/27, 09:49:35",True,"2018/07/27, 10:22:48",11822.0,193178.0,1,See also  http://github.com/NLog/NLog/wiki/MDLC-Layout-Renderer
Datadog,51552249,51524214,0,"2018/07/27, 09:49:35",True,"2018/07/27, 10:22:48",11822.0,193178.0,1,MappedDiagnosticsLogicalContext  uses  CallContext  (and  AsyncLocal  on NetCore) which are thread-safe.
Datadog,51552249,51524214,0,"2018/07/27, 09:49:35",True,"2018/07/27, 10:22:48",11822.0,193178.0,1,"Settings will also support async Task and follow to the chained tasks, but if scheduling a Time-callback based on a user-request, then the Timer-callback will not see the userid."
Datadog,51552249,51524214,0,"2018/07/27, 09:49:35",True,"2018/07/27, 10:22:48",11822.0,193178.0,1,"You should avoid changing  LogManager.Configuration.Variable  at runtime, they are global for all concurrent requests, and might get lost during configuration-reload (If autoreload configured)."
Datadog,52038935,51521838,0,"2018/08/27, 15:02:48",True,"2018/08/27, 15:02:48",1.0,10134087.0,0,"If anyone will need the answer, this is how I did this."
Datadog,52038935,51521838,0,"2018/08/27, 15:02:48",True,"2018/08/27, 15:02:48",1.0,10134087.0,0,"It shows the biggest tables that were not last vacuumed in that past 2 weeks, but limits the list for 20 results."
Datadog,52038935,51521838,0,"2018/08/27, 15:02:48",True,"2018/08/27, 15:02:48",1.0,10134087.0,0,If you want to you can change LIMIT 20 or delete it for shorter/longer list.
Datadog,52038935,51521838,0,"2018/08/27, 15:02:48",True,"2018/08/27, 15:02:48",1.0,10134087.0,0,Also change pg.last_autovacuum for analyze or anything else from pg.stat table you want to check and also 2 week can be changed to whatever time period you want.
Datadog,52039061,51521838,0,"2018/08/27, 15:09:40",False,"2018/08/27, 15:09:40",1.0,10134087.0,0,In datadog under postgres.yaml I added this:
Datadog,52039061,51521838,0,"2018/08/27, 15:09:40",False,"2018/08/27, 15:09:40",1.0,10134087.0,0,Then I added this as a top-list inside a dashboard.
Datadog,52039061,51521838,0,"2018/08/27, 15:09:40",False,"2018/08/27, 15:09:40",1.0,10134087.0,0,"You can also make it as an alert and sum up the counts inside the metrics and decide what's your limit that about it you want to start vacuuming, although it's just recommended to run it regularly and not just when it's just too big, that's why we use this only as a list in our dashboard."
Datadog,52039061,51521838,0,"2018/08/27, 15:09:40",False,"2018/08/27, 15:09:40",1.0,10134087.0,0,To get eyes only.
Datadog,49291336,49280325,0,"2018/03/15, 05:58:25",True,"2018/03/15, 05:58:25",1390.0,8748848.0,1,"Datadog, like OMS and other monitoring software uses the Azure VM agent to steam the information."
Datadog,49291336,49280325,0,"2018/03/15, 05:58:25",True,"2018/03/15, 05:58:25",1390.0,8748848.0,1,Once this agent is installed on the system we are able to gather the info needed.
Datadog,49291336,49280325,0,"2018/03/15, 05:58:25",True,"2018/03/15, 05:58:25",1390.0,8748848.0,1,The VM agent is not something that goes out over the internet like other connections.
Datadog,49291336,49280325,0,"2018/03/15, 05:58:25",True,"2018/03/15, 05:58:25",1390.0,8748848.0,1,"Hence, you should still see the reporting available."
Datadog,49291336,49280325,0,"2018/03/15, 05:58:25",True,"2018/03/15, 05:58:25",1390.0,8748848.0,1,"Rather, it should be a direct connection from the Hyper-V manager and the VM itself."
Datadog,49291336,49280325,0,"2018/03/15, 05:58:25",True,"2018/03/15, 05:58:25",1390.0,8748848.0,1,"This therefore, bypassing any NSG rules you would have in place."
Datadog,49292913,49280325,0,"2018/03/15, 08:32:47",False,"2018/03/15, 08:44:04",12857.0,6851908.0,0,I have installed data dog agent on one of my virtual machines
Datadog,49292913,49280325,0,"2018/03/15, 08:32:47",False,"2018/03/15, 08:44:04",12857.0,6851908.0,0,Datadog agent will collect system metrics and  forward  to Datadog.
Datadog,49292913,49280325,0,"2018/03/15, 08:32:47",False,"2018/03/15, 08:44:04",12857.0,6851908.0,0,Datadog agent works like this:
Datadog,49292913,49280325,0,"2018/03/15, 08:32:47",False,"2018/03/15, 08:44:04",12857.0,6851908.0,0,"Also you can try to perform a network capture on your Azure VM, then we are able to find the detailed of the agent behavior."
Datadog,49292913,49280325,0,"2018/03/15, 08:32:47",False,"2018/03/15, 08:44:04",12857.0,6851908.0,0,Here is the network capture in my test VM:
Datadog,49292913,49280325,0,"2018/03/15, 08:32:47",False,"2018/03/15, 08:44:04",12857.0,6851908.0,0,We can find that  Datadog agent forward over HTTPS(443) to Datadog HQ .
Datadog,49292913,49280325,0,"2018/03/15, 08:32:47",False,"2018/03/15, 08:44:04",12857.0,6851908.0,0,"After you deny port 443 in NSG outbound rules, the datadog will not get your metrics:"
Datadog,49292913,49280325,0,"2018/03/15, 08:32:47",False,"2018/03/15, 08:44:04",12857.0,6851908.0,0,"More information about datadog agent, please refer to this official  article ."
Datadog,48265003,48262289,0,"2018/01/15, 16:28:30",False,"2018/01/15, 16:28:30",9230.0,63308.0,0,Anything printed to STDOUT will be sent to your logging addons like SumoLogic.
Datadog,48265003,48262289,0,"2018/01/15, 16:28:30",False,"2018/01/15, 16:28:30",9230.0,63308.0,0,The options you've shown should take care of that.
Datadog,48265003,48262289,0,"2018/01/15, 16:28:30",False,"2018/01/15, 16:28:30",9230.0,63308.0,0,"The mechanism that addons like SumoLogic use is call  Log Drains , and you can tap into that your self to get your log stream over HTTP."
Datadog,45557292,45554437,2,"2017/08/08, 02:56:57",False,"2017/08/08, 02:56:57",1371.0,5540166.0,1,"hmm, so you're trying to use autodiscovery to find which container the dd-agent should be running the etcd check on?"
Datadog,45557292,45554437,2,"2017/08/08, 02:56:57",False,"2017/08/08, 02:56:57",1371.0,5540166.0,1,and you're using the auto_conf files approach?
Datadog,45557292,45554437,2,"2017/08/08, 02:56:57",False,"2017/08/08, 02:56:57",1371.0,5540166.0,1,"And there, you're wondering how to apply the  %%host%%  template variable?"
Datadog,45557292,45554437,2,"2017/08/08, 02:56:57",False,"2017/08/08, 02:56:57",1371.0,5540166.0,1,"If that's what you're interested in, I think you'll want to add it into your  etcd.yaml  on the  url  line, as shown in  the example file  like so:"
Datadog,43724984,43721365,3,"2017/05/01, 22:11:46",False,"2017/05/01, 22:11:46",261.0,4172512.0,1,When submitting histograms via dogstatsD you should be automatically creating 5 metrics as shown here:
Datadog,43724984,43721365,3,"2017/05/01, 22:11:46",False,"2017/05/01, 22:11:46",261.0,4172512.0,1,dog.histogram(...)
Datadog,43724984,43721365,3,"2017/05/01, 22:11:46",False,"2017/05/01, 22:11:46",261.0,4172512.0,1,Usage: Used to track the statistical distribution of a set of values over a statsd flush period.
Datadog,43724984,43721365,3,"2017/05/01, 22:11:46",False,"2017/05/01, 22:11:46",261.0,4172512.0,1,Actually submits as multiple metrics:
Datadog,43724984,43721365,3,"2017/05/01, 22:11:46",False,"2017/05/01, 22:11:46",261.0,4172512.0,1,Additional details on metric types and their submission sources can be found here:
Datadog,43724984,43721365,3,"2017/05/01, 22:11:46",False,"2017/05/01, 22:11:46",261.0,4172512.0,1,https://help.datadoghq.com/hc/en-us/articles/206955236-Metric-types-in-Datadog
Datadog,43724984,43721365,3,"2017/05/01, 22:11:46",False,"2017/05/01, 22:11:46",261.0,4172512.0,1,It appears for your use case  metric.count  would be the closest match for calculating the total length of your word.
Datadog,43724984,43721365,3,"2017/05/01, 22:11:46",False,"2017/05/01, 22:11:46",261.0,4172512.0,1,"Once selected, you can make use of the  as_count()  modifier which will calculate the total count rather than the average over the flushing period."
Datadog,43724984,43721365,3,"2017/05/01, 22:11:46",False,"2017/05/01, 22:11:46",261.0,4172512.0,1,More information on this use case can be found here:
Datadog,43724984,43721365,3,"2017/05/01, 22:11:46",False,"2017/05/01, 22:11:46",261.0,4172512.0,1,https://help.datadoghq.com/hc/en-us/articles/204271195-Why-is-a-counter-metric-being-displayed-as-a-decimal-value-
Datadog,43724984,43721365,3,"2017/05/01, 22:11:46",False,"2017/05/01, 22:11:46",261.0,4172512.0,1,If you find yourself still running into any issues with this submission feel free to reach out to support@datadoghq.com
Datadog,39513221,39513085,0,"2016/09/15, 17:06:29",False,"2016/09/15, 17:06:29",13863.0,6162307.0,0,You can modify the following according to your needs.
Datadog,39513221,39513085,0,"2016/09/15, 17:06:29",False,"2016/09/15, 17:06:29",13863.0,6162307.0,0,"What this basically does, is that it prevents  print()  from writing the default end character ( end='' ) and at the same time, it write a carriage return ( '\r' ) before anything else."
Datadog,39513221,39513085,0,"2016/09/15, 17:06:29",False,"2016/09/15, 17:06:29",13863.0,6162307.0,0,"In simple terms, you are overwriting the previous  print()  statement."
Datadog,39513348,39513085,1,"2016/09/15, 17:12:09",False,"2016/09/15, 17:16:06",254.0,3785588.0,0,"the naive solution would be to just use the total amount of rows in your dataset and the index your are at, then calculate the progress:"
Datadog,39513348,39513085,1,"2016/09/15, 17:12:09",False,"2016/09/15, 17:16:06",254.0,3785588.0,0,This will only be somewhat reliable if every row takes around the same time to complete.
Datadog,39513348,39513085,1,"2016/09/15, 17:12:09",False,"2016/09/15, 17:16:06",254.0,3785588.0,0,"Because you have a large dataset, it might average out over time, but if some rows take a millisecond, and another takes 10 minutes, the percentage will be garbage."
Datadog,39513348,39513085,1,"2016/09/15, 17:12:09",False,"2016/09/15, 17:16:06",254.0,3785588.0,0,Also consider rounding the percentage to one decimal:
Datadog,39513348,39513085,1,"2016/09/15, 17:12:09",False,"2016/09/15, 17:16:06",254.0,3785588.0,0,Printing for every row might slow your task down significantly so consider this improvement:
Datadog,39513348,39513085,1,"2016/09/15, 17:12:09",False,"2016/09/15, 17:16:06",254.0,3785588.0,0,"There are, of course, also modules for this:"
Datadog,39513348,39513085,1,"2016/09/15, 17:12:09",False,"2016/09/15, 17:16:06",254.0,3785588.0,0,progressbar
Datadog,39513348,39513085,1,"2016/09/15, 17:12:09",False,"2016/09/15, 17:16:06",254.0,3785588.0,0,progress
Datadog,43644895,43641450,0,"2017/04/27, 00:34:11",False,"2017/04/27, 00:34:11",261.0,4172512.0,2,You could use Datadog's Outlier detection to identify instances which exhibit behavior outside the normal for it's peer set.
Datadog,43644895,43641450,0,"2017/04/27, 00:34:11",False,"2017/04/27, 00:34:11",261.0,4172512.0,2,"As an example, you could create an outlier detection monitor:"
Datadog,43644895,43641450,0,"2017/04/27, 00:34:11",False,"2017/04/27, 00:34:11",261.0,4172512.0,2,http://docs.datadoghq.com/guides/outliers/#alerts
Datadog,43644895,43641450,0,"2017/04/27, 00:34:11",False,"2017/04/27, 00:34:11",261.0,4172512.0,2,Which would be scoped to a system metric like  aws.ec2.cpuutilization  and be alerted if any host spiked abnormally or had very low utilization in comparison to its group.
Datadog,43644895,43641450,0,"2017/04/27, 00:34:11",False,"2017/04/27, 00:34:11",261.0,4172512.0,2,There are some additional blog posts which discuss the use of the algorithms that can be found here:
Datadog,43644895,43641450,0,"2017/04/27, 00:34:11",False,"2017/04/27, 00:34:11",261.0,4172512.0,2,https://www.datadoghq.com/blog/introducing-outlier-detection-in-datadog/
Datadog,43644895,43641450,0,"2017/04/27, 00:34:11",False,"2017/04/27, 00:34:11",261.0,4172512.0,2,https://www.datadoghq.com/blog/outlier-detection-algorithms-at-datadog/
Datadog,43644895,43641450,0,"2017/04/27, 00:34:11",False,"2017/04/27, 00:34:11",261.0,4172512.0,2,https://www.datadoghq.com/blog/scaling-outlier-algorithms/
Datadog,43644895,43641450,0,"2017/04/27, 00:34:11",False,"2017/04/27, 00:34:11",261.0,4172512.0,2,"That said, if you find yourself needing additional assistance with outlier detection you can always reach out to the Support team at support@datadoghq.com or by using the internal support features found here:"
Datadog,43644895,43641450,0,"2017/04/27, 00:34:11",False,"2017/04/27, 00:34:11",261.0,4172512.0,2,https://app.datadoghq.com/help
Datadog,43644895,43641450,0,"2017/04/27, 00:34:11",False,"2017/04/27, 00:34:11",261.0,4172512.0,2,Hope this helps!
Datadog,66203307,66202271,0,"2021/02/15, 07:52:17",False,"2021/02/15, 07:52:17",124.0,1534712.0,0,I have found that  Blackfire  is doing the trick.
Datadog,66203307,66202271,0,"2021/02/15, 07:52:17",False,"2021/02/15, 07:52:17",124.0,1534712.0,0,Seems to be relatively easy to install and can run it free locally.
Datadog,65834510,65790261,0,"2021/01/21, 21:32:18",True,"2021/01/21, 21:32:18",493.0,244037.0,0,"Datadog Tags are generally strings, but also support &quot;key:value&quot; strings, which is most useful, since then the  key  can act as a dimension."
Datadog,65834510,65790261,0,"2021/01/21, 21:32:18",True,"2021/01/21, 21:32:18",493.0,244037.0,0,"There's no support that I know of that allows for a single key with multiple values, so I don't think Datadog will support the syntax you're attempting."
Datadog,65834510,65790261,0,"2021/01/21, 21:32:18",True,"2021/01/21, 21:32:18",493.0,244037.0,0,You  may  want to try:
Datadog,65834510,65790261,0,"2021/01/21, 21:32:18",True,"2021/01/21, 21:32:18",493.0,244037.0,0,in your config.
Datadog,65834510,65790261,0,"2021/01/21, 21:32:18",True,"2021/01/21, 21:32:18",493.0,244037.0,0,General reference here:  https://docs.datadoghq.com/getting_started/tagging/
Datadog,65153895,65151286,0,"2020/12/05, 06:52:20",False,"2020/12/05, 06:52:20",17233.0,9576186.0,1,"You're welcome to go look through the source code yourself , but generally my comment is correct."
Datadog,65153895,65151286,0,"2020/12/05, 06:52:20",False,"2020/12/05, 06:52:20",17233.0,9576186.0,1,"Nest binds all route handlers and enhancers (guards, interceptors, pipes, and filters) as a large anonymous function, in a very abstract way (does the same thing for Fastify as far as I can tell)."
Datadog,64842339,64821299,4,"2020/11/15, 10:08:00",False,"2020/11/15, 10:08:00",327.0,2882619.0,0,You can use Fluentd as a  daemonset  on your cluster.
Datadog,64842339,64821299,4,"2020/11/15, 10:08:00",False,"2020/11/15, 10:08:00",327.0,2882619.0,0,see this repo and docker images -&gt;  fluent/fluentd-docker-image
Datadog,64842339,64821299,4,"2020/11/15, 10:08:00",False,"2020/11/15, 10:08:00",327.0,2882619.0,0,and use this filter to add  Kubernetes metadata  to every log collected by Fluentd and then use a grep filter to exclude logs that are not in your namespaces.
Datadog,64842339,64821299,4,"2020/11/15, 10:08:00",False,"2020/11/15, 10:08:00",327.0,2882619.0,0,something like this:
Datadog,60992355,60987316,3,"2020/04/02, 16:08:59",False,"2020/04/02, 16:08:59",239.0,2268923.0,0,did u try bulk publish?
Datadog,60992355,60987316,3,"2020/04/02, 16:08:59",False,"2020/04/02, 16:08:59",239.0,2268923.0,0,"publish(topic,[]Message{1,2,3,4,.....})"
Datadog,59738942,59734858,0,"2020/01/14, 19:30:22",False,"2020/01/14, 19:30:22",2803.0,10809602.0,1,I believe your requirement can be accomplished using cmdlet  Invoke-AzVMRunCommand  /  Invoke-AzureRmVMRunCommand  or  Set-AzVMCustomScriptExtension  /  Set-AzureRmVMCustomScriptExtension .
Datadog,59738942,59734858,0,"2020/01/14, 19:30:22",False,"2020/01/14, 19:30:22",2803.0,10809602.0,1,Related scripts can be found  here  and  here .
Datadog,59738942,59734858,0,"2020/01/14, 19:30:22",False,"2020/01/14, 19:30:22",2803.0,10809602.0,1,"Just FYI,  this  and  this  are actual references for the above information."
Datadog,59738942,59734858,0,"2020/01/14, 19:30:22",False,"2020/01/14, 19:30:22",2803.0,10809602.0,1,Hope this update helps!
Datadog,60614784,59519717,0,"2020/03/10, 11:34:48",False,"2020/03/10, 11:34:48",57.0,339202.0,0,This is possible by adding the annotation below to nginx ingress:
Datadog,60614784,59519717,0,"2020/03/10, 11:34:48",False,"2020/03/10, 11:34:48",57.0,339202.0,0,See full answer at  https://github.com/DataDog/dd-opentracing-cpp/issues/118
Datadog,57763275,57762582,0,"2019/09/03, 01:43:07",True,"2019/09/03, 01:43:07",104968.0,1766831.0,1,"Turn on the ""general log"" and have it write to a file."
Datadog,57763275,57762582,0,"2019/09/03, 01:43:07",True,"2019/09/03, 01:43:07",104968.0,1766831.0,1,Wait a finite amount of time.
Datadog,57763275,57762582,0,"2019/09/03, 01:43:07",True,"2019/09/03, 01:43:07",104968.0,1766831.0,1,Then use  pt-query-digest  to summarize the results.
Datadog,57763275,57762582,0,"2019/09/03, 01:43:07",True,"2019/09/03, 01:43:07",104968.0,1766831.0,1,Turn off the general log before it fills up disk.
Datadog,57763275,57762582,0,"2019/09/03, 01:43:07",True,"2019/09/03, 01:43:07",104968.0,1766831.0,1,The slowlog (with a small value in  long_query_time ) is more useful for finding naughty queries.
Datadog,55752352,55737494,0,"2019/04/18, 22:06:04",False,"2019/04/18, 22:06:04",175.0,2356037.0,0,The issue was the size of the HTTPRequest was to large the higher the parallelism which makes sense.
Datadog,55752352,55737494,0,"2019/04/18, 22:06:04",False,"2019/04/18, 22:06:04",175.0,2356037.0,0,"I was getting back ""Request Entity Too Large"" however the exception wasn't logging out correctly so I missed it."
Datadog,55752352,55737494,0,"2019/04/18, 22:06:04",False,"2019/04/18, 22:06:04",175.0,2356037.0,0,It seems that the Flink  DatadogHttpReporter  does not take the size of the request into consideration when building it.
Datadog,55752352,55737494,0,"2019/04/18, 22:06:04",False,"2019/04/18, 22:06:04",175.0,2356037.0,0,I modified the Reporter to limit the number of metrics per request to 1000.
Datadog,55752352,55737494,0,"2019/04/18, 22:06:04",False,"2019/04/18, 22:06:04",175.0,2356037.0,0,Now the metrics are showing up just fine.
Datadog,54751580,54735683,2,"2019/02/18, 18:30:16",False,"2019/02/18, 18:30:16",1371.0,5540166.0,0,"This becomes pretty easy with Datadog's Log Management product -- you can measure lots of things by endpoint, including hits, unique client-ip count, latency (if you add response time to your nginx logs)."
Datadog,54751580,54735683,2,"2019/02/18, 18:30:16",False,"2019/02/18, 18:30:16",1371.0,5540166.0,0,More info on the setup and these use cases  in this blogpost .
Datadog,54751580,54735683,2,"2019/02/18, 18:30:16",False,"2019/02/18, 18:30:16",1371.0,5540166.0,0,Documentation on the logs part of the nginx integration  here .
Datadog,53449811,53441210,1,"2018/11/23, 18:04:49",True,"2018/11/23, 18:04:49",1371.0,5540166.0,1,"This is definitely possible, but you will want to change your tag setup a little."
Datadog,53449811,53441210,1,"2018/11/23, 18:04:49",True,"2018/11/23, 18:04:49",1371.0,5540166.0,1,"You want to take advantage of  key:value  syntax with your tags, so that you can group out the tags by their common  key ."
Datadog,53449811,53441210,1,"2018/11/23, 18:04:49",True,"2018/11/23, 18:04:49",1371.0,5540166.0,1,"So in your case, instead of tagging by  entity.count.payment , you would want to tag by  entity.count:payment  or better yet  entity:payment ."
Datadog,53449811,53441210,1,"2018/11/23, 18:04:49",True,"2018/11/23, 18:04:49",1371.0,5540166.0,1,That way you can write one query of your metric and use the  group by  functionality on the shared  entity  tag key to see it's values for all the different  entity  tags.
Datadog,53449811,53441210,1,"2018/11/23, 18:04:49",True,"2018/11/23, 18:04:49",1371.0,5540166.0,1,"From there, you can use the  top  function to always see just the top n values, whether that be  payment  or  cart  or  visit  etc."
Datadog,53449811,53441210,1,"2018/11/23, 18:04:49",True,"2018/11/23, 18:04:49",1371.0,5540166.0,1,This doc here about tags  is definitely worth a read!
Datadog,53449811,53441210,1,"2018/11/23, 18:04:49",True,"2018/11/23, 18:04:49",1371.0,5540166.0,1,Tags can make graphing and monitoring much easier and more scalable.
Datadog,52971077,50468823,0,"2018/10/24, 17:06:01",False,"2018/10/24, 17:06:01",338.0,8335684.0,0,It is necessary to  add AspectJ weaver as Java Agent  when you're starting your Akka aplication:  -javaagent:aspectjweaver.jar
Datadog,52971077,50468823,0,"2018/10/24, 17:06:01",False,"2018/10/24, 17:06:01",338.0,8335684.0,0,You can add the following settings in your project SBT configuration:
Datadog,52971077,50468823,0,"2018/10/24, 17:06:01",False,"2018/10/24, 17:06:01",338.0,8335684.0,0,So AspectJ weaver JAR will be copied to  ./lib_managed/jars/org.aspectj/aspectjweaver/aspectjweaver-[aspectJWeaverV].jar  in your project root.
Datadog,52971077,50468823,0,"2018/10/24, 17:06:01",False,"2018/10/24, 17:06:01",338.0,8335684.0,0,Then you can refer this JAR in your Dockerfile:
Datadog,49608019,49607708,6,"2018/04/02, 11:42:22",False,"2018/04/02, 11:42:22",1443.0,4333853.0,1,Few things to debug
Datadog,49277969,49274395,0,"2018/03/14, 14:36:12",True,"2018/04/29, 00:26:33",2024.0,698082.0,1,You can easily get all needed data via querying dmv and other resources inside SQL Server.
Datadog,49277969,49274395,0,"2018/03/14, 14:36:12",True,"2018/04/29, 00:26:33",2024.0,698082.0,1,Good start is  here .
Datadog,46871722,46867883,2,"2017/10/22, 10:49:32",False,"2017/10/22, 10:49:32",2208.0,5259881.0,1,I have 35 Cassandra nodes (different clusters) monitored without any problems with graphite + carbon + whisper + grafana.
Datadog,46871722,46867883,2,"2017/10/22, 10:49:32",False,"2017/10/22, 10:49:32",2208.0,5259881.0,1,But i have to tell that re-configuring collection and aggregations windows with whisper is a pain.
Datadog,46871722,46867883,2,"2017/10/22, 10:49:32",False,"2017/10/22, 10:49:32",2208.0,5259881.0,1,"There's many alternatives today for this job, you can use influxdb (+ telegraf) stack for example."
Datadog,46871722,46867883,2,"2017/10/22, 10:49:32",False,"2017/10/22, 10:49:32",2208.0,5259881.0,1,"Also with datadog you don't need grafana, they're also a visualizing platform."
Datadog,46871722,46867883,2,"2017/10/22, 10:49:32",False,"2017/10/22, 10:49:32",2208.0,5259881.0,1,"I've worked with it some time ago, but they have some misleading names for some metrics in their plugin, and some metrics were just missing."
Datadog,46871722,46867883,2,"2017/10/22, 10:49:32",False,"2017/10/22, 10:49:32",2208.0,5259881.0,1,"As a pros for this platform, it's really easy to install and use."
Datadog,46949745,46867883,2,"2017/10/26, 11:49:53",False,"2017/10/26, 11:49:53",341.0,529321.0,1,"We have a cassandra cluster of 36 nodes in production right now (we had 51 but migrated the instance type since then so we need less C* servers now), monitored using a single graphite server."
Datadog,46949745,46867883,2,"2017/10/26, 11:49:53",False,"2017/10/26, 11:49:53",341.0,529321.0,1,We are also saving data for 30 days but in a 60s resolution.
Datadog,46949745,46867883,2,"2017/10/26, 11:49:53",False,"2017/10/26, 11:49:53",341.0,529321.0,1,We excluded the internode metrics (e.g.
Datadog,46949745,46867883,2,"2017/10/26, 11:49:53",False,"2017/10/26, 11:49:53",341.0,529321.0,1,"open connections from a to b) because of the scaling of the metric count, but keep all other."
Datadog,46949745,46867883,2,"2017/10/26, 11:49:53",False,"2017/10/26, 11:49:53",341.0,529321.0,1,"This totals to ~510k metrics, each whisper file being ~500kb in size =  ~250GB."
Datadog,46949745,46867883,2,"2017/10/26, 11:49:53",False,"2017/10/26, 11:49:53",341.0,529321.0,1,"iostat tells me, that we have write peaks to ~70k writes/s."
Datadog,46949745,46867883,2,"2017/10/26, 11:49:53",False,"2017/10/26, 11:49:53",341.0,529321.0,1,This all is done on a single AWS i3.2xlarge instance which include 1.9TB nvme instance storage and 61GB of RAM.
Datadog,46949745,46867883,2,"2017/10/26, 11:49:53",False,"2017/10/26, 11:49:53",341.0,529321.0,1,To fully utilize the power of the this disk type we increased the number of carbon caches.
Datadog,46949745,46867883,2,"2017/10/26, 11:49:53",False,"2017/10/26, 11:49:53",341.0,529321.0,1,The cpu usage is very low (&lt;20%) and so is the iowait (&lt;1%).
Datadog,46949745,46867883,2,"2017/10/26, 11:49:53",False,"2017/10/26, 11:49:53",341.0,529321.0,1,"I guess we could get away with a less beefy machine, but this gives us a lot of headroom for growing the cluster and we are constantly adding new servers."
Datadog,46949745,46867883,2,"2017/10/26, 11:49:53",False,"2017/10/26, 11:49:53",341.0,529321.0,1,"For the monitoring: Be prepared that AWS will terminate these machines more often than others, so backup and restore are more likely a regular operation."
Datadog,46949745,46867883,2,"2017/10/26, 11:49:53",False,"2017/10/26, 11:49:53",341.0,529321.0,1,I hope this little insight helped you.
Datadog,41809571,39172174,0,"2017/01/23, 17:14:15",False,"2017/01/23, 17:14:15",89.0,4256613.0,1,It looks like that you have not set your JMX_PORT for kafka from where your datadog agent can listen information abouot the metrics.
Datadog,41809571,39172174,0,"2017/01/23, 17:14:15",False,"2017/01/23, 17:14:15",89.0,4256613.0,1,"Restart your Kafka with the following additional key/value pair parameter:
'JMX_PORT=9999'"
Datadog,41809571,39172174,0,"2017/01/23, 17:14:15",False,"2017/01/23, 17:14:15",89.0,4256613.0,1,$ JMX_PORT=9999 ./kafka-server-start.sh ../config/server.properties
Datadog,50008438,39172174,0,"2018/04/24, 21:19:55",False,"2018/04/24, 21:19:55",3040.0,4460737.0,0,This error essentially means that the Datadog Agent is unable to connect to the Kafka instance to retrieve metrics from the exposed mBeans over the RMI protocol.
Datadog,50008438,39172174,0,"2018/04/24, 21:19:55",False,"2018/04/24, 21:19:55",3040.0,4460737.0,0,"This error can be resolved by including the following JVM (Java Virtual Machine) arguments when starting the Kafka instance (required for Producer, Consumer, and Broker as they are all separate Java instances)
please"
Datadog,50008438,39172174,0,"2018/04/24, 21:19:55",False,"2018/04/24, 21:19:55",3040.0,4460737.0,0,Please read this article
Datadog,39174290,39163880,0,"2016/08/26, 23:35:25",True,"2016/08/26, 23:35:25",961.0,247700.0,2,"Nexus 3.0.1 exposes authenticated access to metrics using  http://metrics.dropwizard.io/3.1.0/manual/servlets/ 
You have these endpoints available for different purposes:
 
        {host:port}/service/metrics/healthcheck
        {host:port}/service/metrics/data
        {host:port}/service/metrics/ping
        {host:port}/service/metrics/threads"
Datadog,30168310,30155496,3,"2015/05/11, 15:58:22",True,"2015/05/11, 15:58:22",1802.0,1368626.0,2,"I recall the default behavior being that each gear can handle 16 concurrent connections, then auto-scaling would kick in and you would get a new gear."
Datadog,30168310,30155496,3,"2015/05/11, 15:58:22",True,"2015/05/11, 15:58:22",1802.0,1368626.0,2,Therefore I would think it makes sense to start by testing that a gear works well with 16 users at once.
Datadog,30168310,30155496,3,"2015/05/11, 15:58:22",True,"2015/05/11, 15:58:22",1802.0,1368626.0,2,"If not, then you can  change the scaling policy  to what works best for you application."
Datadog,30168310,30155496,3,"2015/05/11, 15:58:22",True,"2015/05/11, 15:58:22",1802.0,1368626.0,2,BlazeMeter  is a tool that could probably help with creating the connections.
Datadog,30168310,30155496,3,"2015/05/11, 15:58:22",True,"2015/05/11, 15:58:22",1802.0,1368626.0,2,"They mention 100,000 concurrent users on that main page so I don't think you have to worry about getting banned for this sort of test."
Datadog,66554834,66554063,2,"2021/03/09, 23:11:35",False,"2021/03/09, 23:11:35",1693.0,971735.0,2,spring.sleuth.baggage.correlation-fields  automatically sets baggage values to Slf4j’s MDC so you only need to set the baggage field.
Datadog,66554834,66554063,2,"2021/03/09, 23:11:35",False,"2021/03/09, 23:11:35",1693.0,971735.0,2,I suppose you use Sleuth out of the box (uses Brave):
Datadog,66554834,66554063,2,"2021/03/09, 23:11:35",False,"2021/03/09, 23:11:35",1693.0,971735.0,2,The  spring.sleuth.baggage.correlation-fields  property automatically sets baggage values to Slf4j’s MDC so you only need to set the baggage field.
Datadog,66554834,66554063,2,"2021/03/09, 23:11:35",False,"2021/03/09, 23:11:35",1693.0,971735.0,2,"Also, using  MDCScopeDecorator , you can set the baggage values to Slf4j’s MDC programmatically, you can see how to do it in  Sleuth docs :"
Datadog,66538157,66533080,0,"2021/03/09, 00:29:39",True,"2021/03/09, 00:29:39",1693.0,971735.0,1,"We figured this out in the comments, I'm posting an answer that summarizes it all up: it seems the root cause was using different versions of different spring-boot modules."
Datadog,66538157,66533080,0,"2021/03/09, 00:29:39",True,"2021/03/09, 00:29:39",1693.0,971735.0,1,"It is a good rule of thumb to not define the versions yourself but use BOMs and let them define the versions for you, e.g."
Datadog,66538157,66533080,0,"2021/03/09, 00:29:39",True,"2021/03/09, 00:29:39",1693.0,971735.0,1,see:  spring-boot-dependencies .
Datadog,66538157,66533080,0,"2021/03/09, 00:29:39",True,"2021/03/09, 00:29:39",1693.0,971735.0,1,This way you will use the compatible (and tested) versions.
Datadog,66538157,66533080,0,"2021/03/09, 00:29:39",True,"2021/03/09, 00:29:39",1693.0,971735.0,1,management.metrics.tags.your-tag  is the way to add tags to all of your metrics.
Datadog,66538157,66533080,0,"2021/03/09, 00:29:39",True,"2021/03/09, 00:29:39",1693.0,971735.0,1,A good way to check this is looking at  /actuator/metrics .
Datadog,65636279,65619895,3,"2021/01/08, 22:56:19",True,"2021/01/08, 22:56:19",385.0,2051074.0,2,"I ended up going with the  sbt-javaagent  plugin to avoid extra code to exclude the agent jar from the classpath, which the plugin handles automatically."
Datadog,65636279,65619895,3,"2021/01/08, 22:56:19",True,"2021/01/08, 22:56:19",385.0,2051074.0,2,"The trick/hack was to filter out the default  addJava -javaagent  line the  sbt-javaagent  plugin adds automatically , and then appending a new script snippent to only enable the javaagent when a certain env."
Datadog,65636279,65619895,3,"2021/01/08, 22:56:19",True,"2021/01/08, 22:56:19",385.0,2051074.0,2,variable is set.
Datadog,63061286,63054587,2,"2020/07/23, 21:56:23",False,"2020/07/23, 21:56:23",2661.0,1184752.0,2,You can use simple  jcmd  command line tool
Datadog,63061286,63054587,2,"2020/07/23, 21:56:23",False,"2020/07/23, 21:56:23",2661.0,1184752.0,2,As an example of running this on my simple Clojure application:
Datadog,62333814,62331584,0,"2020/06/12, 00:23:58",False,"2020/06/12, 00:23:58",3.0,5605817.0,0,"As shown in the documentation in  your link , WHL files are also supported."
Datadog,62333814,62331584,0,"2020/06/12, 00:23:58",False,"2020/06/12, 00:23:58",3.0,5605817.0,0,It says:
Datadog,62333814,62331584,0,"2020/06/12, 00:23:58",False,"2020/06/12, 00:23:58",3.0,5605817.0,0,You might already have one or more Python libraries packaged as an .egg or a .whl file.
Datadog,62333814,62331584,0,"2020/06/12, 00:23:58",False,"2020/06/12, 00:23:58",3.0,5605817.0,0,There is a .whl file available for the DataDog python library here:  https://pypi.org/project/datadog/#files .
Datadog,62333814,62331584,0,"2020/06/12, 00:23:58",False,"2020/06/12, 00:23:58",3.0,5605817.0,0,"You might try downloading that file, uploading it to your S3 bucket, and using that as your Python library for your Glue job."
Datadog,62333814,62331584,0,"2020/06/12, 00:23:58",False,"2020/06/12, 00:23:58",3.0,5605817.0,0,You might be more successful using that than trying to build your own .egg file.
Datadog,62032453,62030485,0,"2020/05/27, 01:30:00",False,"2020/05/27, 01:30:00",101971.0,248823.0,0,The AWS API calls to start a task are:
Datadog,62032453,62030485,0,"2020/05/27, 01:30:00",False,"2020/05/27, 01:30:00",101971.0,248823.0,0,StartTask :
Datadog,62032453,62030485,0,"2020/05/27, 01:30:00",False,"2020/05/27, 01:30:00",101971.0,248823.0,0,Starts a new task from the specified task definition on the specified container instance or instances.
Datadog,62032453,62030485,0,"2020/05/27, 01:30:00",False,"2020/05/27, 01:30:00",101971.0,248823.0,0,RunTask :
Datadog,62032453,62030485,0,"2020/05/27, 01:30:00",False,"2020/05/27, 01:30:00",101971.0,248823.0,0,Starts a new task using the specified task definition.
Datadog,62032453,62030485,0,"2020/05/27, 01:30:00",False,"2020/05/27, 01:30:00",101971.0,248823.0,0,"You can allow Amazon ECS to place tasks for you, or you can customize how Amazon ECS places tasks using placement constraints and placement strategies."
Datadog,62032453,62030485,0,"2020/05/27, 01:30:00",False,"2020/05/27, 01:30:00",101971.0,248823.0,0,"Since this is AWS API calls, there are equivalent calls in CLI and SDK."
Datadog,61873777,61872153,0,"2020/05/18, 18:57:42",False,"2020/05/18, 18:57:42",19520.0,2332336.0,0,"A colleaque of mine informed me that, since we're using docker, we can by-pass supervisor and just run the horizon artisan command directly as the entrypoint of the container."
Datadog,61873777,61872153,0,"2020/05/18, 18:57:42",False,"2020/05/18, 18:57:42",19520.0,2332336.0,0,"So, I removed all things related to supervisor and my service yaml is simplified to the following and the logs are coming into datadog:"
Datadog,61912692,61644174,0,"2020/05/20, 15:05:31",False,"2020/05/20, 15:05:31",23.0,12932875.0,0,"you can open it by navigating to the directory it is in, and then typing"
Datadog,61912692,61644174,0,"2020/05/20, 15:05:31",False,"2020/05/20, 15:05:31",23.0,12932875.0,0,You need root permissions to view the file as far as I know.
Datadog,61590972,61588585,6,"2020/05/04, 14:18:01",True,"2020/05/04, 14:34:48",204.0,11981028.0,0,Replace  key  and  value  to what you want to use.
Datadog,61590972,61588585,6,"2020/05/04, 14:18:01",True,"2020/05/04, 14:34:48",204.0,11981028.0,0,"In my case  key  is ""testKey"" and  value  is ""testValue"""
Datadog,61590972,61588585,6,"2020/05/04, 14:18:01",True,"2020/05/04, 14:34:48",204.0,11981028.0,0,it it my full sample code and xml configuration info.
Datadog,61590972,61588585,6,"2020/05/04, 14:18:01",True,"2020/05/04, 14:34:48",204.0,11981028.0,0,code
Datadog,61590972,61588585,6,"2020/05/04, 14:18:01",True,"2020/05/04, 14:34:48",204.0,11981028.0,0,log4j2.xml
Datadog,61590972,61588585,6,"2020/05/04, 14:18:01",True,"2020/05/04, 14:34:48",204.0,11981028.0,0,output
Datadog,60951103,60933143,2,"2020/03/31, 15:56:50",False,"2020/03/31, 15:56:50",11561.0,970308.0,1,You are configuring the filter on a new  StatsdMeterRegistry .
Datadog,60951103,60933143,2,"2020/03/31, 15:56:50",False,"2020/03/31, 15:56:50",11561.0,970308.0,1,When using a  MeterRegistryCustomizer  you need to operate on the registry that was passed in.
Datadog,60951103,60933143,2,"2020/03/31, 15:56:50",False,"2020/03/31, 15:56:50",11561.0,970308.0,1,"Since the customizer will be used against all registries, you also would need to add an if statement to only filter against the registry you want filtered."
Datadog,62521519,60896119,0,"2020/06/22, 21:52:24",False,"2020/06/22, 21:52:24",374.0,5117002.0,0,It doesn't have Grafana support yet (coming in a week or 2) but Questdb supports traditional SQL on a Time Series database and might be able to do what you want.
Datadog,62521519,60896119,0,"2020/06/22, 21:52:24",False,"2020/06/22, 21:52:24",374.0,5117002.0,0,https://questdb.io  or  https://github.com/questdb  on GitHub.
Datadog,60471660,60465436,0,"2020/03/01, 05:14:06",False,"2020/03/01, 05:14:06",4659.0,3709060.0,0,Datadog can process logs through their pipeline fitering feature
Datadog,60471660,60465436,0,"2020/03/01, 05:14:06",False,"2020/03/01, 05:14:06",4659.0,3709060.0,0,https://docs.datadoghq.com/logs/processing/pipelines/
Datadog,60493178,60465436,0,"2020/03/02, 18:42:31",True,"2020/03/02, 18:42:31",1371.0,5540166.0,1,"If you already have an attribute that contains the url, a really easy way to do this would be to use the  processing pipelines  and add a processor of the "" url parser "" type to these logs."
Datadog,60493178,60465436,0,"2020/03/02, 18:42:31",True,"2020/03/02, 18:42:31",1371.0,5540166.0,1,"You just plug in the attribute that contains the url and an attribute path that you'd like to contain all the outputs from it (usually  http.url_details ), and then all new logs will get the extra url parsing applied."
Datadog,60493178,60465436,0,"2020/03/02, 18:42:31",True,"2020/03/02, 18:42:31",1371.0,5540166.0,1,"If your logs have the ""source:nginx"" applied to them (configured in the log shipper), then you'll already have an out-of-the-box Nginx processing pipeline that Datadog has for structuring standard Nginx syntax logs."
Datadog,60493178,60465436,0,"2020/03/02, 18:42:31",True,"2020/03/02, 18:42:31",1371.0,5540166.0,1,You can clone that and then just add your new url parser there.
Datadog,60493178,60465436,0,"2020/03/02, 18:42:31",True,"2020/03/02, 18:42:31",1371.0,5540166.0,1,"Or, if your syntax is similar to the standard syntax, you can just modify their default suggested parsers (in the cloned pipeline)."
Datadog,60493178,60465436,0,"2020/03/02, 18:42:31",True,"2020/03/02, 18:42:31",1371.0,5540166.0,1,"In any case, it'd be worth looking at that default pipeline for inspiration for other valuable things to do beyond url parsing."
Datadog,59633846,59632924,0,"2020/01/07, 20:08:11",True,"2020/01/07, 20:08:11",13615.0,1061413.0,1,Assuming you'd like the output to look like the following:
Datadog,59633846,59632924,0,"2020/01/07, 20:08:11",True,"2020/01/07, 20:08:11",13615.0,1061413.0,1,"you need to escape the  {  and and use  \""  instead of  \' :"
Datadog,57978792,57978390,9,"2019/09/17, 19:52:43",False,"2019/09/17, 19:52:43",2237.0,11486670.0,0,Unfortuanetly I can not propose an exact solution/workaround to you but you might have a look at the following documentations/API's:
Datadog,57978792,57978390,9,"2019/09/17, 19:52:43",False,"2019/09/17, 19:52:43",2237.0,11486670.0,0,Indices Stats API
Datadog,57978792,57978390,9,"2019/09/17, 19:52:43",False,"2019/09/17, 19:52:43",2237.0,11486670.0,0,Cluster Stats API
Datadog,57978792,57978390,9,"2019/09/17, 19:52:43",False,"2019/09/17, 19:52:43",2237.0,11486670.0,0,Nodes Stats API
Datadog,57978792,57978390,9,"2019/09/17, 19:52:43",False,"2019/09/17, 19:52:43",2237.0,11486670.0,0,The cpu usage is not included in the exported fields but maybe you can derive a high cpu usage behaviour from the other fields.
Datadog,57978792,57978390,9,"2019/09/17, 19:52:43",False,"2019/09/17, 19:52:43",2237.0,11486670.0,0,I hope I could help you in some way.
Datadog,55661352,55621848,0,"2019/04/13, 04:58:31",True,"2019/04/13, 04:58:31",1371.0,5540166.0,1,You can reference  this doc  to find where the default logging path is for Jenkins depending on your OS.
Datadog,55661352,55621848,0,"2019/04/13, 04:58:31",True,"2019/04/13, 04:58:31",1371.0,5540166.0,1,"(For linux, it's  /var/log/jenkins/jenkins.log  if you don't configure it to be something else."
Datadog,55661352,55621848,0,"2019/04/13, 04:58:31",True,"2019/04/13, 04:58:31",1371.0,5540166.0,1,Then as long as your  Datadog agent  is v6+ you can use the Datadog agent to tail your jenkins.log file by following  this doc .
Datadog,55661352,55621848,0,"2019/04/13, 04:58:31",True,"2019/04/13, 04:58:31",1371.0,5540166.0,1,"Specifically, you'd add this line to your  dadatod.yaml :"
Datadog,55661352,55621848,0,"2019/04/13, 04:58:31",True,"2019/04/13, 04:58:31",1371.0,5540166.0,1,"and add this content to any old  conf.yaml  file nested in your  conf.d/  directory, such as  conf.d/jenkins.d/conf.yaml :"
Datadog,55661352,55621848,0,"2019/04/13, 04:58:31",True,"2019/04/13, 04:58:31",1371.0,5540166.0,1,"Then the agent will tail your log file as it's written to, and will forward it to your Datadog account so you can query, graph, and monitor on your log data there."
Datadog,55661352,55621848,0,"2019/04/13, 04:58:31",True,"2019/04/13, 04:58:31",1371.0,5540166.0,1,"Once you have the logs coming in, you may want to write a  processing pipeline  to get the critical attributes parsed out, but that would be material for a new question :) ."
Datadog,54284969,54280115,2,"2019/01/21, 09:09:17",True,"2019/01/21, 09:14:28",125340.0,6464308.0,1,This command will only take a split second.
Datadog,54284969,54280115,2,"2019/01/21, 09:09:17",True,"2019/01/21, 09:14:28",125340.0,6464308.0,1,You must have spent 35 minutes waitung for the  ACCESS EXCLUSIVE  lock on the table to be granted (all the while blocking any transaction unfortunate enough to be queued behind you).
Datadog,54284969,54280115,2,"2019/01/21, 09:09:17",True,"2019/01/21, 09:14:28",125340.0,6464308.0,1,You probably have a problem with long transactions.
Datadog,54284969,54280115,2,"2019/01/21, 09:09:17",True,"2019/01/21, 09:14:28",125340.0,6464308.0,1,"Normally they should be as short as possible, otherwise they hold locks for a long time and also keep  VACUUM  from cleaning up dead row versions."
Datadog,54284969,54280115,2,"2019/01/21, 09:09:17",True,"2019/01/21, 09:14:28",125340.0,6464308.0,1,"The lock is necessary, but is should not pose a problem with a well behaved database workload."
Datadog,52703484,52594973,0,"2018/10/08, 16:32:20",False,"2018/10/08, 16:32:20",66.0,10121786.0,0,You can create one PowerShell script to execute your Batch scripts remotely.
Datadog,52703484,52594973,0,"2018/10/08, 16:32:20",False,"2018/10/08, 16:32:20",66.0,10121786.0,0,And Even you can schedule your PowerShell script using Windows Task Scheduler which will run as per your settings.
Datadog,52065295,52064818,0,"2018/08/28, 22:53:03",False,"2018/08/28, 22:53:03",892.0,1757329.0,0,Rubber Duck.
Datadog,52065295,52064818,0,"2018/08/28, 22:53:03",False,"2018/08/28, 22:53:03",892.0,1757329.0,0,"Turns out because we changed  .set  to  .default , we lost the ability to have the variables properly set during the first run."
Datadog,52065295,52064818,0,"2018/08/28, 22:53:03",False,"2018/08/28, 22:53:03",892.0,1757329.0,0,.normal  will do it for us.
Datadog,50526751,50526252,1,"2018/05/25, 13:10:38",True,"2018/05/25, 13:10:38",629.0,7202531.0,2,"Lots of threads are in WAITING state, and it's absolutely ok for them."
Datadog,50526751,50526252,1,"2018/05/25, 13:10:38",True,"2018/05/25, 13:10:38",629.0,7202531.0,2,"For example, there are thread which have the following stack trace:"
Datadog,50526751,50526252,1,"2018/05/25, 13:10:38",True,"2018/05/25, 13:10:38",629.0,7202531.0,2,This only means threads are waiting for any tasks to do.
Datadog,50526751,50526252,1,"2018/05/25, 13:10:38",True,"2018/05/25, 13:10:38",629.0,7202531.0,2,"However, other stacks do not look good."
Datadog,50526751,50526252,1,"2018/05/25, 13:10:38",True,"2018/05/25, 13:10:38",629.0,7202531.0,2,Those threads are waiting for connection to be free in the pool.
Datadog,50526751,50526252,1,"2018/05/25, 13:10:38",True,"2018/05/25, 13:10:38",629.0,7202531.0,2,C3P0 is a pool of database connections.
Datadog,50526751,50526252,1,"2018/05/25, 13:10:38",True,"2018/05/25, 13:10:38",629.0,7202531.0,2,"Instead of creating a new connection every time, they are cached in the pool."
Datadog,50526751,50526252,1,"2018/05/25, 13:10:38",True,"2018/05/25, 13:10:38",629.0,7202531.0,2,"Upon closing, the connection itself is not closed, but only returned to the pool."
Datadog,50526751,50526252,1,"2018/05/25, 13:10:38",True,"2018/05/25, 13:10:38",629.0,7202531.0,2,"So, if hibernate for some reason (or other user) do not close connection after releasing it, then pool can get exhausted."
Datadog,50526751,50526252,1,"2018/05/25, 13:10:38",True,"2018/05/25, 13:10:38",629.0,7202531.0,2,"In order to resolve an issue, you have to find out why some connections are not closed after using."
Datadog,50526751,50526252,1,"2018/05/25, 13:10:38",True,"2018/05/25, 13:10:38",629.0,7202531.0,2,Try to look at your code to do this.
Datadog,50526751,50526252,1,"2018/05/25, 13:10:38",True,"2018/05/25, 13:10:38",629.0,7202531.0,2,The other option is to temporarily go without C3P0 (pooling).
Datadog,50526751,50526252,1,"2018/05/25, 13:10:38",True,"2018/05/25, 13:10:38",629.0,7202531.0,2,"This is not forever, but at least you can check whether this guess is right."
Datadog,49475152,49468467,1,"2018/03/25, 14:11:43",True,"2018/03/25, 14:11:43",16823.0,1075289.0,1,"In Grails 3, You should put the below code to  grails-app/conf/spring/resources.groovy :"
Datadog,48892885,48892401,0,"2018/02/20, 21:33:25",True,"2018/02/20, 21:33:25",43078.0,78722.0,2,"Neither, you want something like this I think:"
Datadog,48892885,48892401,0,"2018/02/20, 21:33:25",True,"2018/02/20, 21:33:25",43078.0,78722.0,2,Also putting the key into node attributes like that is very unsafe and kind of defeats the point of encrypted bags since node attributes are all written back to the Chef Server and so the key will be sent unencrypted.
Datadog,47779901,47779576,0,"2017/12/12, 21:09:55",False,"2017/12/12, 21:09:55",3545.0,1831118.0,0,"I still don't know why it makes a difference, but adding the  -4  option made it work"
Datadog,47779901,47779576,0,"2017/12/12, 21:09:55",False,"2017/12/12, 21:09:55",3545.0,1831118.0,0,Here's the man page on the option:
Datadog,47779901,47779576,0,"2017/12/12, 21:09:55",False,"2017/12/12, 21:09:55",3545.0,1831118.0,0,-4      Forces nc to use IPv4 addresses only.
Datadog,36027695,36024447,2,"2016/03/16, 07:49:29",False,"2016/03/16, 19:40:27",824.0,2493338.0,0,Problem in this case is not running scripts via JMeter GUI.
Datadog,36027695,36024447,2,"2016/03/16, 07:49:29",False,"2016/03/16, 19:40:27",824.0,2493338.0,0,Instead it is related to network.
Datadog,36027695,36024447,2,"2016/03/16, 07:49:29",False,"2016/03/16, 19:40:27",824.0,2493338.0,0,I had a similar distributed setup in EC2-environment and I successfully executed heavy load tests in GUI mode.
Datadog,36027695,36024447,2,"2016/03/16, 07:49:29",False,"2016/03/16, 19:40:27",824.0,2493338.0,0,"In my case, all my JMeter (master/slaves) were running on EC2 instances (windows environment)."
Datadog,36027695,36024447,2,"2016/03/16, 07:49:29",False,"2016/03/16, 19:40:27",824.0,2493338.0,0,"So, I will recommend you to setup your  JMeter   (Master)  on EC2 and run scripts via GUI mode."
Datadog,36027695,36024447,2,"2016/03/16, 07:49:29",False,"2016/03/16, 19:40:27",824.0,2493338.0,0,If you still want to run in command line mode then you simply need to pass command to create jtl file while the script runs on command line.
Datadog,36027695,36024447,2,"2016/03/16, 07:49:29",False,"2016/03/16, 19:40:27",824.0,2493338.0,0,Later on you can use this JTL to generate any JMeter report as per requirement.
Datadog,36027695,36024447,2,"2016/03/16, 07:49:29",False,"2016/03/16, 19:40:27",824.0,2493338.0,0,For more details check..
Datadog,36027695,36024447,2,"2016/03/16, 07:49:29",False,"2016/03/16, 19:40:27",824.0,2493338.0,0,Jmeter - Run .jmx file through command line and get the summary report in a excel
Datadog,36027695,36024447,2,"2016/03/16, 07:49:29",False,"2016/03/16, 19:40:27",824.0,2493338.0,0,jmeter -n -t /path/to/your/test.jmx  -l /path/to/results/file.jtl
Datadog,36027695,36024447,2,"2016/03/16, 07:49:29",False,"2016/03/16, 19:40:27",824.0,2493338.0,0,Please refer to Dmitri answer in following question to reduce JTL size.
Datadog,36027695,36024447,2,"2016/03/16, 07:49:29",False,"2016/03/16, 19:40:27",824.0,2493338.0,0,How can we control size of JTL file while running test from Non GUI Mode
Datadog,31340312,30227260,0,"2015/07/10, 15:03:45",False,"2015/07/10, 15:03:45",491.0,4923204.0,0,"Before implementing the code, you need to look around in Widows ""registry"" using ""regedit"" and find the exact registry key value for the software."
Datadog,31340312,30227260,0,"2015/07/10, 15:03:45",False,"2015/07/10, 15:03:45",491.0,4923204.0,0,"Below example shows, how to fetch the version number of ""internet explorer""."
Datadog,31340312,30227260,0,"2015/07/10, 15:03:45",False,"2015/07/10, 15:03:45",491.0,4923204.0,0,"Also recommended to have basic knowledge on Ruby array and hash, to understand the code"
Datadog,31340312,30227260,0,"2015/07/10, 15:03:45",False,"2015/07/10, 15:03:45",491.0,4923204.0,0,I've used registry_key_XXXXX Chef methods.
Datadog,31340312,30227260,0,"2015/07/10, 15:03:45",False,"2015/07/10, 15:03:45",491.0,4923204.0,0,Note: Registry key entry may differ for Windows 32bit and 64bit
Datadog,65043190,65042688,8,"2020/11/27, 21:56:03",True,"2020/11/28, 01:23:09",14946.0,190823.0,1,The problem is your  sendData()  function.
Datadog,65043190,65042688,8,"2020/11/27, 21:56:03",True,"2020/11/28, 01:23:09",14946.0,190823.0,1,This function is called in your for loop and has the following line:
Datadog,65043190,65042688,8,"2020/11/27, 21:56:03",True,"2020/11/28, 01:23:09",14946.0,190823.0,1,"This line will create a new DataDog client, which uses a Unix socket."
Datadog,65043190,65042688,8,"2020/11/27, 21:56:03",True,"2020/11/28, 01:23:09",14946.0,190823.0,1,This explains your error message.
Datadog,65043190,65042688,8,"2020/11/27, 21:56:03",True,"2020/11/28, 01:23:09",14946.0,190823.0,1,"With every iteration of your loop, a new socket is &quot;allocated&quot;."
Datadog,65043190,65042688,8,"2020/11/27, 21:56:03",True,"2020/11/28, 01:23:09",14946.0,190823.0,1,"After a sufficient amount of loops no sockets can be opened, resulting in:"
Datadog,65043190,65042688,8,"2020/11/27, 21:56:03",True,"2020/11/28, 01:23:09",14946.0,190823.0,1,socket: too many open files
Datadog,65043190,65042688,8,"2020/11/27, 21:56:03",True,"2020/11/28, 01:23:09",14946.0,190823.0,1,To fix this you should create the client only once and pass it to your method as parameter.
Datadog,49457754,49457370,0,"2018/03/23, 22:21:44",True,"2018/03/24, 05:00:40",2191.0,6084559.0,3,The variable  $LASTEXITCODE  will give you the exit code of the last native command (executable) that was run.
Datadog,44265020,44258173,0,"2017/05/30, 17:28:04",False,"2017/05/30, 17:28:04",11.0,8014459.0,0,"I'm not familiar with the program, but I've had to solve a problem like this before."
Datadog,44265020,44258173,0,"2017/05/30, 17:28:04",False,"2017/05/30, 17:28:04",11.0,8014459.0,0,"I'm making the assumption that you're always going to start the output you want with a line 'Dogstatsd', and always end with several equals signs."
Datadog,44265020,44258173,0,"2017/05/30, 17:28:04",False,"2017/05/30, 17:28:04",11.0,8014459.0,0,"Based on that, you could script out your output like this:"
Datadog,44265020,44258173,0,"2017/05/30, 17:28:04",False,"2017/05/30, 17:28:04",11.0,8014459.0,0,"We get the values defining the length of the file, the length until we hit the first line you want, the length where the output ends, and trim accordingly."
Datadog,42575657,42556276,4,"2017/03/03, 11:53:34",True,"2017/03/03, 11:53:34",541.0,1153563.0,0,I would strongly suggest to setup a pre-production environment and run load tests (with tools like  JMeter ) in conjunction with server-side monitoring.
Datadog,42575657,42556276,4,"2017/03/03, 11:53:34",True,"2017/03/03, 11:53:34",541.0,1153563.0,0,Tomcat backends can be monitored using the JMX protocol.
Datadog,42575657,42556276,4,"2017/03/03, 11:53:34",True,"2017/03/03, 11:53:34",541.0,1153563.0,0,You have 2 solutions :
Datadog,42575657,42556276,4,"2017/03/03, 11:53:34",True,"2017/03/03, 11:53:34",541.0,1153563.0,0,"Like always, free software costs nothing but your time, and paid software gets you straight to the issue in exchanges for some pennies."
Elastic APM,55021893,53428661,0,"2019/03/06, 13:22:41",False,"2019/03/06, 13:22:41",89.0,989723.0,2,As it is reported on the official site it could be released in the future.
Elastic APM,55021893,53428661,0,"2019/03/06, 13:22:41",False,"2019/03/06, 13:22:41",89.0,989723.0,2,"Some of the features we are very excited to provide in the near future
  include distributed tracing and providing framework-specific
  information (e.g., route change times) for some of the frontend
  frameworks such as React, Angular, Vue.js, etc."
Elastic APM,55021893,53428661,0,"2019/03/06, 13:22:41",False,"2019/03/06, 13:22:41",89.0,989723.0,2,https://www.elastic.co/blog/elastic-apm-rum-js-agent-is-generally-available
Elastic APM,55021893,53428661,0,"2019/03/06, 13:22:41",False,"2019/03/06, 13:22:41",89.0,989723.0,2,For now you can rely on Elastic APM RUM JS Agent using JS tags:
Elastic APM,55021893,53428661,0,"2019/03/06, 13:22:41",False,"2019/03/06, 13:22:41",89.0,989723.0,2,https://www.elastic.co/guide/en/apm/agent/js-base/3.x/getting-started.html#using-script-tags
Elastic APM,58086472,54623543,0,"2019/09/24, 21:46:42",False,"2019/09/24, 21:46:42",3095.0,1532769.0,3,You can try to increase:
Elastic APM,58086472,54623543,0,"2019/09/24, 21:46:42",False,"2019/09/24, 21:46:42",3095.0,1532769.0,3,Please take a look on documentation:  Tune APM Server
Elastic APM,60540275,58780229,0,"2020/03/05, 09:37:36",False,"2020/03/05, 09:45:37",1.0,13010745.0,0,You can attach your ElasticApmAttacher.attach() in the Spring Application main class
Elastic APM,60540275,58780229,0,"2020/03/05, 09:37:36",False,"2020/03/05, 09:45:37",1.0,13010745.0,0,"For a SpringBootApplication packaged as a war file, and deployed to Tomcat server, this can be added to the configure method"
Elastic APM,60540275,58780229,0,"2020/03/05, 09:37:36",False,"2020/03/05, 09:45:37",1.0,13010745.0,0,Below code might help:
Elastic APM,61101616,61098374,0,"2020/04/08, 16:15:18",True,"2020/04/08, 16:15:18",36.0,13258898.0,2,unfortunately oracle is not supported by elastic apm agent.
Elastic APM,61101616,61098374,0,"2020/04/08, 16:15:18",True,"2020/04/08, 16:15:18",36.0,13258898.0,2,you should wrap your  oracleQueryRunner  in order to start and end agent spans manually.
Elastic APM,61101616,61098374,0,"2020/04/08, 16:15:18",True,"2020/04/08, 16:15:18",36.0,13258898.0,2,put this code in your  main.ts  file:
Elastic APM,62516533,62511256,2,"2020/06/22, 17:17:56",True,"2020/06/22, 17:17:56",86.0,6020497.0,5,"It's true, there isn't an Elixir agent for Elastic APM - you can upvote  this issue  to get the topic more attention."
Elastic APM,62516533,62511256,2,"2020/06/22, 17:17:56",True,"2020/06/22, 17:17:56",86.0,6020497.0,5,"As you discovered, you can use the OpenTelemetry in the meantime."
Elastic APM,62516533,62511256,2,"2020/06/22, 17:17:56",True,"2020/06/22, 17:17:56",86.0,6020497.0,5,"To do that, run the OpenTelemetry contrib collector (otel collector) and configure it to export to  elastic  - there is a full explanation  in the docs  along with this sample configuration:"
Elastic APM,62516533,62511256,2,"2020/06/22, 17:17:56",True,"2020/06/22, 17:17:56",86.0,6020497.0,5,"In your application, configure the tracer use the  opentelemetry exporter ."
Elastic APM,62516533,62511256,2,"2020/06/22, 17:17:56",True,"2020/06/22, 17:17:56",86.0,6020497.0,5,At that point you'll have a tracer in your application sending traces to the otel collector.
Elastic APM,62516533,62511256,2,"2020/06/22, 17:17:56",True,"2020/06/22, 17:17:56",86.0,6020497.0,5,"From there, traces will be exported to the Elastic Stack via APM Server."
Elastic APM,62516533,62511256,2,"2020/06/22, 17:17:56",True,"2020/06/22, 17:17:56",86.0,6020497.0,5,In summary:  your app -&gt; otel collector -&gt; apm-server -&gt; elasticsearch
Elastic APM,62516533,62511256,2,"2020/06/22, 17:17:56",True,"2020/06/22, 17:17:56",86.0,6020497.0,5,The  Erlang/Elixir Agent Docs  have sample code for starting and decorating spans.
Elastic APM,63573627,63573207,4,"2020/08/25, 10:04:20",True,"2020/08/25, 10:04:20",28262.0,45691.0,1,transaction.duration.us  should indeed be what you're looking for.
Elastic APM,63573627,63573207,4,"2020/08/25, 10:04:20",True,"2020/08/25, 10:04:20",28262.0,45691.0,1,It's the duration in microseconds as an integer.
Elastic APM,63573627,63573207,4,"2020/08/25, 10:04:20",True,"2020/08/25, 10:04:20",28262.0,45691.0,1,"Divide it by 1000 to get milliseconds, or by 1'000'000 to get seconds."
Elastic APM,63573627,63573207,4,"2020/08/25, 10:04:20",True,"2020/08/25, 10:04:20",28262.0,45691.0,1,https://www.elastic.co/guide/en/apm/server/7.9/exported-fields-apm-transaction.html#_duration_2
Elastic APM,50283028,50282965,0,"2018/05/11, 02:51:21",False,"2018/05/11, 02:51:21",7900.0,2259934.0,0,"When you bring up containers using compose, each container has its own networking stack (so they can each talk to themselves on  localhost , but they need an ip address or dns name to talk to a different container!"
Elastic APM,50283028,50282965,0,"2018/05/11, 02:51:21",False,"2018/05/11, 02:51:21",7900.0,2259934.0,0,).
Elastic APM,50283028,50282965,0,"2018/05/11, 02:51:21",False,"2018/05/11, 02:51:21",7900.0,2259934.0,0,Compose by default connects each of the containers to a default network and gives each a dns name with the name of the service.
Elastic APM,50283028,50282965,0,"2018/05/11, 02:51:21",False,"2018/05/11, 02:51:21",7900.0,2259934.0,0,If your compose file looks like
Elastic APM,50283028,50282965,0,"2018/05/11, 02:51:21",False,"2018/05/11, 02:51:21",7900.0,2259934.0,0,A process in the  apm  container could access elasticsearch at  http://elasticsearch:9200
Elastic APM,51649953,50282965,0,"2018/08/02, 12:19:26",False,"2018/08/02, 12:19:26",41.0,8848390.0,4,"If you are starting all the services with single docker compose file, the app-server.yaml should have the value like this
 
output:
  elasticsearch:
    hosts: elasticsearch:9200
 
The ""hosts: elasticsearch:9200"" should be service name of the elasticsearch you mentioned in the docker-compose."
Elastic APM,51649953,50282965,0,"2018/08/02, 12:19:26",False,"2018/08/02, 12:19:26",41.0,8848390.0,4,Like in the followiing
Elastic APM,51649953,50282965,0,"2018/08/02, 12:19:26",False,"2018/08/02, 12:19:26",41.0,8848390.0,4,"
    version: '2'
    services:
      elasticsearch:
          image: elasticsearch:latest"
Elastic APM,56741976,56665160,0,"2019/06/24, 21:20:03",False,"2019/06/24, 21:20:03",528.0,9619535.0,1,Answer to the Errors - I have custom resole.extensions in the  webpack.config.js :
Elastic APM,56741976,56665160,0,"2019/06/24, 21:20:03",False,"2019/06/24, 21:20:03",528.0,9619535.0,1,That was missing the default  .json :
Elastic APM,56741976,56665160,0,"2019/06/24, 21:20:03",False,"2019/06/24, 21:20:03",528.0,9619535.0,1,Now only the warnings are left:
Elastic APM,56741976,56665160,0,"2019/06/24, 21:20:03",False,"2019/06/24, 21:20:03",528.0,9619535.0,1,I addressed them to the developer:  https://github.com/elastic/apm-agent-nodejs/issues/1154
Elastic APM,58717230,56989849,0,"2019/11/05, 20:16:57",False,"2019/11/05, 20:16:57",552.0,1518708.0,4,To listen on  0.0.0.0  try:
Elastic APM,57257482,57211703,2,"2019/07/29, 18:58:36",True,"2020/05/16, 21:59:41",445.0,3834445.0,3,Try using a configuration:
Elastic APM,57257482,57211703,2,"2019/07/29, 18:58:36",True,"2020/05/16, 21:59:41",445.0,3834445.0,3,where the file apm-server/config/apm-server.yml has your config content:
Elastic APM,57257482,57211703,2,"2019/07/29, 18:58:36",True,"2020/05/16, 21:59:41",445.0,3834445.0,3,Note the rum.allow_origins option that you can configure to resolve the CORS issue.
Elastic APM,57257482,57211703,2,"2019/07/29, 18:58:36",True,"2020/05/16, 21:59:41",445.0,3834445.0,3,https://www.elastic.co/guide/en/apm/agent/rum-js/master/configuring-cors.html
Elastic APM,61340495,57696669,1,"2020/04/21, 12:44:16",True,"2020/04/21, 12:44:16",36.0,2155647.0,1,you have to modify your elastic APM python agent code.
Elastic APM,61340495,57696669,1,"2020/04/21, 12:44:16",True,"2020/04/21, 12:44:16",36.0,2155647.0,1,"In general, you can add labels to your span example"
Elastic APM,61340495,57696669,1,"2020/04/21, 12:44:16",True,"2020/04/21, 12:44:16",36.0,2155647.0,1,"also, you can add directly to span object."
Elastic APM,61340495,57696669,1,"2020/04/21, 12:44:16",True,"2020/04/21, 12:44:16",36.0,2155647.0,1,You will get the ip from request object flask
Elastic APM,61340495,57696669,1,"2020/04/21, 12:44:16",True,"2020/04/21, 12:44:16",36.0,2155647.0,1,request.remote_addr   set this to the desired key.
Elastic APM,61340495,57696669,1,"2020/04/21, 12:44:16",True,"2020/04/21, 12:44:16",36.0,2155647.0,1,more details of APIs on elastic APM python agent can be found here -  https://www.elastic.co/guide/en/apm/agent/python/current/api.html
Elastic APM,61340495,57696669,1,"2020/04/21, 12:44:16",True,"2020/04/21, 12:44:16",36.0,2155647.0,1,Thanks
Elastic APM,60827426,60820578,0,"2020/03/24, 10:35:00",True,"2020/03/24, 10:35:00",6576.0,547452.0,2,These metrics come directly from  java.lang.management.GarbageCollectorMXBean .
Elastic APM,60827426,60820578,0,"2020/03/24, 10:35:00",True,"2020/03/24, 10:35:00",6576.0,547452.0,2,"The value of the  jvm.gc.time  metric is taken from  GarbageCollectorMXBean.getCollectionTime , which is indeed accumulating since the process started."
Elastic APM,60827426,60820578,0,"2020/03/24, 10:35:00",True,"2020/03/24, 10:35:00",6576.0,547452.0,2,"Assuming you're looking at metrics from a single JVM, there are a couple of possible reasons for why the value would appear to have gone backwards:"
Elastic APM,60827426,60820578,0,"2020/03/24, 10:35:00",True,"2020/03/24, 10:35:00",6576.0,547452.0,2,"If the process had restarted (which I expect you would know about anyway), the metrics documents in Elasticsearch would have different values for the field  agent.ephemeral_id ."
Elastic APM,60827426,60820578,0,"2020/03/24, 10:35:00",True,"2020/03/24, 10:35:00",6576.0,547452.0,2,"The more likely answer is that you're seeing values for two different memory managers/GC generations, in which case the metrics documents in Elasticsearch would have different values for the field  labels.name ."
Elastic APM,60892441,60890836,2,"2020/03/27, 20:53:59",True,"2020/03/27, 21:21:59",3264.0,1783306.0,2,You don't end  trans1  and  trans2 .
Elastic APM,60892441,60890836,2,"2020/03/27, 20:53:59",True,"2020/03/27, 21:21:59",3264.0,1783306.0,2,"Just put these 2 lines to the point where these end, and everything should show up fine:"
Elastic APM,60892441,60890836,2,"2020/03/27, 20:53:59",True,"2020/03/27, 21:21:59",3264.0,1783306.0,2,"There is the  CaptureTransaction , which is a convenient method that can wrap you any code and makes sure the transaction is ended and all exceptions are captured - so you use that method and it does ""everything"" for you."
Elastic APM,60892441,60890836,2,"2020/03/27, 20:53:59",True,"2020/03/27, 21:21:59",3264.0,1783306.0,2,"Then there is the  StartTransaction  method - this is the one you use in your code -, which starts the transaction and does not do anything else."
Elastic APM,60892441,60890836,2,"2020/03/27, 20:53:59",True,"2020/03/27, 21:21:59",3264.0,1783306.0,2,The advantage here is that you get an  ITransaction  instance which you can use wherever and whenever you want.
Elastic APM,60892441,60890836,2,"2020/03/27, 20:53:59",True,"2020/03/27, 21:21:59",3264.0,1783306.0,2,But in this case you need to call  .End()  on it manually once the transaction (aka the code you want to capture) is executed.
Elastic APM,60892441,60890836,2,"2020/03/27, 20:53:59",True,"2020/03/27, 21:21:59",3264.0,1783306.0,2,Same with  CaptureSpan  and  StartSpan .
Elastic APM,60892441,60890836,2,"2020/03/27, 20:53:59",True,"2020/03/27, 21:21:59",3264.0,1783306.0,2,"So you used  CaptureSpan  for your spans, so those where ended automatically when the lambda with  Task.Delay  finished, on the other hand you started your transactions with  StartTransaction  but only called  .End()  on  trans3  and not on the 2 other transactions."
Elastic APM,60892441,60890836,2,"2020/03/27, 20:53:59",True,"2020/03/27, 21:21:59",3264.0,1783306.0,2,There is some explanation with a demo  here  - sample code of that demo is  here .
Elastic APM,61066068,61065570,6,"2020/04/06, 21:03:11",True,"2020/04/06, 21:03:11",3264.0,1783306.0,1,Currently background services are not captured out of the box.
Elastic APM,61066068,61065570,6,"2020/04/06, 21:03:11",True,"2020/04/06, 21:03:11",3264.0,1783306.0,1,What you can do is to use the  Public Agent API  and with a little bit of an additional code you can capture those also as transactions.
Elastic APM,61066068,61065570,6,"2020/04/06, 21:03:11",True,"2020/04/06, 21:03:11",3264.0,1783306.0,1,Something like this in the background service:
Elastic APM,61196239,61115721,0,"2020/04/13, 23:31:21",False,"2020/04/13, 23:31:21",64.0,12032134.0,1,"I run via docker-compose elasticsearch, apm, kibana and tomcat application in docker."
Elastic APM,61196239,61115721,0,"2020/04/13, 23:31:21",False,"2020/04/13, 23:31:21",64.0,12032134.0,1,In apm- -transaction-  index exist this meta information:  container.id .
Elastic APM,61196239,61115721,0,"2020/04/13, 23:31:21",False,"2020/04/13, 23:31:21",64.0,12032134.0,1,And in apm- -metrics-  index this information is also stored.
Elastic APM,61196239,61115721,0,"2020/04/13, 23:31:21",False,"2020/04/13, 23:31:21",64.0,12032134.0,1,"Try to look at json structure at Discover tab by index pattern ""apm-*"""
Elastic APM,61196239,61115721,0,"2020/04/13, 23:31:21",False,"2020/04/13, 23:31:21",64.0,12032134.0,1,enter image description here
Elastic APM,61509232,61262895,0,"2020/04/29, 21:38:55",False,"2020/04/29, 21:38:55",64.0,12032134.0,0,did you try to give permissions to folder /opt/elastic ?
Elastic APM,64329014,64323562,2,"2020/10/13, 08:23:52",False,"2020/10/13, 08:23:52",156911.0,4668.0,1,"It's hard to say without knowing exactly how you're using NestJS and GraphQL together, and without knowing which parts of Apollo Server that NestJS itself uses."
Elastic APM,64329014,64323562,2,"2020/10/13, 08:23:52",False,"2020/10/13, 08:23:52",156911.0,4668.0,1,Seeing a small sample of what  I am using the graphql feature of nestjs  means would be useful.
Elastic APM,64329014,64323562,2,"2020/10/13, 08:23:52",False,"2020/10/13, 08:23:52",156911.0,4668.0,1,Here's a few datapoints that might help you narrow things down.
Elastic APM,64329014,64323562,2,"2020/10/13, 08:23:52",False,"2020/10/13, 08:23:52",156911.0,4668.0,1,"Also, opening  an issue  in the Agent repository or  a question in their forums  might get more of the right eyes on this."
Elastic APM,64329014,64323562,2,"2020/10/13, 08:23:52",False,"2020/10/13, 08:23:52",156911.0,4668.0,1,"The Elastic APM instrumentation for Apollo Server works by wrapping the  runHttpQuery  function of the  apollo-server-core  module, and  marking the transaction with  trans._graphqlRoute ."
Elastic APM,64329014,64323562,2,"2020/10/13, 08:23:52",False,"2020/10/13, 08:23:52",156911.0,4668.0,1,"When the agent  sees this  _graphqlRoute  property , it runs some code that will set a default name for the transaction"
Elastic APM,64329014,64323562,2,"2020/10/13, 08:23:52",False,"2020/10/13, 08:23:52",156911.0,4668.0,1,"In your application, either the  _graphqlRoute  property isn't getting set, or the renaming code above is doing something weird, or something about NestJS comes along and renames the transaction after it's been named with the above code."
Elastic APM,64329014,64323562,2,"2020/10/13, 08:23:52",False,"2020/10/13, 08:23:52",156911.0,4668.0,1,Knowing more specifically  what  you're doing would help folks narrow in on your problems.
Elastic APM,65496630,65488128,2,"2020/12/29, 20:01:53",True,"2020/12/29, 20:01:53",633.0,13659075.0,1,The metrics shown in Kibana are sent by the APM agent that like you said has limited access to your environment.
Elastic APM,65496630,65488128,2,"2020/12/29, 20:01:53",True,"2020/12/29, 20:01:53",633.0,13659075.0,1,It basically says anything that is collected by the JVM running your JAR.
Elastic APM,65496630,65488128,2,"2020/12/29, 20:01:53",True,"2020/12/29, 20:01:53",633.0,13659075.0,1,If you want to get further visibility into the CPU details of your local environment then you must augment your setup using  Elastic MetricBeats  that ships O.S level details about your machine that sees beyond what the JVM can see.
Elastic APM,65496630,65488128,2,"2020/12/29, 20:01:53",True,"2020/12/29, 20:01:53",633.0,13659075.0,1,"In the presentation below I show how to configure logs, metrics, and APM altogether."
Elastic APM,65496630,65488128,2,"2020/12/29, 20:01:53",True,"2020/12/29, 20:01:53",633.0,13659075.0,1,https://www.youtube.com/watch?v=aXbg9pZCjpk
Elastic APM,54385310,52708201,1,"2019/01/27, 07:21:01",False,"2019/01/27, 07:21:01",1.0,10973722.0,-1,"currently,we run apm-agent for java application, but after 1 day, server cpu has over 100% and java application killed by system"
Elastic APM,55333285,55319800,0,"2019/03/25, 09:50:57",False,"2019/03/25, 09:50:57",3915.0,837717.0,0,"I am not familiar with what is Elastic APM, but if it says it supports Spring Boot, then it means it supports any spring-boot-based framework which Spring Cloud Stream is."
Elastic APM,57318627,57297752,1,"2019/08/02, 03:41:47",False,"2019/08/02, 03:41:47",9394.0,573153.0,0,"The  JVM GC metrics tracked  right now are  jvm.gc.alloc ,  jvm.gc.time , and  jvm.gc.count ."
Elastic APM,57318627,57297752,1,"2019/08/02, 03:41:47",False,"2019/08/02, 03:41:47",9394.0,573153.0,0,"If you are looking for additional ones, which ones would those be?"
Elastic APM,57318627,57297752,1,"2019/08/02, 03:41:47",False,"2019/08/02, 03:41:47",9394.0,573153.0,0,And could you  open an issue with the details .
Elastic APM,61072797,57297752,1,"2020/04/07, 07:20:17",False,"2020/04/07, 07:20:17",23.0,6332568.0,0,Please import from saved objects option -  https://github.com/elastic/apm-contrib/blob/master/apm-agent-java/dashboards/java_metrics_dashboard_7.x.json
Elastic APM,57298815,57298727,3,"2019/08/01, 00:04:16",False,"2019/08/01, 00:04:16",151.0,639333.0,0,To include spans into the transactions you should start the spans from the transaction object
Elastic APM,57298815,57298727,3,"2019/08/01, 00:04:16",False,"2019/08/01, 00:04:16",151.0,639333.0,0,"
 
 ...
var span = transaction.startSpan('My custom span')
..."
Elastic APM,57298815,57298727,3,"2019/08/01, 00:04:16",False,"2019/08/01, 00:04:16",151.0,639333.0,0,And ending the parent transaction object all the nested spans will be also ended in cascade
Elastic APM,57298815,57298727,3,"2019/08/01, 00:04:16",False,"2019/08/01, 00:04:16",151.0,639333.0,0,https://www.elastic.co/guide/en/apm/agent/js-base/4.x/transaction-api.html#transaction-start-span
Elastic APM,59268282,57782547,8,"2019/12/10, 15:25:10",False,"2019/12/10, 15:25:10",228.0,4349519.0,1,You can start your application with argument  active=false .
Elastic APM,59268282,57782547,8,"2019/12/10, 15:25:10",False,"2019/12/10, 15:25:10",228.0,4349519.0,1,C:\Users\dsm\Documents&gt;java -jar apm-agent-attach-1.9.0-standalone.jar --pid 16832 --args 'service_name=test;server_urls=http://localhost:8200;active=false'
Elastic APM,58075056,58074198,0,"2019/09/24, 10:22:09",True,"2019/09/24, 10:22:09",163369.0,4604579.0,5,You simply need to change your query to this:
Elastic APM,63339911,58074198,0,"2020/08/10, 15:09:15",False,"2020/08/10, 15:09:15",383.0,10119759.0,3,"The accepted answer no longer works, you can use the following"
Elastic APM,58087113,58076441,0,"2019/09/24, 22:36:31",True,"2019/09/24, 22:36:31",9394.0,573153.0,1,The key features of APM agents is normally in their framework integrations.
Elastic APM,58087113,58076441,0,"2019/09/24, 22:36:31",True,"2019/09/24, 22:36:31",9394.0,573153.0,1,The Java APM agent is mostly focussed on web frameworks — see the list of  supported technologies .
Elastic APM,58087113,58076441,0,"2019/09/24, 22:36:31",True,"2019/09/24, 22:36:31",9394.0,573153.0,1,"But you already mentioned the  public API  — if you manually instrument your code with that, you will still be able to use it."
Elastic APM,58087113,58076441,0,"2019/09/24, 22:36:31",True,"2019/09/24, 22:36:31",9394.0,573153.0,1,It just doesn't automatically understand the framework and you need to help it with that.
Elastic APM,58087113,58076441,0,"2019/09/24, 22:36:31",True,"2019/09/24, 22:36:31",9394.0,573153.0,1,"Alternatively, if your tool supports OpenTracing then you could use the  OpenTracing bridge  for that."
Elastic APM,58750060,58728047,0,"2019/11/07, 15:43:30",True,"2019/11/07, 15:51:40",26.0,9528014.0,1,When routing to a different subpage you have to set the Route Name manually.
Elastic APM,58750060,58728047,0,"2019/11/07, 15:43:30",True,"2019/11/07, 15:51:40",26.0,9528014.0,1,You can achieve this via a filter on the 'change-route' type.
Elastic APM,58750060,58728047,0,"2019/11/07, 15:43:30",True,"2019/11/07, 15:51:40",26.0,9528014.0,1,"See  apm.addFilter()  
docs:  https://www.elastic.co/guide/en/apm/agent/rum-js/current/agent-api.html#apm-add-filter"
Elastic APM,58750060,58728047,0,"2019/11/07, 15:43:30",True,"2019/11/07, 15:51:40",26.0,9528014.0,1,Something like this should work:
Elastic APM,60703633,58728047,0,"2020/03/16, 11:47:36",False,"2020/03/16, 11:47:36",55.0,3588136.0,1,Another way to do it would be to observe for transaction events which is fired whenever transaction starts and ends.
Elastic APM,60703633,58728047,0,"2020/03/16, 11:47:36",False,"2020/03/16, 11:47:36",55.0,3588136.0,1,API docs -  https://www.elastic.co/guide/en/apm/agent/rum-js/current/agent-api.html#observe
Elastic APM,60703633,58728047,0,"2020/03/16, 11:47:36",False,"2020/03/16, 11:47:36",55.0,3588136.0,1,"The agent also supports frameworks such as Vue.js, React and Angular out of the box, so the above code should not be necessary."
Elastic APM,60703633,58728047,0,"2020/03/16, 11:47:36",False,"2020/03/16, 11:47:36",55.0,3588136.0,1,Vue docs -  https://www.elastic.co/guide/en/apm/agent/rum-js/current/vue-integration.html
Elastic APM,58742793,58738337,0,"2019/11/07, 08:00:56",False,"2019/11/07, 08:00:56",9394.0,573153.0,0,"Logging is separate from APM / tracing, but can be integrated."
Elastic APM,58742793,58738337,0,"2019/11/07, 08:00:56",False,"2019/11/07, 08:00:56",9394.0,573153.0,0,"https://github.com/elastic/ecs-logging-java  is a curated logging library that will also correlate the trace IDs, so you can tie both together."
Elastic APM,58742793,58738337,0,"2019/11/07, 08:00:56",False,"2019/11/07, 08:00:56",9394.0,573153.0,0,Keep using SLF4J and just add the right logging backend.
Elastic APM,58742793,58738337,0,"2019/11/07, 08:00:56",False,"2019/11/07, 08:00:56",9394.0,573153.0,0,The output can then be picked up by Filebeat (as described in the repository) and you're ready to go.
Elastic APM,61196812,58738337,0,"2020/04/14, 00:09:21",False,"2020/04/14, 00:09:21",64.0,12032134.0,1,"Elastic-apm-agent-java automatically capture exceptions when you use slf4j implementation  Logger#error(""message"", Throwable) ."
Elastic APM,61196812,58738337,0,"2020/04/14, 00:09:21",False,"2020/04/14, 00:09:21",64.0,12032134.0,1,More information you can find  here
Elastic APM,61801017,59328108,0,"2020/05/14, 18:18:04",False,"2020/05/14, 18:18:04",396.0,4191904.0,0,Try  this  official docker-compose set up:
Elastic APM,60702327,59352981,0,"2020/03/16, 10:08:57",True,"2020/03/16, 10:08:57",1083.0,3074424.0,0,"The Elastic RUM agent has support for  click user interactions , therefore you shouldn't need to manually start these type of transactions."
Elastic APM,60702327,59352981,0,"2020/03/16, 10:08:57",True,"2020/03/16, 10:08:57",1083.0,3074424.0,0,Regarding the failure in your code the correct API call is  getCurrentTransaction  and not  currentTransaction .
Elastic APM,60702327,59352981,0,"2020/03/16, 10:08:57",True,"2020/03/16, 10:08:57",1083.0,3074424.0,0,Hope this helps.
Elastic APM,60983706,60920952,0,"2020/04/02, 06:15:04",False,"2020/04/02, 06:15:04",6576.0,547452.0,0,"So, I'm guessing that the handler wrappers are dropping the buffalo.Context information."
Elastic APM,60983706,60920952,0,"2020/04/02, 06:15:04",False,"2020/04/02, 06:15:04",6576.0,547452.0,0,That's correct.
Elastic APM,60983706,60920952,0,"2020/04/02, 06:15:04",False,"2020/04/02, 06:15:04",6576.0,547452.0,0,The problem is that  buffalo.WrapHandler  ( Source ) throws away all of the context other than the underlying  http.Request / http.Response :
Elastic APM,60983706,60920952,0,"2020/04/02, 06:15:04",False,"2020/04/02, 06:15:04",6576.0,547452.0,0,"So, what would I need to do to be able to integrate Sentry and Elastic in Buffalo asides from trying to reimplement their wrappers?"
Elastic APM,60983706,60920952,0,"2020/04/02, 06:15:04",False,"2020/04/02, 06:15:04",6576.0,547452.0,0,I can see two options:
Elastic APM,60983706,60920952,0,"2020/04/02, 06:15:04",False,"2020/04/02, 06:15:04",6576.0,547452.0,0,There's an open issue in the Elastic APM agent for the latter option:  elastic/apm#39 .
Elastic APM,61067229,61066877,2,"2020/04/06, 22:11:02",True,"2020/04/06, 22:11:02",3264.0,1783306.0,1,There are basically 2 ways the agent captures things:
Elastic APM,61067229,61066877,2,"2020/04/06, 22:11:02",True,"2020/04/06, 22:11:02",3264.0,1783306.0,1,"In a typical ASP.NET Classic MVC application the agent has auto instrumentation for outgoing HTTP calls with  HttpClient , Database calls with EF6 (Make sure to  add the interceptor ) ( SqlClient  support is already work-in-progress, hopefully released soon)."
Elastic APM,61067229,61066877,2,"2020/04/06, 22:11:02",True,"2020/04/06, 22:11:02",3264.0,1783306.0,1,"So unless you have any of these within those requests, the agent won't capture things out of the box."
Elastic APM,61067229,61066877,2,"2020/04/06, 22:11:02",True,"2020/04/06, 22:11:02",3264.0,1783306.0,1,"If you want to capture more things, currently the way to go is to place some agent specific code - so basically manual code instrumentation - into your application and use the public agent API."
Elastic APM,61171704,61108716,0,"2020/04/12, 15:46:20",True,"2020/04/12, 15:46:20",33.0,9358877.0,0,Finally noticed issue.
Elastic APM,61171704,61108716,0,"2020/04/12, 15:46:20",True,"2020/04/12, 15:46:20",33.0,9358877.0,0,application API is returning service name as Bootstrap because.
Elastic APM,61171704,61108716,0,"2020/04/12, 15:46:20",True,"2020/04/12, 15:46:20",33.0,9358877.0,0,Id is not set so it is using default value
Elastic APM,61171704,61108716,0,"2020/04/12, 15:46:20",True,"2020/04/12, 15:46:20",33.0,9358877.0,0,The Id can be set like this
Elastic APM,61195913,61163723,0,"2020/04/13, 23:09:07",True,"2020/04/13, 23:09:07",64.0,12032134.0,1,"You can try the method described here  disscuss-elastic , via ElasticApmAttacher#attach(map of properties)."
Elastic APM,61524739,61524132,3,"2020/04/30, 16:40:12",True,"2020/04/30, 16:40:12",3264.0,1783306.0,1,"Is this not yet included in the .NET agent, or is there additional configuration necessary to get this working?"
Elastic APM,61524739,61524132,3,"2020/04/30, 16:40:12",True,"2020/04/30, 16:40:12",3264.0,1783306.0,1,This is not yet included in the .NET Agent unfortunately.
Elastic APM,61652111,61642883,0,"2020/05/07, 10:28:33",False,"2020/05/07, 10:37:54",3606.0,2042827.0,0,WebFlux can run on Servlet containers with support for the Servlet 3.1 Non-Blocking IO API as well as on other async runtimes such as  Netty  and Undertow.
Elastic APM,61652111,61642883,0,"2020/05/07, 10:28:33",False,"2020/05/07, 10:37:54",3606.0,2042827.0,0,Each Spring Boot web application includes an embedded web server.
Elastic APM,61652111,61642883,0,"2020/05/07, 10:28:33",False,"2020/05/07, 10:37:54",3606.0,2042827.0,0,"For reactive stack applications, the  spring-boot-starter-webflux  includes Reactor Netty by default I guess."
Elastic APM,61652111,61642883,0,"2020/05/07, 10:28:33",False,"2020/05/07, 10:37:54",3606.0,2042827.0,0,"And it does not include  Servlet API  (Netty is non-Servlet runtime), but it looks like your  Elastic APM  expects this API to be present."
Elastic APM,61652111,61642883,0,"2020/05/07, 10:28:33",False,"2020/05/07, 10:37:54",3606.0,2042827.0,0,Try to use  spring-boot-starter-tomcat  instead of Netty.
Elastic APM,61652111,61642883,0,"2020/05/07, 10:28:33",False,"2020/05/07, 10:37:54",3606.0,2042827.0,0,"When  switching to a different HTTP server , you need to exclude the default dependencies in addition to including the one you need."
Elastic APM,61652111,61642883,0,"2020/05/07, 10:28:33",False,"2020/05/07, 10:37:54",3606.0,2042827.0,0,Here is an example:
Elastic APM,61652111,61642883,0,"2020/05/07, 10:28:33",False,"2020/05/07, 10:37:54",3606.0,2042827.0,0,Tomcat dependency brings Servlet API.
Elastic APM,61652111,61642883,0,"2020/05/07, 10:28:33",False,"2020/05/07, 10:37:54",3606.0,2042827.0,0,Perhaps it will resolve your issue.
Elastic APM,66580927,61642883,0,"2021/03/11, 12:42:08",False,"2021/03/11, 12:42:08",47.0,2800145.0,0,Looks like there is no support from elastic for WebFlux yet
Elastic APM,66580927,61642883,0,"2021/03/11, 12:42:08",False,"2021/03/11, 12:42:08",47.0,2800145.0,0,Check here  https://github.com/elastic/apm-agent-java/issues/60
Elastic APM,66580927,61642883,0,"2021/03/11, 12:42:08",False,"2021/03/11, 12:42:08",47.0,2800145.0,0,"They are currently working on it, but there is not a date to be ready yet"
Elastic APM,62680831,62677891,0,"2020/07/01, 19:02:17",True,"2020/07/01, 19:02:17",45.0,13409135.0,0,I got the answer after posting the same on  Elastic Support Forum .
Elastic APM,62680831,62677891,0,"2020/07/01, 19:02:17",True,"2020/07/01, 19:02:17",45.0,13409135.0,0,It was a very prompt response.
Elastic APM,62680831,62677891,0,"2020/07/01, 19:02:17",True,"2020/07/01, 19:02:17",45.0,13409135.0,0,"This was not a problem from Elastic APM side, and was more of a silly problem from my side."
Elastic APM,62680831,62677891,0,"2020/07/01, 19:02:17",True,"2020/07/01, 19:02:17",45.0,13409135.0,0,Refer the  discussion  to find the problem and solution.
Elastic APM,62903745,62815678,1,"2020/07/14, 23:46:57",True,"2020/07/14, 23:46:57",1274.0,1370767.0,1,"This question was cross-posted to discuss.elastic.co, and you can see the answer that was provided there:  https://discuss.elastic.co/t/elastic-apm-python-system-metrics-dont-show-process-related-metrics-like-memory-on-kibana/240531/2?u=basepi"
Elastic APM,63118018,63116459,4,"2020/07/27, 17:58:27",False,"2020/07/27, 17:58:27",163369.0,4604579.0,1,"In your ES Cloud console, you need to Edit the cluster configuration, scroll to the APM section and then click &quot;User override settings&quot;."
Elastic APM,63118018,63116459,4,"2020/07/27, 17:58:27",False,"2020/07/27, 17:58:27",163369.0,4604579.0,1,In there you can override the target index by adding the following property:
Elastic APM,63118018,63116459,4,"2020/07/27, 17:58:27",False,"2020/07/27, 17:58:27",163369.0,4604579.0,1,"Note that if you change this setting, you also need to modify the corresponding index template to match the new index name."
Elastic APM,63291099,63129247,0,"2020/08/06, 23:12:47",False,"2020/08/06, 23:12:47",64.0,12032134.0,0,currently elastic-apm-agent support natively Quartz framework(since 1.8).
Elastic APM,63291099,63129247,0,"2020/08/06, 23:12:47",False,"2020/08/06, 23:12:47",64.0,12032134.0,0,"If you use it, instrumentation should work."
Elastic APM,63291099,63129247,0,"2020/08/06, 23:12:47",False,"2020/08/06, 23:12:47",64.0,12032134.0,0,But you should add your packages to  application_packages .
Elastic APM,63291099,63129247,0,"2020/08/06, 23:12:47",False,"2020/08/06, 23:12:47",64.0,12032134.0,0,It would be good if you can share mini-demo project.
Elastic APM,63291099,63129247,0,"2020/08/06, 23:12:47",False,"2020/08/06, 23:12:47",64.0,12032134.0,0,And I can reproduce your problem locally.
Elastic APM,63291099,63129247,0,"2020/08/06, 23:12:47",False,"2020/08/06, 23:12:47",64.0,12032134.0,0,Information from  supported-technologies-details
Elastic APM,63351255,63339667,1,"2020/08/11, 06:53:42",True,"2020/08/11, 06:53:42",6576.0,547452.0,0,"All of the Elastic APM agents, with the exception of the RUM JavaScript agent, have a  verify_server_cert  configuration variable."
Elastic APM,63351255,63339667,1,"2020/08/11, 06:53:42",True,"2020/08/11, 06:53:42",6576.0,547452.0,0,You can set this to  false  to disable server TLS certificate verification.
Elastic APM,63353158,63339667,1,"2020/08/11, 10:06:22",False,"2020/08/11, 10:06:22",32819.0,433570.0,0,I am using docker-compose
Elastic APM,63353158,63339667,1,"2020/08/11, 10:06:22",False,"2020/08/11, 10:06:22",32819.0,433570.0,0,and the following option doesn't work.
Elastic APM,63353158,63339667,1,"2020/08/11, 10:06:22",False,"2020/08/11, 10:06:22",32819.0,433570.0,0,I think its a bug for apm..
Elastic APM,63353158,63339667,1,"2020/08/11, 10:06:22",False,"2020/08/11, 10:06:22",32819.0,433570.0,0,When I use same option in apm-server.yml it works fine.
Elastic APM,63934347,63931550,0,"2020/09/17, 11:40:33",True,"2020/09/17, 11:40:33",3264.0,1783306.0,1,One thing you can use is the  Filter API  for this.
Elastic APM,63934347,63931550,0,"2020/09/17, 11:40:33",True,"2020/09/17, 11:40:33",3264.0,1783306.0,1,With that you have access to all transactions and spans before they are sent to the APM Server.
Elastic APM,63934347,63931550,0,"2020/09/17, 11:40:33",True,"2020/09/17, 11:40:33",3264.0,1783306.0,1,"You can't run through all the spans on a given transaction, so you need some tweaking - for this I use a  Dictionary  in my sample."
Elastic APM,63934347,63931550,0,"2020/09/17, 11:40:33",True,"2020/09/17, 11:40:33",3264.0,1783306.0,1,Couple of thing here:
Elastic APM,65484213,65441133,0,"2020/12/28, 23:19:29",True,"2020/12/28, 23:19:29",109.0,1175351.0,0,Looks like it can be done using drop_event processor in api-server.yml.
Elastic APM,65484213,65441133,0,"2020/12/28, 23:19:29",True,"2020/12/28, 23:19:29",109.0,1175351.0,0,and in code set custom context:
Elastic APM,66062278,66026533,0,"2021/02/05, 13:15:44",True,"2021/02/05, 13:15:44",59.0,8278891.0,0,Commenting out  'SERVER_URL': '127.0.0.1:8200'  solved the problem.
Elastic APM,66089252,66088061,1,"2021/02/07, 16:52:15",True,"2021/02/07, 16:52:15",3264.0,1783306.0,1,I want to know if there is a way to create a transaction (or span) using a traceparent that is being sent from another system not using a HTTP protocol.
Elastic APM,66089252,66088061,1,"2021/02/07, 16:52:15",True,"2021/02/07, 16:52:15",3264.0,1783306.0,1,"Yes, this is possible and there is an API for it."
Elastic APM,66089252,66088061,1,"2021/02/07, 16:52:15",True,"2021/02/07, 16:52:15",3264.0,1783306.0,1,This part of the documentation  explains it.
Elastic APM,66089252,66088061,1,"2021/02/07, 16:52:15",True,"2021/02/07, 16:52:15",3264.0,1783306.0,1,So you'll need to do this when you start your transaction - I imagine in your scenario this will be when you read a message from RabbitMQ.
Elastic APM,66089252,66088061,1,"2021/02/07, 16:52:15",True,"2021/02/07, 16:52:15",3264.0,1783306.0,1,"When you  start the transaction  there is an optional parameter called  distributedTracingData  - if you pass it, then the transaction will reuse the traceid which you passed through RabbitMQ, and this way the new transaction will be part of the whole trace."
Elastic APM,66089252,66088061,1,"2021/02/07, 16:52:15",True,"2021/02/07, 16:52:15",3264.0,1783306.0,1,"If you don't pass this parameter, a new traceid will be generated and a new trace will be started."
Elastic APM,66089252,66088061,1,"2021/02/07, 16:52:15",True,"2021/02/07, 16:52:15",3264.0,1783306.0,1,Another comment that may help: you pass the trace id into the method where you start the transaction and each span will inherit this trace id within a transaction - so you control this on the transaction level and accordingly you don't pass it into a span.
Elastic APM,66089252,66088061,1,"2021/02/07, 16:52:15",True,"2021/02/07, 16:52:15",3264.0,1783306.0,1,Here is a small code snippet on how this would look:
Elastic APM,66094811,66088061,0,"2021/02/08, 03:19:53",False,"2021/02/08, 03:19:53",417.0,2803435.0,1,"@gregkalapos, again thank you for the information."
Elastic APM,66094811,66088061,0,"2021/02/08, 03:19:53",False,"2021/02/08, 03:19:53",417.0,2803435.0,1,I checked how to acquire the neccessary trace information as in  node.js agent documentation  and when I debugged noticed that it was the trace id.
Elastic APM,66094811,66088061,0,"2021/02/08, 03:19:53",False,"2021/02/08, 03:19:53",417.0,2803435.0,1,Next in the C# consumer end I placed a code snippet as mentioned in the  .Net agent  and gave it a run.
Elastic APM,66094811,66088061,0,"2021/02/08, 03:19:53",False,"2021/02/08, 03:19:53",417.0,2803435.0,1,Kibana displayed the transactions from two different services in a single trace as I hoped it would.
Elastic APM,66221229,66219545,0,"2021/02/16, 10:51:24",False,"2021/02/16, 10:51:24",417.0,2803435.0,0,Initially asked this questions because Visual Studio did not show the source as expected in the editor.
Elastic APM,66221229,66219545,0,"2021/02/16, 10:51:24",False,"2021/02/16, 10:51:24",417.0,2803435.0,0,So what I did was clicked Build after right clicking on the .sln (solution) then noticed that necessary version of .Net SDK was not there and version of MSBuild related to it.
Elastic APM,66221229,66219545,0,"2021/02/16, 10:51:24",False,"2021/02/16, 10:51:24",417.0,2803435.0,0,Once I updated Visual Studio the sources were visible.
Elastic APM,66221229,66219545,0,"2021/02/16, 10:51:24",False,"2021/02/16, 10:51:24",417.0,2803435.0,0,Hope this would help if someone faced a similar situation.
Elastic APM,66324541,66316731,1,"2021/02/23, 00:45:53",False,"2021/02/23, 00:45:53",633.0,13659075.0,1,Using labels should be the best way to add custom details to the transaction/span but you can also use the  addCustomContext()  method:
Elastic APM,66324541,66316731,1,"2021/02/23, 00:45:53",False,"2021/02/23, 00:45:53",633.0,13659075.0,1,https://www.elastic.co/guide/en/apm/agent/java/current/public-api.html#api-transaction-add-custom-context
Elastic APM,66782709,66781290,0,"2021/03/24, 16:01:20",False,"2021/03/24, 16:01:20",11.0,13628343.0,1,That error indicates the agent can't connect to apm-server.
Elastic APM,66782709,66781290,0,"2021/03/24, 16:01:20",False,"2021/03/24, 16:01:20",11.0,13628343.0,1,SERVER_URL  should be  ELASTIC_APM_SERVER_URL  in the apm-agent-container env.
Elastic APM,66798562,66781290,1,"2021/03/25, 13:27:56",False,"2021/03/25, 13:27:56",1.0,15469419.0,0,"Thanks for the reply, I'm able to connect apm-server with the agent, but in kibana dashboard, I'm getting &quot; No data has been received from agents yet&quot; ."
Elastic APM,66798562,66781290,1,"2021/03/25, 13:27:56",False,"2021/03/25, 13:27:56",1.0,15469419.0,0,My application is running fine
Elastic APM,66869872,66868273,2,"2021/03/30, 14:09:32",True,"2021/03/30, 15:56:11",163369.0,4604579.0,1,Good job so far.
Elastic APM,66869872,66868273,2,"2021/03/30, 14:09:32",True,"2021/03/30, 15:56:11",163369.0,4604579.0,1,"Your pipeline is almost good, however, the grok pattern needs some fixing and you have some orphan curly braces."
Elastic APM,66869872,66868273,2,"2021/03/30, 14:09:32",True,"2021/03/30, 15:56:11",163369.0,4604579.0,1,Here is a working example:
Elastic APM,66869872,66868273,2,"2021/03/30, 14:09:32",True,"2021/03/30, 15:56:11",163369.0,4604579.0,1,Response:
Elastic APM,66869872,66868273,2,"2021/03/30, 14:09:32",True,"2021/03/30, 15:56:11",163369.0,4604579.0,1,Just note that the exact date is missing so the @timestamp field resolve to January 1st this year.
Elastic APM,67065273,67057172,1,"2021/04/12, 23:27:40",True,"2021/04/13, 00:52:22",633.0,13659075.0,0,Try to specify the  config_file  using the following notation:
Elastic APM,67065273,67057172,1,"2021/04/12, 23:27:40",True,"2021/04/13, 00:52:22",633.0,13659075.0,0,-Delastic.apm.config_file=elasticapm.properties
Elastic APM,67065273,67057172,1,"2021/04/12, 23:27:40",True,"2021/04/13, 00:52:22",633.0,13659075.0,0,The attacher can create the log file depending on the settings configured during startup.
Elastic APM,67065273,67057172,1,"2021/04/12, 23:27:40",True,"2021/04/13, 00:52:22",633.0,13659075.0,0,See the [1] current code for a better understanding.
Elastic APM,67065273,67057172,1,"2021/04/12, 23:27:40",True,"2021/04/13, 00:52:22",633.0,13659075.0,0,[1]  https://github.com/elastic/apm-agent-java/blob/0465d479430172c3e745afd2ef5b62a3da6b60aa/apm-agent-attach-cli/src/main/java/co/elastic/apm/attach/AgentAttacher.java#L79
Elastic APM,61196475,60947830,0,"2020/04/13, 23:48:14",False,"2020/04/13, 23:48:14",64.0,12032134.0,0,"Do you mean that you need new ""Transaction type""?"
Elastic APM,61196475,60947830,0,"2020/04/13, 23:48:14",False,"2020/04/13, 23:48:14",64.0,12032134.0,0,"If yes, so you should set  type  annotation parameter."
Elastic APM,61196475,60947830,0,"2020/04/13, 23:48:14",False,"2020/04/13, 23:48:14",64.0,12032134.0,0,But @CaptureTranscation annotation work  in case :
Elastic APM,64793314,55457083,0,"2020/11/11, 21:58:43",True,"2020/11/11, 21:58:43",2674.0,3019008.0,0,"I worked with the Elastic APM team, who had just rolled out this package:  https://www.npmjs.com/package/elastic-apm-node"
Elastic APM,64793314,55457083,0,"2020/11/11, 21:58:43",True,"2020/11/11, 21:58:43",2674.0,3019008.0,0,"The directions are pretty self-explanatory, works like a charm."
Elastic APM,52714271,52696696,1,"2018/10/09, 09:00:41",False,"2018/10/09, 09:00:41",48155.0,2989261.0,0,Hard to tell without debugging but since some connections are getting dropped when you add more load + concurrency it's likely that you need more replicas on your  Kubernetes deployments  and possibly adjusts the  Resources  on your container pod specs.
Elastic APM,52714271,52696696,1,"2018/10/09, 09:00:41",False,"2018/10/09, 09:00:41",48155.0,2989261.0,0,If this turns out to be the case you can also configure an  HPA  (Horizontal Pod Autoscaler) to handle your load.
Elastic APM,56835665,56832275,0,"2019/07/01, 15:24:24",False,"2019/07/01, 15:24:24",9394.0,573153.0,0,Are you not building a custom Dockerfile and you could just add it there (using wget or curl probably)?
Elastic APM,56835665,56832275,0,"2019/07/01, 15:24:24",False,"2019/07/01, 15:24:24",9394.0,573153.0,0,"If you really want a build dependency,  https://search.maven.org/artifact/co.elastic.apm/elastic-apm-agent/1.7.0/jar  should be what you want."
Elastic APM,56835665,56832275,0,"2019/07/01, 15:24:24",False,"2019/07/01, 15:24:24",9394.0,573153.0,0,"PS: IMO it's a feature that this is only a runtime dependency and you can just add, remove, change it independently of your application; unless you want to do some custom instrumentation."
Elastic APM,65706105,65703890,6,"2021/01/13, 18:38:19",False,"2021/01/13, 18:38:19",633.0,13659075.0,0,If the objective is to attach labels to a transaction over multiple spans then using the  public APIs from the Elastic APM for Java  is a better choice instead of instrumenting the JVM with ByteBuddy.
Elastic APM,65706105,65703890,6,"2021/01/13, 18:38:19",False,"2021/01/13, 18:38:19",633.0,13659075.0,0,You will have much more freedom to do what you want to do without relying on a hacking.
Elastic APM,65706105,65703890,6,"2021/01/13, 18:38:19",False,"2021/01/13, 18:38:19",633.0,13659075.0,0,"FYI, the Elastic APM agent for Java already instrument the JVM with additional bytecode so what you are doing may get even more confusing because of this."
Elastic APM,65706105,65703890,6,"2021/01/13, 18:38:19",False,"2021/01/13, 18:38:19",633.0,13659075.0,0,"Alternatively, you can also use the  OpenTracing Bridge  to set labels in a transaction."
Elastic APM,65712813,65703890,0,"2021/01/14, 05:02:19",False,"2021/01/14, 05:02:19",49309.0,1082681.0,0,Disclaimer: This answer is a stub for now.
Elastic APM,65712813,65703890,0,"2021/01/14, 05:02:19",False,"2021/01/14, 05:02:19",49309.0,1082681.0,0,You should first try to explore a more canonical way of doing things like Ricardo suggested.
Elastic APM,65712813,65703890,0,"2021/01/14, 05:02:19",False,"2021/01/14, 05:02:19",49309.0,1082681.0,0,"If for some reason that does not work, then we could explore ways to instrument your agent class - not so much because I think it is a good idea but because it is technically interesting."
Elastic APM,65712813,65703890,0,"2021/01/14, 05:02:19",False,"2021/01/14, 05:02:19",49309.0,1082681.0,0,"Basically, we would have to find out if maybe the class you want to instrument was already loaded before your ByteBuddy agent gets active."
Elastic APM,65712813,65703890,0,"2021/01/14, 05:02:19",False,"2021/01/14, 05:02:19",49309.0,1082681.0,0,Then you would have to use class retransformation rather than redefinition.
Elastic APM,65712813,65703890,0,"2021/01/14, 05:02:19",False,"2021/01/14, 05:02:19",49309.0,1082681.0,0,You would have to make sure the advice you apply can do its job without the need to change the class structure with regard to method signatures and fields.
Elastic APM,65712813,65703890,0,"2021/01/14, 05:02:19",False,"2021/01/14, 05:02:19",49309.0,1082681.0,0,"You would also need to make sure that the advice and ByteBuddy are visible to the other agent's classloader, e.g."
Elastic APM,65712813,65703890,0,"2021/01/14, 05:02:19",False,"2021/01/14, 05:02:19",49309.0,1082681.0,0,by putting both on the boot class path.
Elastic APM,65712813,65703890,0,"2021/01/14, 05:02:19",False,"2021/01/14, 05:02:19",49309.0,1082681.0,0,But let's not get ahead of ourselves.
Elastic APM,65712813,65703890,0,"2021/01/14, 05:02:19",False,"2021/01/14, 05:02:19",49309.0,1082681.0,0,"Explore Ricardo's ideas first, please."
Elastic APM,65498418,65484826,1,"2020/12/29, 22:34:40",True,"2020/12/29, 22:34:40",156911.0,4668.0,1,Based on what I've seen it looks like there isn't a &quot;right&quot; way to do this with the stock  nuxt  command line application.
Elastic APM,65498418,65484826,1,"2020/12/29, 22:34:40",True,"2020/12/29, 22:34:40",156911.0,4668.0,1,"The problem seems to be that while  nuxt.config.js  is the first time a user has a chance to add some javascript, that the  nuxt  command line application bootstraps the Node's HTTP frameworks before this config file is  required ."
Elastic APM,65498418,65484826,1,"2020/12/29, 22:34:40",True,"2020/12/29, 22:34:40",156911.0,4668.0,1,This means the elastic agent (or any APM agent) doesn't have a chance to hook into the modules.
Elastic APM,65498418,65484826,1,"2020/12/29, 22:34:40",True,"2020/12/29, 22:34:40",156911.0,4668.0,1,The  current recommendations  from the Nuxt team appears to be
Elastic APM,65498418,65484826,1,"2020/12/29, 22:34:40",True,"2020/12/29, 22:34:40",156911.0,4668.0,1,Invoke  nuxt  manually via  -r
Elastic APM,65498418,65484826,1,"2020/12/29, 22:34:40",True,"2020/12/29, 22:34:40",156911.0,4668.0,1,Skip  nuxt  and  use NuxtJS programmatically  as a middleware in your framework of choice
Elastic APM,65978053,65484826,0,"2021/01/31, 12:30:39",False,"2021/01/31, 12:36:15",31.0,5761761.0,1,Based on Alan Storm answer (from Nuxt team) I made it work but with a little modification:
Elastic APM,65384228,65380989,4,"2020/12/20, 21:58:43",True,"2020/12/20, 22:08:14",58.0,14860242.0,1,"actually there are many reasons why your app not starting depending on how you setup and configured your ELK stack , but for me I did the following and it's working fine :"
Elastic APM,65384228,65380989,4,"2020/12/20, 21:58:43",True,"2020/12/20, 22:08:14",58.0,14860242.0,1,create image from this Dockerfile:
Elastic APM,65384228,65380989,4,"2020/12/20, 21:58:43",True,"2020/12/20, 22:08:14",58.0,14860242.0,1,run the created image :
Elastic APM,59233925,59228679,1,"2019/12/08, 11:07:43",True,"2019/12/08, 11:07:43",37971.0,1237575.0,0,The exception comes from the constructur of the  Kernel32  class which is a class of the Maven coordinate  net.java.dev.jna:jna-platform  which itself depends on  net.java.dev.jna:jna .
Elastic APM,59233925,59228679,1,"2019/12/08, 11:07:43",True,"2019/12/08, 11:07:43",37971.0,1237575.0,0,It seems to me like you have to incompatible versions of those dependencies on the class path.
Elastic APM,59233925,59228679,1,"2019/12/08, 11:07:43",True,"2019/12/08, 11:07:43",37971.0,1237575.0,0,I assume that you use version 4 of JNA core and version 5 of JNA platform.
Elastic APM,59233925,59228679,1,"2019/12/08, 11:07:43",True,"2019/12/08, 11:07:43",37971.0,1237575.0,0,Upgrade the first or downgrade the latter and the error should disappear.
Elastic APM,58917376,58916834,9,"2019/11/18, 16:59:13",True,"2020/03/24, 22:19:43",18074.0,4728685.0,1,"Well, as an option, you can use something like that"
Elastic APM,56067795,56065263,1,"2019/05/10, 00:25:17",False,"2019/05/10, 00:25:17",2737.0,7330758.0,0,It seems that you are using the oss distribution of elasticsearch but the defaut version of apm.
Elastic APM,56067795,56065263,1,"2019/05/10, 00:25:17",False,"2019/05/10, 00:25:17",2737.0,7330758.0,0,upgrade the elasticsearch cluster to the default disto or use this oss apm docker image: docker.elastic.co/apm/apm-server-oss:7.0.1
Elastic APM,57365645,55962977,3,"2019/08/05, 23:23:09",True,"2019/08/05, 23:23:09",5287.0,780798.0,2,OpenTracing   is a set of standard APIs that consistently model and describe the behavior of distributed systems )
Elastic APM,57365645,55962977,3,"2019/08/05, 23:23:09",True,"2019/08/05, 23:23:09",5287.0,780798.0,2,"OpenTracing did not describe how to collect, report, store or represent the data of interrelated traces and spans."
Elastic APM,57365645,55962977,3,"2019/08/05, 23:23:09",True,"2019/08/05, 23:23:09",5287.0,780798.0,2,It is implementation details (such as  jaeger  or  wavefront ).
Elastic APM,57365645,55962977,3,"2019/08/05, 23:23:09",True,"2019/08/05, 23:23:09",5287.0,780798.0,2,jaeger-client-csharp is very jaeger-specific.
Elastic APM,57365645,55962977,3,"2019/08/05, 23:23:09",True,"2019/08/05, 23:23:09",5287.0,780798.0,2,"But there is one exception, called  zipkin  which in turns is not fully OpenTracing compliant, even it has similar terms."
Elastic APM,57365645,55962977,3,"2019/08/05, 23:23:09",True,"2019/08/05, 23:23:09",5287.0,780798.0,2,"If you are OK with  opentracing-contrib/csharp-netcore  (hope you are using this library) then if you want to achieve ""no code change"" (in target microservice) in order to configure tracing subsystem, you should use some plug-in model."
Elastic APM,57365645,55962977,3,"2019/08/05, 23:23:09",True,"2019/08/05, 23:23:09",5287.0,780798.0,2,"Good news that aspnetcore has concept of  hosted startup assemblies , which allow you to configure tracing system."
Elastic APM,57365645,55962977,3,"2019/08/05, 23:23:09",True,"2019/08/05, 23:23:09",5287.0,780798.0,2,"So, you can have some library called  JaegerStartup  where you will implement IHostedStartup like follows:"
Elastic APM,57365645,55962977,3,"2019/08/05, 23:23:09",True,"2019/08/05, 23:23:09",5287.0,780798.0,2,"When you decide to switch the tracing system - you need to create another library, which can be loaded automatically, and target microservice code will not be touched."
Elastic APM,67181477,67173030,2,"2021/04/20, 18:01:19",False,"2021/04/20, 18:01:19",633.0,13659075.0,0,The best way to troubleshoot what is going on is to check if the events from Heartbeat are being collected.
Elastic APM,67181477,67173030,2,"2021/04/20, 18:01:19",False,"2021/04/20, 18:01:19",633.0,13659075.0,0,"The Uptime application only displays events from Heartbeat, and therefore — this is the Beat that you need to check."
Elastic APM,67181477,67173030,2,"2021/04/20, 18:01:19",False,"2021/04/20, 18:01:19",633.0,13659075.0,0,"First, check the connectivity of Heartbeat and the configured output:"
Elastic APM,67181477,67173030,2,"2021/04/20, 18:01:19",False,"2021/04/20, 18:01:19",633.0,13659075.0,0,"Secondly, check if the events are being generated."
Elastic APM,67181477,67173030,2,"2021/04/20, 18:01:19",False,"2021/04/20, 18:01:19",633.0,13659075.0,0,You can check this by commenting out your existing output (Likely Elasticsearc/Elastic Cloud) and enabling either the  Console  output or the  File  output.
Elastic APM,67181477,67173030,2,"2021/04/20, 18:01:19",False,"2021/04/20, 18:01:19",633.0,13659075.0,0,Then start your Metricbeat and check if events are being generated.
Elastic APM,67181477,67173030,2,"2021/04/20, 18:01:19",False,"2021/04/20, 18:01:19",633.0,13659075.0,0,"If they are, then it might be something with the backend side of things; maybe Elasticsearch is rejecting the documents sent and refusing to index them."
Elastic APM,67181477,67173030,2,"2021/04/20, 18:01:19",False,"2021/04/20, 18:01:19",633.0,13659075.0,0,"Apropos, Elastic is implementing a native  Jenkins  plugin that allows you to observe your CI pipeline using OpenTelemetry compatible backends such as  Elastic APM ."
Elastic APM,67181477,67173030,2,"2021/04/20, 18:01:19",False,"2021/04/20, 18:01:19",633.0,13659075.0,0,You can learn more about this plugin  here .
Elastic APM,65005103,64923213,0,"2020/11/25, 14:52:51",True,"2020/11/25, 14:52:51",176.0,14018385.0,0,"The only way to get configuration is to check apm-server.yml in your instance, but if you want to check your agent configuration you can use Agent Configuration API, for more information check  https://www.elastic.co/guide/en/apm/server/current/agent-configuration-api.html ."
Elastic APM,64854473,64854472,0,"2020/11/16, 10:09:07",False,"2020/11/16, 10:09:07",31.0,1973221.0,0,I can by pass it when I comment entityframework-&gt;Interceptor node in web.config file
Elastic APM,64854473,64854472,0,"2020/11/16, 10:09:07",False,"2020/11/16, 10:09:07",31.0,1973221.0,0,And I can continue after uncomment it
Elastic APM,63598396,63576369,1,"2020/08/26, 16:09:20",False,"2020/08/26, 16:09:20",15.0,12391397.0,1,About the possibility of using some kind of Proxy between your gke cluster and elastic apm.
Elastic APM,63598396,63576369,1,"2020/08/26, 16:09:20",False,"2020/08/26, 16:09:20",15.0,12391397.0,1,"You can check the following link [1], to see if it can fit your necessities."
Elastic APM,63598396,63576369,1,"2020/08/26, 16:09:20",False,"2020/08/26, 16:09:20",15.0,12391397.0,1,[1]  https://cloud.google.com/vpc/docs/special-configurations#proxyvm
Elastic APM,63480663,63480314,1,"2020/08/19, 08:40:43",True,"2020/08/19, 08:40:43",6576.0,547452.0,1,Since you didn't mention it above: did you instrument a Go application?
Elastic APM,63480663,63480314,1,"2020/08/19, 08:40:43",True,"2020/08/19, 08:40:43",6576.0,547452.0,1,The Elastic APM Go &quot;Agent&quot; is a package which you use to instrument your application source code.
Elastic APM,63480663,63480314,1,"2020/08/19, 08:40:43",True,"2020/08/19, 08:40:43",6576.0,547452.0,1,"It is not an independent process, but runs within your application."
Elastic APM,63480663,63480314,1,"2020/08/19, 08:40:43",True,"2020/08/19, 08:40:43",6576.0,547452.0,1,"So, first (if you haven't already) instrument your application."
Elastic APM,63480663,63480314,1,"2020/08/19, 08:40:43",True,"2020/08/19, 08:40:43",6576.0,547452.0,1,See  https://www.elastic.co/guide/en/apm/agent/go/current/getting-started.html#instrumenting-source
Elastic APM,63480663,63480314,1,"2020/08/19, 08:40:43",True,"2020/08/19, 08:40:43",6576.0,547452.0,1,"Here's an example web server using  Echo , and the  apmechov4  instrumentation module:"
Elastic APM,63480663,63480314,1,"2020/08/19, 08:40:43",True,"2020/08/19, 08:40:43",6576.0,547452.0,1,"If you run that and send some requests to  http://localhost:8080/hello/world , you should soon see requests in the APM app in Kibana."
Elastic APM,63480663,63480314,1,"2020/08/19, 08:40:43",True,"2020/08/19, 08:40:43",6576.0,547452.0,1,"If you still don't see anything in Kibana, you can follow  https://www.elastic.co/guide/en/apm/agent/go/current/troubleshooting.html#agent-logging  to enable logging."
Elastic APM,63480663,63480314,1,"2020/08/19, 08:40:43",True,"2020/08/19, 08:40:43",6576.0,547452.0,1,Here's what you can expect to see if the agent is able to successfully send data to the server:
Elastic APM,63480663,63480314,1,"2020/08/19, 08:40:43",True,"2020/08/19, 08:40:43",6576.0,547452.0,1,"If on the other hand the server is inaccessible, you would see something like this:"
Elastic APM,62871884,62865170,0,"2020/07/13, 11:20:58",True,"2020/07/13, 11:20:58",31.0,2259926.0,0,"Sorry my bad, the server urls weren't correctly passed to docker."
Elastic APM,60327076,60326859,0,"2020/02/20, 21:16:01",False,"2020/02/20, 21:16:01",1428.0,2924546.0,0,"This is by design in Django, and it is intentionally designed in this way."
Elastic APM,60327076,60326859,0,"2020/02/20, 21:16:01",False,"2020/02/20, 21:16:01",1428.0,2924546.0,0,this is a parametrized way.
Elastic APM,60327076,60326859,0,"2020/02/20, 21:16:01",False,"2020/02/20, 21:16:01",1428.0,2924546.0,0,suppose someone has a column name with spaces like  test column name  then think what would happen.
Elastic APM,60327076,60326859,0,"2020/02/20, 21:16:01",False,"2020/02/20, 21:16:01",1428.0,2924546.0,0,"it will lead to some unwanted errors, so don't change the underlying logic of the framework."
Elastic APM,60327397,60326859,0,"2020/02/20, 21:40:06",True,"2020/02/20, 21:40:06",1.0,7133255.0,0,"Thanks @BjarniRagnarsson, The upper case letters was making this behavior of framework as @Sanjay mentioned."
Elastic APM,60327397,60326859,0,"2020/02/20, 21:40:06",True,"2020/02/20, 21:40:06",1.0,7133255.0,0,Solution:
Elastic APM,59466931,58615241,0,"2019/12/24, 11:52:49",False,"2019/12/24, 11:52:49",2686.0,1360278.0,0,"For a high-level overview type of information, have a look at  Elastic Stack Monitoring ."
Elastic APM,59466931,58615241,0,"2019/12/24, 11:52:49",False,"2019/12/24, 11:52:49",2686.0,1360278.0,0,"If you want to look at any monitoring in more detail, have a look at the  monitoring APIs themselves ."
Elastic APM,59466931,58615241,0,"2019/12/24, 11:52:49",False,"2019/12/24, 11:52:49",2686.0,1360278.0,0,"If you want to log this sort of information, you should set thresholds  for your Elasticsearch slow log ."
Elastic APM,59466931,58615241,0,"2019/12/24, 11:52:49",False,"2019/12/24, 11:52:49",2686.0,1360278.0,0,"If you want to index and then view data from the slow log,  you can always use Filebeat to ingest that slow log data back into Elasticsearch ."
Elastic APM,54616102,54559465,0,"2019/02/10, 13:55:02",False,"2019/02/10, 13:55:02",8336.0,1773866.0,1,You're calling one method from another.
Elastic APM,54616102,54559465,0,"2019/02/10, 13:55:02",False,"2019/02/10, 13:55:02",8336.0,1773866.0,1,Spring is creating a proxy around your method.
Elastic APM,54616102,54559465,0,"2019/02/10, 13:55:02",False,"2019/02/10, 13:55:02",8336.0,1773866.0,1,If you call one method from another from the same class then you're not going through the proxy.
Elastic APM,54616102,54559465,0,"2019/02/10, 13:55:02",False,"2019/02/10, 13:55:02",8336.0,1773866.0,1,Extract the method annotated with new span to a separate class and it will work fine.
Jaeger,51790666,51785812,2,"2018/08/10, 19:32:28",True,"2019/07/15, 20:44:47",373.0,10206966.0,14,"After searching a solution for some time, I found a docker-compose.yml file which had the Jaeger Query,Agent,collector and Elasticsearch configurations."
Jaeger,51790666,51785812,2,"2018/08/10, 19:32:28",True,"2019/07/15, 20:44:47",373.0,10206966.0,14,docker-compose.yml
Jaeger,51790666,51785812,2,"2018/08/10, 19:32:28",True,"2019/07/15, 20:44:47",373.0,10206966.0,14,"The docker-compose.yml file installs the elasticsearch, Jaeger collector,query and agent."
Jaeger,51790666,51785812,2,"2018/08/10, 19:32:28",True,"2019/07/15, 20:44:47",373.0,10206966.0,14,"Install docker and docker compose first
 https://docs.docker.com/compose/install/#install-compose"
Jaeger,51790666,51785812,2,"2018/08/10, 19:32:28",True,"2019/07/15, 20:44:47",373.0,10206966.0,14,"Then, execute these commands in order"
Jaeger,51790666,51785812,2,"2018/08/10, 19:32:28",True,"2019/07/15, 20:44:47",373.0,10206966.0,14,"start all the docker containers - Jaeger agent,collector,query and elasticsearch."
Jaeger,51790666,51785812,2,"2018/08/10, 19:32:28",True,"2019/07/15, 20:44:47",373.0,10206966.0,14,sudo docker start container-id
Jaeger,51790666,51785812,2,"2018/08/10, 19:32:28",True,"2019/07/15, 20:44:47",373.0,10206966.0,14,access -   http://localhost:16686/
Jaeger,61147810,51785812,0,"2020/04/10, 23:00:13",False,"2020/04/10, 23:00:13",71.0,9731647.0,0,"If Jaeger needs to be set up in Kubernetes cluster as a helm chart, one can use this:  https://github.com/jaegertracing/helm-charts/tree/master/charts/jaeger 
It can delploy either Elasticsearch or Cassandara as a storage backend."
Jaeger,61147810,51785812,0,"2020/04/10, 23:00:13",False,"2020/04/10, 23:00:13",71.0,9731647.0,0,Which is just a matter of right value being passed in to the chart:
Jaeger,61147810,51785812,0,"2020/04/10, 23:00:13",False,"2020/04/10, 23:00:13",71.0,9731647.0,0,"This section shows the helm command as an example:
 https://github.com/jaegertracing/helm-charts/tree/master/charts/jaeger#installing-the-chart-using-a-new-elasticsearch-cluster"
Jaeger,62418210,51785812,0,"2020/06/17, 00:57:19",False,"2020/06/17, 00:57:19",488.0,5789008.0,1,If you would like to deploy the Jaeger with Elasticsearch and Kibana to quickly validate and check the stack e.g.
Jaeger,62418210,51785812,0,"2020/06/17, 00:57:19",False,"2020/06/17, 00:57:19",488.0,5789008.0,1,"in kind or Minikube, the following snippet may help you."
Jaeger,63530756,51785812,0,"2020/08/22, 01:23:06",False,"2020/09/02, 13:12:23",30911.0,2000548.0,0,"For people who are using OpenTelemetry, Jaeger, and Elasticsearch, here is the way."
Jaeger,63530756,51785812,0,"2020/08/22, 01:23:06",False,"2020/09/02, 13:12:23",30911.0,2000548.0,0,Note the image being used are  jaegertracing/jaeger-opentelemetry-collector  and  jaegertracing/jaeger-opentelemetry-agent .
Jaeger,63530756,51785812,0,"2020/08/22, 01:23:06",False,"2020/09/02, 13:12:23",30911.0,2000548.0,0,Then just need
Jaeger,63530756,51785812,0,"2020/08/22, 01:23:06",False,"2020/09/02, 13:12:23",30911.0,2000548.0,0,Reference:  https://github.com/jaegertracing/jaeger/blob/master/crossdock/jaeger-opentelemetry-docker-compose.yml
Jaeger,64755733,51785812,0,"2020/11/09, 18:47:27",False,"2020/11/09, 18:47:27",328.0,1221718.0,1,"As I mentioned in my comment on the OP's first answer above, I was getting an error when running the docker-compose exactly as given:"
Jaeger,64755733,51785812,0,"2020/11/09, 18:47:27",False,"2020/11/09, 18:47:27",328.0,1221718.0,1,Error: unknown flag: --collector.host-port
Jaeger,64755733,51785812,0,"2020/11/09, 18:47:27",False,"2020/11/09, 18:47:27",328.0,1221718.0,1,I think this CLI flag has been deprecated by the Jaeger folks since that answer was written.
Jaeger,64755733,51785812,0,"2020/11/09, 18:47:27",False,"2020/11/09, 18:47:27",328.0,1221718.0,1,So I poked around in the jaeger-agent documentation a bit:
Jaeger,64755733,51785812,0,"2020/11/09, 18:47:27",False,"2020/11/09, 18:47:27",328.0,1221718.0,1,And I got this to work with a couple of small modifications:
Jaeger,64755733,51785812,0,"2020/11/09, 18:47:27",False,"2020/11/09, 18:47:27",328.0,1221718.0,1,The updated docker-compose.yaml:
Jaeger,50985064,50855480,0,"2018/06/22, 12:38:46",True,"2018/06/22, 12:38:46",231.0,4158442.0,7,https://github.com/opentracing-contrib/java-spring-cloud  project automatically sends standard logging to the active span.
Jaeger,50985064,50855480,0,"2018/06/22, 12:38:46",True,"2018/06/22, 12:38:46",231.0,4158442.0,7,Just add the following dependency to your pom.xml
Jaeger,50985064,50855480,0,"2018/06/22, 12:38:46",True,"2018/06/22, 12:38:46",231.0,4158442.0,7,Or use this  https://github.com/opentracing-contrib/java-spring-cloud/tree/master/instrument-starters/opentracing-spring-cloud-core  starter if you want only logging integration.
Jaeger,56222158,50855480,2,"2019/05/20, 16:39:29",False,"2019/05/20, 16:39:29",61.0,9483992.0,0,Then I use  opentracing-spring-jaeger-cloud-starter
Jaeger,56222158,50855480,2,"2019/05/20, 16:39:29",False,"2019/05/20, 16:39:29",61.0,9483992.0,0,"I got just one line in console with current trace and span
i.j.internal.reporters.LoggingReporter   : Span reported: f1a264bbe2c7eae9:f1a264bbe2c7eae9:0:1 - my_method"
Jaeger,56222158,50855480,2,"2019/05/20, 16:39:29",False,"2019/05/20, 16:39:29",61.0,9483992.0,0,Then I use  spring-cloud-starter-sleuth
Jaeger,56222158,50855480,2,"2019/05/20, 16:39:29",False,"2019/05/20, 16:39:29",61.0,9483992.0,0,"I got the Trace and Spans like [my-service,90e1114e35c897d6,90e1114e35c897d6,false] in each line and it's helpfull for filebeat in ELK"
Jaeger,56222158,50855480,2,"2019/05/20, 16:39:29",False,"2019/05/20, 16:39:29",61.0,9483992.0,0,How could I get the same log in console using opentracing-spring-jaeger-cloud-starter ?
Jaeger,56222158,50855480,2,"2019/05/20, 16:39:29",False,"2019/05/20, 16:39:29",61.0,9483992.0,0,my opentracing config
Jaeger,58798104,50855480,0,"2019/11/11, 10:48:18",False,"2019/11/11, 10:48:18",11.0,10465104.0,1,Here is what I did to make jdbc related logs from Logback (Slf4j) write into Jaeger server:
Jaeger,58798104,50855480,0,"2019/11/11, 10:48:18",False,"2019/11/11, 10:48:18",11.0,10465104.0,1,Beginning with Logback config (logback-spring.xml):
Jaeger,58798104,50855480,0,"2019/11/11, 10:48:18",False,"2019/11/11, 10:48:18",11.0,10465104.0,1,Here is my appender:
Jaeger,62995670,57268147,0,"2020/07/20, 15:44:52",False,"2020/07/20, 15:44:52",537.0,2841947.0,0,I was facing similar issue.
Jaeger,62995670,57268147,0,"2020/07/20, 15:44:52",False,"2020/07/20, 15:44:52",537.0,2841947.0,0,ConnectionInfo was getting traced but not the SQL statements.
Jaeger,62995670,57268147,0,"2020/07/20, 15:44:52",False,"2020/07/20, 15:44:52",537.0,2841947.0,0,"In my case, I had to enable traceWithActiveSpanOnly=true."
Jaeger,62995670,57268147,0,"2020/07/20, 15:44:52",False,"2020/07/20, 15:44:52",537.0,2841947.0,0,"For e.g, : jdbc:tracing:h2:mem:test?traceWithActiveSpanOnly=true"
Jaeger,62995670,57268147,0,"2020/07/20, 15:44:52",False,"2020/07/20, 15:44:52",537.0,2841947.0,0,After that the statements started getting traced.
Jaeger,62995670,57268147,0,"2020/07/20, 15:44:52",False,"2020/07/20, 15:44:52",537.0,2841947.0,0,Check the documentation of opentracing java-jdbc module here
Jaeger,53464346,53459759,2,"2018/11/25, 05:17:34",False,"2018/11/26, 04:04:55",2451.0,1432067.0,4,"Based on my experience and reading online, I found this interesting line in Istio  mixer faq"
Jaeger,53464346,53459759,2,"2018/11/25, 05:17:34",False,"2018/11/26, 04:04:55",2451.0,1432067.0,4,"Mixer trace generation is controlled by command-line flags: trace_zipkin_url, trace_jaeger_url, and trace_log_spans."
Jaeger,53464346,53459759,2,"2018/11/25, 05:17:34",False,"2018/11/26, 04:04:55",2451.0,1432067.0,4,"If any of those flag values are set, trace data will be written directly to those locations."
Jaeger,53464346,53459759,2,"2018/11/25, 05:17:34",False,"2018/11/26, 04:04:55",2451.0,1432067.0,4,"If no tracing options are provided, Mixer will not generate any application-level trace information."
Jaeger,53464346,53459759,2,"2018/11/25, 05:17:34",False,"2018/11/26, 04:04:55",2451.0,1432067.0,4,"Also, if you go deep into mixer  helm chart , you will find traces of Zipkin and Jaeger signifying that it’s mixer that is passing trace info to Jaeger."
Jaeger,53464346,53459759,2,"2018/11/25, 05:17:34",False,"2018/11/26, 04:04:55",2451.0,1432067.0,4,I also got confused which reading this line in one of the articles
Jaeger,53464346,53459759,2,"2018/11/25, 05:17:34",False,"2018/11/26, 04:04:55",2451.0,1432067.0,4,Istio injects a sidecar proxy (Envoy) in the pod in which your application container is running.
Jaeger,53464346,53459759,2,"2018/11/25, 05:17:34",False,"2018/11/26, 04:04:55",2451.0,1432067.0,4,This sidecar proxy transparently intercepts (iptables magic) all network traffic going in and out of your application.
Jaeger,53464346,53459759,2,"2018/11/25, 05:17:34",False,"2018/11/26, 04:04:55",2451.0,1432067.0,4,"Because of this interception, the sidecar proxy is in a unique position to automatically trace all network requests (HTTP/1.1, HTTP/2.0 &amp; gRPC)."
Jaeger,53464346,53459759,2,"2018/11/25, 05:17:34",False,"2018/11/26, 04:04:55",2451.0,1432067.0,4,"On Istio mixer documentation, The Envoy sidecar logically calls Mixer before each request to perform precondition checks, and after each request to report telemetry."
Jaeger,53464346,53459759,2,"2018/11/25, 05:17:34",False,"2018/11/26, 04:04:55",2451.0,1432067.0,4,The sidecar has local caching such that a large percentage of precondition checks can be performed from cache.
Jaeger,53464346,53459759,2,"2018/11/25, 05:17:34",False,"2018/11/26, 04:04:55",2451.0,1432067.0,4,"Additionally, the sidecar buffers outgoing telemetry such that it only calls Mixer infrequently."
Jaeger,53464346,53459759,2,"2018/11/25, 05:17:34",False,"2018/11/26, 04:04:55",2451.0,1432067.0,4,Update:  You can enable tracing to understand what happens to a request in Istio and also the role of mixer and envoy.
Jaeger,53464346,53459759,2,"2018/11/25, 05:17:34",False,"2018/11/26, 04:04:55",2451.0,1432067.0,4,Read more information  here
Jaeger,50230156,50173643,1,"2018/05/08, 12:19:17",True,"2018/05/08, 12:19:17",193.0,9725840.0,13,"I found the solution to my problem, in case anybody is facing similar issues."
Jaeger,50230156,50173643,1,"2018/05/08, 12:19:17",True,"2018/05/08, 12:19:17",193.0,9725840.0,13,"I was missing the environment variable  JAEGER_SAMPLER_MANAGER_HOST_PORT , which is necessary if the (default) remote controlled sampler is used for tracing."
Jaeger,50230156,50173643,1,"2018/05/08, 12:19:17",True,"2018/05/08, 12:19:17",193.0,9725840.0,13,This is the working docker-compose file:
Jaeger,66317822,50173643,0,"2021/02/22, 16:41:31",False,"2021/02/23, 08:30:03",89.0,668438.0,0,For anyone finding themselves on this page looking at a simialr issue for jaeger opentracing in .net core below is a working docker compose snippet and working code in Startup.cs.
Jaeger,66317822,50173643,0,"2021/02/22, 16:41:31",False,"2021/02/23, 08:30:03",89.0,668438.0,0,The piece I was missing was getting jaeger to read the environment variables from docker-compose as by default it was trying to send the traces over udp on localhost.
Jaeger,66317822,50173643,0,"2021/02/22, 16:41:31",False,"2021/02/23, 08:30:03",89.0,668438.0,0,Add following nuget packages to your api's.
Jaeger,66317822,50173643,0,"2021/02/22, 16:41:31",False,"2021/02/23, 08:30:03",89.0,668438.0,0,"Then inside your Startup.cs Configure method, you will need to configure jaeger and opentracing as below."
Jaeger,62348886,62216558,0,"2020/06/12, 19:51:43",False,"2020/06/12, 19:51:43",19192.0,4275342.0,2,The jaeger-agent service should look like
Jaeger,62348886,62216558,0,"2020/06/12, 19:51:43",False,"2020/06/12, 19:51:43",19192.0,4275342.0,2,"Don't use IP, use FQDN."
Jaeger,62348886,62216558,0,"2020/06/12, 19:51:43",False,"2020/06/12, 19:51:43",19192.0,4275342.0,2,"First, try to hardcode value for  jaegerHost"
Jaeger,62348886,62216558,0,"2020/06/12, 19:51:43",False,"2020/06/12, 19:51:43",19192.0,4275342.0,2,"where  jaeger-agent  - service name,  jaeger  - namespace of service"
Jaeger,62348886,62216558,0,"2020/06/12, 19:51:43",False,"2020/06/12, 19:51:43",19192.0,4275342.0,2,"Also, you should create  jaeger-collector  service"
Jaeger,52894567,52886127,2,"2018/10/19, 17:34:17",True,"2018/10/31, 13:44:01",13313.0,524946.0,2,Are you closing the tracer and the scope?
Jaeger,52894567,52886127,2,"2018/10/19, 17:34:17",True,"2018/10/31, 13:44:01",13313.0,524946.0,2,"If you are using a version before 0.32.0, you should manually call  tracer.close()  before your process terminates, otherwise the spans in the buffer might not get dispatched."
Jaeger,52894567,52886127,2,"2018/10/19, 17:34:17",True,"2018/10/31, 13:44:01",13313.0,524946.0,2,"As for the scope, it's common to wrap it in a try-with-resources statement:"
Jaeger,52894567,52886127,2,"2018/10/19, 17:34:17",True,"2018/10/31, 13:44:01",13313.0,524946.0,2,You might also want to check the OpenTracing tutorial at  https://github.com/yurishkuro/opentracing-tutorial  or the Katacoda-based version at  https://www.katacoda.com/courses/opentracing
Jaeger,52894567,52886127,2,"2018/10/19, 17:34:17",True,"2018/10/31, 13:44:01",13313.0,524946.0,2,-- EDIT
Jaeger,52894567,52886127,2,"2018/10/19, 17:34:17",True,"2018/10/31, 13:44:01",13313.0,524946.0,2,and is deployed on a different hostname and port
Jaeger,52894567,52886127,2,"2018/10/19, 17:34:17",True,"2018/10/31, 13:44:01",13313.0,524946.0,2,Then you do need to tell the tracer where to send the traces.
Jaeger,52894567,52886127,2,"2018/10/19, 17:34:17",True,"2018/10/31, 13:44:01",13313.0,524946.0,2,"Either export the  JAEGER_ENDPOINT  environment variable, pointing to a collector endpoint, or set  JAEGER_AGENT_HOST / JAEGER_AGENT_PORT , with the location of the agent."
Jaeger,52894567,52886127,2,"2018/10/19, 17:34:17",True,"2018/10/31, 13:44:01",13313.0,524946.0,2,You can check the available environment variables for your client on the following URL:  https://www.jaegertracing.io/docs/1.7/client-features/
Jaeger,47756263,47706011,7,"2017/12/11, 17:35:05",True,"2017/12/11, 17:35:05",91.0,629338.0,2,You need to add some more properties to your config options.
Jaeger,47756263,47706011,7,"2017/12/11, 17:35:05",True,"2017/12/11, 17:35:05",91.0,629338.0,2,For reporter deployed on localhost and local sampler strategy :
Jaeger,47756263,47706011,7,"2017/12/11, 17:35:05",True,"2017/12/11, 17:35:05",91.0,629338.0,2,Replace  localhost  by server or route name to target another host for Jeager runtime.
Jaeger,64818358,63407052,0,"2020/11/13, 11:22:51",False,"2020/11/13, 11:22:51",51.0,8394088.0,0,This link ( https://objectpartners.com/2019/04/25/distributed-tracing-with-apache-kafka-and-jaeger/ ) provides the details of how to enable jaeger traces.
Jaeger,64818358,63407052,0,"2020/11/13, 11:22:51",False,"2020/11/13, 11:22:51",51.0,8394088.0,0,The simplest way to enable jaeger to spring-boot application is add the dependency and the required properties.
Jaeger,64818358,63407052,0,"2020/11/13, 11:22:51",False,"2020/11/13, 11:22:51",51.0,8394088.0,0,Dependency:
Jaeger,64818358,63407052,0,"2020/11/13, 11:22:51",False,"2020/11/13, 11:22:51",51.0,8394088.0,0,Example Properties
Jaeger,65029618,63407052,0,"2020/11/26, 23:49:03",True,"2020/11/26, 23:49:03",36.0,4216591.0,1,Answering to your question about dependencies it is explained here in Dependencies section ( https://github.com/opentracing-contrib/java-spring-jaeger ):
Jaeger,65029618,63407052,0,"2020/11/26, 23:49:03",True,"2020/11/26, 23:49:03",36.0,4216591.0,1,"The opentracing-spring-jaeger-web-starter starter is convenience starter that includes both opentracing-spring-jaeger-starter and opentracing-spring-web-starter This means that by including it, simple web Spring Boot microservices include all the necessary dependencies to instrument Web requests / responses and send traces to Jaeger."
Jaeger,65029618,63407052,0,"2020/11/26, 23:49:03",True,"2020/11/26, 23:49:03",36.0,4216591.0,1,"The opentracing-spring-jaeger-cloud-starter starter is convenience starter that includes both opentracing-spring-jaeger-starter and opentracing-spring-cloud-starter This means that by including it, all parts of the Spring Cloud stack supported by Opentracing will be instrumented"
Jaeger,65029618,63407052,0,"2020/11/26, 23:49:03",True,"2020/11/26, 23:49:03",36.0,4216591.0,1,And by the way:
Jaeger,65029618,63407052,0,"2020/11/26, 23:49:03",True,"2020/11/26, 23:49:03",36.0,4216591.0,1,same as:
Jaeger,61154134,61154096,3,"2020/04/11, 11:43:10",False,"2020/04/11, 11:43:10",27576.0,1839482.0,2,You can use  Jaeger Operator  to deploy Jaeger on kubernetes.The Jaeger Operator is an implementation of a Kubernetes Operator.
Jaeger,61154134,61154096,3,"2020/04/11, 11:43:10",False,"2020/04/11, 11:43:10",27576.0,1839482.0,2,Operators are pieces of software that ease the operational complexity of running another piece of software.
Jaeger,61154134,61154096,3,"2020/04/11, 11:43:10",False,"2020/04/11, 11:43:10",27576.0,1839482.0,2,"More technically, Operators are a method of packaging, deploying, and managing a Kubernetes application"
Jaeger,61154180,61154096,0,"2020/04/11, 11:47:26",True,"2020/04/11, 22:39:56",382.0,7847042.0,1,Follow this link for steps to deploy JAEGER on kubernetes .
Jaeger,61154180,61154096,0,"2020/04/11, 11:47:26",True,"2020/04/11, 22:39:56",382.0,7847042.0,1,https://www.upnxtblog.com/index.php/2018/07/16/kubernetes-tutorial-distributed-tracing-with-jaeger/
Jaeger,61154180,61154096,0,"2020/04/11, 11:47:26",True,"2020/04/11, 22:39:56",382.0,7847042.0,1,make following changes in application.properties
Jaeger,61160646,61154096,0,"2020/04/11, 20:07:08",False,"2020/04/11, 20:07:08",181.0,12530105.0,1,You can use this link for a better understanding -  https://www.upnxtblog.com/index.php/2018/07/16/kubernetes-tutorial-distributed-tracing-with-jaeger/
Jaeger,61152144,61149872,7,"2020/04/11, 07:27:43",False,"2020/04/11, 10:29:27",27576.0,1839482.0,1,You add  opentracing-spring-jaeger-starter   library  into the project which simply contains the code needed to provide a Jaeger implementation of the OpenTracing's  io.opentracing.Tracer  interface.
Jaeger,61152144,61149872,7,"2020/04/11, 07:27:43",False,"2020/04/11, 10:29:27",27576.0,1839482.0,1,Since you have the jaeger deployed in Kubernetes and exposed it via a loadbalancer service you can use the loadbalancer IP and port to connect to it from outside the Kubernetes cluster.
Jaeger,61154289,61149872,1,"2020/04/11, 11:56:40",False,"2021/04/10, 03:12:05",382.0,7847042.0,1,"So the solution that works for me is -
I have made the following changes in my application.properties file of application"
Jaeger,62832644,61149872,3,"2020/07/10, 13:45:36",False,"2020/07/10, 13:45:36",9.0,8694436.0,0,You should config:
Jaeger,59260272,58409078,0,"2019/12/10, 06:32:34",False,"2019/12/10, 06:32:34",471.0,2673284.0,0,"This is why tracing is a useful tool, it often reveals issues like this that you wouldn't suspect otherwise."
Jaeger,59260272,58409078,0,"2019/12/10, 06:32:34",False,"2019/12/10, 06:32:34",471.0,2673284.0,0,"If your application is using async framework, these gaps may indicate execution waiting on available threads."
Jaeger,59260272,58409078,0,"2019/12/10, 06:32:34",False,"2019/12/10, 06:32:34",471.0,2673284.0,0,Or your application may be CPU throttled during and between the spans.
Jaeger,59260272,58409078,0,"2019/12/10, 06:32:34",False,"2019/12/10, 06:32:34",471.0,2673284.0,0,"You cannot really explain the gaps from the trace itself, but you surely do have them."
Jaeger,59260272,58409078,0,"2019/12/10, 06:32:34",False,"2019/12/10, 06:32:34",471.0,2673284.0,0,Time to whip out the profiler.
Jaeger,55138790,53453160,1,"2019/03/13, 11:48:49",True,"2019/03/13, 11:48:49",1678.0,2104921.0,1,"Turns out that  Feign  clients are  currently not supported  or to be precise, the spring startes do not configure the Feign clients accordingly."
Jaeger,55138790,53453160,1,"2019/03/13, 11:48:49",True,"2019/03/13, 11:48:49",1678.0,2104921.0,1,"If you want to use Jaeger with your Feign clients, you have to provide an integration of your own."
Jaeger,55138790,53453160,1,"2019/03/13, 11:48:49",True,"2019/03/13, 11:48:49",1678.0,2104921.0,1,"In my experience so far, the jaeger community is a lesser supportive one, thus you have to gain such knowledge on your own, which in my opinion is a big downside and you should probably consider, using an alternative."
Jaeger,63367857,53453160,0,"2020/08/12, 02:50:32",False,"2020/08/12, 02:50:32",1.0,8149668.0,0,"In some cases it might be necessary to explicitly expose the  Feign Client  in the Spring configuration, in order to get the traceId propagated."
Jaeger,63367857,53453160,0,"2020/08/12, 02:50:32",False,"2020/08/12, 02:50:32",1.0,8149668.0,0,This can be done easily by adding the following into one of your configuration classes
Jaeger,53343467,53322900,1,"2018/11/16, 20:27:10",True,"2018/11/16, 20:27:10",463.0,1749786.0,1,The best way to move forward in order to use Jaegar is NOT TO USE JAEGAR CLIENT!
Jaeger,53343467,53322900,1,"2018/11/16, 20:27:10",True,"2018/11/16, 20:27:10",463.0,1749786.0,1,Jaegar has the ability to collect Zipkin spans:
Jaeger,53343467,53322900,1,"2018/11/16, 20:27:10",True,"2018/11/16, 20:27:10",463.0,1749786.0,1,https://www.jaegertracing.io/docs/1.8/getting-started/#migrating-from-zipkin
Jaeger,53343467,53322900,1,"2018/11/16, 20:27:10",True,"2018/11/16, 20:27:10",463.0,1749786.0,1,You should take advantage of this and use the below Sleuth+Zipkin dependency and exclude Jaegar agent jars in your spring boot app.
Jaeger,53343467,53322900,1,"2018/11/16, 20:27:10",True,"2018/11/16, 20:27:10",463.0,1749786.0,1,The above will send Zipkin spans to  http://localhost:9411  by default.
Jaeger,53343467,53322900,1,"2018/11/16, 20:27:10",True,"2018/11/16, 20:27:10",463.0,1749786.0,1,You can override this in your Spring Boot app to point to your Jaegar server easily by overriding the zipkin base URL.
Jaeger,53343467,53322900,1,"2018/11/16, 20:27:10",True,"2018/11/16, 20:27:10",463.0,1749786.0,1,spring.zipkin.base-url= http://your-jaegar-server:9411
Jaeger,53343467,53322900,1,"2018/11/16, 20:27:10",True,"2018/11/16, 20:27:10",463.0,1749786.0,1,Sleuth will do all the heavy lifting and the default logging will log the span and traceIds.
Jaeger,53343467,53322900,1,"2018/11/16, 20:27:10",True,"2018/11/16, 20:27:10",463.0,1749786.0,1,"In the log4j2.xml file, all you have to mention is"
Jaeger,53343467,53322900,1,"2018/11/16, 20:27:10",True,"2018/11/16, 20:27:10",463.0,1749786.0,1,[%X]
Jaeger,53343467,53322900,1,"2018/11/16, 20:27:10",True,"2018/11/16, 20:27:10",463.0,1749786.0,1,You can find the sample code here:
Jaeger,53343467,53322900,1,"2018/11/16, 20:27:10",True,"2018/11/16, 20:27:10",463.0,1749786.0,1,https://github.com/anoophp777/spring-webflux-jaegar-log4j2
Jaeger,52111563,52038025,1,"2018/08/31, 11:36:35",True,"2018/08/31, 11:36:35",284.0,4828509.0,3,Some web frameworks return empty string if a non-existent header is queried.
Jaeger,52111563,52038025,1,"2018/08/31, 11:36:35",True,"2018/08/31, 11:36:35",284.0,4828509.0,3,I have seen this in Spring Boot and KoaJS.
Jaeger,52111563,52038025,1,"2018/08/31, 11:36:35",True,"2018/08/31, 11:36:35",284.0,4828509.0,3,"If any of the tracing headers is not sent by Istio, this header logic causes us to send empty string for those non-existent headers which breaks tracing."
Jaeger,52111563,52038025,1,"2018/08/31, 11:36:35",True,"2018/08/31, 11:36:35",284.0,4828509.0,3,My suggestion is after getting the values for headers filter out the ones with empty string as their values and propogate the remaining ones.
Jaeger,51929991,51838896,0,"2018/08/20, 14:39:54",False,"2018/08/20, 14:39:54",116.0,7510189.0,0,"Also, do you get an Error Message?"
Jaeger,51929991,51838896,0,"2018/08/20, 14:39:54",False,"2018/08/20, 14:39:54",116.0,7510189.0,0,"If so, please post it."
Jaeger,51720029,51719575,0,"2018/08/07, 09:18:44",False,"2018/08/07, 09:34:22",8280.0,1057291.0,3,BINGO!
Jaeger,51720029,51719575,0,"2018/08/07, 09:18:44",False,"2018/08/07, 09:34:22",8280.0,1057291.0,3,We need to setup the ReporterConfigurations as below.
Jaeger,51720029,51719575,0,"2018/08/07, 09:18:44",False,"2018/08/07, 09:34:22",8280.0,1057291.0,3,previously my ones were default ones that's why it always connected to local.
Jaeger,51720029,51719575,0,"2018/08/07, 09:18:44",False,"2018/08/07, 09:34:22",8280.0,1057291.0,3,"Even better, you can create the Configuration from Environment as below providing the environment variables as below"
Jaeger,51720029,51719575,0,"2018/08/07, 09:18:44",False,"2018/08/07, 09:34:22",8280.0,1057291.0,3,You can provide this when you run the docker container
Jaeger,51720029,51719575,0,"2018/08/07, 09:18:44",False,"2018/08/07, 09:34:22",8280.0,1057291.0,3,"-e JAVA_OPTS="""
Jaeger,51720029,51719575,0,"2018/08/07, 09:18:44",False,"2018/08/07, 09:34:22",8280.0,1057291.0,3,""" ...."
Jaeger,59170296,51719575,2,"2019/12/04, 08:43:41",False,"2019/12/04, 13:31:17",11.0,12478106.0,1,Step 1 : First we need to configure remote host address and port.
Jaeger,59170296,51719575,2,"2019/12/04, 08:43:41",False,"2019/12/04, 13:31:17",11.0,12478106.0,1,"Step 2 : configure sender configuration and pass the remote host and port in                           withAgentHost, withAgentPort."
Jaeger,59170296,51719575,2,"2019/12/04, 08:43:41",False,"2019/12/04, 13:31:17",11.0,12478106.0,1,"SenderConfiguration senderConfig = Configuration.SenderConfiguration.fromEnv()
                    .withAgentHost(JAEGER_HOST)
                    .withAgentPort(JAEGER_PORT);"
Jaeger,59170296,51719575,2,"2019/12/04, 08:43:41",False,"2019/12/04, 13:31:17",11.0,12478106.0,1,Step 3 : Pass sender configuration in reporter configuration
Jaeger,59170296,51719575,2,"2019/12/04, 08:43:41",False,"2019/12/04, 13:31:17",11.0,12478106.0,1,"Configuration.ReporterConfiguration reporterConfig = Configuration.ReporterConfiguration.fromEnv()
                .withLogSpans(true)
                .withSender(senderConfig);"
Jaeger,52813394,48423910,0,"2018/10/15, 12:16:23",False,"2018/10/15, 12:16:23",13313.0,524946.0,0,This seems a bit old and it's a bit hard to tell what's wrong.
Jaeger,52813394,48423910,0,"2018/10/15, 12:16:23",False,"2018/10/15, 12:16:23",13313.0,524946.0,0,"My first guess would be the sampling strategy, as Jaeger samples one trace in one thousand, but looks like you did set it."
Jaeger,52813394,48423910,0,"2018/10/15, 12:16:23",False,"2018/10/15, 12:16:23",13313.0,524946.0,0,"
JAEGER_SAMPLER_TYPE=const
JAEGER_SAMPLER_PARAM=1"
Jaeger,52813394,48423910,0,"2018/10/15, 12:16:23",False,"2018/10/15, 12:16:23",13313.0,524946.0,0,I would recommend you start by using a simple  Configuration.fromEnv().getTracer()  to get your tracer.
Jaeger,52813394,48423910,0,"2018/10/15, 12:16:23",False,"2018/10/15, 12:16:23",13313.0,524946.0,0,"Then, control it via env vars, probably setting  JAEGER_REPORTER_LOG_SPANS  to  true ."
Jaeger,52813394,48423910,0,"2018/10/15, 12:16:23",False,"2018/10/15, 12:16:23",13313.0,524946.0,0,"With this option, you should be able to see in the logs whenever Jaeger emits a span."
Jaeger,52813394,48423910,0,"2018/10/15, 12:16:23",False,"2018/10/15, 12:16:23",13313.0,524946.0,0,"You can also set the  --log-level=debug  option to the agent and collector (or all-in-one, in your case) to see when these components receive a span from a client."
Jaeger,64399197,64391505,1,"2020/10/17, 08:06:03",False,"2020/10/17, 08:06:03",4190.0,1733929.0,1,You can see that jaeger-query configuration includes: SPAN_STORAGE_TYPE: &quot;kafka&quot;
Jaeger,64399197,64391505,1,"2020/10/17, 08:06:03",False,"2020/10/17, 08:06:03",4190.0,1733929.0,1,"The error indicates that a kafka client used by jaeger-query to store spans in Kafka cannot in fact reach Kafka, and therefore the jaeger storage factory fails to initialize."
Jaeger,64399197,64391505,1,"2020/10/17, 08:06:03",False,"2020/10/17, 08:06:03",4190.0,1733929.0,1,This can be either because Kafka failed to start (did you check)?
Jaeger,64399197,64391505,1,"2020/10/17, 08:06:03",False,"2020/10/17, 08:06:03",4190.0,1733929.0,1,Or a misconfig of the network in your docker.
Jaeger,64572391,64391505,0,"2020/10/28, 13:57:45",True,"2020/10/28, 13:57:45",1101.0,1141160.0,1,I was missing a lot of information.
Jaeger,64572391,64391505,0,"2020/10/28, 13:57:45",True,"2020/10/28, 13:57:45",1101.0,1141160.0,1,I managed to get it working:
Jaeger,64090692,64018706,9,"2020/09/27, 19:21:51",False,"2020/09/28, 12:43:21",3656.0,3503019.0,0,Update :
Jaeger,64090692,64018706,9,"2020/09/27, 19:21:51",False,"2020/09/28, 12:43:21",3656.0,3503019.0,0,I got the same exception ( Tracer bean is not configured!.. )
Jaeger,64090692,64018706,9,"2020/09/27, 19:21:51",False,"2020/09/28, 12:43:21",3656.0,3503019.0,0,when I use your version of spring cloud jaeger dependencies.
Jaeger,64090692,64018706,9,"2020/09/27, 19:21:51",False,"2020/09/28, 12:43:21",3656.0,3503019.0,0,This is irrespective of the  RxJava  package.
Jaeger,64090692,64018706,9,"2020/09/27, 19:21:51",False,"2020/09/28, 12:43:21",3656.0,3503019.0,0,I think you can directly use  opentracing-spring-jaeger-cloud-starter  which is a combination of  opentracing-spring-cloud-starter  and  opentracing-spring-jaeger-starter .
Jaeger,64090692,64018706,9,"2020/09/27, 19:21:51",False,"2020/09/28, 12:43:21",3656.0,3503019.0,0,Read  this details  for java spring jaeger.
Jaeger,64090692,64018706,9,"2020/09/27, 19:21:51",False,"2020/09/28, 12:43:21",3656.0,3503019.0,0,"The opentracing-spring-jaeger-cloud-starter starter is convenience
starter that includes both opentracing-spring-jaeger-starter and
opentracing-spring-cloud-starter This means that by including it, all
parts of the Spring Cloud stack supported by Opentracing will be
instrumented"
Jaeger,64090692,64018706,9,"2020/09/27, 19:21:51",False,"2020/09/28, 12:43:21",3656.0,3503019.0,0,Note :
Jaeger,64090692,64018706,9,"2020/09/27, 19:21:51",False,"2020/09/28, 12:43:21",3656.0,3503019.0,0,Maybe RxJava tracing won't work without registering the tracer using the decorators provided by  opentracing-contrib .
Jaeger,64090692,64018706,9,"2020/09/27, 19:21:51",False,"2020/09/28, 12:43:21",3656.0,3503019.0,0,Please see the working app  here .
Jaeger,64090692,64018706,9,"2020/09/27, 19:21:51",False,"2020/09/28, 12:43:21",3656.0,3503019.0,0,I have followed  this spring guide  for Reactive Restful webservice and  jaeger  worked with below  pom.xml  without any  Tracer bean  -
Jaeger,63455905,63443921,4,"2020/08/17, 20:36:56",True,"2020/08/17, 20:36:56",111.0,2035350.0,0,"Thing is, you should use opentelemetry collector if you use opentelemetry exporter."
Jaeger,63455905,63443921,4,"2020/08/17, 20:36:56",True,"2020/08/17, 20:36:56",111.0,2035350.0,0,Pls see schema in attachment
Jaeger,63455905,63443921,4,"2020/08/17, 20:36:56",True,"2020/08/17, 20:36:56",111.0,2035350.0,0,"Also I created a gist, which will help you to setup
pls see"
Jaeger,63455905,63443921,4,"2020/08/17, 20:36:56",True,"2020/08/17, 20:36:56",111.0,2035350.0,0,https://gist.github.com/AndrewGrachov/11a18bc7268e43f1a36960d630a0838f
Jaeger,63455905,63443921,4,"2020/08/17, 20:36:56",True,"2020/08/17, 20:36:56",111.0,2035350.0,0,"(just tune the values, export to jaeger-all-in-one instead of separate + cassandra, etc)"
Jaeger,59260037,59153293,1,"2019/12/10, 06:02:36",True,"2019/12/10, 14:41:51",471.0,2673284.0,4,From the official FAQ ( https://www.jaegertracing.io/docs/latest/faq/#do-i-need-to-run-jaeger-agent ):
Jaeger,59260037,59153293,1,"2019/12/10, 06:02:36",True,"2019/12/10, 14:41:51",471.0,2673284.0,4,jaeger-agent  is not always necessary.
Jaeger,59260037,59153293,1,"2019/12/10, 06:02:36",True,"2019/12/10, 14:41:51",471.0,2673284.0,4,Jaeger client libraries can be configured to export trace data directly to  jaeger-collector .
Jaeger,59260037,59153293,1,"2019/12/10, 06:02:36",True,"2019/12/10, 14:41:51",471.0,2673284.0,4,"However, the following are the reasons why running  jaeger-agent  is recommended:"
Jaeger,58511794,58511640,0,"2019/10/22, 23:25:32",True,"2019/10/22, 23:39:29",34894.0,3288890.0,0,If you check the your  base image  it from scratch.
Jaeger,58511794,58511640,0,"2019/10/22, 23:25:32",True,"2019/10/22, 23:39:29",34894.0,3288890.0,0,"So there is no  Bash, ash  as the image is from scratch so it will only cotnain  hotrod-linux ."
Jaeger,58511794,58511640,0,"2019/10/22, 23:25:32",True,"2019/10/22, 23:39:29",34894.0,3288890.0,0,"To get sh or bash in such cases you need to use multi-stage Dockerfile, you can use the base image in Dockerfile and then copy the binaries from the base image in multi-stage in Dockerfile."
Jaeger,58511794,58511640,0,"2019/10/22, 23:25:32",True,"2019/10/22, 23:39:29",34894.0,3288890.0,0,Here you go
Jaeger,58511794,58511640,0,"2019/10/22, 23:25:32",True,"2019/10/22, 23:39:29",34894.0,3288890.0,0,"so now you can build and test and you will able to run command inside container using docker exec, here is the example"
Jaeger,58511799,58511640,0,"2019/10/22, 23:25:50",False,"2019/10/22, 23:25:50",76.0,12259556.0,2,https://github.com/jaegertracing/jaeger/blob/master/examples/hotrod/Dockerfile
Jaeger,58511799,58511640,0,"2019/10/22, 23:25:50",False,"2019/10/22, 23:25:50",76.0,12259556.0,2,As I can see hotrod image was built from scratch image.
Jaeger,58511799,58511640,0,"2019/10/22, 23:25:50",False,"2019/10/22, 23:25:50",76.0,12259556.0,2,And from the docker hub:
Jaeger,58511799,58511640,0,"2019/10/22, 23:25:50",False,"2019/10/22, 23:25:50",76.0,12259556.0,2,"""an explicitly empty image, especially for building images ""FROM
  scratch""..."
Jaeger,58511799,58511640,0,"2019/10/22, 23:25:50",False,"2019/10/22, 23:25:50",76.0,12259556.0,2,"""This image is most useful in the context of building base images
  (such as debian and busybox) or super minimal images (that contain
  only a single binary and whatever it requires, such as hello-world)."""
Jaeger,58511799,58511640,0,"2019/10/22, 23:25:50",False,"2019/10/22, 23:25:50",76.0,12259556.0,2,https://hub.docker.com/_/scratch
Jaeger,58511799,58511640,0,"2019/10/22, 23:25:50",False,"2019/10/22, 23:25:50",76.0,12259556.0,2,"So, I think there is not bash inside this image"
Jaeger,57986473,57976158,0,"2019/09/18, 09:36:52",True,"2019/09/18, 09:36:52",31.0,10489752.0,1,"'reporting_host' must be not 'localhost', but 'jaeger', just as service in docker-compose.yml called."
Jaeger,57986473,57976158,0,"2019/09/18, 09:36:52",True,"2019/09/18, 09:36:52",31.0,10489752.0,1,"'reporting_host' =&gt; 'jaeger',"
Jaeger,57986473,57976158,0,"2019/09/18, 09:36:52",True,"2019/09/18, 09:36:52",31.0,10489752.0,1,"Also, I needed to add  $tracer-&gt;flush();  after all, it closes all the entities and does sending via UDP behind the scenes."
Jaeger,56927362,56847558,1,"2019/07/08, 04:26:18",True,"2019/07/08, 04:26:18",1084.0,2915603.0,1,"So, answering my own question."
Jaeger,56927362,56847558,1,"2019/07/08, 04:26:18",True,"2019/07/08, 04:26:18",1084.0,2915603.0,1,Jaeger does not support cross system spans.
Jaeger,56927362,56847558,1,"2019/07/08, 04:26:18",True,"2019/07/08, 04:26:18",1084.0,2915603.0,1,Every sub-system is responsible for its own span in the whole system.
Jaeger,56927362,56847558,1,"2019/07/08, 04:26:18",True,"2019/07/08, 04:26:18",1084.0,2915603.0,1,"For reference, check this answer  https://github.com/opentracing/specification/issues/143"
Jaeger,56581518,55256948,2,"2019/06/13, 16:23:02",True,"2019/06/13, 16:23:02",2678.0,3349358.0,5,"If anyone else would like to set up Jaeger in spring project, here's what I did:"
Jaeger,56581518,55256948,2,"2019/06/13, 16:23:02",True,"2019/06/13, 16:23:02",2678.0,3349358.0,5,Add dependencies to pom:
Jaeger,56581518,55256948,2,"2019/06/13, 16:23:02",True,"2019/06/13, 16:23:02",2678.0,3349358.0,5,Set up you web.xml to register new tracing filter  tracingFilter  to intercept REST API:
Jaeger,56581518,55256948,2,"2019/06/13, 16:23:02",True,"2019/06/13, 16:23:02",2678.0,3349358.0,5,Register jaeger tracer in spring mvc:
Jaeger,56581518,55256948,2,"2019/06/13, 16:23:02",True,"2019/06/13, 16:23:02",2678.0,3349358.0,5,Set up the  tracingFilter  bean we described in web.xml:
Jaeger,56581518,55256948,2,"2019/06/13, 16:23:02",True,"2019/06/13, 16:23:02",2678.0,3349358.0,5,Finally define jaeger tracer spring configuration:
Jaeger,63082785,55256948,2,"2020/07/25, 02:41:06",False,"2020/07/25, 02:41:06",176.0,1446358.0,0,"I have got following gradle dependencies working,"
Jaeger,63082785,55256948,2,"2020/07/25, 02:41:06",False,"2020/07/25, 02:41:06",176.0,1446358.0,0,"Following tracer bean configuration,"
Jaeger,63082785,55256948,2,"2020/07/25, 02:41:06",False,"2020/07/25, 02:41:06",176.0,1446358.0,0,And then the spans can be recorded as
Jaeger,63082785,55256948,2,"2020/07/25, 02:41:06",False,"2020/07/25, 02:41:06",176.0,1446358.0,0,Here is the working example you can refer  https://github.com/krushnatkhawale/jaeger-with-spring-boot-web-app
Jaeger,53757860,53754605,0,"2018/12/13, 10:35:24",False,"2018/12/13, 10:35:24",39.0,10384577.0,1,So did you install direct  or created a yaml from the templates ?
Jaeger,53757860,53754605,0,"2018/12/13, 10:35:24",False,"2018/12/13, 10:35:24",39.0,10384577.0,1,"I would run the command you used to install but with template function and then add the options for jaeger,Kiali and grafana."
Jaeger,53761233,53754605,3,"2018/12/13, 13:50:31",True,"2018/12/13, 13:50:31",2167.0,9485673.0,6,Here is  howto : from official repository.
Jaeger,53761233,53754605,3,"2018/12/13, 13:50:31",True,"2018/12/13, 13:50:31",2167.0,9485673.0,6,you need to update  values.yaml .
Jaeger,53761233,53754605,3,"2018/12/13, 13:50:31",True,"2018/12/13, 13:50:31",2167.0,9485673.0,6,"and turn on  grafana,  kiali and jaeger."
Jaeger,53761233,53754605,3,"2018/12/13, 13:50:31",True,"2018/12/13, 13:50:31",2167.0,9485673.0,6,For example with kiali change:
Jaeger,53761233,53754605,3,"2018/12/13, 13:50:31",True,"2018/12/13, 13:50:31",2167.0,9485673.0,6,to
Jaeger,53761233,53754605,3,"2018/12/13, 13:50:31",True,"2018/12/13, 13:50:31",2167.0,9485673.0,6,than rebuild the Helm dependencies:
Jaeger,53761233,53754605,3,"2018/12/13, 13:50:31",True,"2018/12/13, 13:50:31",2167.0,9485673.0,6,than upgrade your istio inside kubernetes:
Jaeger,53761233,53754605,3,"2018/12/13, 13:50:31",True,"2018/12/13, 13:50:31",2167.0,9485673.0,6,"that's it, hope it was helpful"
Jaeger,53705114,53654993,0,"2018/12/10, 13:51:09",False,"2018/12/10, 13:51:09",63.0,326167.0,0,Scalabilty is dependent on sampling frequency and volumes.
Jaeger,53705114,53654993,0,"2018/12/10, 13:51:09",False,"2018/12/10, 13:51:09",63.0,326167.0,0,The agent supports adaptive sampling which is a feedback loop from the collector to your instrumented app.
Jaeger,53705114,53654993,0,"2018/12/10, 13:51:09",False,"2018/12/10, 13:51:09",63.0,326167.0,0,You can statically define this up front in your instrumentation but you lose the adaptive features.
Jaeger,53706156,53654993,0,"2018/12/10, 14:56:58",False,"2018/12/10, 14:56:58",21.0,10770815.0,1,It is possible to bypass agent all together and send metrics directly to collector.
Jaeger,53706156,53654993,0,"2018/12/10, 14:56:58",False,"2018/12/10, 14:56:58",21.0,10770815.0,1,Just define variable JAEGER_ENDPOINT in your app running environment.
Jaeger,53706156,53654993,0,"2018/12/10, 14:56:58",False,"2018/12/10, 14:56:58",21.0,10770815.0,1,This behaviour is documented but buried down in the Jager git repo:
Jaeger,53706156,53654993,0,"2018/12/10, 14:56:58",False,"2018/12/10, 14:56:58",21.0,10770815.0,1,https://github.com/jaegertracing/jaeger-client-java/blob/master/jaeger-core/README.md
Jaeger,54325898,53654993,0,"2019/01/23, 13:09:57",False,"2019/01/23, 13:09:57",13313.0,524946.0,0,Would a single agent colocated with a single collector be possible in a Jaeger deployment?
Jaeger,54325898,53654993,0,"2019/01/23, 13:09:57",False,"2019/01/23, 13:09:57",13313.0,524946.0,0,"It's possible, and that's how the  ""all-in-one""  image works."
Jaeger,54325898,53654993,0,"2019/01/23, 13:09:57",False,"2019/01/23, 13:09:57",13313.0,524946.0,0,Would it be advisable?
Jaeger,54325898,53654993,0,"2019/01/23, 13:09:57",False,"2019/01/23, 13:09:57",13313.0,524946.0,0,Depends on your architecture.
Jaeger,54325898,53654993,0,"2019/01/23, 13:09:57",False,"2019/01/23, 13:09:57",13313.0,524946.0,0,"If you don't expect your Jaeger infra to grow, using the all-in-one is easier from the maintenance perspective."
Jaeger,54325898,53654993,0,"2019/01/23, 13:09:57",False,"2019/01/23, 13:09:57",13313.0,524946.0,0,"If you need your Jaeger infra to be highly available, then you probably want to place your agents closer to your instrumented applications than to your collector and scale the collectors separately."
Jaeger,54325898,53654993,0,"2019/01/23, 13:09:57",False,"2019/01/23, 13:09:57",13313.0,524946.0,0,More about the Jaeger Agent is discussed in the following blog posts:
Jaeger,54325898,53654993,0,"2019/01/23, 13:09:57",False,"2019/01/23, 13:09:57",13313.0,524946.0,0,"Running Jaeger Agent on bare metal 
 Deployment strategies for the Jaeger Agent"
Jaeger,54325898,53654993,0,"2019/01/23, 13:09:57",False,"2019/01/23, 13:09:57",13313.0,524946.0,0,Is it possible to skip the agent altogether and submit spans directly to the collector over HTTP?
Jaeger,54325898,53654993,0,"2019/01/23, 13:09:57",False,"2019/01/23, 13:09:57",13313.0,524946.0,0,"For some clients (Java, NodeJS and C#), yes."
Jaeger,54325898,53654993,0,"2019/01/23, 13:09:57",False,"2019/01/23, 13:09:57",13313.0,524946.0,0,Look for the  JAEGER_ENDPOINT  option .
Jaeger,53409452,53172758,0,"2018/11/21, 11:57:59",False,"2018/11/21, 11:57:59",13313.0,524946.0,0,"You can turn on the debugging in the client by setting the option  JAEGER_REPORTER_LOG_SPANS  to true (or use the related option in the  ReporterConfiguration , as it seems that's how you are using it)."
Jaeger,53409452,53172758,0,"2018/11/21, 11:57:59",False,"2018/11/21, 11:57:59",13313.0,524946.0,0,https://www.jaegertracing.io/docs/1.8/client-features/
Jaeger,53409452,53172758,0,"2018/11/21, 11:57:59",False,"2018/11/21, 11:57:59",13313.0,524946.0,0,"Once you confirm the traces are being generated and sent to the agent, set the log-level in the agent to  debug :"
Jaeger,53409452,53172758,0,"2018/11/21, 11:57:59",False,"2018/11/21, 11:57:59",13313.0,524946.0,0,"
docker run -p... jaegertracing/jaeger-agent:1.8 --log-level=debug"
Jaeger,53409452,53172758,0,"2018/11/21, 11:57:59",False,"2018/11/21, 11:57:59",13313.0,524946.0,0,"If you don't see anything in the logs there indicating that the agent received a span (or span batch), then you might need to configure your client with the Agent's address ( JAEGER_AGENT_HOST  and  JAEGER_AGENT_PORT , or related options in the  Configuration  object)."
Jaeger,53409452,53172758,0,"2018/11/21, 11:57:59",False,"2018/11/21, 11:57:59",13313.0,524946.0,0,"You mentioned that you are deploying in Azure AKS, so, I guess that the agent isn't available at  localhost , which is the default location where the client sends the spans."
Jaeger,53409452,53172758,0,"2018/11/21, 11:57:59",False,"2018/11/21, 11:57:59",13313.0,524946.0,0,"Typically, the agent would be deployed as a sidecar in such a scenario:"
Jaeger,53409452,53172758,0,"2018/11/21, 11:57:59",False,"2018/11/21, 11:57:59",13313.0,524946.0,0,https://github.com/jaegertracing/jaeger-kubernetes#deploying-the-agent-as-sidecar
Jaeger,53341599,51929412,0,"2018/11/16, 18:11:14",False,"2020/12/19, 21:55:48",463.0,1749786.0,2,The best way to move forward in order to use Jaegar is NOT TO USE JAEGAR CLIENT!
Jaeger,53341599,51929412,0,"2018/11/16, 18:11:14",False,"2020/12/19, 21:55:48",463.0,1749786.0,2,Jaegar has the ability to collect Zipkin spans.
Jaeger,53341599,51929412,0,"2018/11/16, 18:11:14",False,"2020/12/19, 21:55:48",463.0,1749786.0,2,https://www.jaegertracing.io/docs/1.8/getting-started/#migrating-from-zipkin
Jaeger,53341599,51929412,0,"2018/11/16, 18:11:14",False,"2020/12/19, 21:55:48",463.0,1749786.0,2,You should take advantage of this and use the below Sleuth+Zipkin dependency and exclude Jaegar agent jars in your spring boot app.
Jaeger,53341599,51929412,0,"2018/11/16, 18:11:14",False,"2020/12/19, 21:55:48",463.0,1749786.0,2,The above will send Zipkin spans to http://localhost:9411 by default.
Jaeger,53341599,51929412,0,"2018/11/16, 18:11:14",False,"2020/12/19, 21:55:48",463.0,1749786.0,2,You can override this in your Spring Boot app to point to your Jaegar server easily by overriding the zipkin base URL.
Jaeger,53341599,51929412,0,"2018/11/16, 18:11:14",False,"2020/12/19, 21:55:48",463.0,1749786.0,2,Sleuth will do all the heavy lifting and the default logging will log the span and traceIds.
Jaeger,53341599,51929412,0,"2018/11/16, 18:11:14",False,"2020/12/19, 21:55:48",463.0,1749786.0,2,"In the  log4j2.xml  file, all you have to mention is"
Jaeger,53341599,51929412,0,"2018/11/16, 18:11:14",False,"2020/12/19, 21:55:48",463.0,1749786.0,2,I'll be uploading a working example of this approach into my GitHub and sharing the link.
Jaeger,53341599,51929412,0,"2018/11/16, 18:11:14",False,"2020/12/19, 21:55:48",463.0,1749786.0,2,EDIT 1:
Jaeger,53341599,51929412,0,"2018/11/16, 18:11:14",False,"2020/12/19, 21:55:48",463.0,1749786.0,2,You can find the sample code here:
Jaeger,53341599,51929412,0,"2018/11/16, 18:11:14",False,"2020/12/19, 21:55:48",463.0,1749786.0,2,https://github.com/anoophp777/spring-webflux-jaegar-log4j2
Jaeger,59675721,50179555,0,"2020/01/10, 06:59:31",True,"2020/01/10, 06:59:31",842.0,6907909.0,1,The Jaeger helm chart is now available  here .
Jaeger,59675721,50179555,0,"2020/01/10, 06:59:31",True,"2020/01/10, 06:59:31",842.0,6907909.0,1,You need to add the helm repo first using the following:
Jaeger,59675721,50179555,0,"2020/01/10, 06:59:31",True,"2020/01/10, 06:59:31",842.0,6907909.0,1,This can be installed with:
Jaeger,66917361,66916739,0,"2021/04/02, 12:39:37",True,"2021/04/02, 12:39:37",8336.0,1773866.0,1,"Yes you can, and I have shown that numerous times during my presentations ( https://toomuchcoding.com/talks ) and we describe it extensively in the documentation ( https://docs.spring.io/spring-cloud-sleuth/docs/current/reference/html/ )."
Jaeger,66917361,66916739,0,"2021/04/02, 12:39:37",True,"2021/04/02, 12:39:37",8336.0,1773866.0,1,Sleuth will set up your logging pattern which you can then parse and visualize using the ELK stack.
Jaeger,66917361,66916739,0,"2021/04/02, 12:39:37",True,"2021/04/02, 12:39:37",8336.0,1773866.0,1,Sleuth takes care of tracing context propagation and can send the spans to a span storage (e.g.
Jaeger,66917361,66916739,0,"2021/04/02, 12:39:37",True,"2021/04/02, 12:39:37",8336.0,1773866.0,1,Zipkin or Jaeger).
Jaeger,66917361,66916739,0,"2021/04/02, 12:39:37",True,"2021/04/02, 12:39:37",8336.0,1773866.0,1,Sleuth does take care of updating the MDC for you.
Jaeger,66917361,66916739,0,"2021/04/02, 12:39:37",True,"2021/04/02, 12:39:37",8336.0,1773866.0,1,Please always read the documentation and the project page before filing a question
Jaeger,66584227,66527792,0,"2021/03/11, 16:09:56",True,"2021/03/11, 16:09:56",2573.0,2519395.0,2,The solution for this one is to simply increase the memory size in the  istio-config.yaml  file.
Jaeger,66584227,66527792,0,"2021/03/11, 16:09:56",True,"2021/03/11, 16:09:56",2573.0,2519395.0,2,"in my case, I'm updating the PVC and it looks like it's already filled with data and decreasing it wasn't an option for istio, so I increased it in the config file instead:"
Jaeger,66314862,66299365,0,"2021/02/22, 13:29:44",True,"2021/02/22, 13:29:44",4113.0,12201084.0,1,Based on following example from  jaeger docs :
Jaeger,66314862,66299365,0,"2021/02/22, 13:29:44",True,"2021/02/22, 13:29:44",4113.0,12201084.0,1,and on example  cli falgs :
Jaeger,66314862,66299365,0,"2021/02/22, 13:29:44",True,"2021/02/22, 13:29:44",4113.0,12201084.0,1,I infere that you should be able to do the following:
Jaeger,66314862,66299365,0,"2021/02/22, 13:29:44",True,"2021/02/22, 13:29:44",4113.0,12201084.0,1,Notice that I split the cli options with the dot and added it as a nested fields in yaml.
Jaeger,66314862,66299365,0,"2021/02/22, 13:29:44",True,"2021/02/22, 13:29:44",4113.0,12201084.0,1,Do the same to other parameters by analogy.
Jaeger,65163350,65138941,0,"2020/12/06, 02:12:58",False,"2020/12/06, 02:12:58",166.0,13922022.0,0,"See the answer on this  jaeger issue , you will need to query elastic search or the source where the data is stored."
Jaeger,65163350,65138941,0,"2020/12/06, 02:12:58",False,"2020/12/06, 02:12:58",166.0,13922022.0,0,"Alternatively, you should raise an issue on  jaeger-ui  detailing your case."
Jaeger,65299006,65138941,0,"2020/12/15, 04:01:30",False,"2020/12/15, 04:01:30",471.0,2673284.0,0,"When you open an individual trace in the Jaeger UI, there is a View dropdown in the top right corner."
Jaeger,65299006,65138941,0,"2020/12/15, 04:01:30",False,"2020/12/15, 04:01:30",471.0,2673284.0,0,One of the options is to view/download the given trace as a JSON file.
Jaeger,65299006,65138941,0,"2020/12/15, 04:01:30",False,"2020/12/15, 04:01:30",471.0,2673284.0,0,"You can also programmatically query the Jaeger query service via JSON/Protobuf API, but those endpoints will not result in a data format that you can load back into the UI."
Jaeger,65299006,65138941,0,"2020/12/15, 04:01:30",False,"2020/12/15, 04:01:30",471.0,2673284.0,0,https://www.jaegertracing.io/docs/latest/apis/#trace-retrieval-apis
Jaeger,64970836,64938228,0,"2020/11/23, 16:58:26",True,"2020/11/23, 16:58:26",328.0,1221718.0,1,"OK, I figured out the issue here which may be obvious to those with more expertise."
Jaeger,64970836,64938228,0,"2020/11/23, 16:58:26",True,"2020/11/23, 16:58:26",328.0,1221718.0,1,The guide I linked to above that describes how to make an Ingress spec for gRPC  is specific to NGINX.
Jaeger,64970836,64938228,0,"2020/11/23, 16:58:26",True,"2020/11/23, 16:58:26",328.0,1221718.0,1,"Meanwhile, I am using K3S which came out of the box with Traefik as the Ingress Controller."
Jaeger,64970836,64938228,0,"2020/11/23, 16:58:26",True,"2020/11/23, 16:58:26",328.0,1221718.0,1,"Therefore, the annotations I used in my Ingress spec had no affect:"
Jaeger,64970836,64938228,0,"2020/11/23, 16:58:26",True,"2020/11/23, 16:58:26",328.0,1221718.0,1,So I found  another Stack Overflow post discussing Traefik and gRPC  and modified my original Ingress spec above a bit to include the annotations mentioned there:
Jaeger,64970836,64938228,0,"2020/11/23, 16:58:26",True,"2020/11/23, 16:58:26",328.0,1221718.0,1,These are the changes I made:
Jaeger,64970836,64938228,0,"2020/11/23, 16:58:26",True,"2020/11/23, 16:58:26",328.0,1221718.0,1,Hopefully this helps someone else running into this same confusion.
Jaeger,64814852,64755896,0,"2020/11/13, 05:00:24",True,"2020/11/13, 05:00:24",121.0,3792242.0,1,"It's not clear in the documentation, but I managed to get it working by providing the  SPAN_STORAGE_TYPE  and the respective connection details to allow the jaeger components to talk to the storage running outside of the all-in-one container."
Jaeger,64814852,64755896,0,"2020/11/13, 05:00:24",True,"2020/11/13, 05:00:24",121.0,3792242.0,1,"For instance, I'm running elasticsearch on my Mac, so I used the following command to run all-in-one:"
Jaeger,64532290,64484169,0,"2020/10/26, 08:17:55",True,"2020/10/26, 08:17:55",5749.0,11977760.0,0,According to istio  documentation
Jaeger,64532290,64484169,0,"2020/10/26, 08:17:55",True,"2020/10/26, 08:17:55",5749.0,11977760.0,0,"To see trace data, you must send requests to your service."
Jaeger,64532290,64484169,0,"2020/10/26, 08:17:55",True,"2020/10/26, 08:17:55",5749.0,11977760.0,0,The number of requests depends on Istio’s sampling rate.
Jaeger,64532290,64484169,0,"2020/10/26, 08:17:55",True,"2020/10/26, 08:17:55",5749.0,11977760.0,0,You set this rate when you install Istio.
Jaeger,64532290,64484169,0,"2020/10/26, 08:17:55",True,"2020/10/26, 08:17:55",5749.0,11977760.0,0,The default sampling rate is 1%.
Jaeger,64532290,64484169,0,"2020/10/26, 08:17:55",True,"2020/10/26, 08:17:55",5749.0,11977760.0,0,You need to send at least 100 requests before the first trace is visible.
Jaeger,64532290,64484169,0,"2020/10/26, 08:17:55",True,"2020/10/26, 08:17:55",5749.0,11977760.0,0,Could you try to send at least 100 requests and check if it works?
Jaeger,64532290,64484169,0,"2020/10/26, 08:17:55",True,"2020/10/26, 08:17:55",5749.0,11977760.0,0,If you wan't to change the default sampling rate then there is istio  documentation  about that.
Jaeger,64532290,64484169,0,"2020/10/26, 08:17:55",True,"2020/10/26, 08:17:55",5749.0,11977760.0,0,Customizing Trace sampling
Jaeger,64532290,64484169,0,"2020/10/26, 08:17:55",True,"2020/10/26, 08:17:55",5749.0,11977760.0,0,The sampling rate option can be used to control what percentage of requests get reported to your tracing system.
Jaeger,64532290,64484169,0,"2020/10/26, 08:17:55",True,"2020/10/26, 08:17:55",5749.0,11977760.0,0,This should be configured depending upon your traffic in the mesh and the amount of tracing data you want to collect.
Jaeger,64532290,64484169,0,"2020/10/26, 08:17:55",True,"2020/10/26, 08:17:55",5749.0,11977760.0,0,The default rate is 1%.
Jaeger,64532290,64484169,0,"2020/10/26, 08:17:55",True,"2020/10/26, 08:17:55",5749.0,11977760.0,0,"To modify the default random sampling to 50, add the following option to your tracing.yaml file."
Jaeger,64532290,64484169,0,"2020/10/26, 08:17:55",True,"2020/10/26, 08:17:55",5749.0,11977760.0,0,The sampling rate should be in the range of 0.0 to 100.0 with a precision of 0.01.
Jaeger,64532290,64484169,0,"2020/10/26, 08:17:55",True,"2020/10/26, 08:17:55",5749.0,11977760.0,0,"For example, to trace 5 requests out of every 10000, use 0.05 as the value here."
Jaeger,64039073,64036098,0,"2020/09/24, 06:22:45",True,"2020/09/24, 06:22:45",328.0,1221718.0,1,After digging around in the OpenTracing C# .NET Core source ( https://github.com/opentracing-contrib/csharp-netcore ) I figured out how to override the top level Span.OperationName.
Jaeger,64039073,64036098,0,"2020/09/24, 06:22:45",True,"2020/09/24, 06:22:45",328.0,1221718.0,1,I had to update my  Startup.ConfigureServices()  call to  services.AddOpenTracing()  to the following:
Jaeger,63116714,62992614,0,"2020/07/27, 16:44:44",False,"2020/07/27, 16:44:44",43.0,7017126.0,0,I have resolved it after configuring port as 14250 as JaegerGrpcSpanExporter internally uses grpc port which has been configured to 14250 for jaeger-collector
Jaeger,62966022,62948787,1,"2020/07/18, 10:50:12",False,"2020/07/18, 10:50:12",669.0,9291851.0,1,"I was studying Opentracing and Jeager and I've used this tutorial to get familiar with the basic possibilities:
 https://github.com/yurishkuro/opentracing-tutorial/tree/master/java"
Jaeger,62966022,62948787,1,"2020/07/18, 10:50:12",False,"2020/07/18, 10:50:12",669.0,9291851.0,1,"If you take a look in the case 1 (Hello World), it explains how to &quot; Annotate the Trace with Tags and Logs &quot;."
Jaeger,62966022,62948787,1,"2020/07/18, 10:50:12",False,"2020/07/18, 10:50:12",669.0,9291851.0,1,"That would answer your questions 1, 2 and 3, as with that you can add all the info that you would like within spans and logs."
Jaeger,62966022,62948787,1,"2020/07/18, 10:50:12",False,"2020/07/18, 10:50:12",669.0,9291851.0,1,"Here is a snippet from the repository (but I'd recommend checking there, as it has a more detailed explanation):"
Jaeger,62966022,62948787,1,"2020/07/18, 10:50:12",False,"2020/07/18, 10:50:12",669.0,9291851.0,1,"In this case  helloTo  is a variable containing a name, to whom the app will say hello."
Jaeger,62966022,62948787,1,"2020/07/18, 10:50:12",False,"2020/07/18, 10:50:12",669.0,9291851.0,1,It would create a span tag called hello-to with the value that is coming from the execution.
Jaeger,62966022,62948787,1,"2020/07/18, 10:50:12",False,"2020/07/18, 10:50:12",669.0,9291851.0,1,"Below we have an example for the logs case, where the whole  helloStr  message is added to the logs:"
Jaeger,62966022,62948787,1,"2020/07/18, 10:50:12",False,"2020/07/18, 10:50:12",669.0,9291851.0,1,"Regarding the last question, that would be easier, you can use the Jaeger UI to search for the trace you would like, there is a field for that on the top left corner:"
Jaeger,63082657,62948787,0,"2020/07/25, 02:23:11",False,"2020/07/25, 02:23:11",176.0,1446358.0,1,There you go.
Jaeger,63082657,62948787,0,"2020/07/25, 02:23:11",False,"2020/07/25, 02:23:11",176.0,1446358.0,1,There are various overloaded methods as follows
Jaeger,63082657,62948787,0,"2020/07/25, 02:23:11",False,"2020/07/25, 02:23:11",176.0,1446358.0,1,I want to add some fields to span tags so that it's easy to search in JaegerUI.
Jaeger,63082657,62948787,0,"2020/07/25, 02:23:11",False,"2020/07/25, 02:23:11",176.0,1446358.0,1,"Jaeger API provides  log  method to log multiple fields that needs to be added to a map, the method signature is as follows,"
Jaeger,63082657,62948787,0,"2020/07/25, 02:23:11",False,"2020/07/25, 02:23:11",176.0,1446358.0,1,"Span log(Map&lt;String, ?&gt; fields);"
Jaeger,63082657,62948787,0,"2020/07/25, 02:23:11",False,"2020/07/25, 02:23:11",176.0,1446358.0,1,eg:
Jaeger,63082657,62948787,0,"2020/07/25, 02:23:11",False,"2020/07/25, 02:23:11",176.0,1446358.0,1,"spanId and traceId are stored in JaegerSpanContext class, which can be obtained from context() method of Span class."
Jaeger,63082657,62948787,0,"2020/07/25, 02:23:11",False,"2020/07/25, 02:23:11",176.0,1446358.0,1,There is a search box in the navigation bar of Jaeger UI where you can search traces by trace ID.
Jaeger,64036080,62420728,0,"2020/09/23, 23:55:37",False,"2020/09/23, 23:55:37",1048.0,1112106.0,1,"What you did is for http 1.x, and it doesn't work for http2/grpc."
Jaeger,64036080,62420728,0,"2020/09/23, 23:55:37",False,"2020/09/23, 23:55:37",1048.0,1112106.0,1,Please dive into grpc impl in springboot doc.
Jaeger,62300680,62273414,0,"2020/06/10, 12:52:33",True,"2020/06/10, 12:52:33",155.0,1222237.0,0,Thanks Yuri.
Jaeger,62300680,62273414,0,"2020/06/10, 12:52:33",True,"2020/06/10, 12:52:33",155.0,1222237.0,0,Yes it was a clock issue.
Jaeger,62300680,62273414,0,"2020/06/10, 12:52:33",True,"2020/06/10, 12:52:33",155.0,1222237.0,0,"Although the host machine (VM) updated its clock on every unpause, docker for windows did not."
Jaeger,62300680,62273414,0,"2020/06/10, 12:52:33",True,"2020/06/10, 12:52:33",155.0,1222237.0,0,The timezones were correct for all containers but the times were all out by the exact same amount.
Jaeger,62300680,62273414,0,"2020/06/10, 12:52:33",True,"2020/06/10, 12:52:33",155.0,1222237.0,0,This must be the docker internal clock that seems to only get updated once on launch and not at the launch of every new container.
Jaeger,62300680,62273414,0,"2020/06/10, 12:52:33",True,"2020/06/10, 12:52:33",155.0,1222237.0,0,"Although all container clocks were out by the same amount, the windows host machine was correct."
Jaeger,62300680,62273414,0,"2020/06/10, 12:52:33",True,"2020/06/10, 12:52:33",155.0,1222237.0,0,The messages were arriving but the times/dates were outside the time frame window the UI was displaying.
Jaeger,62300680,62273414,0,"2020/06/10, 12:52:33",True,"2020/06/10, 12:52:33",155.0,1222237.0,0,If I set a custom date range I'm sure they would appear.
Jaeger,62300680,62273414,0,"2020/06/10, 12:52:33",True,"2020/06/10, 12:52:33",155.0,1222237.0,0,The containers must have been out by a few days with the continual stopping and starting and not by just a few hours.
Jaeger,66277887,62273414,0,"2021/02/19, 14:49:58",False,"2021/02/19, 14:49:58",169.0,3301685.0,0,Time drift in docker is a known issue on Mac and Windows OS.
Jaeger,66277887,62273414,0,"2021/02/19, 14:49:58",False,"2021/02/19, 14:49:58",169.0,3301685.0,0,Check the date/time in a docker container using this (apologies in advance)
Jaeger,66277887,62273414,0,"2021/02/19, 14:49:58",False,"2021/02/19, 14:49:58",169.0,3301685.0,0,or calculate the drift...
Jaeger,66277887,62273414,0,"2021/02/19, 14:49:58",False,"2021/02/19, 14:49:58",169.0,3301685.0,0,Unfortunately there doesnt seem to be a better solution than occasionally restarting the container.
Jaeger,61677933,61565012,0,"2020/05/08, 14:18:10",False,"2020/05/08, 14:18:10",167.0,11335868.0,0,The problem is that your Jaeger collector is not accessible from outside docker network host as you specified in your docker command.
Jaeger,61677933,61565012,0,"2020/05/08, 14:18:10",False,"2020/05/08, 14:18:10",167.0,11335868.0,0,This would only work if your spring boot application is deployed on the host network too.
Jaeger,61677933,61565012,0,"2020/05/08, 14:18:10",False,"2020/05/08, 14:18:10",167.0,11335868.0,0,Try to run Jaeger as follows:
Jaeger,61677933,61565012,0,"2020/05/08, 14:18:10",False,"2020/05/08, 14:18:10",167.0,11335868.0,0,It should trigger.
Jaeger,61171646,61163242,0,"2020/04/12, 15:42:14",False,"2020/04/14, 21:51:42",79.0,9827874.0,0,Have you tried looking at the logs being generated by your pods?
Jaeger,61171646,61163242,0,"2020/04/12, 15:42:14",False,"2020/04/14, 21:51:42",79.0,9827874.0,0,In my case I got the following
Jaeger,61171646,61163242,0,"2020/04/12, 15:42:14",False,"2020/04/14, 21:51:42",79.0,9827874.0,0,"ERROR Failed to flush spans in reporter: error sending spans over UDP:
  Error: getaddrinfo ENOTFOUND  http://jaeger-agent , packet size: 984,
  bytes sent: undefined"
Jaeger,61171646,61163242,0,"2020/04/12, 15:42:14",False,"2020/04/14, 21:51:42",79.0,9827874.0,0,Changing it to jaeger-agent worked for me.
Jaeger,61171646,61163242,0,"2020/04/12, 15:42:14",False,"2020/04/14, 21:51:42",79.0,9827874.0,0,Also if it helps I have declared this under my jaeger image in docker-compose.yml:
Jaeger,60500953,60489344,0,"2020/03/03, 08:01:57",True,"2020/03/03, 08:01:57",5749.0,11977760.0,2,The answer here is to install istio with  --set values.global.tracer.zipkin.address  as provided in  istio documentation
Jaeger,60500953,60489344,0,"2020/03/03, 08:01:57",True,"2020/03/03, 08:01:57",5749.0,11977760.0,2,And
Jaeger,60500953,60489344,0,"2020/03/03, 08:01:57",True,"2020/03/03, 08:01:57",5749.0,11977760.0,2,"Use the original TracingService  setting: service: ""zipkin.istio-system:9411""  as Donato Szilagyi confirmed in comments."
Jaeger,60500953,60489344,0,"2020/03/03, 08:01:57",True,"2020/03/03, 08:01:57",5749.0,11977760.0,2,Great!
Jaeger,60500953,60489344,0,"2020/03/03, 08:01:57",True,"2020/03/03, 08:01:57",5749.0,11977760.0,2,It works.
Jaeger,60500953,60489344,0,"2020/03/03, 08:01:57",True,"2020/03/03, 08:01:57",5749.0,11977760.0,2,"And this time I used the original TracingService setting: service: ""zipkin.istio-system:9411"" – Donato Szilagy"
Jaeger,59678839,59655255,6,"2020/01/10, 11:30:18",False,"2020/01/10, 11:30:18",23024.0,9773937.0,0,"Not sure, but it seems you miss setting the TLS for Cassandra Storage in Azure Cosmos DB."
Jaeger,59678839,59655255,6,"2020/01/10, 11:30:18",False,"2020/01/10, 11:30:18",23024.0,9773937.0,0,"When you use the Cassandra client to connect the Cassandra Storage in Azure Cosmos DB, it will give out the time out error, but if you enable the SSL, the connection works well."
Jaeger,59678839,59655255,6,"2020/01/10, 11:30:18",False,"2020/01/10, 11:30:18",23024.0,9773937.0,0,So I think you can try to enable the TLS for Cassandra in your values.yaml following the steps in the Github which provided.
Jaeger,59578042,59577629,0,"2020/01/03, 13:44:16",False,"2020/01/03, 13:53:57",nan,nan,1,A colleague of mine provided the answer...
Jaeger,59578042,59577629,0,"2020/01/03, 13:44:16",False,"2020/01/03, 13:53:57",nan,nan,1,"It was hidden in the Makefile, which hadn't worked for me as I don't use Golang (and it had been more complex than just installing Golang and running it, but I digress...)."
Jaeger,59578042,59577629,0,"2020/01/03, 13:44:16",False,"2020/01/03, 13:53:57",nan,nan,1,The following .sh will do the trick.
Jaeger,59578042,59577629,0,"2020/01/03, 13:44:16",False,"2020/01/03, 13:53:57",nan,nan,1,"This assumes the query.proto file is a subdirectory from the same location as the script below, under model/proto/api_v2/ (as it appears in the main Jaeger repo)."
Jaeger,59578042,59577629,0,"2020/01/03, 13:44:16",False,"2020/01/03, 13:53:57",nan,nan,1,"This will definitely generate the needed Python file, but it will still be missing dependencies."
Jaeger,59380813,59303550,0,"2019/12/17, 21:23:20",False,"2019/12/17, 21:23:20",471.0,2673284.0,3,"Jaeger clients implement so-called  head-based sampling , where a sampling decision is made at the root of the call tree and propagated down the tree along with the trace context."
Jaeger,59380813,59303550,0,"2019/12/17, 21:23:20",False,"2019/12/17, 21:23:20",471.0,2673284.0,3,"This is done to guarantee consistent sampling of all spans of a given trace (or none of them), because we don't want to make the coin flip at every node and end up with partial/broken traces."
Jaeger,59380813,59303550,0,"2019/12/17, 21:23:20",False,"2019/12/17, 21:23:20",471.0,2673284.0,3,Implementing on-error sampling in the head-based sampling system is not really possible.
Jaeger,59380813,59303550,0,"2019/12/17, 21:23:20",False,"2019/12/17, 21:23:20",471.0,2673284.0,3,"Imaging that your service is calling service A, which returns successfully, and then service B, which returns an error."
Jaeger,59380813,59303550,0,"2019/12/17, 21:23:20",False,"2019/12/17, 21:23:20",471.0,2673284.0,3,Let's assume the root of the trace was not sampled (because otherwise you'd catch the error normally).
Jaeger,59380813,59303550,0,"2019/12/17, 21:23:20",False,"2019/12/17, 21:23:20",471.0,2673284.0,3,"That means by the time you know of an error from B, the whole sub-tree at A has been already executed and all spans discarded because of the earlier decision not to sample."
Jaeger,59380813,59303550,0,"2019/12/17, 21:23:20",False,"2019/12/17, 21:23:20",471.0,2673284.0,3,The sub-tree at B has also finished executing.
Jaeger,59380813,59303550,0,"2019/12/17, 21:23:20",False,"2019/12/17, 21:23:20",471.0,2673284.0,3,The only thing you can sample at this point is the spans in the current service.
Jaeger,59380813,59303550,0,"2019/12/17, 21:23:20",False,"2019/12/17, 21:23:20",471.0,2673284.0,3,You could also implement a reverse propagation of the sampling decision via response to your caller.
Jaeger,59380813,59303550,0,"2019/12/17, 21:23:20",False,"2019/12/17, 21:23:20",471.0,2673284.0,3,"So in the best case you could end up with a sub-branch of the whole trace sampled, and possible future branches if the trace continues from above (e.g."
Jaeger,59380813,59303550,0,"2019/12/17, 21:23:20",False,"2019/12/17, 21:23:20",471.0,2673284.0,3,via retries).
Jaeger,59380813,59303550,0,"2019/12/17, 21:23:20",False,"2019/12/17, 21:23:20",471.0,2673284.0,3,"But you can never capture the full trace, and sometimes the reason B failed was because A (successfully) returned some data that caused the error later."
Jaeger,59380813,59303550,0,"2019/12/17, 21:23:20",False,"2019/12/17, 21:23:20",471.0,2673284.0,3,"Note that reverse propagation is not supported by the OpenTracing or OpenTelemetry today, but it has been discussed in the last meetings of the W3C Trace Context working group."
Jaeger,59380813,59303550,0,"2019/12/17, 21:23:20",False,"2019/12/17, 21:23:20",471.0,2673284.0,3,"The alternative way to implement sampling is with  tail-based sampling , a technique employed by some of the commercial vendors today, such as Lightstep, DataDog."
Jaeger,59380813,59303550,0,"2019/12/17, 21:23:20",False,"2019/12/17, 21:23:20",471.0,2673284.0,3,It is also on the roadmap for Jaeger (we're working on it right now at Uber).
Jaeger,59380813,59303550,0,"2019/12/17, 21:23:20",False,"2019/12/17, 21:23:20",471.0,2673284.0,3,"With tail-based sampling 100% of spans are captured from the application, but only stored in memory in a collection tier, until the full trace is gathered and a sampling decision is made."
Jaeger,59380813,59303550,0,"2019/12/17, 21:23:20",False,"2019/12/17, 21:23:20",471.0,2673284.0,3,"The decision making code has a lot more information now, including errors, unusual latencies, etc."
Jaeger,59380813,59303550,0,"2019/12/17, 21:23:20",False,"2019/12/17, 21:23:20",471.0,2673284.0,3,"If we decide to sample the trace, only then it goes to disk storage, otherwise we evict it from memory, so that we only need to keep spans in memory for a few seconds on average."
Jaeger,59380813,59303550,0,"2019/12/17, 21:23:20",False,"2019/12/17, 21:23:20",471.0,2673284.0,3,Tail-based sampling imposes heavier performance penalty on the traced applications because 100% of traffic needs to be profiled by tracing instrumentation.
Jaeger,59380813,59303550,0,"2019/12/17, 21:23:20",False,"2019/12/17, 21:23:20",471.0,2673284.0,3,"You can read more about head-based and tail-based sampling either in Chapter 3 of my book ( https://www.shkuro.com/books/2019-mastering-distributed-tracing/ ) or in the awesome paper  ""So, you want to trace your distributed system?"
Jaeger,59380813,59303550,0,"2019/12/17, 21:23:20",False,"2019/12/17, 21:23:20",471.0,2673284.0,3,"Key design insights from years of practical experience""  by Raja R. Sambasivan, Rodrigo Fonseca, Ilari Shafer, Gregory R. Ganger ( http://www.pdl.cmu.edu/PDL-FTP/SelfStar/CMU-PDL-14-102.pdf )."
Jaeger,59047238,59044026,1,"2019/11/26, 11:05:04",False,"2019/11/26, 11:05:04",13313.0,524946.0,1,"You can bind it to metrics and logging frameworks, but you don't have to."
Jaeger,59047238,59044026,1,"2019/11/26, 11:05:04",False,"2019/11/26, 11:05:04",13313.0,524946.0,1,"You can simply just call  cfg.NewTracer() , like in this example:"
Jaeger,59047238,59044026,1,"2019/11/26, 11:05:04",False,"2019/11/26, 11:05:04",13313.0,524946.0,1,Source:  https://github.com/jaegertracing/jaeger-client-go/blob/3585cc566102e0ea2225177423e3fcc3d2e5fd7a/config/example_test.go#L88-L105
Jaeger,59047238,59044026,1,"2019/11/26, 11:05:04",False,"2019/11/26, 11:05:04",13313.0,524946.0,1,Check the Jaeger Go Client readme for more information on the metrics/logging integration:  https://github.com/jaegertracing/jaeger-client-go
Jaeger,59260094,59044026,0,"2019/12/10, 06:10:40",True,"2019/12/10, 06:10:40",471.0,2673284.0,2,Jaeger clients are designed to have a minimum set of dependencies.
Jaeger,59260094,59044026,0,"2019/12/10, 06:10:40",True,"2019/12/10, 06:10:40",471.0,2673284.0,2,We don't know if your application is using Prometheus metrics or Zap logger.
Jaeger,59260094,59044026,0,"2019/12/10, 06:10:40",True,"2019/12/10, 06:10:40",471.0,2673284.0,2,This is why  jaeger-client-go  (as well as many other Jaeger clients in other languages) provide two lightweight interfaces for a Logger and MetricsFactory that can be implemented for a specific logs/metrics backend that your application is using.
Jaeger,59260094,59044026,0,"2019/12/10, 06:10:40",True,"2019/12/10, 06:10:40",471.0,2673284.0,2,"Of course, the bindings for Prometheus and Zap are already implemented in the  jaeger-lib  and can be included optionally."
Jaeger,59507206,58947001,1,"2019/12/28, 02:23:31",True,"2019/12/28, 02:23:31",721.0,1563297.0,1,It looks like you have different versions of opentracing.
Jaeger,59507206,58947001,1,"2019/12/28, 02:23:31",True,"2019/12/28, 02:23:31",721.0,1563297.0,1,"The spring-starter-jaeger version 2.x upgrade the version of opentracing, so you might have introduced this breaking changes when you upgraded the dependency version."
Jaeger,64808184,58763972,1,"2020/11/12, 18:51:10",False,"2020/11/12, 18:51:10",163.0,823789.0,0,"Unfortunatelly, the reporter interface is used to report FINISHED spans, it is invoked on JaegerSpan.finish."
Jaeger,64808184,58763972,1,"2020/11/12, 18:51:10",False,"2020/11/12, 18:51:10",163.0,823789.0,0,I presume this is why it does not appear in logs.
Jaeger,58919712,58642294,0,"2019/11/18, 19:08:56",False,"2019/11/18, 19:08:56",721.0,1563297.0,0,"If you are using spring boot with auto configuration, the logs printed using log4j will be instrumented and sent automatically in the span."
Jaeger,59260222,58642294,0,"2019/12/10, 06:26:49",False,"2019/12/10, 06:26:49",471.0,2673284.0,0,"In Go this is not very straightforward, and largely depends on the logging library you use and the interface it provides."
Jaeger,59260222,58642294,0,"2019/12/10, 06:26:49",False,"2019/12/10, 06:26:49",471.0,2673284.0,0,One example is implemented in the  HotROD demo  in the Jaeger repository and it is described in the  blog post accompanying the demo .
Jaeger,59260222,58642294,0,"2019/12/10, 06:26:49",False,"2019/12/10, 06:26:49",471.0,2673284.0,0,It uses  go.uber.org/zap  logging library underneath and allows to write log statements accepting the Context argument:
Jaeger,59260222,58642294,0,"2019/12/10, 06:26:49",False,"2019/12/10, 06:26:49",471.0,2673284.0,0,"Behind the scenes  logger.For(ctx)  captures the current tracing spans and adds all log statements to that span, in addition to sending then to  stdout  as usual."
Jaeger,59260222,58642294,0,"2019/12/10, 06:26:49",False,"2019/12/10, 06:26:49",471.0,2673284.0,0,"To my knowledge,  go.uber.org/zap  does not yet support this mode of logging natively, and therefore requires a wrapper."
Jaeger,59260222,58642294,0,"2019/12/10, 06:26:49",False,"2019/12/10, 06:26:49",471.0,2673284.0,0,"If you use another logging library, than on the high level this is what needs to happen:"
Jaeger,58696749,58504344,2,"2019/11/04, 17:45:56",False,"2019/11/04, 17:45:56",721.0,1563297.0,1,spring-cloud-openfeign  since is from the spring-cloud family should be instrumented automatically once you add  opentracing-spring-jaeger-cloud-starter  the  as stated  here .
Jaeger,58696749,58504344,2,"2019/11/04, 17:45:56",False,"2019/11/04, 17:45:56",721.0,1563297.0,1,"But sometimes (depending on how you create your feign client bean) you need to explicitly expose the bean to the spring context, so that the autoconfiguration can instrument your Feign Client."
Jaeger,58696749,58504344,2,"2019/11/04, 17:45:56",False,"2019/11/04, 17:45:56",721.0,1563297.0,1,Something like this:
Jaeger,58696749,58504344,2,"2019/11/04, 17:45:56",False,"2019/11/04, 17:45:56",721.0,1563297.0,1,it is kotlin but you can adapt.
Jaeger,58440891,58406367,0,"2019/10/18, 00:21:43",True,"2019/10/18, 00:21:43",105.0,8754016.0,1,This was due to the helidon dependency.
Jaeger,58440891,58406367,0,"2019/10/18, 00:21:43",True,"2019/10/18, 00:21:43",105.0,8754016.0,1,https://helidon.io/docs/latest/#/guides/03_quickstart-mp
Jaeger,58440891,58406367,0,"2019/10/18, 00:21:43",True,"2019/10/18, 00:21:43",105.0,8754016.0,1,Also I had to upgrade  opentracing-api  version to  0.33.0
Jaeger,59260378,58134618,0,"2019/12/10, 06:43:54",True,"2019/12/10, 06:43:54",471.0,2673284.0,1,"If you are already using Istio in the deployment, then enabling tracing in it will provide more complete picture of request processing, such as accounting for the time spent in the network between the proxies."
Jaeger,59260378,58134618,0,"2019/12/10, 06:43:54",True,"2019/12/10, 06:43:54",471.0,2673284.0,1,"You also don't need to have full tracing instrumentation in your services as long as they pass through certain headers, then Istio can still provide a pretty accurate picture of the traces (but you cannot capture any business specific data in the traces)."
Jaeger,59260378,58134618,0,"2019/12/10, 06:43:54",True,"2019/12/10, 06:43:54",471.0,2673284.0,1,"Traces generated by Istio will have standardized span names that you can use to reason about the SLAs across the whole infrastructure, whereas explicit tracing instrumentation inside the services can often use different naming schemes, especially when services are written in different languages and using different frameworks."
Jaeger,59260378,58134618,0,"2019/12/10, 06:43:54",True,"2019/12/10, 06:43:54",471.0,2673284.0,1,"For the best of both worlds, I would recommend adding instrumentation inside the services for full fidelity, and also enabling tracing in Istio to capture full picture of request execution (and all network latencies)."
Jaeger,58058871,57870219,0,"2019/09/23, 11:55:31",False,"2019/09/23, 11:55:31",13313.0,524946.0,0,You most likely have a mismatch with your OpenTracing libraries.
Jaeger,58058871,57870219,0,"2019/09/23, 11:55:31",False,"2019/09/23, 11:55:31",13313.0,524946.0,0,It looks like your Servlet Filter integration (io.opentracing.contrib.web.servlet.filter.TracingFilter) is making use of a method that doesn't exist.
Jaeger,57855614,57854200,3,"2019/09/09, 17:02:34",True,"2019/09/09, 17:02:34",3892.0,3603660.0,2,You can use opentracing  java-jdbc  extension  it will works in Quarkus (I didn't test the native mode).
Jaeger,57855614,57854200,3,"2019/09/09, 17:02:34",True,"2019/09/09, 17:02:34",3892.0,3603660.0,2,You need to use the version 0.0.12 as the latest one is based on Opentracing 0.33 but Quarkus use the version 0.31.
Jaeger,57855614,57854200,3,"2019/09/09, 17:02:34",True,"2019/09/09, 17:02:34",3892.0,3603660.0,2,Add the dependency to your pom.xml:
Jaeger,57855614,57854200,3,"2019/09/09, 17:02:34",True,"2019/09/09, 17:02:34",3892.0,3603660.0,2,"Update your application.properties to use the opentracing-jdbc driver, the following are for a Postgres database:"
Jaeger,57855614,57854200,3,"2019/09/09, 17:02:34",True,"2019/09/09, 17:02:34",3892.0,3603660.0,2,You will then saw the SQL queries in Jaeger as spans.
Jaeger,57679113,57608146,1,"2019/08/27, 19:45:11",False,"2019/08/27, 19:45:11",55.0,11863447.0,1,What I eventually did is create a JaegerTraces and annotated with Bean
Jaeger,66138678,57608146,0,"2021/02/10, 16:19:25",True,"2021/02/10, 16:19:25",12820.0,5277820.0,0,"Apache Camel doesn't provide an implementation of  OpenTracing , so you have to add also an implementation to your dependencies."
Jaeger,66138678,57608146,0,"2021/02/10, 16:19:25",True,"2021/02/10, 16:19:25",12820.0,5277820.0,0,For example  Jaeger .
Jaeger,66138678,57608146,0,"2021/02/10, 16:19:25",True,"2021/02/10, 16:19:25",12820.0,5277820.0,0,Maven POM:
Jaeger,66138678,57608146,0,"2021/02/10, 16:19:25",True,"2021/02/10, 16:19:25",12820.0,5277820.0,0,"Also you have to enable OpenTracing for Apache Camel on your Spring Boot application class, see  Spring Boot :"
Jaeger,66138678,57608146,0,"2021/02/10, 16:19:25",True,"2021/02/10, 16:19:25",12820.0,5277820.0,0,"If you are using Spring Boot then you can add the  camel-opentracing-starter  dependency, and turn on OpenTracing by annotating the main class with  @CamelOpenTracing ."
Jaeger,66138678,57608146,0,"2021/02/10, 16:19:25",True,"2021/02/10, 16:19:25",12820.0,5277820.0,0,"The Tracer will be implicitly obtained from the camel context’s Registry, or the ServiceLoader, unless a Tracer bean has been defined by the application."
Jaeger,66138678,57608146,0,"2021/02/10, 16:19:25",True,"2021/02/10, 16:19:25",12820.0,5277820.0,0,Spring Boot application class:
Jaeger,55247113,55239593,1,"2019/03/19, 19:47:45",False,"2019/03/19, 19:47:45",76.0,1496147.0,1,"Could you try using a more recent version of Jaeger:  https://www.jaegertracing.io/docs/latest/getting-started/#all-in-one  - actually 1.11 is now out, so could try that."
Jaeger,55255528,55239593,0,"2019/03/20, 09:29:23",True,"2019/03/20, 09:29:23",48481.0,2504224.0,3,The problem is that you are using  RestTemplate template = new RestTemplate();  to get an instance of the  RestTemplate  to make a REST call.
Jaeger,55255528,55239593,0,"2019/03/20, 09:29:23",True,"2019/03/20, 09:29:23",48481.0,2504224.0,3,Doing that means that Opentracing cannot instrument the call to add necessary HTTP headers.
Jaeger,55255528,55239593,0,"2019/03/20, 09:29:23",True,"2019/03/20, 09:29:23",48481.0,2504224.0,3,Please consider using  @Autowired RestTemplate restTemplate
Jaeger,55225031,55216969,2,"2019/03/18, 17:41:35",True,"2019/03/18, 17:41:35",48481.0,2504224.0,0,"While doing  mvnDebug quarkus:dev  (without  jvm.args ) and placing a breakpoint  here , I see that you all your params are being passed except  quarkus.jaeger.sampler.parameter  which is wrong."
Jaeger,55225031,55216969,2,"2019/03/18, 17:41:35",True,"2019/03/18, 17:41:35",48481.0,2504224.0,0,It should be  quarkus.jaeger.sampler.param
Jaeger,53949856,53914858,5,"2018/12/27, 21:21:46",True,"2019/01/01, 03:38:13",6500.0,9521610.0,1,When you are accessing the service from the pod in the  same namespace  you can use just the service name.
Jaeger,53949856,53914858,5,"2018/12/27, 21:21:46",True,"2019/01/01, 03:38:13",6500.0,9521610.0,1,Example:
Jaeger,53949856,53914858,5,"2018/12/27, 21:21:46",True,"2019/01/01, 03:38:13",6500.0,9521610.0,1,If you are accessing the service from the pod in the  different namespace  you should also specify the namespace.
Jaeger,53949856,53914858,5,"2018/12/27, 21:21:46",True,"2019/01/01, 03:38:13",6500.0,9521610.0,1,Example:
Jaeger,53949856,53914858,5,"2018/12/27, 21:21:46",True,"2019/01/01, 03:38:13",6500.0,9521610.0,1,"To check in what namespace the service is located, use the following command:"
Jaeger,53949856,53914858,5,"2018/12/27, 21:21:46",True,"2019/01/01, 03:38:13",6500.0,9521610.0,1,Note : Changing ConfigMap does not apply it to deployment instantly.
Jaeger,53949856,53914858,5,"2018/12/27, 21:21:46",True,"2019/01/01, 03:38:13",6500.0,9521610.0,1,"Usually, you need to restart all pods in the deployment to apply new ConfigMap values."
Jaeger,53949856,53914858,5,"2018/12/27, 21:21:46",True,"2019/01/01, 03:38:13",6500.0,9521610.0,1,"There is no rolling-restart functionality at the moment, but you can use the following command as a workaround: 
 (replace deployment name and pod name with the real ones)"
Jaeger,54367583,53709029,0,"2019/01/25, 16:46:35",False,"2019/01/25, 16:46:35",13313.0,524946.0,0,"I don't think it's possible at the moment and you should definitely ask for this feature in the mailing list, Gitter or GitHub issue."
Jaeger,54367583,53709029,0,"2019/01/25, 16:46:35",False,"2019/01/25, 16:46:35",13313.0,524946.0,0,"The current assumption is that a clear TChannel connection can be made between the agent and collector(s), all being part of the same trusted network."
Jaeger,54367583,53709029,0,"2019/01/25, 16:46:35",False,"2019/01/25, 16:46:35",13313.0,524946.0,0,"If you are using the Java, Node or C# client, my recommendation in your situation is to have your Jaeger Client to talk directly to the collector."
Jaeger,54367583,53709029,0,"2019/01/25, 16:46:35",False,"2019/01/25, 16:46:35",13313.0,524946.0,0,Look for the env var  JAEGER_ENDPOINT  in the  Client Features  documentation page.
Jaeger,53537859,53518480,0,"2018/11/29, 13:19:37",True,"2018/11/29, 13:19:37",321.0,4125383.0,2,"The problem was, that the Report instance used a NoopSender -- thus ignoring the connection settings."
Jaeger,53537859,53518480,0,"2018/11/29, 13:19:37",True,"2018/11/29, 13:19:37",321.0,4125383.0,2,Using
Jaeger,53537859,53518480,0,"2018/11/29, 13:19:37",True,"2018/11/29, 13:19:37",321.0,4125383.0,2,in your POM will provide an appropriate Sender to the SenderFactory used by Jaeger's SenderResolver::resolve method.
Jaeger,53537859,53518480,0,"2018/11/29, 13:19:37",True,"2018/11/29, 13:19:37",321.0,4125383.0,2,This solved my problem.
Jaeger,52813216,52764397,2,"2018/10/15, 12:05:59",False,"2018/10/15, 12:05:59",13313.0,524946.0,0,The Jaeger tracer (the part that runs along with your application) will send spans via UDP to an agent running on localhost by default.
Jaeger,52813216,52764397,2,"2018/10/15, 12:05:59",False,"2018/10/15, 12:05:59",13313.0,524946.0,0,"If your agent is somewhere else, set the  JAEGER_AGENT_HOST / JAEGER_AGENT_PORT  env vars accordingly."
Jaeger,52813216,52764397,2,"2018/10/15, 12:05:59",False,"2018/10/15, 12:05:59",13313.0,524946.0,0,"If you don't want an agent running on localhost and want to access a Jaeger Collector directly via HTTP, then set the  JAEGER_ENDPOINT  env var."
Jaeger,52813216,52764397,2,"2018/10/15, 12:05:59",False,"2018/10/15, 12:05:59",13313.0,524946.0,0,"More info about these env vars can be found in the  documentation  or here:
 https://github.com/jaegertracing/jaeger-client-java/tree/master/jaeger-core#configuration-via-environment"
Jaeger,52721877,52628054,0,"2018/10/09, 16:09:20",True,"2018/10/09, 16:09:20",13313.0,524946.0,0,"No,  it cannot , but it wouldn't hurt to open an issue there with this suggestion."
Jaeger,57364908,52522049,0,"2019/08/05, 22:22:29",False,"2019/12/18, 12:08:53",5287.0,780798.0,1,There is several components which works together and can fully satisfy your requirement.
Jaeger,57364908,52522049,0,"2019/08/05, 22:22:29",False,"2019/12/18, 12:08:53",5287.0,780798.0,1,"Common  opentracing library , consisted of abstract layer for span, tracer, injectors and extractors, etc."
Jaeger,57364908,52522049,0,"2019/08/05, 22:22:29",False,"2019/12/18, 12:08:53",5287.0,780798.0,1,Official  jaeger-client-csharp .
Jaeger,57364908,52522049,0,"2019/08/05, 22:22:29",False,"2019/12/18, 12:08:53",5287.0,780798.0,1,"Full list of clients can be found  here , which implement  opentracing abstraction layer  mentioned earlier."
Jaeger,57364908,52522049,0,"2019/08/05, 22:22:29",False,"2019/12/18, 12:08:53",5287.0,780798.0,1,"The final piece is the  OpenTracing API for .NET , which is glue between  opentracing library  and  DiagnosticSource  concept in dotnet."
Jaeger,57364908,52522049,0,"2019/08/05, 22:22:29",False,"2019/12/18, 12:08:53",5287.0,780798.0,1,"Actually, the final library has  sample  which uses jaeger csharp implementation of ITracer and configure it as default GlobalTracer."
Jaeger,57364908,52522049,0,"2019/08/05, 22:22:29",False,"2019/12/18, 12:08:53",5287.0,780798.0,1,"At the rest in your Startup.cs, you will end up with something like from that sample (services is IServiceCollection):"
Jaeger,51977735,51878525,0,"2018/08/23, 05:56:58",True,"2018/08/23, 05:56:58",4269.0,1047335.0,1,I resolved this.
Jaeger,51977735,51878525,0,"2018/08/23, 05:56:58",True,"2018/08/23, 05:56:58",4269.0,1047335.0,1,It related to the sample rate.
Jaeger,51977735,51878525,0,"2018/08/23, 05:56:58",True,"2018/08/23, 05:56:58",4269.0,1047335.0,1,"After I configured the  JAEGER_SAMPLER_TYPE  and  JAEGER_SAMPLER_PARAM , I can see the data."
Jaeger,51792533,51720184,0,"2018/08/10, 21:53:47",False,"2018/08/10, 22:38:32",373.0,10206966.0,3,"In server 2 , Install jaeger"
Jaeger,51792533,51720184,0,"2018/08/10, 21:53:47",False,"2018/08/10, 22:38:32",373.0,10206966.0,3,"In server 1, set these environment variables."
Jaeger,51792533,51720184,0,"2018/08/10, 21:53:47",False,"2018/08/10, 22:38:32",373.0,10206966.0,3,"Change the tracer registration application code as below in server 1, so that it will get the configurations from the environment variables."
Jaeger,51792533,51720184,0,"2018/08/10, 21:53:47",False,"2018/08/10, 22:38:32",373.0,10206966.0,3,Hope this works!
Jaeger,51792533,51720184,0,"2018/08/10, 21:53:47",False,"2018/08/10, 22:38:32",373.0,10206966.0,3,Check this link for integrating elasticsearch as the persistence storage backend so that the traces will not remove once the Jaeger instance is stopped.
Jaeger,51792533,51720184,0,"2018/08/10, 21:53:47",False,"2018/08/10, 22:38:32",373.0,10206966.0,3,How to configure Jaeger with elasticsearch?
Jaeger,49311159,49179249,0,"2018/03/16, 01:55:29",True,"2018/03/16, 01:55:29",1116.0,1953109.0,2,I finally figured this out after trying out different combinations.
Jaeger,49311159,49179249,0,"2018/03/16, 01:55:29",True,"2018/03/16, 01:55:29",1116.0,1953109.0,2,This is happening because Jaeger agent is not receiving any UDP packets from my application.
Jaeger,49311159,49179249,0,"2018/03/16, 01:55:29",True,"2018/03/16, 01:55:29",1116.0,1953109.0,2,"You need to tell the tracer where to send UDP packets, which in this case is  docker-machine ip  
I added:"
Jaeger,49311159,49179249,0,"2018/03/16, 01:55:29",True,"2018/03/16, 01:55:29",1116.0,1953109.0,2,and then I was able to see my services in Jaeger UI.
Jaeger,48432561,48095718,0,"2018/01/25, 00:12:09",False,"2018/01/25, 00:12:09",471.0,2673284.0,1,Service graph data must be generated in Jaeger.
Jaeger,48432561,48095718,0,"2018/01/25, 00:12:09",False,"2018/01/25, 00:12:09",471.0,2673284.0,1,Currently it's possible with via a Spark job here:  https://github.com/jaegertracing/spark-dependencies
Jaeger,48432603,46561079,0,"2018/01/25, 00:16:25",True,"2019/12/10, 06:06:47",471.0,2673284.0,2,"The Downloads page ( https://www.jaegertracing.io/download/ ) lists both the Docker images and the raw binaries built for various platforms (Linux, macOS, windows)."
Jaeger,48432603,46561079,0,"2018/01/25, 00:16:25",True,"2019/12/10, 06:06:47",471.0,2673284.0,2,You can also build binaries from source.
Jaeger,66277779,46561079,0,"2021/02/19, 14:43:21",False,"2021/02/19, 14:43:21",169.0,3301685.0,0,"Just to add to Yuris answer, you can also download the source from github -  Github - Jaeger  This is useful for diagnosing issues, or just getting a better understanding of how it all works."
Jaeger,66277779,46561079,0,"2021/02/19, 14:43:21",False,"2021/02/19, 14:43:21",169.0,3301685.0,0,I have run both the released apps and custom versions on both windows and linux servers without issues.
Jaeger,66277779,46561079,0,"2021/02/19, 14:43:21",False,"2021/02/19, 14:43:21",169.0,3301685.0,0,For windows I would recommend running as a service using Nssm.
Jaeger,66277779,46561079,0,"2021/02/19, 14:43:21",False,"2021/02/19, 14:43:21",169.0,3301685.0,0,Nssm details
Jaeger,66277685,66236610,0,"2021/02/19, 14:35:11",False,"2021/02/19, 14:35:11",169.0,3301685.0,0,Elastic search works fine for this.
Jaeger,66277685,66236610,0,"2021/02/19, 14:35:11",False,"2021/02/19, 14:35:11",169.0,3301685.0,0,And Kibana allows you to build nice aggregated views of the traffic.
Jaeger,66277685,66236610,0,"2021/02/19, 14:35:11",False,"2021/02/19, 14:35:11",169.0,3301685.0,0,A recommendation from my experience is to use the  --es.tags-as-fields.dot-replacement  option and specify a character.
Jaeger,66277685,66236610,0,"2021/02/19, 14:35:11",False,"2021/02/19, 14:35:11",169.0,3301685.0,0,This flattens the data structure.
Jaeger,66277685,66236610,0,"2021/02/19, 14:35:11",False,"2021/02/19, 14:35:11",169.0,3301685.0,0,Its very useful because ElasticSearch/Kibana struggle with the tags data as an array.
Jaeger,65888740,65490691,0,"2021/01/25, 18:32:56",True,"2021/01/25, 18:32:56",1869.0,3511252.0,1,I had the same problem.
Jaeger,65888740,65490691,0,"2021/01/25, 18:32:56",True,"2021/01/25, 18:32:56",1869.0,3511252.0,1,Found this page that explains how to configure Thrift sender:  https://github.com/jaegertracing/jaeger-client-csharp/blob/master/src/Senders/Jaeger.Senders.Thrift/README.md
Jaeger,65888740,65490691,0,"2021/01/25, 18:32:56",True,"2021/01/25, 18:32:56",1869.0,3511252.0,1,The C# tutorial does not mention it though ...
Jaeger,65888740,65490691,0,"2021/01/25, 18:32:56",True,"2021/01/25, 18:32:56",1869.0,3511252.0,1,And here is my InitTracer().
Jaeger,65888740,65490691,0,"2021/01/25, 18:32:56",True,"2021/01/25, 18:32:56",1869.0,3511252.0,1,Works fine with Jaeger launched from binary:
Jaeger,65207511,65149873,0,"2020/12/08, 23:57:48",False,"2020/12/08, 23:57:48",357.0,3703933.0,1,I solved it by using this library instead  https://github.com/opentracing-contrib/java-spring-cloud
Jaeger,65207511,65149873,0,"2020/12/08, 23:57:48",False,"2020/12/08, 23:57:48",357.0,3703933.0,1,It seem to have an option to enable or disable different instrumentation feature.
Jaeger,65207511,65149873,0,"2020/12/08, 23:57:48",False,"2020/12/08, 23:57:48",357.0,3703933.0,1,Read about  opentracing.spring.cloud.async.enabled  for more info.
Jaeger,65059275,65059109,1,"2020/11/29, 12:48:21",True,"2020/11/29, 12:48:21",1058.0,1609014.0,0,"Looks like your DaemonSet misses the  hostNetwork  property, to be able to listen on the node IP."
Jaeger,65059275,65059109,1,"2020/11/29, 12:48:21",True,"2020/11/29, 12:48:21",1058.0,1609014.0,0,You can check that article for further info:  https://medium.com/@masroor.hasan/tracing-infrastructure-with-jaeger-on-kubernetes-6800132a677
Jaeger,65299081,65056880,0,"2020/12/15, 04:11:35",False,"2020/12/15, 04:11:35",471.0,2673284.0,0,You have two options:
Jaeger,65299081,65056880,0,"2020/12/15, 04:11:35",False,"2020/12/15, 04:11:35",471.0,2673284.0,0,"For (2), you can pass the environment variable to you applications:"
Jaeger,65299081,65056880,0,"2020/12/15, 04:11:35",False,"2020/12/15, 04:11:35",471.0,2673284.0,0,Additional references:
Jaeger,63654371,63634257,3,"2020/08/30, 08:47:00",True,"2020/08/30, 08:47:00",366.0,10202496.0,1,"By default, OpenTracing doesn't log automatically into span logs, only important messages that Jaeger feels it needs to be logged and is needed for tracing would be there :)."
Jaeger,63654371,63634257,3,"2020/08/30, 08:47:00",True,"2020/08/30, 08:47:00",366.0,10202496.0,1,"The idea is to separate responsibilities between Tracing and Log management,  Check this GitHub discussion ."
Jaeger,63654371,63634257,3,"2020/08/30, 08:47:00",True,"2020/08/30, 08:47:00",366.0,10202496.0,1,An alternative would be to use centralized log management and print traceId &amp; spanId into your logs for troubleshooting and correlating logs and tracing.
Jaeger,63686699,63634257,0,"2020/09/01, 14:23:48",False,"2020/09/01, 14:23:48",363.0,977959.0,0,"As iabughosh said, the main focus on jaeger is traceability, monitoring and performance, not for logging."
Jaeger,63686699,63634257,0,"2020/09/01, 14:23:48",False,"2020/09/01, 14:23:48",363.0,977959.0,0,"Anyway, i found that using the @traced Bean injection you can insert a tag into the current span, that will be printed on Jaeger UI."
Jaeger,63686699,63634257,0,"2020/09/01, 14:23:48",False,"2020/09/01, 14:23:48",363.0,977959.0,0,this example will added the first 4 lines of an excpetion to the Tags seccion.
Jaeger,63686699,63634257,0,"2020/09/01, 14:23:48",False,"2020/09/01, 14:23:48",363.0,977959.0,0,(I use it on my global ExceptionHandler to add more info about the error):
Jaeger,63686699,63634257,0,"2020/09/01, 14:23:48",False,"2020/09/01, 14:23:48",363.0,977959.0,0,}
Jaeger,63686699,63634257,0,"2020/09/01, 14:23:48",False,"2020/09/01, 14:23:48",363.0,977959.0,0,and you will see the little stacktrace at JaegerUI.
Jaeger,63686699,63634257,0,"2020/09/01, 14:23:48",False,"2020/09/01, 14:23:48",363.0,977959.0,0,Hope helps
Jaeger,55568365,55220796,0,"2019/04/08, 10:37:58",False,"2019/04/08, 10:37:58",76.0,1496147.0,0,Did you install the operator on openshift using the instructions listed:  https://github.com/jaegertracing/jaeger-operator#openshift  ?
Jaeger,55568365,55220796,0,"2019/04/08, 10:37:58",False,"2019/04/08, 10:37:58",76.0,1496147.0,0,"Did the operator start up ok, if you not were there errors in the log?"
Jaeger,55568365,55220796,0,"2019/04/08, 10:37:58",False,"2019/04/08, 10:37:58",76.0,1496147.0,0,"The  Creating a new Jaeger Instance  section starts with a link to some examples, including  simple-prod.yaml , which creates a Jaeger instance that uses an Elasticsearch cluster at the specified URL."
Jaeger,55568365,55220796,0,"2019/04/08, 10:37:58",False,"2019/04/08, 10:37:58",76.0,1496147.0,0,You simply run:
Jaeger,67077099,67003774,0,"2021/04/13, 17:44:16",False,"2021/04/13, 17:44:16",193.0,3774803.0,0,It doesn't work in golang grpc client.
Jaeger,67077099,67003774,0,"2021/04/13, 17:44:16",False,"2021/04/13, 17:44:16",193.0,3774803.0,0,I used openTelemetry  load balancing  Another option - use kubernetes to balance requests to backends.
Jaeger,66621994,66597254,0,"2021/03/14, 09:14:12",False,"2021/03/14, 09:14:12",13.0,15381526.0,1,I realized that I had got into a completely wrong direction.
Jaeger,66621994,66597254,0,"2021/03/14, 09:14:12",False,"2021/03/14, 09:14:12",13.0,15381526.0,1,"I thought that I have to access the backend storage to get the trace data, which actually make the problem much more complex."
Jaeger,66621994,66597254,0,"2021/03/14, 09:14:12",False,"2021/03/14, 09:14:12",13.0,15381526.0,1,I got the answer from github discussion and here is the address  https://github.com/jaegertracing/jaeger/discussions/2876#discussioncomment-477176
Jaeger,65650224,65595635,0,"2021/01/10, 06:52:21",False,"2021/01/10, 06:52:21",31.0,3289465.0,0,Can you paste the Collector config file?
Jaeger,65650224,65595635,0,"2021/01/10, 06:52:21",False,"2021/01/10, 06:52:21",31.0,3289465.0,0,It seems you are using the gRPC protocol and it's not supported on the system where the collector is running.
Jaeger,65650224,65595635,0,"2021/01/10, 06:52:21",False,"2021/01/10, 06:52:21",31.0,3289465.0,0,https://github.com/open-telemetry/opentelemetry-collector/blob/master/exporter/otlpexporter/README.md
Jaeger,65910568,65595635,1,"2021/01/27, 01:06:48",True,"2021/01/27, 01:06:48",483.0,4895267.0,0,gRPC port isn't enabled in your jaeger instance.
Jaeger,65910568,65595635,1,"2021/01/27, 01:06:48",True,"2021/01/27, 01:06:48",483.0,4895267.0,0,You can try a docker-compose file like this
Jaeger,65910568,65595635,1,"2021/01/27, 01:06:48",True,"2021/01/27, 01:06:48",483.0,4895267.0,0,And you can connect to it without problems
Jaeger,65853125,65202244,0,"2021/01/22, 23:41:07",False,"2021/01/22, 23:41:07",483.0,4895267.0,0,Remove your dependencies and use the following one that will include also the instrumentation you need
Jaeger,65041185,65040519,0,"2020/11/27, 19:06:58",True,"2020/11/27, 19:06:58",328.0,1221718.0,0,I figured it out... the Jaeger Operator doesn't create a  Service  exposing the metrics endpoints.
Jaeger,65041185,65040519,0,"2020/11/27, 19:06:58",True,"2020/11/27, 19:06:58",328.0,1221718.0,0,These endpoints are just exposed via the pods for the Collector and Query components.
Jaeger,65041185,65040519,0,"2020/11/27, 19:06:58",True,"2020/11/27, 19:06:58",328.0,1221718.0,0,An example from the Collector pod spec:
Jaeger,65041185,65040519,0,"2020/11/27, 19:06:58",True,"2020/11/27, 19:06:58",328.0,1221718.0,0,Note the  admin-http  port there.
Jaeger,65041185,65040519,0,"2020/11/27, 19:06:58",True,"2020/11/27, 19:06:58",328.0,1221718.0,0,"So to get the Prometheus Operator to scrape these metrics, I created a  PodMonitor  which covers both the Collector and Query components because both of them have the  labels/app: jaeger  and  admin-http  ports defined:"
Jaeger,64592134,64486524,0,"2020/10/29, 15:30:59",False,"2020/10/29, 15:30:59",51.0,8394088.0,0,referring to the documentation provided in below link helped to resolve the issue.
Jaeger,64592134,64486524,0,"2020/10/29, 15:30:59",False,"2020/10/29, 15:30:59",51.0,8394088.0,0,java.lang.IllegalStateException: This should not happen as headers() should only be called while a record is processed
Jaeger,64481993,64465875,0,"2020/10/22, 15:14:22",True,"2020/10/22, 16:39:57",2287.0,261708.0,1,I got it working as mentioned below
Jaeger,64481993,64465875,0,"2020/10/22, 15:14:22",True,"2020/10/22, 16:39:57",2287.0,261708.0,1,I would assume that it may not be the right way of exposing the services.
Jaeger,64481993,64465875,0,"2020/10/22, 15:14:22",True,"2020/10/22, 16:39:57",2287.0,261708.0,1,Instead
Jaeger,64356244,64342820,0,"2020/10/14, 18:06:36",True,"2020/10/14, 18:06:36",39034.0,785745.0,0,This is the simplest working example that I was able to find.
Jaeger,64356244,64342820,0,"2020/10/14, 18:06:36",True,"2020/10/14, 18:06:36",39034.0,785745.0,0,Here is a more realistic example that builds the tracer from a configuration.
Jaeger,64536471,64266056,0,"2020/10/26, 13:43:36",False,"2020/10/26, 13:43:36",1157.0,1700378.0,0,"GitLab Helm charts support tracing, and you can configure it with:"
Jaeger,64536471,64266056,0,"2020/10/26, 13:43:36",False,"2020/10/26, 13:43:36",1157.0,1700378.0,0,For more details refer : https://docs.gitlab.com/charts/charts/globals.html#tracing
Jaeger,64209717,64201190,0,"2020/10/05, 16:34:52",False,"2020/10/05, 16:34:52",2576.0,12014434.0,1,According to  istio  documentation:
Jaeger,64209717,64201190,0,"2020/10/05, 16:34:52",False,"2020/10/05, 16:34:52",2576.0,12014434.0,1,"Consult the   Jaeger documentation   to
get started."
Jaeger,64209717,64201190,0,"2020/10/05, 16:34:52",False,"2020/10/05, 16:34:52",2576.0,12014434.0,1,"No special changes are needed for Jaeger to work with
Istio."
Jaeger,64209717,64201190,0,"2020/10/05, 16:34:52",False,"2020/10/05, 16:34:52",2576.0,12014434.0,1,"Once Jaeger is installed, you will need to point Istio proxies to send
traces to the deployment."
Jaeger,64209717,64201190,0,"2020/10/05, 16:34:52",False,"2020/10/05, 16:34:52",2576.0,12014434.0,1,"This can be configured with   --set values.global.tracer.zipkin.address=&lt;jaeger-collector-address&gt;:9411 
at installation time."
Jaeger,64209717,64201190,0,"2020/10/05, 16:34:52",False,"2020/10/05, 16:34:52",2576.0,12014434.0,1,"See the
 ProxyConfig.Tracing 
for advanced configuration such as TLS settings."
Jaeger,64209717,64201190,0,"2020/10/05, 16:34:52",False,"2020/10/05, 16:34:52",2576.0,12014434.0,1,Istio documentation states to use jaeger collector address in  global.tracer.zipkin.address .
Jaeger,64209717,64201190,0,"2020/10/05, 16:34:52",False,"2020/10/05, 16:34:52",2576.0,12014434.0,1,"As for the Jaeger agent host, according to  Jaeger  Operator documentation:"
Jaeger,64209717,64201190,0,"2020/10/05, 16:34:52",False,"2020/10/05, 16:34:52",2576.0,12014434.0,1,"&lt;9&gt; By default, the operator assumes that agents are deployed as
sidecars within the target pods."
Jaeger,64209717,64201190,0,"2020/10/05, 16:34:52",False,"2020/10/05, 16:34:52",2576.0,12014434.0,1,"Specifying the strategy as
“DaemonSet” changes that and makes the operator deploy the agent as
DaemonSet."
Jaeger,64209717,64201190,0,"2020/10/05, 16:34:52",False,"2020/10/05, 16:34:52",2576.0,12014434.0,1,"Note that your tracer client will probably have to override
the “JAEGER_AGENT_HOST” environment variable to use the node’s IP."
Jaeger,64209717,64201190,0,"2020/10/05, 16:34:52",False,"2020/10/05, 16:34:52",2576.0,12014434.0,1,Your tracer client will then most likely need to be told where the agent is located.
Jaeger,64209717,64201190,0,"2020/10/05, 16:34:52",False,"2020/10/05, 16:34:52",2576.0,12014434.0,1,"This is usually done by setting the environment variable   JAEGER_AGENT_HOST   to the value of the Kubernetes node’s IP, for example:"
Jaeger,63837122,63835165,0,"2020/09/10, 23:16:48",True,"2020/09/10, 23:16:48",147.0,1729409.0,0,Solved by adding dependency in pom file on jaeger-thrift.
Jaeger,64240007,63745859,0,"2020/10/07, 11:18:29",True,"2020/10/07, 11:18:29",391.0,6452043.0,1,I enabled instrumentation on the services using those two dependencies:
Jaeger,64240007,63745859,0,"2020/10/07, 11:18:29",True,"2020/10/07, 11:18:29",391.0,6452043.0,1,"And, I used jaeger-client to configure the tracer using environment variables:"
Jaeger,64240007,63745859,0,"2020/10/07, 11:18:29",True,"2020/10/07, 11:18:29",391.0,6452043.0,1,Getting a Tracer Instance:
Jaeger,64240007,63745859,0,"2020/10/07, 11:18:29",True,"2020/10/07, 11:18:29",391.0,6452043.0,1,"Finally, in the dropwizard application, you have to register the tracer like so"
Jaeger,63583340,63581052,1,"2020/08/25, 19:34:35",False,"2020/08/25, 20:14:13",336.0,8095979.0,2,You need to keep double quotes as it is.
Jaeger,63583340,63581052,1,"2020/08/25, 19:34:35",False,"2020/08/25, 20:14:13",336.0,8095979.0,2,An issue has been identified similar to this [1] and has been fixed recently.
Jaeger,63583340,63581052,1,"2020/08/25, 19:34:35",False,"2020/08/25, 20:14:13",336.0,8095979.0,2,Can you try to get the latest WUM updated API Manager 3.1.0 and try enabling Jaeger open tracing?
Jaeger,63583340,63581052,1,"2020/08/25, 19:34:35",False,"2020/08/25, 20:14:13",336.0,8095979.0,2,"Alternatively, this issue will not occur when using &quot;localhost&quot; as the hostname."
Jaeger,63583340,63581052,1,"2020/08/25, 19:34:35",False,"2020/08/25, 20:14:13",336.0,8095979.0,2,[1]  https://github.com/wso2/product-apim/issues/7940
Jaeger,63583870,63581052,0,"2020/08/25, 20:08:00",False,"2020/12/03, 10:54:28",2466.0,3176125.0,0,Run Jager using the docker image as follows.
Jaeger,63583870,63581052,0,"2020/08/25, 20:08:00",False,"2020/12/03, 10:54:28",2466.0,3176125.0,0,Then add the following config to the deployment.toml.
Jaeger,63583870,63581052,0,"2020/08/25, 20:08:00",False,"2020/12/03, 10:54:28",2466.0,3176125.0,0,Side Note: For zipkin you can use the following.
Jaeger,63470131,63221267,0,"2020/08/18, 16:51:58",False,"2020/08/18, 16:51:58",213.0,7933630.0,0,"I'm not sure if what you're looking for exists today per se, but you could accomplish this with OpenTelemetry by writing traces through the LogReporter then using a serverless function to read the cloudwatch stream and send it to Jaeger (or to an OpenTelemetry Collector that sends them to Jaeger)."
Jaeger,63470131,63221267,0,"2020/08/18, 16:51:58",False,"2020/08/18, 16:51:58",213.0,7933630.0,0,You could also write a custom plugin for the OpenTelemetry Collector that read a cloudwatch stream into OTLP then exported it to any endpoint supported by the collector.
Jaeger,63218533,63218451,2,"2020/08/02, 19:38:53",False,"2020/08/02, 19:38:53",721.0,1563297.0,0,You are missing the configuration of Jaeger address.
Jaeger,63218533,63218451,2,"2020/08/02, 19:38:53",False,"2020/08/02, 19:38:53",721.0,1563297.0,0,"Since you did not provided it, it is trying to connect to the default one, which is TCP protocol,  127.0.0.1  and port 5778."
Jaeger,63218533,63218451,2,"2020/08/02, 19:38:53",False,"2020/08/02, 19:38:53",721.0,1563297.0,0,Check for details the configuration section  here .
Jaeger,63198835,63198673,0,"2020/07/31, 23:41:17",True,"2020/07/31, 23:41:17",7166.0,3838328.0,0,You just need to make use of  tags.value  instead of  value  in your match query.
Jaeger,63198835,63198673,0,"2020/07/31, 23:41:17",True,"2020/07/31, 23:41:17",7166.0,3838328.0,0,Below query should help:
Jaeger,62910785,62830150,0,"2020/07/15, 11:34:09",False,"2020/07/15, 11:34:09",184.0,8575474.0,0,"if it throws error then it is unable to reach host, this method is costly I agree but this is the only viable solution I find"
Jaeger,62141984,62112814,0,"2020/06/02, 01:29:32",False,"2020/06/02, 01:34:32",113.0,4573609.0,0,so I give it a try and my answer to the question above are:
Jaeger,62141984,62112814,0,"2020/06/02, 01:29:32",False,"2020/06/02, 01:34:32",113.0,4573609.0,0,"Q1) yes it is possible (congrats to the jaeger team, package pretty easy to grasp with a good documentation)"
Jaeger,62141984,62112814,0,"2020/06/02, 01:29:32",False,"2020/06/02, 01:34:32",113.0,4573609.0,0,"Q2) I did struggle a bit with this one and thanks to  https://github.com/CHOMNANP/jaeger-js-text-map-demo  I implemented a solution by adding a ""textCarrier"" with a ref."
Jaeger,62141984,62112814,0,"2020/06/02, 01:29:32",False,"2020/06/02, 01:34:32",113.0,4573609.0,0,"to the span context formatted as ""FORMAT_TEXT_MAP"" to the message Component 1 was publishing towards Component 2."
Jaeger,62141984,62112814,0,"2020/06/02, 01:29:32",False,"2020/06/02, 01:34:32",113.0,4573609.0,0,Code snipper in C1 on the first API invocation
Jaeger,62141984,62112814,0,"2020/06/02, 01:29:32",False,"2020/06/02, 01:34:32",113.0,4573609.0,0,followed by this part when sending the msg on redis:
Jaeger,62141984,62112814,0,"2020/06/02, 01:29:32",False,"2020/06/02, 01:34:32",113.0,4573609.0,0,The getTextCarrierBySpanObject function is coming from  https://github.com/CHOMNANP/jaeger-js-text-map-demo
Jaeger,62141984,62112814,0,"2020/06/02, 01:29:32",False,"2020/06/02, 01:34:32",113.0,4573609.0,0,Code snippet in C2 receiving the msg from redis
Jaeger,62141984,62112814,0,"2020/06/02, 01:29:32",False,"2020/06/02, 01:34:32",113.0,4573609.0,0,I tested with version 1.13
Jaeger,62141984,62112814,0,"2020/06/02, 01:29:32",False,"2020/06/02, 01:34:32",113.0,4573609.0,0,"So all in all a pretty convincing prototyping exercise with Jaeger, will most probably pilot it now on a real project before trying it in production."
Jaeger,61666591,61619122,0,"2020/05/07, 22:54:39",True,"2020/05/07, 22:54:39",13.0,7069613.0,0,According to  https://helm.sh/docs/chart_template_guide/control_structures/  a string is converted to a boolean of True.
Jaeger,61666591,61619122,0,"2020/05/07, 22:54:39",True,"2020/05/07, 22:54:39",13.0,7069613.0,0,So even a string of false would get evaluated as a Boolean of True by Helm.
Jaeger,61666591,61619122,0,"2020/05/07, 22:54:39",True,"2020/05/07, 22:54:39",13.0,7069613.0,0,"I was using Spinnaker which handles all overrides as a string unless the ""Raw Overrides"" box is checked."
Jaeger,61666591,61619122,0,"2020/05/07, 22:54:39",True,"2020/05/07, 22:54:39",13.0,7069613.0,0,If that box is checked than it converts the string to primitives where applicable.
Jaeger,61666591,61619122,0,"2020/05/07, 22:54:39",True,"2020/05/07, 22:54:39",13.0,7069613.0,0,"My issue was that even though I was overriding with a value of false, Spinnaker would pass that as a string to Helm which would then evaluate that as True."
Jaeger,61666591,61619122,0,"2020/05/07, 22:54:39",True,"2020/05/07, 22:54:39",13.0,7069613.0,0,"The solution was to check the ""Raw Overrides"" box in Spinnaker."
Jaeger,65852979,61431244,0,"2021/01/22, 23:26:39",False,"2021/01/22, 23:26:39",483.0,4895267.0,1,You can set a tag to Span creating a new custom Span
Jaeger,65852979,61431244,0,"2021/01/22, 23:26:39",False,"2021/01/22, 23:26:39",483.0,4895267.0,1,or retrieving the current active Span
Jaeger,61163147,61073781,0,"2020/04/11, 23:15:30",False,"2020/04/11, 23:15:30",67.0,1466001.0,0,Here is a working example:  https://github.com/jobinesh/cloud-native-applications/tree/master/helidon-example-mp-jaeger .
Jaeger,61163147,61073781,0,"2020/04/11, 23:15:30",False,"2020/04/11, 23:15:30",67.0,1466001.0,0,See if that helps you.
Jaeger,61163147,61073781,0,"2020/04/11, 23:15:30",False,"2020/04/11, 23:15:30",67.0,1466001.0,0,"If you are interested, see the details captured here:  https://www.jobinesh.com/2020/04/tracing-api-calls-in-your-helidon.html"
Jaeger,60796655,60793517,1,"2020/03/22, 08:47:00",False,"2020/03/22, 08:47:00",27576.0,1839482.0,0,You need to enable open tracing in nginx ingress controller.
Jaeger,60796655,60793517,1,"2020/03/22, 08:47:00",False,"2020/03/22, 08:47:00",27576.0,1839482.0,0,To enable the instrumentation we must enable OpenTracing in the configuration ConfigMap:
Jaeger,60796655,60793517,1,"2020/03/22, 08:47:00",False,"2020/03/22, 08:47:00",27576.0,1839482.0,0,"To enable or disable instrumentation for a single Ingress, use the enable-opentracing annotation:"
Jaeger,60796655,60793517,1,"2020/03/22, 08:47:00",False,"2020/03/22, 08:47:00",27576.0,1839482.0,0,You must also set the host to use when uploading traces:
Jaeger,60796655,60793517,1,"2020/03/22, 08:47:00",False,"2020/03/22, 08:47:00",27576.0,1839482.0,0,https://kubernetes.github.io/ingress-nginx/user-guide/third-party-addons/opentracing/
Jaeger,62828373,60655072,0,"2020/07/10, 09:20:22",False,"2020/07/10, 09:20:22",81.0,12550162.0,0,Are you connecting it to only elasticsearch or stack like ELK/EFK?.
Jaeger,62828373,60655072,0,"2020/07/10, 09:20:22",False,"2020/07/10, 09:20:22",81.0,12550162.0,0,"I had tried but we cannot configure ELK in jeager-all-one.exe alone in windows without docker.You can do it by running Jeager-collector, Jeager agent and Jeager query individually by mentioning configurations related to ELK."
Jaeger,62828373,60655072,0,"2020/07/10, 09:20:22",False,"2020/07/10, 09:20:22",81.0,12550162.0,0,In Jeager collector and Jeager query you need to set up variables  SPAN_STORAGE_TYPES  and  ES_SERVER_URLS .
Jaeger,61860648,60506609,0,"2020/05/18, 03:34:12",True,"2020/05/18, 03:34:12",829.0,9428538.0,2,To start a jaeger container:
Jaeger,61860648,60506609,0,"2020/05/18, 03:34:12",True,"2020/05/18, 03:34:12",829.0,9428538.0,2,Then you should by able to access to the Jaeger UI at  http://localhost:16686
Jaeger,61860648,60506609,0,"2020/05/18, 03:34:12",True,"2020/05/18, 03:34:12",829.0,9428538.0,2,"Once you've have a Jaeger up and running, you need to configure a Jaeger exporter to forward spans to Jaeger."
Jaeger,61860648,60506609,0,"2020/05/18, 03:34:12",True,"2020/05/18, 03:34:12",829.0,9428538.0,2,This will depends of the language used.
Jaeger,61860648,60506609,0,"2020/05/18, 03:34:12",True,"2020/05/18, 03:34:12",829.0,9428538.0,2,Here  is the straightforward documentation to do so in python.
Jaeger,59790526,59782209,0,"2020/01/17, 17:42:43",False,"2020/01/17, 17:42:43",2105.0,2461073.0,0,"Not out of the box, you have to plug a behaviour into NSB that uses open telemetry
 https://github.com/open-telemetry/opentelemetry-dotnet 
You will have to write custom code."
Jaeger,59790526,59782209,0,"2020/01/17, 17:42:43",False,"2020/01/17, 17:42:43",2105.0,2461073.0,0,"Plus you can do push metrics as well as shown in our app insights, Prometheus and other samples."
Jaeger,59790526,59782209,0,"2020/01/17, 17:42:43",False,"2020/01/17, 17:42:43",2105.0,2461073.0,0,Let's continue the conversation in our support channels?
Jaeger,66675598,59782209,0,"2021/03/17, 16:55:15",False,"2021/03/17, 17:02:16",970.0,5253437.0,0,Not sure if you are still looking for a solution for this.
Jaeger,66675598,59782209,0,"2021/03/17, 16:55:15",False,"2021/03/17, 17:02:16",970.0,5253437.0,0,You should be able to do this currently using the  NServiceBus.Extensions.Diagnostics.OpenTelemetry  package from  nuget .
Jaeger,66675598,59782209,0,"2021/03/17, 16:55:15",False,"2021/03/17, 17:02:16",970.0,5253437.0,0,This is built by Jimmy Bogard and instruments NServiceBus with the required support for  Open Telemetry .
Jaeger,66675598,59782209,0,"2021/03/17, 16:55:15",False,"2021/03/17, 17:02:16",970.0,5253437.0,0,The source for this is available  here .
Jaeger,66675598,59782209,0,"2021/03/17, 16:55:15",False,"2021/03/17, 17:02:16",970.0,5253437.0,0,You can connect this to any backend of your choice that supports  Open Telemetry  including but not limited to  Jaeger  and  Zipkin .
Jaeger,66675598,59782209,0,"2021/03/17, 16:55:15",False,"2021/03/17, 17:02:16",970.0,5253437.0,0,"Additionally, here is an  example  that shows this in action."
Jaeger,59628491,59561262,0,"2020/01/07, 14:31:30",False,"2020/01/07, 14:31:30",13.0,4977370.0,0,Got it!
Jaeger,59628491,59561262,0,"2020/01/07, 14:31:30",False,"2020/01/07, 14:31:30",13.0,4977370.0,0,We need to enable sampling strategy to reach the collector endpoint.
Jaeger,59391314,59372759,0,"2019/12/18, 13:44:56",True,"2019/12/18, 13:44:56",425.0,9112151.0,0,Found out how.
Jaeger,59391314,59372759,0,"2019/12/18, 13:44:56",True,"2019/12/18, 13:44:56",425.0,9112151.0,0,I just added one single line of code into tracing.py of django_opentracing lib:
Jaeger,59391314,59372759,0,"2019/12/18, 13:44:56",True,"2019/12/18, 13:44:56",425.0,9112151.0,0,And the result:
Jaeger,59076610,59076369,1,"2019/11/27, 20:38:47",False,"2019/11/27, 20:38:47",12299.0,369759.0,0,I see..
Jaeger,59076610,59076369,1,"2019/11/27, 20:38:47",False,"2019/11/27, 20:38:47",12299.0,369759.0,0,I thought  @Traced  will be somehow propagated to my db-services/repositories.
Jaeger,59076610,59076369,1,"2019/11/27, 20:38:47",False,"2019/11/27, 20:38:47",12299.0,369759.0,0,"No, I have to put it explicitly:"
Jaeger,59076610,59076369,1,"2019/11/27, 20:38:47",False,"2019/11/27, 20:38:47",12299.0,369759.0,0,That fixes the issue.
Jaeger,58745678,58704937,0,"2019/11/07, 11:30:57",False,"2019/11/07, 12:06:45",2049.0,11207414.0,0,According to the documentation  Remotely Accessing Telemetry Addons .
Jaeger,58745678,58704937,0,"2019/11/07, 11:30:57",False,"2019/11/07, 12:06:45",2049.0,11207414.0,0,There are different ways how to acces telemetry.
Jaeger,58745678,58704937,0,"2019/11/07, 11:30:57",False,"2019/11/07, 12:06:45",2049.0,11207414.0,0,The Recommended way is to create Secure acces using https instead of http.
Jaeger,58745678,58704937,0,"2019/11/07, 11:30:57",False,"2019/11/07, 12:06:45",2049.0,11207414.0,0,Note for both methods:
Jaeger,58745678,58704937,0,"2019/11/07, 11:30:57",False,"2019/11/07, 12:06:45",2049.0,11207414.0,0,This option covers securing the transport layer only.
Jaeger,58745678,58704937,0,"2019/11/07, 11:30:57",False,"2019/11/07, 12:06:45",2049.0,11207414.0,0,You should also configure the telemetry addons to require authentication when exposing them externally.
Jaeger,58745678,58704937,0,"2019/11/07, 11:30:57",False,"2019/11/07, 12:06:45",2049.0,11207414.0,0,Please note that jaeger itself doesn't support authentication methods  github  and workaround using Apache httpd server  here .
Jaeger,58745678,58704937,0,"2019/11/07, 11:30:57",False,"2019/11/07, 12:06:45",2049.0,11207414.0,0,With your recruitments you can use Gateways (SDS)  with self-signed certificates :
Jaeger,58745678,58704937,0,"2019/11/07, 11:30:57",False,"2019/11/07, 12:06:45",2049.0,11207414.0,0,a .)
Jaeger,58745678,58704937,0,"2019/11/07, 11:30:57",False,"2019/11/07, 12:06:45",2049.0,11207414.0,0,Make sure your that during istio instalation youe have enabled SDS at ingress gateway  --set gateways.istio-ingressgateway.sds.enabled=true  and  --set tracing.enabled=true  for tacing purposes.
Jaeger,58745678,58704937,0,"2019/11/07, 11:30:57",False,"2019/11/07, 12:06:45",2049.0,11207414.0,0,b .)
Jaeger,58745678,58704937,0,"2019/11/07, 11:30:57",False,"2019/11/07, 12:06:45",2049.0,11207414.0,0,Create self signed certificates for testing purposes you can use this  example and repository .
Jaeger,58745678,58704937,0,"2019/11/07, 11:30:57",False,"2019/11/07, 12:06:45",2049.0,11207414.0,0,c .)
Jaeger,58745678,58704937,0,"2019/11/07, 11:30:57",False,"2019/11/07, 12:06:45",2049.0,11207414.0,0,Please follow  Generate client and server certificates and keys   and  Configure a TLS ingress gateway using SDS .
Jaeger,58745678,58704937,0,"2019/11/07, 11:30:57",False,"2019/11/07, 12:06:45",2049.0,11207414.0,0,Create Virtualservice and Gateway:
Jaeger,58745678,58704937,0,"2019/11/07, 11:30:57",False,"2019/11/07, 12:06:45",2049.0,11207414.0,0,Hope this help
Jaeger,58561231,58514716,0,"2019/10/25, 18:12:58",False,"2019/10/25, 18:12:58",721.0,1563297.0,1,The prometheus-es-exporter provides a way to create metrics using queries.
Jaeger,58561231,58514716,0,"2019/10/25, 18:12:58",False,"2019/10/25, 18:12:58",721.0,1563297.0,1,For further details you can check  prometheus-es-exporter#query-metrics
Jaeger,58138322,58011545,0,"2019/09/27, 19:26:25",False,"2019/09/27, 19:26:25",1.0,4734979.0,0,Great question and a very popular one too.
Jaeger,58138322,58011545,0,"2019/09/27, 19:26:25",False,"2019/09/27, 19:26:25",1.0,4734979.0,0,"In short, yes, code changes are required."
Jaeger,58138322,58011545,0,"2019/09/27, 19:26:25",False,"2019/09/27, 19:26:25",1.0,4734979.0,0,Not just in one service but in all the services that a request will go through.
Jaeger,58138322,58011545,0,"2019/09/27, 19:26:25",False,"2019/09/27, 19:26:25",1.0,4734979.0,0,You need to instrument all services to get continuous traces that will be able to tell you the story of a request as it travels through the system.
Jaeger,57474156,57473539,0,"2019/08/13, 11:45:41",False,"2019/08/13, 11:45:41",1642.0,2255344.0,1,"I believe you can reuse Elasticsearch for multiple purposes - each would use a different set of indices, so separation is good."
Jaeger,57474156,57473539,0,"2019/08/13, 11:45:41",False,"2019/08/13, 11:45:41",1642.0,2255344.0,1,from:  https://www.jaegertracing.io/docs/1.11/deployment/  :
Jaeger,57474156,57473539,0,"2019/08/13, 11:45:41",False,"2019/08/13, 11:45:41",1642.0,2255344.0,1,Collectors require a persistent storage backend.
Jaeger,57474156,57473539,0,"2019/08/13, 11:45:41",False,"2019/08/13, 11:45:41",1642.0,2255344.0,1,Cassandra and Elasticsearch are the primary supported storage backends
Jaeger,57474156,57473539,0,"2019/08/13, 11:45:41",False,"2019/08/13, 11:45:41",1642.0,2255344.0,1,"Tying the networking all together, a docker-compose example:
 How to configure Jaeger with elasticsearch?"
Jaeger,57476852,57473539,0,"2019/08/13, 14:30:22",False,"2019/08/13, 14:30:22",4433.0,243104.0,1,"While this isn't exactly what you asked, it sounds like what you're trying to achieve is seeing tracing for your JMS calls in Jaegar."
Jaeger,57476852,57473539,0,"2019/08/13, 14:30:22",False,"2019/08/13, 14:30:22",4433.0,243104.0,1,"If that is the case, you could use an OpenTracing tracing solution for JMS or ActiveMQ to report tracing data directly to Jaegar."
Jaeger,57476852,57473539,0,"2019/08/13, 14:30:22",False,"2019/08/13, 14:30:22",4433.0,243104.0,1,Here's one potential solution I found with a quick google.
Jaeger,57476852,57473539,0,"2019/08/13, 14:30:22",False,"2019/08/13, 14:30:22",4433.0,243104.0,1,There may be others.
Jaeger,57476852,57473539,0,"2019/08/13, 14:30:22",False,"2019/08/13, 14:30:22",4433.0,243104.0,1,https://github.com/opentracing-contrib/java-jms
Jaeger,57420811,57419866,0,"2019/08/09, 00:26:00",False,"2019/08/09, 00:26:00",121.0,5799778.0,0,"Maybe you should check whether the application services which you set up in a hurry are both in the same azure resource group as the VM running the Jaeger all-in-one instance, otherwise the second application might not be able to communicate with the Jaeger instance at all."
Jaeger,57295501,57278976,0,"2019/07/31, 19:41:53",False,"2019/07/31, 19:41:53",13313.0,524946.0,0,"It doesn't really matter in which language your individual microservices are written, you should see them all in the same trace."
Jaeger,57295501,57278976,0,"2019/07/31, 19:41:53",False,"2019/07/31, 19:41:53",13313.0,524946.0,0,"Given that you are seeing three traces instead of one trace with three spans, it appears that the context propagation isn't working."
Jaeger,57295501,57278976,0,"2019/07/31, 19:41:53",False,"2019/07/31, 19:41:53",13313.0,524946.0,0,"Check your HTTP client in your nodejs services, they should perform the  ""inject"" operation ."
Jaeger,57295501,57278976,0,"2019/07/31, 19:41:53",False,"2019/07/31, 19:41:53",13313.0,524946.0,0,"Your service ""B"" and ""C"" should then perform the ""extract"" operation."
Jaeger,57295501,57278976,0,"2019/07/31, 19:41:53",False,"2019/07/31, 19:41:53",13313.0,524946.0,0,"If you haven't yet, check  Yuri Shkuro's OpenTracing Tutorial ."
Jaeger,57295501,57278976,0,"2019/07/31, 19:41:53",False,"2019/07/31, 19:41:53",13313.0,524946.0,0,"The lesson 3 is about the context propagation, including the inject and extract operations."
Jaeger,57295501,57278976,0,"2019/07/31, 19:41:53",False,"2019/07/31, 19:41:53",13313.0,524946.0,0,"I'm not quite sure how it works in the NodeJS world, but in Java, it should be sufficient to have the  opentracing-contrib/java-web-servlet-filter  instrumentation library in your classpath, as it would register the necessary pieces in the right hooks and make the trace context available for each incoming HTTP request."
Jaeger,57039417,57038416,0,"2019/07/15, 15:06:42",False,"2019/07/15, 15:06:42",4887.0,6700019.0,0,It seems that PyInstaller can't resolve  jaeger_client  import.
Jaeger,57039417,57038416,0,"2019/07/15, 15:06:42",False,"2019/07/15, 15:06:42",4887.0,6700019.0,0,So an easy way is to just edit your spec file and add the whole  jaeger_client  library as a  Tree  class:
Jaeger,57039417,57038416,0,"2019/07/15, 15:06:42",False,"2019/07/15, 15:06:42",4887.0,6700019.0,0,And generate your executable with  pyinstaller script.spec .
Jaeger,55241560,55236000,1,"2019/03/19, 14:53:31",False,"2019/03/19, 14:53:31",9399.0,502575.0,0,You can create a  NodePort  service using the  app: jaeger  selector to expose the UI outside the cluster.
Jaeger,55487339,55236000,0,"2019/04/03, 07:54:28",False,"2019/04/03, 07:54:28",44552.0,308174.0,0,"kubectl port-forward  command default is expose to  localhost  network only, try to add  --address 0.0.0.0"
Jaeger,55487339,55236000,0,"2019/04/03, 07:54:28",False,"2019/04/03, 07:54:28",44552.0,308174.0,0,see  kubectl command reference
Jaeger,55490575,55236000,0,"2019/04/03, 11:33:16",False,"2019/04/03, 11:33:16",5244.0,5564578.0,1,There are several ways of doing this.
Jaeger,55490575,55236000,0,"2019/04/03, 11:33:16",False,"2019/04/03, 11:33:16",5244.0,5564578.0,1,The  port-forward  works fine on Google Cloud Shell.
Jaeger,55490575,55236000,0,"2019/04/03, 11:33:16",False,"2019/04/03, 11:33:16",5244.0,5564578.0,1,"If you are using GKE, then I strongly recommend using Cloud Shell, and  port-forward  as it is the easiest way."
Jaeger,55490575,55236000,0,"2019/04/03, 11:33:16",False,"2019/04/03, 11:33:16",5244.0,5564578.0,1,"On other clouds, I don't know."
Jaeger,55490575,55236000,0,"2019/04/03, 11:33:16",False,"2019/04/03, 11:33:16",5244.0,5564578.0,1,What is suggesting Stefan would work.
Jaeger,55490575,55236000,0,"2019/04/03, 11:33:16",False,"2019/04/03, 11:33:16",5244.0,5564578.0,1,"You can edit the jaeger service with  kubectl edit svc jaeger-query , then change the type of the service from  ClusterIP  to  NodePort ."
Jaeger,55490575,55236000,0,"2019/04/03, 11:33:16",False,"2019/04/03, 11:33:16",5244.0,5564578.0,1,"Finally, you can access the service with  NODE_IP:PORT  (any node)."
Jaeger,55490575,55236000,0,"2019/04/03, 11:33:16",False,"2019/04/03, 11:33:16",5244.0,5564578.0,1,"If you do  kubectl get svc , you will see the new port assigned to the service."
Jaeger,55490575,55236000,0,"2019/04/03, 11:33:16",False,"2019/04/03, 11:33:16",5244.0,5564578.0,1,Note: You might need to open a firewall rule for that port.
Jaeger,55490575,55236000,0,"2019/04/03, 11:33:16",False,"2019/04/03, 11:33:16",5244.0,5564578.0,1,"You can also make the service type  LoadBalancer , if you have a control plane to set up an external IP address."
Jaeger,55490575,55236000,0,"2019/04/03, 11:33:16",False,"2019/04/03, 11:33:16",5244.0,5564578.0,1,"This would be a more expensive solution, but you would have a dedicated external IP address for your service."
Jaeger,55490575,55236000,0,"2019/04/03, 11:33:16",False,"2019/04/03, 11:33:16",5244.0,5564578.0,1,"There are more ways, but I would say these are the appropriate ones."
Jaeger,55277542,55222810,0,"2019/03/21, 11:44:21",True,"2019/03/21, 11:44:21",654.0,955379.0,0,This issue looks more to have to do with Java it self then either Opentracing and Jaeger.
Jaeger,55277542,55222810,0,"2019/03/21, 11:44:21",True,"2019/03/21, 11:44:21",654.0,955379.0,0,as  ex.getStackTrace()  is more of the problem.
Jaeger,55277542,55222810,0,"2019/03/21, 11:44:21",True,"2019/03/21, 11:44:21",654.0,955379.0,0,As it should be more like
Jaeger,55277542,55222810,0,"2019/03/21, 11:44:21",True,"2019/03/21, 11:44:21",654.0,955379.0,0,Problem solved.
Jaeger,54867389,54675467,0,"2019/02/25, 15:32:18",False,"2019/02/25, 15:32:18",13313.0,524946.0,0,Setting a baggage item is  not  the same as setting an HTTP header.
Jaeger,54867389,54675467,0,"2019/02/25, 15:32:18",False,"2019/02/25, 15:32:18",13313.0,524946.0,0,You should use your HTTP client (not shown in your example) to set the HTTP header.
Jaeger,54867389,54675467,0,"2019/02/25, 15:32:18",False,"2019/02/25, 15:32:18",13313.0,524946.0,0,"Baggage items might or not be available as individual HTTP headers: it's a detail implementation of the underlying tracer, such as Jaeger's."
Jaeger,54367285,54365010,0,"2019/01/25, 16:30:16",False,"2019/01/25, 16:30:16",13313.0,524946.0,0,"Each ""dot"" would be a new child node in the YAML file, like:"
Jaeger,54367285,54365010,0,"2019/01/25, 16:30:16",False,"2019/01/25, 16:30:16",13313.0,524946.0,0,"Make sure to run the process with the env var SPAN_STORAGE_TYPE set to elasticsearch, like:"
Jaeger,54367285,54365010,0,"2019/01/25, 16:30:16",False,"2019/01/25, 16:30:16",13313.0,524946.0,0,(as seen on  https://github.com/jaegertracing/jaeger/issues/1299 )
Jaeger,54325706,53885456,1,"2019/01/23, 13:00:46",False,"2019/01/23, 13:00:46",13313.0,524946.0,1,"So as per this config, does it mean that I'm sampling each and every trace or just the few traces randomly?"
Jaeger,54325706,53885456,1,"2019/01/23, 13:00:46",False,"2019/01/23, 13:00:46",13313.0,524946.0,1,Using the sampler type as  const  with  1  as the value means that you are sampling everything.
Jaeger,54325706,53885456,1,"2019/01/23, 13:00:46",False,"2019/01/23, 13:00:46",13313.0,524946.0,1,"Mysteriously, when I'm passing random inputs to create spans for my microservices, the spans are getting generated only after 4 to 5 minutes."
Jaeger,54325706,53885456,1,"2019/01/23, 13:00:46",False,"2019/01/23, 13:00:46",13313.0,524946.0,1,I would like to understand this configuration spec more but not able to.
Jaeger,54325706,53885456,1,"2019/01/23, 13:00:46",False,"2019/01/23, 13:00:46",13313.0,524946.0,1,There are several things that might be happening.
Jaeger,54325706,53885456,1,"2019/01/23, 13:00:46",False,"2019/01/23, 13:00:46",13313.0,524946.0,1,"You might not be closing spans, for instance."
Jaeger,54325706,53885456,1,"2019/01/23, 13:00:46",False,"2019/01/23, 13:00:46",13313.0,524946.0,1,I recommend reading the following two blog posts to try to understand what might be happening:
Jaeger,54325706,53885456,1,"2019/01/23, 13:00:46",False,"2019/01/23, 13:00:46",13313.0,524946.0,1,Help!
Jaeger,54325706,53885456,1,"2019/01/23, 13:00:46",False,"2019/01/23, 13:00:46",13313.0,524946.0,1,Something is wrong with my Jaeger installation!
Jaeger,54325706,53885456,1,"2019/01/23, 13:00:46",False,"2019/01/23, 13:00:46",13313.0,524946.0,1,The life of a span
Jaeger,52329845,52145774,0,"2018/09/14, 13:15:30",True,"2018/09/14, 13:15:30",360.0,1796107.0,0,"Your best chance is to get the data from whatever storage the Jaeger collector is using (Cassandra, Elastic.)"
Jaeger,52329845,52145774,0,"2018/09/14, 13:15:30",True,"2018/09/14, 13:15:30",360.0,1796107.0,0,( https://www.jaegertracing.io/docs/1.6/deployment/  )
Jaeger,52329845,52145774,0,"2018/09/14, 13:15:30",True,"2018/09/14, 13:15:30",360.0,1796107.0,0,My suggestion is to store in Elastic and use Kibana to accomplish what you need.
Jaeger,65919685,52145774,0,"2021/01/27, 14:59:44",False,"2021/01/27, 14:59:44",1869.0,3511252.0,0,Old topic but the current version of Jaeger Query UI is a single page app and has an underlying API that allows the same queries capabilities as the UI.
Jaeger,52167852,52104427,6,"2018/09/04, 16:48:04",False,"2018/09/04, 16:48:04",8336.0,1773866.0,-1,You're using  Camden  release train with boot  2.0  and Sleuth  2.0 .
Jaeger,52167852,52104427,6,"2018/09/04, 16:48:04",False,"2018/09/04, 16:48:04",8336.0,1773866.0,-1,That's completely incompatible.
Jaeger,52167852,52104427,6,"2018/09/04, 16:48:04",False,"2018/09/04, 16:48:04",8336.0,1773866.0,-1,"Please generate a project from start.spring.io from scratch, please don't put any versions manually for spring cloud projects, and please try again."
Jaeger,52167852,52104427,6,"2018/09/04, 16:48:04",False,"2018/09/04, 16:48:04",8336.0,1773866.0,-1,Try using  Finchley  release train instead of  Camden
Jaeger,52722089,50906213,0,"2018/10/09, 16:19:28",True,"2018/10/09, 16:19:28",13313.0,524946.0,1,"The problem is that if I kill the Docker running the jaeger collector- systemctl stop docker and later restart docker and jaegertracing/all-in-one, the services are no longer up at  http://localhost:16686/api/services"
Jaeger,52722089,50906213,0,"2018/10/09, 16:19:28",True,"2018/10/09, 16:19:28",13313.0,524946.0,1,That's because you are using the in-memory storage.
Jaeger,52722089,50906213,0,"2018/10/09, 16:19:28",True,"2018/10/09, 16:19:28",13313.0,524946.0,1,"If you stop and start the container, the storage is reset, so, you'll effectively lose your data."
Jaeger,52722089,50906213,0,"2018/10/09, 16:19:28",True,"2018/10/09, 16:19:28",13313.0,524946.0,1,"For production purposes, you should use a backing storage like Cassandra or Elasticsearch."
Jaeger,52722089,50906213,0,"2018/10/09, 16:19:28",True,"2018/10/09, 16:19:28",13313.0,524946.0,1,Does the Jaeger collector needs to be running before starting the Jaeger clients?
Jaeger,52722089,50906213,0,"2018/10/09, 16:19:28",True,"2018/10/09, 16:19:28",13313.0,524946.0,1,"No, but spans reported by clients when the collector isn't available might get dropped."
Jaeger,52722089,50906213,0,"2018/10/09, 16:19:28",True,"2018/10/09, 16:19:28",13313.0,524946.0,1,"Note that clients will send spans to the agent by default, and will not contact the collector directly."
Jaeger,52722089,50906213,0,"2018/10/09, 16:19:28",True,"2018/10/09, 16:19:28",13313.0,524946.0,1,"So, if the agent isn't available, spans might get dropped as well."
Jaeger,52722089,50906213,0,"2018/10/09, 16:19:28",True,"2018/10/09, 16:19:28",13313.0,524946.0,1,how can I flush the memory used by Jaeger OpenTracing so that my host doesn't run out of memory?
Jaeger,52722089,50906213,0,"2018/10/09, 16:19:28",True,"2018/10/09, 16:19:28",13313.0,524946.0,1,Use the configuration option  --memory.max-traces .
Jaeger,52722089,50906213,0,"2018/10/09, 16:19:28",True,"2018/10/09, 16:19:28",13313.0,524946.0,1,"With this option, older traces will get overwritten by new ones once this limit is reached."
Jaeger,52813470,50118186,1,"2018/10/15, 12:20:43",True,"2018/10/15, 12:20:43",13313.0,524946.0,1,But when I wrap the Spring-Boot application inside a Docker container with the following docker-compose file and start the Jaeger client again I can't see any traces
Jaeger,52813470,50118186,1,"2018/10/15, 12:20:43",True,"2018/10/15, 12:20:43",13313.0,524946.0,1,"That's because the Jaeger client will, by default, send the spans via UDP to an agent at  localhost ."
Jaeger,52813470,50118186,1,"2018/10/15, 12:20:43",True,"2018/10/15, 12:20:43",13313.0,524946.0,1,"When your application is running in a Docker container, your  localhost  there is the container itself, so that the spans are lost."
Jaeger,52813470,50118186,1,"2018/10/15, 12:20:43",True,"2018/10/15, 12:20:43",13313.0,524946.0,1,"As you are linking the Jaeger container with your application, you may want to get it solved by exporting the env var  JAEGER_AGENT_HOST  to  jaeger ."
Jaeger,50339003,49909075,0,"2018/05/15, 00:05:40",True,"2018/05/15, 00:05:40",611.0,118116.0,1,Turns out I don't need neither Zipkin nor Jaeger to have my traces on Stackdriver.
Jaeger,50339003,49909075,0,"2018/05/15, 00:05:40",True,"2018/05/15, 00:05:40",611.0,118116.0,1,All that is needed is a deployment of  zipkin-collector  and a service to point to it and all my traces are now reporting as expected on GCP Stackdriver.
Jaeger,49626058,49624555,0,"2018/04/03, 12:00:37",False,"2018/04/04, 08:31:53",81.0,2819181.0,0,"Not jaeger, able to send traces to zipkin server, using zipkin-simple."
Jaeger,49626058,49624555,0,"2018/04/03, 12:00:37",False,"2018/04/04, 08:31:53",81.0,2819181.0,0,Related code is in repository  https://github.com/debmalya/calculator
Jaeger,49637708,49624555,0,"2018/04/03, 22:24:22",False,"2018/04/03, 22:24:22",21.0,9593079.0,2,node-jaeger-client currently doesn't run in the browser.
Jaeger,49637708,49624555,0,"2018/04/03, 22:24:22",False,"2018/04/03, 22:24:22",21.0,9593079.0,2,There is ongoing  work  to make jaeger-client browser friendly.
Jaeger,49637708,49624555,0,"2018/04/03, 22:24:22",False,"2018/04/03, 22:24:22",21.0,9593079.0,2,This issue:  readFileSync is not a function  contains relevant information to why you're seeing the error message.
Jaeger,49637708,49624555,0,"2018/04/03, 22:24:22",False,"2018/04/03, 22:24:22",21.0,9593079.0,2,"Essentially, you're trying to run jaeger-client (a nodejs library) using react-scripts which doesn't contain the modules that jaeger-client needs."
Jaeger,44337567,43766832,0,"2017/06/03, 00:18:56",True,"2017/06/03, 00:18:56",471.0,2673284.0,0,There are two issues here.
Jaeger,44337567,43766832,0,"2017/06/03, 00:18:56",True,"2017/06/03, 00:18:56",471.0,2673284.0,0,One is that your code sets the port for Jaeger client to 5775.
Jaeger,44337567,43766832,0,"2017/06/03, 00:18:56",True,"2017/06/03, 00:18:56",471.0,2673284.0,0,"This port expects a different data model than what Node.js client sends, you can remove the  agentHost  and  agentPort  parameters and rely on defaults."
Jaeger,44337567,43766832,0,"2017/06/03, 00:18:56",True,"2017/06/03, 00:18:56",471.0,2673284.0,0,The second issue is that you're running the Docker image without exposing the required UDP port.
Jaeger,44337567,43766832,0,"2017/06/03, 00:18:56",True,"2017/06/03, 00:18:56",471.0,2673284.0,0,"The correct command is shown in the  documentation , as of today it should be this (one long line):"
Jaeger,54372644,54371809,0,"2019/01/25, 22:56:14",False,"2019/01/25, 22:56:14",324.0,5600810.0,0,Liberty does not have Open Tracing Tracer implementation for Jaeger yet.
Jaeger,54372644,54371809,0,"2019/01/25, 22:56:14",False,"2019/01/25, 22:56:14",324.0,5600810.0,0,We have a sample Tracer implementation for Zipkin.
Jaeger,54372644,54371809,0,"2019/01/25, 22:56:14",False,"2019/01/25, 22:56:14",324.0,5600810.0,0,You can find it at  https://github.com/WASdev/sample.opentracing.zipkintracer .
Jaeger,54372644,54371809,0,"2019/01/25, 22:56:14",False,"2019/01/25, 22:56:14",324.0,5600810.0,0,Jaegar claims it is backward compatible with Zipkin by accepting spans in Zipkin formats over HTTP.
Jaeger,54372644,54371809,0,"2019/01/25, 22:56:14",False,"2019/01/25, 22:56:14",324.0,5600810.0,0,Feel free to open a RFE at  https://developer.ibm.com/wasdev/help/submit-rfe/
Jaeger,52894461,52881039,0,"2018/10/19, 17:29:25",False,"2018/10/19, 17:29:25",13313.0,524946.0,0,This is most likely caused by the static assets  not  being included in the binary.
Jaeger,52894461,52881039,0,"2018/10/19, 17:29:25",False,"2018/10/19, 17:29:25",13313.0,524946.0,0,You can try that out by running the binary you compiled.
Jaeger,52894461,52881039,0,"2018/10/19, 17:29:25",False,"2018/10/19, 17:29:25",13313.0,524946.0,0,"Instead of compiling on your own, a better approach would be to get the official binaries from the releases page and build your Docker container using that."
Jaeger,52894461,52881039,0,"2018/10/19, 17:29:25",False,"2018/10/19, 17:29:25",13313.0,524946.0,0,https://github.com/jaegertracing/jaeger/releases/latest
Jaeger,56227367,56156809,0,"2019/05/20, 22:55:44",True,"2019/05/20, 22:55:44",386.0,6692626.0,9,"This is a limitation in the Serilog logger factory implementation; in particular, Serilog currently ignores added providers and assumes that Serilog Sinks will replace them instead."
Jaeger,56227367,56156809,0,"2019/05/20, 22:55:44",True,"2019/05/20, 22:55:44",386.0,6692626.0,9,"So, the solutions is implementaion a simple  WriteTo.OpenTracing()  method to connect Serilog directly to  OpenTracing"
Jaeger,62140570,54597464,0,"2020/06/01, 23:33:01",False,"2020/06/01, 23:33:01",1111.0,898472.0,0,I had this problem while using gunicorn with gevent as the worker class.
Jaeger,62140570,54597464,0,"2020/06/01, 23:33:01",False,"2020/06/01, 23:33:01",1111.0,898472.0,0,To resolve and get cloud traces working the solution was to monkey patch grpc like so
Jaeger,62140570,54597464,0,"2020/06/01, 23:33:01",False,"2020/06/01, 23:33:01",1111.0,898472.0,0,See  https://github.com/grpc/grpc/issues/4629#issuecomment-376962677
Jaeger,32155765,32155133,4,"2015/08/22, 14:48:32",True,"2016/11/03, 17:57:34",nan,nan,13,For your exact question create a character class
Jaeger,32155765,32155133,4,"2015/08/22, 14:48:32",True,"2016/11/03, 17:57:34",nan,nan,13,And then you can just add * on the end to get 0 or unlimited number of them or alternatively 1 or an unlimited number with +
Jaeger,32155765,32155133,4,"2015/08/22, 14:48:32",True,"2016/11/03, 17:57:34",nan,nan,13,or
Jaeger,32155765,32155133,4,"2015/08/22, 14:48:32",True,"2016/11/03, 17:57:34",nan,nan,13,"Also there is this below, found at  https://regex101.com/  under the library tab when searching for json"
Jaeger,32155765,32155133,4,"2015/08/22, 14:48:32",True,"2016/11/03, 17:57:34",nan,nan,13,"This should match any valid json, you can also test it at the website above"
Jaeger,32155765,32155133,4,"2015/08/22, 14:48:32",True,"2016/11/03, 17:57:34",nan,nan,13,EDIT:
Jaeger,32155765,32155133,4,"2015/08/22, 14:48:32",True,"2016/11/03, 17:57:34",nan,nan,13,Link to the regex
Jaeger,56941159,56863669,0,"2019/07/08, 22:06:55",False,"2019/07/08, 22:06:55",504.0,9187876.0,2,"No, there is no out-of-the-box possibility to change the HTTP header name."
Jaeger,56941159,56863669,0,"2019/07/08, 22:06:55",False,"2019/07/08, 22:06:55",504.0,9187876.0,2,"However, you can enable B3 header propagation with  opentracing.jaeger.enable-b3-propagation=true ."
Jaeger,56941159,56863669,0,"2019/07/08, 22:06:55",False,"2019/07/08, 22:06:55",504.0,9187876.0,2,"To configure Traefik to send the trace data as B3 headers, see  https://github.com/containous/traefik/blob/master/docs/content/observability/tracing/jaeger.md#propagation ."
Jaeger,56941159,56863669,0,"2019/07/08, 22:06:55",False,"2019/07/08, 22:06:55",504.0,9187876.0,2,traceContextHeaderName  should also be configured as  X-B3-TraceId  then.
Jaeger,44370244,44166525,0,"2017/06/05, 16:35:47",True,"2017/06/05, 16:35:47",8336.0,1773866.0,2,There's an ongoing discussion over here -  https://github.com/spring-cloud/spring-cloud-sleuth/issues/599  .
Jaeger,44370244,44166525,0,"2017/06/05, 16:35:47",True,"2017/06/05, 16:35:47",8336.0,1773866.0,2,In general we don't explicitly use the OpenTracing API but we are Zipkin compatible in terms of header propagation.
Jaeger,44370244,44166525,0,"2017/06/05, 16:35:47",True,"2017/06/05, 16:35:47",8336.0,1773866.0,2,You can also manipulate the header names as you wish so if any sort of library you're using requires other header names for span / trace etc.
Jaeger,44370244,44166525,0,"2017/06/05, 16:35:47",True,"2017/06/05, 16:35:47",8336.0,1773866.0,2,then you can set it yourself as you want to.
Jaeger,53370207,44166525,0,"2018/11/19, 09:38:53",False,"2018/11/19, 09:38:53",463.0,1749786.0,14,Spring Sleuth is now OpenTracing compatible.
Jaeger,53370207,44166525,0,"2018/11/19, 09:38:53",False,"2018/11/19, 09:38:53",463.0,1749786.0,14,All you have to do is use OpenTracing Jars in your class path.
Jaeger,53370207,44166525,0,"2018/11/19, 09:38:53",False,"2018/11/19, 09:38:53",463.0,1749786.0,14,You can then use Sleuth-Zipkin to send instrumentation data to Jaeger's Zipkin collector.
Jaeger,53370207,44166525,0,"2018/11/19, 09:38:53",False,"2018/11/19, 09:38:53",463.0,1749786.0,14,This way you achieve everything you want with minimal configuration.
Jaeger,53370207,44166525,0,"2018/11/19, 09:38:53",False,"2018/11/19, 09:38:53",463.0,1749786.0,14,You can use my sample program as an example here:
Jaeger,53370207,44166525,0,"2018/11/19, 09:38:53",False,"2018/11/19, 09:38:53",463.0,1749786.0,14,https://github.com/anoophp777/spring-webflux-jaegar-log4j2
Jaeger,23347674,23266043,1,"2014/04/28, 20:38:39",True,"2014/04/28, 20:38:39",832.0,23854.0,4,It looks like this is not currently possible with Cordova 3.4 when attempting to read a video file out of the application assets.
Jaeger,23347674,23266043,1,"2014/04/28, 20:38:39",True,"2014/04/28, 20:38:39",832.0,23854.0,4,See  https://issues.apache.org/jira/browse/CB-6079
Jaeger,23347674,23266043,1,"2014/04/28, 20:38:39",True,"2014/04/28, 20:38:39",832.0,23854.0,4,"It is possible to read the file if its copied to a directory outside the application assets, or the file is stored remotely."
Jaeger,23347674,23266043,1,"2014/04/28, 20:38:39",True,"2014/04/28, 20:38:39",832.0,23854.0,4,But not in the app assets folder any longer.
Jaeger,23347674,23266043,1,"2014/04/28, 20:38:39",True,"2014/04/28, 20:38:39",832.0,23854.0,4,I have a similar issue - my application has a welcome screen with a short video explaining the application (~300k) which I cannot play out of the APK itself.
Jaeger,17730606,16491312,0,"2013/07/18, 20:49:47",True,"2013/07/18, 20:49:47",3963.0,2161256.0,4,"jquery.limitkeypress.js has some issue with ie, I recommend you to use a more powerful library."
Jaeger,17730606,16491312,0,"2013/07/18, 20:49:47",True,"2013/07/18, 20:49:47",3963.0,2161256.0,4,http://github.com/RobinHerbots/jquery.inputmask
Jaeger,17730606,16491312,0,"2013/07/18, 20:49:47",True,"2013/07/18, 20:49:47",3963.0,2161256.0,4,With this library you can use something like this:
Jaeger,17730606,16491312,0,"2013/07/18, 20:49:47",True,"2013/07/18, 20:49:47",3963.0,2161256.0,4,It works perfectly on ie.
Jaeger,17730606,16491312,0,"2013/07/18, 20:49:47",True,"2013/07/18, 20:49:47",3963.0,2161256.0,4,:)
Jaeger,35784198,16491312,0,"2016/03/04, 00:18:17",False,"2016/03/04, 16:45:12",11.0,6015643.0,1,Sorry i have not updated that plugin in a few years but...
Jaeger,35784198,16491312,0,"2016/03/04, 00:18:17",False,"2016/03/04, 16:45:12",11.0,6015643.0,1,jquery.limitkeypress  now works with IE9+ there was an issue with how the selection was determined.
Jaeger,35784198,16491312,0,"2016/03/04, 00:18:17",False,"2016/03/04, 16:45:12",11.0,6015643.0,1,IE11 killed support for their document.selection but they kept the document.setSelectionRange which i was using to test what browser was being used...
Jaeger,35784198,16491312,0,"2016/03/04, 00:18:17",False,"2016/03/04, 16:45:12",11.0,6015643.0,1,IE9 enabled document.selectionStart and document.selectionEnd so i now check directly what browser version of IE peoples are using...
Jaeger,35784198,16491312,0,"2016/03/04, 00:18:17",False,"2016/03/04, 16:45:12",11.0,6015643.0,1,I added this to check for IE version:
Jaeger,35784198,16491312,0,"2016/03/04, 00:18:17",False,"2016/03/04, 16:45:12",11.0,6015643.0,1,So my selection functions now look like this:
Jaeger,63582546,63507749,1,"2020/08/25, 18:46:03",False,"2020/08/25, 18:46:03",61.0,14137910.0,3,After few days of digging I've figured it out.
Jaeger,63582546,63507749,1,"2020/08/25, 18:46:03",False,"2020/08/25, 18:46:03",61.0,14137910.0,3,Problem is in the format of the  x-request-id  header that nginx ingress controller uses.
Jaeger,63582546,63507749,1,"2020/08/25, 18:46:03",False,"2020/08/25, 18:46:03",61.0,14137910.0,3,Envoy proxy expects it to be an UUID (e.g.
Jaeger,63582546,63507749,1,"2020/08/25, 18:46:03",False,"2020/08/25, 18:46:03",61.0,14137910.0,3,x-request-id: 3e21578f-cd04-9246-aa50-67188d790051 ) but ingrex controller passes it as a non-formatted random string ( x-request-id: 60e82827a270070cfbda38c6f30f478a ).
Jaeger,63582546,63507749,1,"2020/08/25, 18:46:03",False,"2020/08/25, 18:46:03",61.0,14137910.0,3,When I pass properly formatted x-request-id header in the request to ingress controller its getting passed down to envoy proxy and request is getting sampled as expected.
Jaeger,63582546,63507749,1,"2020/08/25, 18:46:03",False,"2020/08/25, 18:46:03",61.0,14137910.0,3,"I also tried to remove
x-request-id header from the request from ingress controller to ServiceA with a simple EnvoyFilter."
Jaeger,63582546,63507749,1,"2020/08/25, 18:46:03",False,"2020/08/25, 18:46:03",61.0,14137910.0,3,And it also works as expected.
Jaeger,63582546,63507749,1,"2020/08/25, 18:46:03",False,"2020/08/25, 18:46:03",61.0,14137910.0,3,Envoy proxy generates a new x-request-id and request is getting traced.
Jaeger,63486777,63485673,2,"2020/08/19, 15:18:24",True,"2020/08/19, 15:18:24",91.0,1548178.0,4,The demo you tried is using older configuration and opencensus which should be replaced with otlp receiver.
Jaeger,63486777,63485673,2,"2020/08/19, 15:18:24",True,"2020/08/19, 15:18:24",91.0,1548178.0,4,"Having said that this is a working example
 https://github.com/open-telemetry/opentelemetry-js/tree/master/examples/collector-exporter-node/docker 
So I'm copying the files from there:"
Jaeger,63486777,63485673,2,"2020/08/19, 15:18:24",True,"2020/08/19, 15:18:24",91.0,1548178.0,4,docker-compose.yaml
Jaeger,63486777,63485673,2,"2020/08/19, 15:18:24",True,"2020/08/19, 15:18:24",91.0,1548178.0,4,collector-config.yaml
Jaeger,63486777,63485673,2,"2020/08/19, 15:18:24",True,"2020/08/19, 15:18:24",91.0,1548178.0,4,prometheus.yaml
Jaeger,63486777,63485673,2,"2020/08/19, 15:18:24",True,"2020/08/19, 15:18:24",91.0,1548178.0,4,This should work fine with opentelemetry-js ver.
Jaeger,63486777,63485673,2,"2020/08/19, 15:18:24",True,"2020/08/19, 15:18:24",91.0,1548178.0,4,0.10.2
Jaeger,63486777,63485673,2,"2020/08/19, 15:18:24",True,"2020/08/19, 15:18:24",91.0,1548178.0,4,Default port for traces is 55680 and for metrics 55681
Jaeger,63486777,63485673,2,"2020/08/19, 15:18:24",True,"2020/08/19, 15:18:24",91.0,1548178.0,4,"The link I posted previously - you will always find there the latest up to date working example:
 https://github.com/open-telemetry/opentelemetry-js/tree/master/examples/collector-exporter-node 
And for web example you can use the same docker and see all working examples here:
 https://github.com/open-telemetry/opentelemetry-js/tree/master/examples/tracer-web/"
Jaeger,63489195,63485673,2,"2020/08/19, 17:27:00",False,"2020/08/19, 18:54:58",30911.0,2000548.0,3,Thank you sooo much for @BObecny's help!
Jaeger,63489195,63485673,2,"2020/08/19, 17:27:00",False,"2020/08/19, 18:54:58",30911.0,2000548.0,3,This is a complement of @BObecny's answer.
Jaeger,63489195,63485673,2,"2020/08/19, 17:27:00",False,"2020/08/19, 18:54:58",30911.0,2000548.0,3,Since I am more interested in integrating with Jaeger.
Jaeger,63489195,63485673,2,"2020/08/19, 17:27:00",False,"2020/08/19, 18:54:58",30911.0,2000548.0,3,"So here is the config to set up with all Jaeger, Zipkin, Prometheus."
Jaeger,63489195,63485673,2,"2020/08/19, 17:27:00",False,"2020/08/19, 18:54:58",30911.0,2000548.0,3,And now it works on both front end and back end.
Jaeger,63489195,63485673,2,"2020/08/19, 17:27:00",False,"2020/08/19, 18:54:58",30911.0,2000548.0,3,First both front end and back end use same exporter code:
Jaeger,63489195,63485673,2,"2020/08/19, 17:27:00",False,"2020/08/19, 18:54:58",30911.0,2000548.0,3,docker-compose.yaml
Jaeger,63489195,63485673,2,"2020/08/19, 17:27:00",False,"2020/08/19, 18:54:58",30911.0,2000548.0,3,collector-config.yaml
Jaeger,63489195,63485673,2,"2020/08/19, 17:27:00",False,"2020/08/19, 18:54:58",30911.0,2000548.0,3,prometheus.yaml
Jaeger,59684931,59679605,3,"2020/01/10, 17:51:33",False,"2020/01/10, 17:51:33",519.0,12405999.0,3,"Per the log file, there are more than 10,000 started threads."
Jaeger,59684931,59679605,3,"2020/01/10, 17:51:33",False,"2020/01/10, 17:51:33",519.0,12405999.0,3,That's  a lot  even if we don't look at the less that 2 CPUs/cores reserved for the container (limits.cpu = request.cpu = 1600 millicores).
Jaeger,59684931,59679605,3,"2020/01/10, 17:51:33",False,"2020/01/10, 17:51:33",519.0,12405999.0,3,"Each thread, and its stack, is allocated in memory separate from the heap."
Jaeger,59684931,59679605,3,"2020/01/10, 17:51:33",False,"2020/01/10, 17:51:33",519.0,12405999.0,3,It is quite possible that the large number of started threads is the cause for the OOM problem.
Jaeger,59684931,59679605,3,"2020/01/10, 17:51:33",False,"2020/01/10, 17:51:33",519.0,12405999.0,3,"The JVM is started with the Native Memory Tracking related options ( -XX:NativeMemoryTracking=detail, -XX:+UnlockDiagnosticVMOptions, -XX:+PrintNMTStatistics)  that could help to see the memory usage, including what's consumed by those threads."
Jaeger,59684931,59679605,3,"2020/01/10, 17:51:33",False,"2020/01/10, 17:51:33",519.0,12405999.0,3,This doc  could be a starting point for Java 11.
Jaeger,59684931,59679605,3,"2020/01/10, 17:51:33",False,"2020/01/10, 17:51:33",519.0,12405999.0,3,"In any case, it would be highly recommended to  not  have that many threads started."
Jaeger,59684931,59679605,3,"2020/01/10, 17:51:33",False,"2020/01/10, 17:51:33",519.0,12405999.0,3,E.g.
Jaeger,59684931,59679605,3,"2020/01/10, 17:51:33",False,"2020/01/10, 17:51:33",519.0,12405999.0,3,"use a pool, start and stop them when not needed anymore..."
Jaeger,59717806,59679605,0,"2020/01/13, 15:39:55",False,"2020/01/13, 15:39:55",2389.0,12524159.0,2,There are two reasons a container is OOM Killed: Container Quota and System Quota.
Jaeger,59717806,59679605,0,"2020/01/13, 15:39:55",False,"2020/01/13, 15:39:55",2389.0,12524159.0,2,OOM Killer  only  triggers with memory related issues.
Jaeger,59717806,59679605,0,"2020/01/13, 15:39:55",False,"2020/01/13, 15:39:55",2389.0,12524159.0,2,"If your system is far from being out of memory, there is probably a limit in your container."
Jaeger,59717806,59679605,0,"2020/01/13, 15:39:55",False,"2020/01/13, 15:39:55",2389.0,12524159.0,2,"To your process inside the pod, the pod resource limit is like the whole system being OOM."
Jaeger,59717806,59679605,0,"2020/01/13, 15:39:55",False,"2020/01/13, 15:39:55",2389.0,12524159.0,2,"Also, it's worth checking the Resource Requests because by default they are not set."
Jaeger,59717806,59679605,0,"2020/01/13, 15:39:55",False,"2020/01/13, 15:39:55",2389.0,12524159.0,2,Requests must be less than or equal to container limits.
Jaeger,59717806,59679605,0,"2020/01/13, 15:39:55",False,"2020/01/13, 15:39:55",2389.0,12524159.0,2,That means that containers could be overcommitted on nodes and killed by  OOMK if multiple containers are using more memory than their respective requests at the same time.
Jaeger,60649575,59679605,0,"2020/03/12, 09:43:50",True,"2020/03/12, 09:43:50",2227.0,1004374.0,1,In my case the issue was with debugger component that is located in CMD line of Docker file
Jaeger,60649575,59679605,0,"2020/03/12, 09:43:50",True,"2020/03/12, 09:43:50",2227.0,1004374.0,1,After removal application stopped leaking.
Jaeger,60649575,59679605,0,"2020/03/12, 09:43:50",True,"2020/03/12, 09:43:50",2227.0,1004374.0,1,But disappeared only native memory leak.
Jaeger,60649575,59679605,0,"2020/03/12, 09:43:50",True,"2020/03/12, 09:43:50",2227.0,1004374.0,1,As later investigated there also was heap memory leak induced by jaegger tracer component (luckily here we have much more tools).
Jaeger,60649575,59679605,0,"2020/03/12, 09:43:50",True,"2020/03/12, 09:43:50",2227.0,1004374.0,1,After its removal application became stable.
Jaeger,60649575,59679605,0,"2020/03/12, 09:43:50",True,"2020/03/12, 09:43:50",2227.0,1004374.0,1,I don't know if those components were leaky by itself or with combination with other components but fact is that now it is stable.
Jaeger,58318625,58249869,0,"2019/10/10, 11:31:45",False,"2019/10/10, 11:31:45",6049.0,3156333.0,0,"Istio have this feature called  Distributed Tracing , which enables users to track requests in mesh that is distributed across multiple services."
Jaeger,58318625,58249869,0,"2019/10/10, 11:31:45",False,"2019/10/10, 11:31:45",6049.0,3156333.0,0,"This can be used to visualize request latency, serialization and parallelism."
Jaeger,58318625,58249869,0,"2019/10/10, 11:31:45",False,"2019/10/10, 11:31:45",6049.0,3156333.0,0,For this to work Istio uses  Envoy Proxy - Tracing  feature.
Jaeger,58318625,58249869,0,"2019/10/10, 11:31:45",False,"2019/10/10, 11:31:45",6049.0,3156333.0,0,You can deploy  Bookinfo Application  and see how  Trace context propagation  works.
Jaeger,58414249,58249869,0,"2019/10/16, 16:14:47",False,"2019/10/16, 16:14:47",334.0,4734545.0,0,"If you have the same issue explained in this ticket, you need to wait for the next release of micronaut or use the workaround mentioned by micronaut guys there."
Jaeger,58414249,58249869,0,"2019/10/16, 16:14:47",False,"2019/10/16, 16:14:47",334.0,4734545.0,0,https://github.com/micronaut-projects/micronaut-core/issues/2209
Jaeger,58209862,58209785,0,"2019/10/03, 00:55:41",False,"2019/10/03, 00:55:41",966.0,324449.0,0,"latest  is just a tag like any other -- you will want  docker image inspect , which will give you information about the other tags on your image."
Jaeger,58209862,58209785,0,"2019/10/03, 00:55:41",False,"2019/10/03, 00:55:41",966.0,324449.0,0,"In the case of  jaegertracing/jaeger-agent:latest , it doesn't look this image has any other tags, so it's probable that this image is tracking something like the master branch of a source control repository, i.e., it doesn't correspond to a published version at all."
Jaeger,58244607,58209785,0,"2019/10/05, 04:10:08",False,"2019/10/05, 04:10:08",12605.0,11934042.0,1,"As @max-gasner mentioned, it's common for  latest  to be tracking the  master  branch of a git repository."
Jaeger,58244607,58209785,0,"2019/10/05, 04:10:08",False,"2019/10/05, 04:10:08",12605.0,11934042.0,1,This allows the engineers to quickly build and test images before they are released and version tagged.
Jaeger,58244607,58209785,0,"2019/10/05, 04:10:08",False,"2019/10/05, 04:10:08",12605.0,11934042.0,1,This is one of the reasons why it's not recommended to ever use  latest  tags for anything critical where you need reproducibility.
Jaeger,58244607,58209785,0,"2019/10/05, 04:10:08",False,"2019/10/05, 04:10:08",12605.0,11934042.0,1,"jaegertracing/jaeger-agent:latest  doesn't have any other tags so the only way to determine which ""version"" of  latest  you are using is to look at the digest."
Jaeger,58244607,58209785,0,"2019/10/05, 04:10:08",False,"2019/10/05, 04:10:08",12605.0,11934042.0,1,This uniquely identifies the image build.
Jaeger,58244607,58209785,0,"2019/10/05, 04:10:08",False,"2019/10/05, 04:10:08",12605.0,11934042.0,1,Tags actually resolve to digests.
Jaeger,58244607,58209785,0,"2019/10/05, 04:10:08",False,"2019/10/05, 04:10:08",12605.0,11934042.0,1,"So when a new image is built with the  latest  tag, that tag will then resolve to the digest of the new image."
Jaeger,58244607,58209785,0,"2019/10/05, 04:10:08",False,"2019/10/05, 04:10:08",12605.0,11934042.0,1,DockerHub only shows the short version.
Jaeger,58244607,58209785,0,"2019/10/05, 04:10:08",False,"2019/10/05, 04:10:08",12605.0,11934042.0,1,You can inspect the full digest like this:
Jaeger,52908915,52875462,1,"2018/10/20, 21:37:17",True,"2018/10/20, 21:37:17",46.0,4618624.0,2,There are similar ideas in this zipkin/brave repo by @jeqo.
Jaeger,52908915,52875462,1,"2018/10/20, 21:37:17",True,"2018/10/20, 21:37:17",46.0,4618624.0,2,https://github.com/jeqo/brave/tree/kafka-streams-processor/instrumentation/kafka-streams
Jaeger,52908915,52875462,1,"2018/10/20, 21:37:17",True,"2018/10/20, 21:37:17",46.0,4618624.0,2,There also seems to be something available in opentracing-contrib repo but it seems to only at trace producer/consumer level.
Jaeger,52908915,52875462,1,"2018/10/20, 21:37:17",True,"2018/10/20, 21:37:17",46.0,4618624.0,2,https://github.com/opentracing-contrib/java-kafka-client/tree/master/opentracing-kafka-streams
Jaeger,64052707,64050154,0,"2020/09/24, 21:55:46",False,"2020/09/24, 22:01:08",2441.0,10020419.0,4,"As, you can see there are few missing components - There are few pods missing istio-citadel, istio-pilot, istio-policy, istio-sidecar, istio-telemetry, istio-tracing etc."
Jaeger,64052707,64050154,0,"2020/09/24, 21:55:46",False,"2020/09/24, 22:01:08",2441.0,10020419.0,4,These components were available in 1.4.2.
Jaeger,64052707,64050154,0,"2020/09/24, 21:55:46",False,"2020/09/24, 22:01:08",2441.0,10020419.0,4,These components where merged with version 1.5 into one service named  istiod .
Jaeger,64052707,64050154,0,"2020/09/24, 21:55:46",False,"2020/09/24, 22:01:08",2441.0,10020419.0,4,See:  https://istio.io/latest/blog/2020/istiod/
Jaeger,64052707,64050154,0,"2020/09/24, 21:55:46",False,"2020/09/24, 22:01:08",2441.0,10020419.0,4,"In 1.4.2 installation I could see grafana, jaeger, kiali, prometheus, zipkin dashboards."
Jaeger,64052707,64050154,0,"2020/09/24, 21:55:46",False,"2020/09/24, 22:01:08",2441.0,10020419.0,4,But these are now missing.
Jaeger,64052707,64050154,0,"2020/09/24, 21:55:46",False,"2020/09/24, 22:01:08",2441.0,10020419.0,4,These AddonComponents must be installed manually and are not part of  istioctl  since version 1.7.
Jaeger,64052707,64050154,0,"2020/09/24, 21:55:46",False,"2020/09/24, 22:01:08",2441.0,10020419.0,4,See:  https://istio.io/latest/blog/2020/addon-rework/
Jaeger,64052707,64050154,0,"2020/09/24, 21:55:46",False,"2020/09/24, 22:01:08",2441.0,10020419.0,4,So your installation is not broken.
Jaeger,64052707,64050154,0,"2020/09/24, 21:55:46",False,"2020/09/24, 22:01:08",2441.0,10020419.0,4,It's just a lot has changed since 1.4.
Jaeger,64052707,64050154,0,"2020/09/24, 21:55:46",False,"2020/09/24, 22:01:08",2441.0,10020419.0,4,I would suggest to go through the release announcements to read about all changes:  https://istio.io/latest/news/releases/
Jaeger,62861784,62859856,0,"2020/07/12, 16:39:14",True,"2020/07/12, 16:39:14",654.0,955379.0,8,Iv finally found the solution.
Jaeger,62861784,62859856,0,"2020/07/12, 16:39:14",True,"2020/07/12, 16:39:14",654.0,955379.0,8,It seemed to have to do with how the reporter is started up.
Jaeger,62861784,62859856,0,"2020/07/12, 16:39:14",True,"2020/07/12, 16:39:14",654.0,955379.0,8,"Anyhow, I changed my tracer class to this."
Jaeger,62861784,62859856,0,"2020/07/12, 16:39:14",True,"2020/07/12, 16:39:14",654.0,955379.0,8,I know there are several inactive variables here right now.
Jaeger,62861784,62859856,0,"2020/07/12, 16:39:14",True,"2020/07/12, 16:39:14",654.0,955379.0,8,Will see if they still can be of use some how.
Jaeger,62861784,62859856,0,"2020/07/12, 16:39:14",True,"2020/07/12, 16:39:14",654.0,955379.0,8,But none is needed right now to get it rolling.
Jaeger,62861784,62859856,0,"2020/07/12, 16:39:14",True,"2020/07/12, 16:39:14",654.0,955379.0,8,Hope this might help someone else trying to get the .NET Core working properly together with a remote Jeagertracing server.
Jaeger,60706055,60636937,0,"2020/03/16, 14:29:14",False,"2020/03/16, 14:29:14",596.0,5089120.0,1,I use  io.opentracing.contrib:opentracing-spring-rabbitmq-starter:3.0.0 .
Jaeger,60706055,60636937,0,"2020/03/16, 14:29:14",False,"2020/03/16, 14:29:14",596.0,5089120.0,1,You can find some documentation  here .
Jaeger,60706055,60636937,0,"2020/03/16, 14:29:14",False,"2020/03/16, 14:29:14",596.0,5089120.0,1,With this mechanism I achieved exactly what you asked for.
Jaeger,60706055,60636937,0,"2020/03/16, 14:29:14",False,"2020/03/16, 14:29:14",596.0,5089120.0,1,I found 2 important things to make sure:
Jaeger,60706055,60636937,0,"2020/03/16, 14:29:14",False,"2020/03/16, 14:29:14",596.0,5089120.0,1,Hopefully you find this helpful.
Jaeger,57119911,57119864,0,"2019/07/20, 00:47:14",True,"2019/07/20, 00:47:14",1067.0,1795610.0,1,Ugh.
Jaeger,57119911,57119864,0,"2019/07/20, 00:47:14",True,"2019/07/20, 00:47:14",1067.0,1795610.0,1,I am an idiot.
Jaeger,57119911,57119864,0,"2019/07/20, 00:47:14",True,"2019/07/20, 00:47:14",1067.0,1795610.0,1,Here is what was going wrong for anyone else who might be stuck something like this:
Jaeger,57119911,57119864,0,"2019/07/20, 00:47:14",True,"2019/07/20, 00:47:14",1067.0,1795610.0,1,"The frontend application  is  receiving a header, I was just looking in the wrong place."
Jaeger,57119911,57119864,0,"2019/07/20, 00:47:14",True,"2019/07/20, 00:47:14",1067.0,1795610.0,1,The request comes from the load balancer to the node frontend microservice which sends its response to the browser.
Jaeger,57119911,57119864,0,"2019/07/20, 00:47:14",True,"2019/07/20, 00:47:14",1067.0,1795610.0,1,"I was checking the browser for the header, but the node frontend microservice was not forwarding this header to the browser."
Jaeger,56143481,56054438,0,"2019/05/15, 10:07:09",True,"2019/05/15, 10:07:09",1520.0,824434.0,3,If anyone is interested; I ended up solving this by creating some publish and consume MassTransit middleware to do the trace propagation via trace injection and extraction respectively.
Jaeger,56143481,56054438,0,"2019/05/15, 10:07:09",True,"2019/05/15, 10:07:09",1520.0,824434.0,3,I've put the solution up on GitHub -  https://github.com/yesmarket/MassTransit.OpenTracing
Jaeger,56143481,56054438,0,"2019/05/15, 10:07:09",True,"2019/05/15, 10:07:09",1520.0,824434.0,3,Still interested to hear if there's a better way of doing this...
Jaeger,55858211,55506381,1,"2019/04/26, 01:01:46",True,"2019/04/26, 01:01:46",1448.0,5788941.0,2,"In istio 1.1, the default sampling rate is 1%, so you need to send at least 100 requests before the first trace is visible."
Jaeger,55858211,55506381,1,"2019/04/26, 01:01:46",True,"2019/04/26, 01:01:46",1448.0,5788941.0,2,This can be configured through the  pilot.traceSampling  option.
Jaeger,54325585,54325234,2,"2019/01/23, 12:54:00",True,"2019/01/23, 12:54:00",13313.0,524946.0,2,OpenTracing is the API that  your  code will interact with directly.
Jaeger,54325585,54325234,2,"2019/01/23, 12:54:00",True,"2019/01/23, 12:54:00",13313.0,524946.0,2,"Basically, your application would be ""instrumented"" using the OpenTracing API and a concrete tracer (like Jaeger or Brave/Zipkin) would capture the data and send it somewhere."
Jaeger,54325585,54325234,2,"2019/01/23, 12:54:00",True,"2019/01/23, 12:54:00",13313.0,524946.0,2,This allows your application to use a neutral API throughout your code so that you can change from one provider to another without having to change your entire code base.
Jaeger,54325585,54325234,2,"2019/01/23, 12:54:00",True,"2019/01/23, 12:54:00",13313.0,524946.0,2,"Another way to think about it is that OpenTracing is like SLF4J in the Java logging world, whereas Jaeger and Zipkin are the concrete implementations, such as Log4j in the Java logging world."
Jaeger,59524696,48794097,0,"2019/12/30, 03:11:09",False,"2019/12/30, 03:11:09",2172.0,3514300.0,0,Trace context propagation might be missing.
Jaeger,59524696,48794097,0,"2019/12/30, 03:11:09",False,"2019/12/30, 03:11:09",2172.0,3514300.0,0,https://istio.io/docs/tasks/observability/distributed-tracing/overview/#trace-context-propagation
Jaeger,25716102,25715304,4,"2014/09/08, 03:49:31",False,"2014/09/08, 03:49:31",194.0,1478294.0,0,"I cannot test your code, but the only thing i can think off, is that the order of execution is wrong."
Jaeger,25716102,25715304,4,"2014/09/08, 03:49:31",False,"2014/09/08, 03:49:31",194.0,1478294.0,0,"You first make a string, then you make it Base64, then you encrypt it."
Jaeger,25716102,25715304,4,"2014/09/08, 03:49:31",False,"2014/09/08, 03:49:31",194.0,1478294.0,0,Now you undo the Base64 and afterwards you decrypt the encoded string.
Jaeger,25716102,25715304,4,"2014/09/08, 03:49:31",False,"2014/09/08, 03:49:31",194.0,1478294.0,0,These last two must be swapped.
Jaeger,25756628,25715304,1,"2014/09/10, 05:56:20",True,"2014/09/10, 08:14:52",22907.0,2714852.0,7,"A reminder that  security is unusually treacherous territory , and if there's a way to call on other well-tested code even more of your toplevel task than just what Go's OpenPGP package is handling for you, consider it."
Jaeger,25756628,25715304,1,"2014/09/10, 05:56:20",True,"2014/09/10, 08:14:52",22907.0,2714852.0,7,It's good that at least low-level details are outsourced to  openpgp  because they're nasty and so so easy to get wrong.
Jaeger,25756628,25715304,1,"2014/09/10, 05:56:20",True,"2014/09/10, 08:14:52",22907.0,2714852.0,7,"But tiny mistakes at any level can make crypto features worse than useless; if there's a way to write less security-critical code, that's one of the best things anyone can do for security."
Jaeger,25756628,25715304,1,"2014/09/10, 05:56:20",True,"2014/09/10, 08:14:52",22907.0,2714852.0,7,"On the specific question: you have to  Close()  the writer to get everything flushed out (a trait OpenPGP's writer shares with, say,  compress/gzip 's)."
Jaeger,25756628,25715304,1,"2014/09/10, 05:56:20",True,"2014/09/10, 08:14:52",22907.0,2714852.0,7,"Unrelated changes: the way you're printing things is a better fit  log.Println , which just lets you pass a bunch of values you want printed with spaces in between (like, say, Python  print ), rather than needing format specifiers like  ""%s""  or  ""%d"" ."
Jaeger,25756628,25715304,1,"2014/09/10, 05:56:20",True,"2014/09/10, 08:14:52",22907.0,2714852.0,7,"(The ""EXTRA"" in your initial output is what Go's  Printf  emits when you pass more things than you had format specifiers for.)"
Jaeger,25756628,25715304,1,"2014/09/10, 05:56:20",True,"2014/09/10, 08:14:52",22907.0,2714852.0,7,"It's also best practice to check errors (I dropped  if err != nil s where I saw a need, but inelegantly and without much thought, and I may not have gotten all the calls) and to run  go fmt  on your code."
Jaeger,25756628,25715304,1,"2014/09/10, 05:56:20",True,"2014/09/10, 08:14:52",22907.0,2714852.0,7,"Again,  I can't testify to the seaworthiness of this code or anything like that."
Jaeger,25756628,25715304,1,"2014/09/10, 05:56:20",True,"2014/09/10, 08:14:52",22907.0,2714852.0,7,But now it round-trips all the text.
Jaeger,25756628,25715304,1,"2014/09/10, 05:56:20",True,"2014/09/10, 08:14:52",22907.0,2714852.0,7,I wound up with:
Jaeger,62563034,62560345,1,"2020/06/24, 22:52:11",True,"2020/06/24, 23:07:58",3381.0,4108803.0,1,"You could try with  StartSpanFromContext , inside your gRPC handlers:"
Jaeger,62563034,62560345,1,"2020/06/24, 22:52:11",True,"2020/06/24, 23:07:58",3381.0,4108803.0,1,As the documentation of  otgrpc.OpenTracingServerInterceptor  says:
Jaeger,62563034,62560345,1,"2020/06/24, 22:52:11",True,"2020/06/24, 23:07:58",3381.0,4108803.0,1,"[...] the server Span will be embedded in the context.Context for the
application-specific gRPC handler(s) to access."
Jaeger,62563034,62560345,1,"2020/06/24, 22:52:11",True,"2020/06/24, 23:07:58",3381.0,4108803.0,1,If we look at the function implementation:
Jaeger,62563034,62560345,1,"2020/06/24, 22:52:11",True,"2020/06/24, 23:07:58",3381.0,4108803.0,1,"
 Edit : Given the above, you probably can omit this code:"
Jaeger,61468536,60753860,0,"2020/04/28, 00:11:21",True,"2020/04/28, 00:11:21",184.0,8575474.0,0,The issue was related to  yaml  file parsing
Jaeger,57877033,57588551,0,"2019/09/10, 22:21:23",False,"2019/09/10, 22:21:23",1040.0,1688998.0,0,"Jaeger has a UI to look at your data, but no tools to create statistics."
Jaeger,57877033,57588551,0,"2019/09/10, 22:21:23",False,"2019/09/10, 22:21:23",1040.0,1688998.0,0,However all your data is being stored in a DB of your choice.
Jaeger,57877033,57588551,0,"2019/09/10, 22:21:23",False,"2019/09/10, 22:21:23",1040.0,1688998.0,0,Storing it in e.g.
Jaeger,57877033,57588551,0,"2019/09/10, 22:21:23",False,"2019/09/10, 22:21:23",1040.0,1688998.0,0,Elasticsearch gives you a powerful query language to look at the data as well as many other tools that integrate with it.
Jaeger,56349205,56135086,5,"2019/05/28, 22:52:10",False,"2019/05/28, 22:52:10",838.0,10587554.0,0,"Correct me, if I'm wrong."
Jaeger,56349205,56135086,5,"2019/05/28, 22:52:10",False,"2019/05/28, 22:52:10",838.0,10587554.0,0,"If you mean how to find the trace-id on the server side, you can try to access the OpenTracing span by  get_active_span ."
Jaeger,56349205,56135086,5,"2019/05/28, 22:52:10",False,"2019/05/28, 22:52:10",838.0,10587554.0,0,"The trace-id, I suppose, should be one of the tags in it."
Jaeger,56584198,56135086,0,"2019/06/13, 18:46:22",True,"2019/06/13, 18:46:22",5906.0,474819.0,1,I had missed a key piece of documentation.
Jaeger,56584198,56135086,0,"2019/06/13, 18:46:22",True,"2019/06/13, 18:46:22",5906.0,474819.0,1,"In order to get a trace ID, you have to create a span on the client side."
Jaeger,56584198,56135086,0,"2019/06/13, 18:46:22",True,"2019/06/13, 18:46:22",5906.0,474819.0,1,This span will have the trace ID that can be used to examine data in the Jaeger UI.
Jaeger,56584198,56135086,0,"2019/06/13, 18:46:22",True,"2019/06/13, 18:46:22",5906.0,474819.0,1,The span has to be added into the GRPC messages via an  ActiveSpanSource  instance.
Jaeger,56584198,56135086,0,"2019/06/13, 18:46:22",True,"2019/06/13, 18:46:22",5906.0,474819.0,1,"Of course, you could switch the ordering of the  with  statements so that the span is created after the GRPC channel."
Jaeger,56584198,56135086,0,"2019/06/13, 18:46:22",True,"2019/06/13, 18:46:22",5906.0,474819.0,1,That part doesn't make any difference.
Jaeger,53131562,53131007,1,"2018/11/03, 15:00:17",True,"2018/11/03, 15:00:17",17024.0,858913.0,0,"You assumption is correct, the elements are there, but not exactly where you think they are."
Jaeger,53131562,53131007,1,"2018/11/03, 15:00:17",True,"2018/11/03, 15:00:17",17024.0,858913.0,0,To easily check if an element is part of the response html and not being loaded by javascript I normally recommend using a  browser plugin to disable javascript .
Jaeger,53131562,53131007,1,"2018/11/03, 15:00:17",True,"2018/11/03, 15:00:17",17024.0,858913.0,0,"If you want the images, they are still part of the html response, you can get them with:"
Jaeger,53131562,53131007,1,"2018/11/03, 15:00:17",True,"2018/11/03, 15:00:17",17024.0,858913.0,0,the main image appears separately:
Jaeger,53131562,53131007,1,"2018/11/03, 15:00:17",True,"2018/11/03, 15:00:17",17024.0,858913.0,0,Hope that helps you.
Jaeger,48593245,48592875,1,"2018/02/03, 04:04:40",False,"2018/02/03, 04:14:38",31.0,7234392.0,1,You haven't initialized the variables for the next few iterations.
Jaeger,48593245,48592875,1,"2018/02/03, 04:04:40",False,"2018/02/03, 04:14:38",31.0,7234392.0,1,You need to reinitialize the variables used for while loop's condition check outside their respective while loops.
Jaeger,48593245,48592875,1,"2018/02/03, 04:04:40",False,"2018/02/03, 04:14:38",31.0,7234392.0,1,i.e
Jaeger,48593245,48592875,1,"2018/02/03, 04:04:40",False,"2018/02/03, 04:14:38",31.0,7234392.0,1,Similarly do it for the while loops which use variables  c  &amp;  d .
Jaeger,48594819,48592875,0,"2018/02/03, 08:59:49",False,"2018/02/03, 08:59:49",15431.0,3992939.0,0,"Daniel's answer is correct  : the 
structure of the  while  loop should be:"
Jaeger,36821033,36819104,1,"2016/04/24, 11:48:50",True,"2016/04/24, 12:14:06",2648.0,6230407.0,0,"It may not be possible to input text with conditional formatting, but you can change the font color."
Jaeger,36821033,36819104,1,"2016/04/24, 11:48:50",True,"2016/04/24, 12:14:06",2648.0,6230407.0,0,"A solution could be to put the word ""LATE in the specified cell(s) beforehand and set the font-color equal to the background-color, which makes the word invisable."
Jaeger,36821033,36819104,1,"2016/04/24, 11:48:50",True,"2016/04/24, 12:14:06",2648.0,6230407.0,0,"When the condition (formula) evaluates true, the new format will change the font-color and the word LATE appears."
Jaeger,36821033,36819104,1,"2016/04/24, 11:48:50",True,"2016/04/24, 12:14:06",2648.0,6230407.0,0,No VBA requiered.
Jaeger,36821033,36819104,1,"2016/04/24, 11:48:50",True,"2016/04/24, 12:14:06",2648.0,6230407.0,0,On the other hand: wouldn't a simple if-formula be better?
Jaeger,36821033,36819104,1,"2016/04/24, 11:48:50",True,"2016/04/24, 12:14:06",2648.0,6230407.0,0,Something like:
Jaeger,36821033,36819104,1,"2016/04/24, 11:48:50",True,"2016/04/24, 12:14:06",2648.0,6230407.0,0,If you wish you can then change the background with conditional formating
Jaeger,60859448,60846142,1,"2020/03/26, 02:44:07",False,"2020/03/26, 02:44:07",50169.0,594589.0,0,There's really not many options for other than starting a span in each function you'd like to instrument:
Jaeger,60859448,60846142,1,"2020/03/26, 02:44:07",False,"2020/03/26, 02:44:07",50169.0,594589.0,0,"If your functions have a common call signature, or you can coalesce your function into a common call signature, you can write a wrapper."
Jaeger,60859448,60846142,1,"2020/03/26, 02:44:07",False,"2020/03/26, 02:44:07",50169.0,594589.0,0,"Examples of this can be seen in http  ""middleware"" ."
Jaeger,60859448,60846142,1,"2020/03/26, 02:44:07",False,"2020/03/26, 02:44:07",50169.0,594589.0,0,"Consider the http.Handler, you could write a  decorator  for your functions that handles the span lifecycle:"
Jaeger,60859448,60846142,1,"2020/03/26, 02:44:07",False,"2020/03/26, 02:44:07",50169.0,594589.0,0,A similar pattern could be applied by  embedding  structs.
Jaeger,58197357,58148102,0,"2019/10/02, 10:55:46",True,"2019/10/02, 10:55:46",109.0,318536.0,2,Is it possible to set these when working on OpenShift?
Jaeger,58197357,58148102,0,"2019/10/02, 10:55:46",True,"2019/10/02, 10:55:46",109.0,318536.0,2,"Yes, you can configure it for the Che master of your installation."
Jaeger,58197357,58148102,0,"2019/10/02, 10:55:46",True,"2019/10/02, 10:55:46",109.0,318536.0,2,OpenShift is the Saas Che offering
Jaeger,58197357,58148102,0,"2019/10/02, 10:55:46",True,"2019/10/02, 10:55:46",109.0,318536.0,2,As a user of che.openshift.io you can't leverage from tracing capabilities of Che at this moment.
Jaeger,57297860,57292517,0,"2019/07/31, 22:41:03",False,"2019/07/31, 22:41:03",48155.0,2989261.0,2,This 'appears' to be related to the switch from AWS CNI to weave.
Jaeger,57297860,57292517,0,"2019/07/31, 22:41:03",False,"2019/07/31, 22:41:03",48155.0,2989261.0,2,"CNI uses the IP range of your VPC while weave uses its own address range (for pods), so there may be remaining iptables rules from AWS CNI, for example."
Jaeger,57297860,57292517,0,"2019/07/31, 22:41:03",False,"2019/07/31, 22:41:03",48155.0,2989261.0,2,"Internal error occurred: failed calling admission webhook ""pilot.validation.istio.io"": Post  https://istio-galley.istio-system.svc:443/admitpilot?timeout=30s : Address is not allowed"
Jaeger,57297860,57292517,0,"2019/07/31, 22:41:03",False,"2019/07/31, 22:41:03",48155.0,2989261.0,2,"The message above implies that whatever address  istio-galley.istio-system.svc  resolves to, internally in your K8s cluster, is not a valid IP address."
Jaeger,57297860,57292517,0,"2019/07/31, 22:41:03",False,"2019/07/31, 22:41:03",48155.0,2989261.0,2,So I would also try to see what that resolves to.
Jaeger,57297860,57292517,0,"2019/07/31, 22:41:03",False,"2019/07/31, 22:41:03",48155.0,2989261.0,2,(It may be related to  coreDNS ).
Jaeger,57297860,57292517,0,"2019/07/31, 22:41:03",False,"2019/07/31, 22:41:03",48155.0,2989261.0,2,You can also try the following  these steps ;
Jaeger,57297860,57292517,0,"2019/07/31, 22:41:03",False,"2019/07/31, 22:41:03",48155.0,2989261.0,2,"Basically, (quoted)"
Jaeger,57297860,57292517,0,"2019/07/31, 22:41:03",False,"2019/07/31, 22:41:03",48155.0,2989261.0,2,"Furthermore, you can try reinstalling everything from the beginning using weave."
Jaeger,57297860,57292517,0,"2019/07/31, 22:41:03",False,"2019/07/31, 22:41:03",48155.0,2989261.0,2,Hope it helps!
Jaeger,54199894,54084447,2,"2019/01/15, 15:32:19",True,"2019/01/15, 15:32:19",294930.0,208809.0,2,OpenTracing is a framework for Distributed Tracing.
Jaeger,54199894,54084447,2,"2019/01/15, 15:32:19",True,"2019/01/15, 15:32:19",294930.0,208809.0,2,"As such, it is more about performance monitoring and observability than logging (what NLog is about)."
Jaeger,54199894,54084447,2,"2019/01/15, 15:32:19",True,"2019/01/15, 15:32:19",294930.0,208809.0,2,OpenTracing allows you to manually instrument your code to generate traces with relevant spans containing information about code execution in your app.
Jaeger,54199894,54084447,2,"2019/01/15, 15:32:19",True,"2019/01/15, 15:32:19",294930.0,208809.0,2,"This includes annotating spans with errors and arbitrary keys and values, which you  could  use instead of logging."
Jaeger,54199894,54084447,2,"2019/01/15, 15:32:19",True,"2019/01/15, 15:32:19",294930.0,208809.0,2,"However, that's not the same as dedicated structured logging."
Jaeger,54028704,54021635,1,"2019/01/03, 21:35:20",False,"2019/01/03, 21:35:20",113.0,9236736.0,1,Here's an article/guide on how to work with the limit-ranger and its default values [1]
Jaeger,54028704,54021635,1,"2019/01/03, 21:35:20",False,"2019/01/03, 21:35:20",113.0,9236736.0,1,[1] https://medium.com/@betz.mark/understanding-resource-limits-in-kubernetes-cpu-time-9eff74d3161b
Jaeger,54200196,53428114,0,"2019/01/15, 15:48:26",True,"2019/01/15, 15:48:26",294930.0,208809.0,1,"The  span.kind=server  tag denotes an entry span, e.g."
Jaeger,54200196,53428114,0,"2019/01/15, 15:48:26",True,"2019/01/15, 15:48:26",294930.0,208809.0,1,a span created in the local code in response to an external request.
Jaeger,54200196,53428114,0,"2019/01/15, 15:48:26",True,"2019/01/15, 15:48:26",294930.0,208809.0,1,"Likewise,  span.kind=client  denotes an exit span, e.g."
Jaeger,54200196,53428114,0,"2019/01/15, 15:48:26",True,"2019/01/15, 15:48:26",294930.0,208809.0,1,a call made from the local code to another server.
Jaeger,54200196,53428114,0,"2019/01/15, 15:48:26",True,"2019/01/15, 15:48:26",294930.0,208809.0,1,"In your example, the span generated for Foo is a  span.kind=server  and the span recording the call to Buzz is a  span.kind=client ."
Jaeger,51745456,51723902,2,"2018/08/08, 14:22:23",False,"2018/08/08, 14:22:23",4058.0,9928809.0,1,Kubernetes provides quite a big variety of Networking and Load Balancing features from the box.
Jaeger,51745456,51723902,2,"2018/08/08, 14:22:23",False,"2018/08/08, 14:22:23",4058.0,9928809.0,1,"However, the idea to simplify and extend the functionality  of  Istio sidecars  is a good choice as they are used for automatic injection into the Pods in order to proxy the traffic between internal Kubernetes services."
Jaeger,51745456,51723902,2,"2018/08/08, 14:22:23",False,"2018/08/08, 14:22:23",4058.0,9928809.0,1,You can implement  sidecars  manually or automatically.
Jaeger,51745456,51723902,2,"2018/08/08, 14:22:23",False,"2018/08/08, 14:22:23",4058.0,9928809.0,1,"If you choose the manual way, make sure to add the appropriate parameter under Pod's annotation field:"
Jaeger,51745456,51723902,2,"2018/08/08, 14:22:23",False,"2018/08/08, 14:22:23",4058.0,9928809.0,1,"Automatic  sidecar  injection requires  Mutating Webhook admission controller , available since Kubernetes version 1.9 released, therefore  sidecars  can be integrated for Pod's creation process as well."
Jaeger,51745456,51723902,2,"2018/08/08, 14:22:23",False,"2018/08/08, 14:22:23",4058.0,9928809.0,1,Get yourself familiar with this  Article  to shed light on using different monitoring and traffic management tools in Istio.
Jaeger,52545681,51723902,0,"2018/09/28, 00:56:57",True,"2018/09/28, 00:56:57",363.0,3463514.0,1,Yes - it is possible to use external services with istio.
Jaeger,52545681,51723902,0,"2018/09/28, 00:56:57",True,"2018/09/28, 00:56:57",363.0,3463514.0,1,"You can disable grafana and prometheus just by setting proper flags in values.yaml of istio helm chart (grafana.enabled=false, etc)."
Jaeger,52545681,51723902,0,"2018/09/28, 00:56:57",True,"2018/09/28, 00:56:57",363.0,3463514.0,1,"You can check  kyma-project  project to see how istio is integrated with prometheus-operator, grafana deployment with custom dashboards, and custom jaeger deployment."
Jaeger,52545681,51723902,0,"2018/09/28, 00:56:57",True,"2018/09/28, 00:56:57",363.0,3463514.0,1,From your list only certmanager is missing.
Jaeger,66249819,66213790,0,"2021/02/17, 22:55:27",False,"2021/02/17, 22:55:27",2140.0,3697695.0,1,I am not sure why Istio doesn't automatically trace your calls to external APIs.
Jaeger,66249819,66213790,0,"2021/02/17, 22:55:27",False,"2021/02/17, 22:55:27",2140.0,3697695.0,1,"Perhaps it requires an egress gateway to be used, I'm not sure."
Jaeger,66249819,66213790,0,"2021/02/17, 22:55:27",False,"2021/02/17, 22:55:27",2140.0,3697695.0,1,"Note also that Istio creates traces for http(s) traffic, not TCP."
Jaeger,66249819,66213790,0,"2021/02/17, 22:55:27",False,"2021/02/17, 22:55:27",2140.0,3697695.0,1,"However, this is something you can still do programmatically."
Jaeger,66249819,66213790,0,"2021/02/17, 22:55:27",False,"2021/02/17, 22:55:27",2140.0,3697695.0,1,You can use any of the  Jaeger client libraries  to augment&quot;the traces already created by Envoy by appending your own spans.
Jaeger,66249819,66213790,0,"2021/02/17, 22:55:27",False,"2021/02/17, 22:55:27",2140.0,3697695.0,1,"To do so, you need first to extract the trace context from the HTTP headers of the incoming request (assuming that your external API calls are consecutive to an incoming request), and then create a new span as child of that previous span context."
Jaeger,66249819,66213790,0,"2021/02/17, 22:55:27",False,"2021/02/17, 22:55:27",2140.0,3697695.0,1,A good idea would be to use  OpenTracing semantic conventions  when you tag your new span.
Jaeger,66249819,66213790,0,"2021/02/17, 22:55:27",False,"2021/02/17, 22:55:27",2140.0,3697695.0,1,Tools like Kiali will be able to leverage some information if it follows this convention.
Jaeger,66249819,66213790,0,"2021/02/17, 22:55:27",False,"2021/02/17, 22:55:27",2140.0,3697695.0,1,I've found this blog post that explains how to do it with the nodejs jaeger client:  https://rhonabwy.com/2019/01/06/adding-tracing-with-jaeger-to-an-express-application/
Jaeger,65485312,65377607,0,"2020/12/29, 01:18:49",False,"2020/12/29, 01:18:49",166.0,12249096.0,0,"Yes, the OpenCensus collector should be injected with the Linkerd proxy because the proxies themselves send the span info using mTLS."
Jaeger,65485312,65377607,0,"2020/12/29, 01:18:49",False,"2020/12/29, 01:18:49",166.0,12249096.0,0,"With mTLS, the sending (client) and receiving (server) sides of the request must present certificates to each other in to  verify  that identities to each other in a way that validates that the identity was issued by the same trusted source."
Jaeger,65485312,65377607,0,"2020/12/29, 01:18:49",False,"2020/12/29, 01:18:49",166.0,12249096.0,0,The Linkerd service mesh is made up of the control plane and the data plane.
Jaeger,65485312,65377607,0,"2020/12/29, 01:18:49",False,"2020/12/29, 01:18:49",166.0,12249096.0,0,The control plane is a set of services that run within the cluster to implement the features of the service mesh.
Jaeger,65485312,65377607,0,"2020/12/29, 01:18:49",False,"2020/12/29, 01:18:49",166.0,12249096.0,0,Mutual TLS (mTLS) is one of those features and is implemented by the  linkerd-identity  component of the control plane.
Jaeger,65485312,65377607,0,"2020/12/29, 01:18:49",False,"2020/12/29, 01:18:49",166.0,12249096.0,0,"The data plane is comprised of any number of the Linkerd proxies which are injected into the services in the application, like the OpenCensus collector."
Jaeger,65485312,65377607,0,"2020/12/29, 01:18:49",False,"2020/12/29, 01:18:49",166.0,12249096.0,0,"Whenever a proxy is started within a pod, it sends a certificate signing request to the  linkerd-identity  component and receives a certificate in return."
Jaeger,65485312,65377607,0,"2020/12/29, 01:18:49",False,"2020/12/29, 01:18:49",166.0,12249096.0,0,"So, when the Linkerd proxies in the control plane send the spans to the collector, they authenticate themselves with those certificates, which must be verified by the proxy injected into the OpenCensus collector Pod."
Jaeger,65485312,65377607,0,"2020/12/29, 01:18:49",False,"2020/12/29, 01:18:49",166.0,12249096.0,0,"This ensures that all traffic, even distributed traces, are sent securely within the cluster."
Jaeger,65485312,65377607,0,"2020/12/29, 01:18:49",False,"2020/12/29, 01:18:49",166.0,12249096.0,0,"In your case, you should suffix the service account with the namespace."
Jaeger,65485312,65377607,0,"2020/12/29, 01:18:49",False,"2020/12/29, 01:18:49",166.0,12249096.0,0,"By default, Linkerd will use the Pod namespace, so if the service account doesn't exist in the Pod namespace, then the configuration will be invalid."
Jaeger,65485312,65377607,0,"2020/12/29, 01:18:49",False,"2020/12/29, 01:18:49",166.0,12249096.0,0,"The  logic  has a function that checks for a namespace in the annotation name and appends it, if it exists:"
Jaeger,65485312,65377607,0,"2020/12/29, 01:18:49",False,"2020/12/29, 01:18:49",166.0,12249096.0,0,"So, this one is correct:"
Jaeger,64175114,64174906,1,"2020/10/02, 19:29:05",False,"2020/10/02, 19:29:05",1.0,540250.0,0,you are using System.Configuration namespace which causes ambiguity.
Jaeger,64175114,64174906,1,"2020/10/02, 19:29:05",False,"2020/10/02, 19:29:05",1.0,540250.0,0,i would suggest remove the using System.Configuration.
Jaeger,64175114,64174906,1,"2020/10/02, 19:29:05",False,"2020/10/02, 19:29:05",1.0,540250.0,0,And try specifying fully qualified name for the Configuration.
Jaeger,64175114,64174906,1,"2020/10/02, 19:29:05",False,"2020/10/02, 19:29:05",1.0,540250.0,0,visual studio would suggest possible candidates (press Ctrl .
Jaeger,64175114,64174906,1,"2020/10/02, 19:29:05",False,"2020/10/02, 19:29:05",1.0,540250.0,0,on the Class name you want to qualify) provided you have added all required references in project already.
Jaeger,64175147,64174906,4,"2020/10/02, 19:31:24",False,"2020/10/02, 19:31:24",85555.0,880990.0,0,If you have a
Jaeger,64175147,64174906,4,"2020/10/02, 19:31:24",False,"2020/10/02, 19:31:24",85555.0,880990.0,0,then the C# compiler gets confused with  Configuration  and thinks it refers to the namespace  System.Configuration .
Jaeger,64175147,64174906,4,"2020/10/02, 19:31:24",False,"2020/10/02, 19:31:24",85555.0,880990.0,0,You can solve it by using the explicit namespace  Jaeger :
Jaeger,63843385,63843312,1,"2020/09/11, 11:21:58",True,"2020/09/11, 11:21:58",1911.0,11506391.0,2,There is no possibility of doing it in the Dockerfile if you want to keep two separate image.
Jaeger,63843385,63843312,1,"2020/09/11, 11:21:58",True,"2020/09/11, 11:21:58",1911.0,11506391.0,2,How should you know in advance the name/id of the container you're going to link ?
Jaeger,63843385,63843312,1,"2020/09/11, 11:21:58",True,"2020/09/11, 11:21:58",1911.0,11506391.0,2,Below are two solutions :
Jaeger,63843675,63843312,0,"2020/09/11, 11:43:16",False,"2020/09/11, 11:43:16",206.0,9641548.0,0,"I recommend you using  netwoking , by creating:"
Jaeger,63843675,63843312,0,"2020/09/11, 11:43:16",False,"2020/09/11, 11:43:16",206.0,9641548.0,0,"and then run with --network=&quot;network&quot;
using docker-compose with  network  and link to each other
example:"
Jaeger,60463466,60462137,6,"2020/02/29, 10:52:57",False,"2020/03/01, 14:12:35",35494.0,7582247.0,1,-I  is used for  include  paths.
Jaeger,60463466,60462137,6,"2020/02/29, 10:52:57",False,"2020/03/01, 14:12:35",35494.0,7582247.0,1,Use  -L  for library paths:
Jaeger,60463466,60462137,6,"2020/02/29, 10:52:57",False,"2020/03/01, 14:12:35",35494.0,7582247.0,1,It also looks like you've linked with a shared library  libyaml-cppd.so  - not the static library  libyaml-cpp.a .
Jaeger,60463466,60462137,6,"2020/02/29, 10:52:57",False,"2020/03/01, 14:12:35",35494.0,7582247.0,1,I don't recognize the  d  in  libyaml-cppd.so  though.
Jaeger,60463466,60462137,6,"2020/02/29, 10:52:57",False,"2020/03/01, 14:12:35",35494.0,7582247.0,1,I'd check if that's really the library you built.
Jaeger,60463466,60462137,6,"2020/02/29, 10:52:57",False,"2020/03/01, 14:12:35",35494.0,7582247.0,1,"libyaml-cpp  will be built as a static library by default ( libyaml-cpp.a ) and on a 64 bit machine, it will probably default to being installed in  /usr/local/lib64 ."
Jaeger,60463466,60462137,6,"2020/02/29, 10:52:57",False,"2020/03/01, 14:12:35",35494.0,7582247.0,1,You are only allowed to do very limited things in  namespace std .
Jaeger,60463466,60462137,6,"2020/02/29, 10:52:57",False,"2020/03/01, 14:12:35",35494.0,7582247.0,1,Adding new functions/classes are not allowed (unless as template specializations including user defined types) - so remove  namespace std { ... }  around your program.
Jaeger,60463466,60462137,6,"2020/02/29, 10:52:57",False,"2020/03/01, 14:12:35",35494.0,7582247.0,1,Also.
Jaeger,60463466,60462137,6,"2020/02/29, 10:52:57",False,"2020/03/01, 14:12:35",35494.0,7582247.0,1,the  main  function should be in the global namespace.
Jaeger,60463466,60462137,6,"2020/02/29, 10:52:57",False,"2020/03/01, 14:12:35",35494.0,7582247.0,1,The reason it's not found by the linker is because you put it in a namespace ( std ).
Jaeger,60463466,60462137,6,"2020/02/29, 10:52:57",False,"2020/03/01, 14:12:35",35494.0,7582247.0,1,UPDATE : The issue is resolved follow this link exaclty  https://github.com/jaegertracing/jaeger-client-cpp/issues/162#issuecomment-565892473  (use thrift version 0.11 or 0.11+)
Jaeger,59285133,59276096,0,"2019/12/11, 13:47:46",False,"2019/12/11, 13:47:46",2576.0,12014434.0,2,Fisrt of all according to istio  documentation  Prometheus is used as default observation operator in istio mesh by default:
Jaeger,59285133,59276096,0,"2019/12/11, 13:47:46",False,"2019/12/11, 13:47:46",2576.0,12014434.0,2,The  default Istio metrics  are defined by a set of configuration artifacts that ship with Istio and are exported to  Prometheus  by default.
Jaeger,59285133,59276096,0,"2019/12/11, 13:47:46",False,"2019/12/11, 13:47:46",2576.0,12014434.0,2,"Operators are free to modify the shape and content of these metrics, as well as to change their collection mechanism, to meet their individual monitoring needs."
Jaeger,59285133,59276096,0,"2019/12/11, 13:47:46",False,"2019/12/11, 13:47:46",2576.0,12014434.0,2,So by having istio injected prometheus operator You end up with two Prometheus operators in Your istio mesh.
Jaeger,59285133,59276096,0,"2019/12/11, 13:47:46",False,"2019/12/11, 13:47:46",2576.0,12014434.0,2,"Secondly, when you enforce Mutual TLS in Your istio mesh every connection has to be secure ( TLS )."
Jaeger,59285133,59276096,0,"2019/12/11, 13:47:46",False,"2019/12/11, 13:47:46",2576.0,12014434.0,2,And as You mentioned it works when there is no istio injection.
Jaeger,59285133,59276096,0,"2019/12/11, 13:47:46",False,"2019/12/11, 13:47:46",2576.0,12014434.0,2,So the most likely cause is that the readiness probe fails because it is using  HTTP  protocol which is insecure (plain text) and this is one of the reason why You would get  503  error.
Jaeger,59285133,59276096,0,"2019/12/11, 13:47:46",False,"2019/12/11, 13:47:46",2576.0,12014434.0,2,"If you really need prometheus operator within istio mesh, this could be fixed by creating  DestinationRule  to  Disable  tls mode just for the readiness probe."
Jaeger,59285133,59276096,0,"2019/12/11, 13:47:46",False,"2019/12/11, 13:47:46",2576.0,12014434.0,2,Example:
Jaeger,59285133,59276096,0,"2019/12/11, 13:47:46",False,"2019/12/11, 13:47:46",2576.0,12014434.0,2,Note: Make sure to modify it so that it matches Your namespaces and hosts.
Jaeger,59285133,59276096,0,"2019/12/11, 13:47:46",False,"2019/12/11, 13:47:46",2576.0,12014434.0,2,Also there could be some other prometheus collisions within mesh.
Jaeger,59285133,59276096,0,"2019/12/11, 13:47:46",False,"2019/12/11, 13:47:46",2576.0,12014434.0,2,The other solution would be not to have prometheus istio injected in the first place.
Jaeger,59285133,59276096,0,"2019/12/11, 13:47:46",False,"2019/12/11, 13:47:46",2576.0,12014434.0,2,You can disable istio injection in prometheus namespace by using the following commands:
Jaeger,59144234,58999288,0,"2019/12/02, 19:50:35",True,"2019/12/02, 19:50:35",3020.0,1264920.0,0,I was able to publish some spans so I could see them on  http://localhost:16686
Jaeger,59144234,58999288,0,"2019/12/02, 19:50:35",True,"2019/12/02, 19:50:35",3020.0,1264920.0,0,This is the updated main function:
Jaeger,57068778,57068664,0,"2019/07/17, 08:00:18",False,"2019/07/17, 08:00:18",1034326.0,6309.0,1,go run  compiles and runs the named main Go package.
Jaeger,57068778,57068664,0,"2019/07/17, 08:00:18",False,"2019/07/17, 08:00:18",1034326.0,6309.0,1,"Only  go build  or  go install  would compile the packages named by the import paths, along with their dependencies,"
Jaeger,54185032,54134885,0,"2019/01/14, 18:04:36",True,"2019/01/14, 18:14:37",147.0,2560123.0,1,I guess if you set sampler to 0 in the configuration then no traces will be captured.
Jaeger,54185032,54134885,0,"2019/01/14, 18:04:36",True,"2019/01/14, 18:14:37",147.0,2560123.0,1,https://github.com/jaegertracing/jaeger-client-java#testing
Jaeger,54185032,54134885,0,"2019/01/14, 18:04:36",True,"2019/01/14, 18:14:37",147.0,2560123.0,1,But it's specific to Jaeger.
Jaeger,54185032,54134885,0,"2019/01/14, 18:04:36",True,"2019/01/14, 18:14:37",147.0,2560123.0,1,Otherwise you can use NoopTracer like  Tracer tracer = NoopTracerFactory.create();   Maven
Jaeger,66493806,66487682,1,"2021/03/05, 15:54:27",False,"2021/03/05, 15:54:27",1.0,15337079.0,0,You can set the service name in the code as follows:
Jaeger,66519661,66487682,0,"2021/03/07, 19:57:21",True,"2021/03/07, 19:57:21",343.0,2500390.0,0,"Based on some of the hints provided by Opentelemetry Java community, created two providers which indeed creates two services."
Jaeger,66519661,66487682,0,"2021/03/07, 19:57:21",True,"2021/03/07, 19:57:21",343.0,2500390.0,0,"Reusing same exporter, so that multiple connections to the backend can be avoided."
Jaeger,66519661,66487682,0,"2021/03/07, 19:57:21",True,"2021/03/07, 19:57:21",343.0,2500390.0,0,I will learn more about  reusing exporter to create two or more provides in the same application in coming days.
Jaeger,66519661,66487682,0,"2021/03/07, 19:57:21",True,"2021/03/07, 19:57:21",343.0,2500390.0,0,"Thank you to amazing Opentelemetry java community - @John Watson (jkwatson), Bogdan Drutu, Rupinder Singh, Anuraag Agrawal."
Jaeger,66147291,66141019,0,"2021/02/11, 02:50:17",False,"2021/02/11, 02:50:17",1693.0,971735.0,0,Based on this:
Jaeger,66147291,66141019,0,"2021/02/11, 02:50:17",False,"2021/02/11, 02:50:17",1693.0,971735.0,0,The problem is each service is capturing trace without issue but I can't see service to service communication and architectural design.
Jaeger,66147291,66141019,0,"2021/02/11, 02:50:17",False,"2021/02/11, 02:50:17",1693.0,971735.0,0,it seems that the tracing information is not propagated across services.
Jaeger,66147291,66141019,0,"2021/02/11, 02:50:17",False,"2021/02/11, 02:50:17",1693.0,971735.0,0,You can check this by looking into the HTTP headers and check the  traceId .
Jaeger,66147291,66141019,0,"2021/02/11, 02:50:17",False,"2021/02/11, 02:50:17",1693.0,971735.0,0,In order to make this work the  traceId  should be the same across the requests.
Jaeger,66147291,66141019,0,"2021/02/11, 02:50:17",False,"2021/02/11, 02:50:17",1693.0,971735.0,0,You should see the same  traceId  in the logs too.
Jaeger,66147291,66141019,0,"2021/02/11, 02:50:17",False,"2021/02/11, 02:50:17",1693.0,971735.0,0,The documentation gives you some pointers how to troubleshoot this:
Jaeger,66147115,66048959,2,"2021/02/11, 02:26:16",False,"2021/02/11, 02:26:16",4433.0,243104.0,0,"I'm not 100% sure what the problem is you're experiencing, but here's some things to consider."
Jaeger,66147115,66048959,2,"2021/02/11, 02:26:16",False,"2021/02/11, 02:26:16",4433.0,243104.0,0,"According to  this post on the Traefik forums , that message you're seeing is  debug  level because it's not something you should be worried about."
Jaeger,66147115,66048959,2,"2021/02/11, 02:26:16",False,"2021/02/11, 02:26:16",4433.0,243104.0,0,"It's just logging that no trace context was found, so a new one will be created."
Jaeger,66147115,66048959,2,"2021/02/11, 02:26:16",False,"2021/02/11, 02:26:16",4433.0,243104.0,0,"That second part is not in the message, but apparently that's what happens."
Jaeger,66147115,66048959,2,"2021/02/11, 02:26:16",False,"2021/02/11, 02:26:16",4433.0,243104.0,0,You should check to see if you're getting data appearing in Jaeger.
Jaeger,66147115,66048959,2,"2021/02/11, 02:26:16",False,"2021/02/11, 02:26:16",4433.0,243104.0,0,"If you are, that message is probably nothing to worry."
Jaeger,66147115,66048959,2,"2021/02/11, 02:26:16",False,"2021/02/11, 02:26:16",4433.0,243104.0,0,"If you are getting data in Jaeger, but it's not connected, that will be because Traefik can only only work with trace context that is already in inbound requests, but it can't add trace context to outbound requests."
Jaeger,66147115,66048959,2,"2021/02/11, 02:26:16",False,"2021/02/11, 02:26:16",4433.0,243104.0,0,"Within your application, you'll need to implement trace propagation so that your outbound requests include the trace context that was received as part of the incoming request."
Jaeger,66147115,66048959,2,"2021/02/11, 02:26:16",False,"2021/02/11, 02:26:16",4433.0,243104.0,0,"Without doing that, every request will be sent without trace context and will start a new trace when it is received at the next Traefik ingress point."
Jaeger,66151385,66048959,0,"2021/02/11, 10:45:37",True,"2021/02/11, 10:45:37",19808.0,376483.0,0,The problem actually was with the  traceContextHeaderName .
Jaeger,66151385,66048959,0,"2021/02/11, 10:45:37",True,"2021/02/11, 10:45:37",19808.0,376483.0,0,Sadly I can not tell exactly what the problem was as the  git diff  only shows that nothing changed around traefik and jaeger at the point where I fixed it.
Jaeger,66151385,66048959,0,"2021/02/11, 10:45:37",True,"2021/02/11, 10:45:37",19808.0,376483.0,0,I assume config got &quot;stuck&quot; somehow.
Jaeger,66151385,66048959,0,"2021/02/11, 10:45:37",True,"2021/02/11, 10:45:37",19808.0,376483.0,0,"I tracked down the  related lines in source , but as I am no Go-Dev, I can only guess if there's a bug."
Jaeger,66151385,66048959,0,"2021/02/11, 10:45:37",True,"2021/02/11, 10:45:37",19808.0,376483.0,0,"What I did was to switch back to  uber-trace-id , which magically &quot;fixed&quot; it."
Jaeger,66151385,66048959,0,"2021/02/11, 10:45:37",True,"2021/02/11, 10:45:37",19808.0,376483.0,0,"After I ran some traces and connected another service (node, npm  jaeger-client  with  process.env.TRACER_STATE_HEADER_NAME  set to an equal value), I switched back to  traefik-trace-id  and things worked."
Jaeger,65298980,65219729,1,"2020/12/15, 03:56:21",True,"2020/12/15, 03:56:21",471.0,2673284.0,1,"To my knowledge, the design of  ForkJoinPool.commonPool()  makes it impossible to actually replace that pool programmatically with an instrumented version."
Jaeger,65298980,65219729,1,"2020/12/15, 03:56:21",True,"2020/12/15, 03:56:21",471.0,2673284.0,1,So the only workaround is to do it via bytecode manipulation.
Jaeger,65298980,65219729,1,"2020/12/15, 03:56:21",True,"2020/12/15, 03:56:21",471.0,2673284.0,1,"The  OpenTelemetry Java Automatic Instrumentation  libraries perform a lot of magic to be able to take care of correctly propagating context through async/concurrency primitives, you may want to give them a try."
Jaeger,64891826,64878686,1,"2020/11/18, 13:05:30",False,"2020/11/18, 13:05:30",557.0,615104.0,2,"Tracing is enabled by default for JAX-RS endpoints only, not for reactive routes at the moment."
Jaeger,64891826,64878686,1,"2020/11/18, 13:05:30",False,"2020/11/18, 13:05:30",557.0,615104.0,2,You can activate tracing by annotating your route with  @org.eclipse.microprofile.opentracing.Traced .
Jaeger,66257794,64878686,0,"2021/02/18, 12:11:01",True,"2021/02/25, 01:42:56",1332.0,1687162.0,1,"Yes, adding @Traced enable to activate tracing on reactive routes."
Jaeger,66257794,64878686,0,"2021/02/18, 12:11:01",True,"2021/02/25, 01:42:56",1332.0,1687162.0,1,"Unfortunately, using both JAX-RS reactive and reactive routes bugs the tracing on event-loop threads used by JAX-RS reactive endpoint when they get executed."
Jaeger,66257794,64878686,0,"2021/02/18, 12:11:01",True,"2021/02/25, 01:42:56",1332.0,1687162.0,1,"I only started Quarkus 2 days ago so i don't really the reason of this behavior (and whether it's normal or it's a bug), but obviously switching between two completely mess up the tracing."
Jaeger,66257794,64878686,0,"2021/02/18, 12:11:01",True,"2021/02/25, 01:42:56",1332.0,1687162.0,1,Here is an example to easily reproduce it:
Jaeger,66257794,64878686,0,"2021/02/18, 12:11:01",True,"2021/02/25, 01:42:56",1332.0,1687162.0,1,Here is a screenshot that show the issue
Jaeger,66257794,64878686,0,"2021/02/18, 12:11:01",True,"2021/02/25, 01:42:56",1332.0,1687162.0,1,"As you can see, as soon as the JAX-RS resource is it and executed on one of the two threads available, it &quot;corrupts&quot; it, messing the trace_id reported (i don't know if it's the generation or the reporting on logs that is broken) on logs for the next calls of the reactive route."
Jaeger,66257794,64878686,0,"2021/02/18, 12:11:01",True,"2021/02/25, 01:42:56",1332.0,1687162.0,1,"This does not happen on the JAX-RS resource, as you can notice on the screenshot as well."
Jaeger,66257794,64878686,0,"2021/02/18, 12:11:01",True,"2021/02/25, 01:42:56",1332.0,1687162.0,1,So it seems to be related to reactive routes only.
Jaeger,66257794,64878686,0,"2021/02/18, 12:11:01",True,"2021/02/25, 01:42:56",1332.0,1687162.0,1,Another point here is the fact that JAX-RS Reactive resources are incorrectly reported on Jaeger.
Jaeger,66257794,64878686,0,"2021/02/18, 12:11:01",True,"2021/02/25, 01:42:56",1332.0,1687162.0,1,(with a mention to a missing root span) Not sure if it's related to the issue but that's also another annoying point.
Jaeger,66257794,64878686,0,"2021/02/18, 12:11:01",True,"2021/02/25, 01:42:56",1332.0,1687162.0,1,I'm thinking to completely remove the JAX-RS Reactive endpoint and replace them by normal reactive route to eliminate this bug.
Jaeger,66257794,64878686,0,"2021/02/18, 12:11:01",True,"2021/02/25, 01:42:56",1332.0,1687162.0,1,I would appreciate if someone with more experience than me could verify this or tell me what i did wrong :)
Jaeger,66257794,64878686,0,"2021/02/18, 12:11:01",True,"2021/02/25, 01:42:56",1332.0,1687162.0,1,"EDIT 1: I added a route filter with priority 500 to clear the MDC and the bug is still there, so definitely not coming from MDC."
Jaeger,66257794,64878686,0,"2021/02/18, 12:11:01",True,"2021/02/25, 01:42:56",1332.0,1687162.0,1,EDIT 2: I opened a  bug report  on Quarkus
Jaeger,66257794,64878686,0,"2021/02/18, 12:11:01",True,"2021/02/25, 01:42:56",1332.0,1687162.0,1,"EDIT 3: It seems related to how both implementations works (thread locals versus context propagation in actor based context)
So, unless JAX-RS reactive resources are marked @Blocking (and get executed in a separated thread pool), JAX-RS reactive and Vertx reactive routes are incompatible when it comes to tracing (but also probably the same for MDC related informations since MDC is also thread related)"
Jaeger,64067421,64051610,0,"2020/09/25, 18:50:14",False,"2020/09/25, 18:50:14",1532.0,5491213.0,0,"I tried applying your file on a Kubernetes 1.16 cluster, and there are a couple of issues with it:"
Jaeger,64067421,64051610,0,"2020/09/25, 18:50:14",False,"2020/09/25, 18:50:14",1532.0,5491213.0,0,The .spec.selector field defines how the Deployment finds which Pods to manage.
Jaeger,64067421,64051610,0,"2020/09/25, 18:50:14",False,"2020/09/25, 18:50:14",1532.0,5491213.0,0,It seems like you are applying something that is super old.
Jaeger,64067421,64051610,0,"2020/09/25, 18:50:14",False,"2020/09/25, 18:50:14",1532.0,5491213.0,0,"Kubernetes notes the below in its doc, and so I wonder if this used to work on older versions of Kubernetes where selectors were defaulted."
Jaeger,64067421,64051610,0,"2020/09/25, 18:50:14",False,"2020/09/25, 18:50:14",1532.0,5491213.0,0,"As of Kubernetes 1.8, you must specify a pod selector that matches the labels of the .spec.template."
Jaeger,64067421,64051610,0,"2020/09/25, 18:50:14",False,"2020/09/25, 18:50:14",1532.0,5491213.0,0,The pod selector will no longer be defaulted when left empty.
Jaeger,64067421,64051610,0,"2020/09/25, 18:50:14",False,"2020/09/25, 18:50:14",1532.0,5491213.0,0,"It seems like you should take a new approach-- in looking around, I found a couple of good tutorials  here  and  here , and Jaeger themselves offer a similar approach  here ."
Jaeger,64067421,64051610,0,"2020/09/25, 18:50:14",False,"2020/09/25, 18:50:14",1532.0,5491213.0,0,They all make use of  Kubernetes Operators .
Jaeger,64067421,64051610,0,"2020/09/25, 18:50:14",False,"2020/09/25, 18:50:14",1532.0,5491213.0,0,"A Kubernetes operator is an application-specific controller that extends the functionality of the Kubernetes API to create, configure, and manage instances of complex applications on behalf of a Kubernetes user"
Jaeger,64067421,64051610,0,"2020/09/25, 18:50:14",False,"2020/09/25, 18:50:14",1532.0,5491213.0,0,I don't know what you mean by  &quot;So previously I had a binary executable (jaeger-agent) running on a Linux CentOS 8 box alongside a server side application&quot;
Jaeger,64067421,64051610,0,"2020/09/25, 18:50:14",False,"2020/09/25, 18:50:14",1532.0,5491213.0,0,"The file you are applying looks like it  deploys the agent as a daemonset , which means the agent is run as a pod on each node of your cluster."
Jaeger,64067421,64051610,0,"2020/09/25, 18:50:14",False,"2020/09/25, 18:50:14",1532.0,5491213.0,0,"If it is running in your k8's cluster, then  this is how I normally approach troubleshooting kubernetes services ."
Jaeger,64067421,64051610,0,"2020/09/25, 18:50:14",False,"2020/09/25, 18:50:14",1532.0,5491213.0,0,"If it is running outside of your cluster entirely, then you need to make sure the Service it talks to is exposed outside of the cluster probably using type LoadBalancer."
Jaeger,62430615,62418175,0,"2020/06/17, 16:50:21",True,"2020/06/17, 16:50:21",213.0,7933630.0,0,"Metrics in OpenTelemetry are currently undergoing continued development and refinement, so they aren't necessarily available for each language yet (see  this tag in the OpenTelemetry JS repo  for an example of metric instruments that aren't up to date yet with spec), but once they are, I'd expect for metrics to be added to the existing node/web instrumentation packages."
Jaeger,62430615,62418175,0,"2020/06/17, 16:50:21",True,"2020/06/17, 16:50:21",213.0,7933630.0,0,"That said, I would still advise you to try out OpenTelemetry for traces at this point, as it's pretty stable for tracing."
Jaeger,62430615,62418175,0,"2020/06/17, 16:50:21",True,"2020/06/17, 16:50:21",213.0,7933630.0,0,"You can use a Prometheus client to export metrics separately, and once OpenTelemetry metrics are fully supported in the JS library, switch over to that."
Jaeger,60989810,60978505,3,"2020/04/02, 13:49:29",False,"2020/04/02, 13:49:29",2576.0,12014434.0,0,There are a few different reasons why You could be experiencing this issue.
Jaeger,60989810,60978505,3,"2020/04/02, 13:49:29",False,"2020/04/02, 13:49:29",2576.0,12014434.0,0,From  istio  documentation:
Jaeger,60989810,60978505,3,"2020/04/02, 13:49:29",False,"2020/04/02, 13:49:29",2576.0,12014434.0,0,"Since Istio 1.0.3, the sampling rate for tracing has been reduced to 1% in the   default    configuration profile ."
Jaeger,60989810,60978505,3,"2020/04/02, 13:49:29",False,"2020/04/02, 13:49:29",2576.0,12014434.0,0,This means that only 1 out of 100 trace instances captured by Istio will be reported to the tracing backend.
Jaeger,60989810,60978505,3,"2020/04/02, 13:49:29",False,"2020/04/02, 13:49:29",2576.0,12014434.0,0,The sampling rate in the   demo   profile is still set to 100%.
Jaeger,60989810,60978505,3,"2020/04/02, 13:49:29",False,"2020/04/02, 13:49:29",2576.0,12014434.0,0,See   this section   for more information on how to set the sampling rate.
Jaeger,60989810,60978505,3,"2020/04/02, 13:49:29",False,"2020/04/02, 13:49:29",2576.0,12014434.0,0,"If you still do not see any trace data, please confirm that your ports conform to the Istio   port naming conventions   and that the appropriate container port is exposed (via pod spec, for example) to enable traffic capture by the sidecar proxy (Envoy)."
Jaeger,60989810,60978505,3,"2020/04/02, 13:49:29",False,"2020/04/02, 13:49:29",2576.0,12014434.0,0,"If you only see trace data associated with the egress proxy, but not the ingress proxy, it may still be related to the Istio   port naming conventions ."
Jaeger,60989810,60978505,3,"2020/04/02, 13:49:29",False,"2020/04/02, 13:49:29",2576.0,12014434.0,0,Starting with   Istio 1.3   the protocol for   outbound   traffic is automatically detected.
Jaeger,60989810,60978505,3,"2020/04/02, 13:49:29",False,"2020/04/02, 13:49:29",2576.0,12014434.0,0,Hope it helps.
Jaeger,59894312,59875985,0,"2020/01/24, 12:11:13",True,"2020/01/24, 12:23:57",4418.0,127670.0,0,"The short answer is ""you can't."""
Jaeger,59894312,59875985,0,"2020/01/24, 12:11:13",True,"2020/01/24, 12:23:57",4418.0,127670.0,0,My question was based on a very fundamental misunderstanding of what opentracing does.
Jaeger,59894312,59875985,0,"2020/01/24, 12:11:13",True,"2020/01/24, 12:23:57",4418.0,127670.0,0,"Tracing context is only propagated downstream, not upstream."
Jaeger,59894312,59875985,0,"2020/01/24, 12:11:13",True,"2020/01/24, 12:23:57",4418.0,127670.0,0,From the same discussion thread:
Jaeger,59894312,59875985,0,"2020/01/24, 12:11:13",True,"2020/01/24, 12:23:57",4418.0,127670.0,0,"On the wire propagation is only meant to carry ""span context"", which
  is a small set of ID fields and possible baggage."
Jaeger,59894312,59875985,0,"2020/01/24, 12:11:13",True,"2020/01/24, 12:23:57",4418.0,127670.0,0,"Returning the whole
  trace as part of the request is not a use case that was considered."
Jaeger,59894312,59875985,0,"2020/01/24, 12:11:13",True,"2020/01/24, 12:23:57",4418.0,127670.0,0,and
Jaeger,59894312,59875985,0,"2020/01/24, 12:11:13",True,"2020/01/24, 12:23:57",4418.0,127670.0,0,The trace collection is meant to be asynchronous and out of process.
Jaeger,59894312,59875985,0,"2020/01/24, 12:11:13",True,"2020/01/24, 12:23:57",4418.0,127670.0,0,So my understanding is now thus:
Jaeger,59894312,59875985,0,"2020/01/24, 12:11:13",True,"2020/01/24, 12:23:57",4418.0,127670.0,0,"Each individual software component creates its own tracing data, bundles it up, and sends it off to the tracing server (e.g."
Jaeger,59894312,59875985,0,"2020/01/24, 12:11:13",True,"2020/01/24, 12:23:57",4418.0,127670.0,0,Jaeger).
Jaeger,59894312,59875985,0,"2020/01/24, 12:11:13",True,"2020/01/24, 12:23:57",4418.0,127670.0,0,Each software component  must  be configured to use the same tracing provider and the same tracing server - an RPC client cannot tell an RPC server that for a particular trace it should use the Jaeger tracing provider and a Jaeger server at such-and-such an address.
Jaeger,59894312,59875985,0,"2020/01/24, 12:11:13",True,"2020/01/24, 12:23:57",4418.0,127670.0,0,"(At least, the opentracing standard doesn't provide a way to do this.)"
Jaeger,59894312,59875985,0,"2020/01/24, 12:11:13",True,"2020/01/24, 12:23:57",4418.0,127670.0,0,The tracing information injected into a RPC request by the client allows the RPC server to embed a 'parent' ID field into the tracing information.
Jaeger,59894312,59875985,0,"2020/01/24, 12:11:13",True,"2020/01/24, 12:23:57",4418.0,127670.0,0,It's then the responsibility of the tracer (e.g.
Jaeger,59894312,59875985,0,"2020/01/24, 12:11:13",True,"2020/01/24, 12:23:57",4418.0,127670.0,0,Jaeger) to figure out the relationships between the various traces it has received from various software components by matching up ID codes embedded in them.
Jaeger,59894312,59875985,0,"2020/01/24, 12:11:13",True,"2020/01/24, 12:23:57",4418.0,127670.0,0,So what I wanted to do is not a use case considered by opentracing and is not possible.
Jaeger,59841411,59839687,0,"2020/01/21, 14:41:01",False,"2020/01/21, 14:41:01",2576.0,12014434.0,0,My interpretation of this is that We need to keep in mind that communication between services needs to support forwarding/&quot;passing along&quot; the trace ID's so that the tracing works correctly.
Jaeger,59841411,59839687,0,"2020/01/21, 14:41:01",False,"2020/01/21, 14:41:01",2576.0,12014434.0,0,So it warns us against situations where:
Jaeger,59841411,59839687,0,"2020/01/21, 14:41:01",False,"2020/01/21, 14:41:01",2576.0,12014434.0,0,Client calls -&gt; Service A #using http request with trace ID in header.
Jaeger,59841411,59839687,0,"2020/01/21, 14:41:01",False,"2020/01/21, 14:41:01",2576.0,12014434.0,0,Service A -&gt; Service B #using tcp request that does not support headers and the trace ID header is lost.
Jaeger,59841411,59839687,0,"2020/01/21, 14:41:01",False,"2020/01/21, 14:41:01",2576.0,12014434.0,0,This situation could break or limit tracing functionality.
Jaeger,59841411,59839687,0,"2020/01/21, 14:41:01",False,"2020/01/21, 14:41:01",2576.0,12014434.0,0,On the other hand If we have situation where:
Jaeger,59841411,59839687,0,"2020/01/21, 14:41:01",False,"2020/01/21, 14:41:01",2576.0,12014434.0,0,Client calls -&gt; Service A #using http request with trace ID in header.
Jaeger,59841411,59839687,0,"2020/01/21, 14:41:01",False,"2020/01/21, 14:41:01",2576.0,12014434.0,0,Service A -&gt; Service B #using http request the trace ID is forwarded to service B.
Jaeger,59841411,59839687,0,"2020/01/21, 14:41:01",False,"2020/01/21, 14:41:01",2576.0,12014434.0,0,This situation allows for the trace ID header to be present in both connections so the tracing can be logged and then viewed in tracing service dashboard.
Jaeger,59841411,59839687,0,"2020/01/21, 14:41:01",False,"2020/01/21, 14:41:01",2576.0,12014434.0,0,Then We can explore the path taken by the request and view the latency incurred at each hop.
Jaeger,59841411,59839687,0,"2020/01/21, 14:41:01",False,"2020/01/21, 14:41:01",2576.0,12014434.0,0,Hope it helps.
Jaeger,60078995,59839687,0,"2020/02/05, 17:23:23",False,"2020/02/05, 17:23:23",461.0,3707487.0,3,"When doing tracing in a service mesh, behind proxies, the traceID generated upon the initial client call is propagated automatically only so long as the call goes from proxy- proxy."
Jaeger,60078995,59839687,0,"2020/02/05, 17:23:23",False,"2020/02/05, 17:23:23",461.0,3707487.0,3,So:
Jaeger,60078995,59839687,0,"2020/02/05, 17:23:23",False,"2020/02/05, 17:23:23",461.0,3707487.0,3,"To get around this, Microservice A just needs to know which headers represent the traceIDs, how to append into it, and some state to make sure it makes it to outgoing requests."
Jaeger,60078995,59839687,0,"2020/02/05, 17:23:23",False,"2020/02/05, 17:23:23",461.0,3707487.0,3,Then you'll get a full transaction chain.
Jaeger,60078995,59839687,0,"2020/02/05, 17:23:23",False,"2020/02/05, 17:23:23",461.0,3707487.0,3,"Without the service propagating the headers, tracing basically gives you each path that ends in a microservice."
Jaeger,60078995,59839687,0,"2020/02/05, 17:23:23",False,"2020/02/05, 17:23:23",461.0,3707487.0,3,"Still useful, but not as complete of a picture."
Jaeger,59577425,59576942,0,"2020/01/03, 13:02:50",False,"2020/01/03, 13:02:50",5244.0,5564578.0,2,"By default, you don't have a logging system on Istio."
Jaeger,59577425,59576942,0,"2020/01/03, 13:02:50",False,"2020/01/03, 13:02:50",5244.0,5564578.0,2,"I mean, besides the native logging of Kubernetes."
Jaeger,59577425,59576942,0,"2020/01/03, 13:02:50",False,"2020/01/03, 13:02:50",5244.0,5564578.0,2,"Zipkin and Jaeger are tracing systems, meaning for latency, not for logging."
Jaeger,59577425,59576942,0,"2020/01/03, 13:02:50",False,"2020/01/03, 13:02:50",5244.0,5564578.0,2,"You can definitely get this info through Istio components, but you will have to set it up first."
Jaeger,59577425,59576942,0,"2020/01/03, 13:02:50",False,"2020/01/03, 13:02:50",5244.0,5564578.0,2,I found  this  articles; in Istio website about how to collect logs.
Jaeger,59577425,59576942,0,"2020/01/03, 13:02:50",False,"2020/01/03, 13:02:50",5244.0,5564578.0,2,I would say  Fluentd  +  Elasticsearch  would give you something as powerful as you need.
Jaeger,59577425,59576942,0,"2020/01/03, 13:02:50",False,"2020/01/03, 13:02:50",5244.0,5564578.0,2,Unfortunately I don't have any examples.
Jaeger,59580066,59574885,1,"2020/01/03, 16:14:30",True,"2020/01/03, 17:50:28",2576.0,12014434.0,1,According to envoy proxy documentation for envoy  v1.12.0  used by istio  1.3 :
Jaeger,59580066,59574885,1,"2020/01/03, 16:14:30",True,"2020/01/03, 17:50:28",2576.0,12014434.0,1,Envoy provides the capability for reporting tracing information regarding communications between services in the mesh.
Jaeger,59580066,59574885,1,"2020/01/03, 16:14:30",True,"2020/01/03, 17:50:28",2576.0,12014434.0,1,"However, to be able to correlate the pieces of tracing information generated by the various proxies within a call flow, the services must propagate certain trace context between the inbound and outbound requests."
Jaeger,59580066,59574885,1,"2020/01/03, 16:14:30",True,"2020/01/03, 17:50:28",2576.0,12014434.0,1,"Whichever tracing provider is being used, the service should propagate the   x-request-id   to enable logging across the invoked services to be correlated."
Jaeger,59580066,59574885,1,"2020/01/03, 16:14:30",True,"2020/01/03, 17:50:28",2576.0,12014434.0,1,"The tracing providers also require additional context, to enable the parent/child relationships between the spans (logical units of work) to be understood."
Jaeger,59580066,59574885,1,"2020/01/03, 16:14:30",True,"2020/01/03, 17:50:28",2576.0,12014434.0,1,"This can be achieved by using the LightStep (via OpenTracing API) or Zipkin tracer directly within the service itself, to extract the trace context from the inbound request and inject it into any subsequent outbound requests."
Jaeger,59580066,59574885,1,"2020/01/03, 16:14:30",True,"2020/01/03, 17:50:28",2576.0,12014434.0,1,"This approach would also enable the service to create additional spans, describing work being done internally within the service, that may be useful when examining the end-to-end trace."
Jaeger,59580066,59574885,1,"2020/01/03, 16:14:30",True,"2020/01/03, 17:50:28",2576.0,12014434.0,1,Alternatively the trace context can be manually propagated by the service:
Jaeger,59580066,59574885,1,"2020/01/03, 16:14:30",True,"2020/01/03, 17:50:28",2576.0,12014434.0,1,"When using the LightStep tracer, Envoy relies on the service to propagate the 
   x-ot-span-context 
  HTTP header while sending HTTP requests to other services."
Jaeger,59580066,59574885,1,"2020/01/03, 16:14:30",True,"2020/01/03, 17:50:28",2576.0,12014434.0,1,"When using the Zipkin tracer, Envoy relies on the service to propagate the B3 HTTP headers ( 
   x-b3-traceid ,
   x-b3-spanid ,
   x-b3-parentspanid ,
   x-b3-sampled ,
  and 
   x-b3-flags )."
Jaeger,59580066,59574885,1,"2020/01/03, 16:14:30",True,"2020/01/03, 17:50:28",2576.0,12014434.0,1,"The 
   x-b3-sampled 
  header can also be supplied by an external client to either enable or
  disable tracing for a particular request."
Jaeger,59580066,59574885,1,"2020/01/03, 16:14:30",True,"2020/01/03, 17:50:28",2576.0,12014434.0,1,"In addition, the single 
   b3 
  header propagation format is supported, which is a more compressed
  format."
Jaeger,59580066,59574885,1,"2020/01/03, 16:14:30",True,"2020/01/03, 17:50:28",2576.0,12014434.0,1,"When using the Datadog tracer, Envoy relies on the service to propagate the Datadog-specific HTTP headers ( 
   x-datadog-trace-id ,
   x-datadog-parent-id ,
   x-datadog-sampling-priority )."
Jaeger,59580066,59574885,1,"2020/01/03, 16:14:30",True,"2020/01/03, 17:50:28",2576.0,12014434.0,1,TLDR: traceId headers need to be manually added to B3 HTTP headers.
Jaeger,59580066,59574885,1,"2020/01/03, 16:14:30",True,"2020/01/03, 17:50:28",2576.0,12014434.0,1,Additional information:  https://github.com/openzipkin/b3-propagation
Jaeger,58269994,58228451,2,"2019/10/07, 15:54:44",True,"2019/10/07, 15:54:44",6049.0,3156333.0,1,If you have sampling rate set to 1% then error will be seen in Jaeger once it occurs 100 times.
Jaeger,58269994,58228451,2,"2019/10/07, 15:54:44",True,"2019/10/07, 15:54:44",6049.0,3156333.0,1,This is mentioned at  Distributed Tracing - Jaeger :
Jaeger,58269994,58228451,2,"2019/10/07, 15:54:44",True,"2019/10/07, 15:54:44",6049.0,3156333.0,1,"To see trace data, you must send requests to your service."
Jaeger,58269994,58228451,2,"2019/10/07, 15:54:44",True,"2019/10/07, 15:54:44",6049.0,3156333.0,1,The number of requests depends on Istio’s sampling rate.
Jaeger,58269994,58228451,2,"2019/10/07, 15:54:44",True,"2019/10/07, 15:54:44",6049.0,3156333.0,1,You set this rate when you install Istio.
Jaeger,58269994,58228451,2,"2019/10/07, 15:54:44",True,"2019/10/07, 15:54:44",6049.0,3156333.0,1,The default sampling rate is 1%.
Jaeger,58269994,58228451,2,"2019/10/07, 15:54:44",True,"2019/10/07, 15:54:44",6049.0,3156333.0,1,You need to send at least 100 requests before the first trace is visible.
Jaeger,58269994,58228451,2,"2019/10/07, 15:54:44",True,"2019/10/07, 15:54:44",6049.0,3156333.0,1,"To send a 100 requests to the   productpage   service, use the following command:"
Jaeger,58269994,58228451,2,"2019/10/07, 15:54:44",True,"2019/10/07, 15:54:44",6049.0,3156333.0,1,$ for i in  `seq 1 100`;  do  curl -s -o /dev/null http://$GATEWAY_URL/productpage;  done
Jaeger,58269994,58228451,2,"2019/10/07, 15:54:44",True,"2019/10/07, 15:54:44",6049.0,3156333.0,1,"If you are not seeing the error in the current sample, I would advice make the sample higher."
Jaeger,58269994,58228451,2,"2019/10/07, 15:54:44",True,"2019/10/07, 15:54:44",6049.0,3156333.0,1,You can read about  Tracing context propagation  which is being done by  Envoy .
Jaeger,58269994,58228451,2,"2019/10/07, 15:54:44",True,"2019/10/07, 15:54:44",6049.0,3156333.0,1,Envoy automatically sends spans to tracing collectors
Jaeger,58269994,58228451,2,"2019/10/07, 15:54:44",True,"2019/10/07, 15:54:44",6049.0,3156333.0,1,Alternatively the trace context can be manually propagated by the service:
Jaeger,57918375,57913923,1,"2019/09/13, 09:31:04",True,"2019/09/13, 09:31:04",269.0,12020494.0,6,"Just mentioning beforehand (you might already know) that a Kubernetes Service is not a ""service"" as in a piece of code."
Jaeger,57918375,57913923,1,"2019/09/13, 09:31:04",True,"2019/09/13, 09:31:04",269.0,12020494.0,6,"It is a way for Kubernetes components &amp; deployments to communicate with one another through an interface which always stays the same, regardless of how many pods or servers there are."
Jaeger,57918375,57913923,1,"2019/09/13, 09:31:04",True,"2019/09/13, 09:31:04",269.0,12020494.0,6,"When Istio deploys it's tracing mechanism, it deploys modular parts so it can deploy them independently, and also scale them independently, very much like micro-services."
Jaeger,57918375,57913923,1,"2019/09/13, 09:31:04",True,"2019/09/13, 09:31:04",269.0,12020494.0,6,Generally a Kubernetes deployed utility will be deployed as a few parts which make up the bigger picture.
Jaeger,57918375,57913923,1,"2019/09/13, 09:31:04",True,"2019/09/13, 09:31:04",269.0,12020494.0,6,For instance in your case:
Jaeger,57918375,57913923,1,"2019/09/13, 09:31:04",True,"2019/09/13, 09:31:04",269.0,12020494.0,6,jaeger-agent - This is the components which will collect all the traffic and tracing from your nodes.
Jaeger,57918375,57913923,1,"2019/09/13, 09:31:04",True,"2019/09/13, 09:31:04",269.0,12020494.0,6,"jaeger-collector - This is the place where all of the jaeger-agents will push the logs and traces they find on the node, and the collector will aggregate these as a trace may span multiple nodes."
Jaeger,57918375,57913923,1,"2019/09/13, 09:31:04",True,"2019/09/13, 09:31:04",269.0,12020494.0,6,tracing - might be the component which injects the tracing ID's into network traffic for the agent to watch.
Jaeger,57918375,57913923,1,"2019/09/13, 09:31:04",True,"2019/09/13, 09:31:04",269.0,12020494.0,6,"zipkin - could be the UI which allows debugging with traces, or replaying requests etc."
Jaeger,57918375,57913923,1,"2019/09/13, 09:31:04",True,"2019/09/13, 09:31:04",269.0,12020494.0,6,"The above might not be absolutely correct, but I hope you get the idea of why multiple parts would be deployed."
Jaeger,57918375,57913923,1,"2019/09/13, 09:31:04",True,"2019/09/13, 09:31:04",269.0,12020494.0,6,"In the same way we deploy mysql, and our containers separately, Kubernetes projects are generally deployed as a set of deployments or pods."
Jaeger,58058841,57913923,0,"2019/09/23, 11:52:53",False,"2019/09/23, 11:52:53",13313.0,524946.0,1,"To complement @christiaan-vermeulen's answer: the  tracing  service is Jaeger's UI (jaeger-query) so that the same URL can be used for alternative backends, whereas the Zipkin service is a convenience service, allowing applications using Zipkin tracers (like Brave) to send data to Jaeger without requiring complex changes."
Jaeger,58058841,57913923,0,"2019/09/23, 11:52:53",False,"2019/09/23, 11:52:53",13313.0,524946.0,1,"If you look closely, the Zipkin service is backed by the jaeger-collector as well."
Jaeger,57474533,57473932,2,"2019/08/13, 12:10:40",True,"2019/08/13, 12:10:40",246.0,10967175.0,0,I hope you have followed the official documentation of the jager with istio.
Jaeger,57474533,57473932,2,"2019/08/13, 12:10:40",True,"2019/08/13, 12:10:40",246.0,10967175.0,0,If you are using the helm chart make the following changes required.
Jaeger,57474533,57473932,2,"2019/08/13, 12:10:40",True,"2019/08/13, 12:10:40",246.0,10967175.0,0,Export the dashboard via Kube port-forward or ingress.
Jaeger,57474533,57473932,2,"2019/08/13, 12:10:40",True,"2019/08/13, 12:10:40",246.0,10967175.0,0,Official Documentation.
Jaeger,57474533,57473932,2,"2019/08/13, 12:10:40",True,"2019/08/13, 12:10:40",246.0,10967175.0,0,https://istio.io/docs/tasks/telemetry/distributed-tracing/jaeger/
Jaeger,57474533,57473932,2,"2019/08/13, 12:10:40",True,"2019/08/13, 12:10:40",246.0,10967175.0,0,NOTE: The important thing by default jaeger will trace something like 0.1% request i.e.
Jaeger,57474533,57473932,2,"2019/08/13, 12:10:40",True,"2019/08/13, 12:10:40",246.0,10967175.0,0,1 request out of 100 so put a lot of requests only then you can see a trace in UI.
Jaeger,57397006,57380205,0,"2019/08/07, 17:43:53",False,"2019/08/07, 17:43:53",527.0,3966540.0,0,I had a wrong opencensus collector configuration.
Jaeger,57397006,57380205,0,"2019/08/07, 17:43:53",False,"2019/08/07, 17:43:53",527.0,3966540.0,0,The docker container network cannot see port 9411 as it was on the host network.
Jaeger,57397006,57380205,0,"2019/08/07, 17:43:53",False,"2019/08/07, 17:43:53",527.0,3966540.0,0,I was able to fix the issue after noticing this misconfiguration.
Jaeger,57295589,57273076,2,"2019/07/31, 19:48:01",False,"2019/07/31, 19:48:01",13313.0,524946.0,0,"OpenTracing does not define the concrete data model, how the data should be collected, nor how it should be transported."
Jaeger,57295589,57273076,2,"2019/07/31, 19:48:01",False,"2019/07/31, 19:48:01",13313.0,524946.0,0,"As such, there's no specification for the endpoint."
Jaeger,57295589,57273076,2,"2019/07/31, 19:48:01",False,"2019/07/31, 19:48:01",13313.0,524946.0,0,"This allows implementations like Jaeger to use non-HTTP transport by default when sending data from the client (tracer) to the backend, by sending UDP packets to an intermediate ""Jaeger Agent""."
Jaeger,57295589,57273076,2,"2019/07/31, 19:48:01",False,"2019/07/31, 19:48:01",13313.0,524946.0,0,"Given that the base model is pretty much similar among implementations, it's common to have tracing solutions to support each other's endpoints."
Jaeger,57295589,57273076,2,"2019/07/31, 19:48:01",False,"2019/07/31, 19:48:01",13313.0,524946.0,0,"For instance, Jaeger is able to expose an endpoint with  Zipkin compatibility ."
Jaeger,57295589,57273076,2,"2019/07/31, 19:48:01",False,"2019/07/31, 19:48:01",13313.0,524946.0,0,"Based on your question, I think you might be interested in the OpenTelemetry project, a successor to the OpenTracing project, as the result of a merge with the OpenCensus project."
Jaeger,57295589,57273076,2,"2019/07/31, 19:48:01",False,"2019/07/31, 19:48:01",13313.0,524946.0,0,"OpenTelemetry provides its own tracer and is able to ""receive"" data in several formats (including Jaeger), and ""export"" to several backends."
Jaeger,56462471,56202494,2,"2019/06/05, 17:23:36",True,"2019/06/05, 17:23:36",3681.0,11300382.0,2,Assuming that your services are  defined in Istio’s internal service registry.
Jaeger,56462471,56202494,2,"2019/06/05, 17:23:36",True,"2019/06/05, 17:23:36",3681.0,11300382.0,2,If not please configure it according to instruction  service-defining .
Jaeger,56462471,56202494,2,"2019/06/05, 17:23:36",True,"2019/06/05, 17:23:36",3681.0,11300382.0,2,"In HTTPS all the HTTP-related information like method, URL path, response code, is encrypted so Istio  cannot  see and cannot monitor that information for HTTPS."
Jaeger,56462471,56202494,2,"2019/06/05, 17:23:36",True,"2019/06/05, 17:23:36",3681.0,11300382.0,2,"If you need to monitor HTTP-related information in access to external HTTPS services, you may want to let your applications issue HTTP requests and configure Istio to perform TLS origination."
Jaeger,56462471,56202494,2,"2019/06/05, 17:23:36",True,"2019/06/05, 17:23:36",3681.0,11300382.0,2,First you have to  redefine  your ServiceEntry and create VirtualService  to rewrite the HTTP request port and add a DestinationRule to perform TLS origination.
Jaeger,56462471,56202494,2,"2019/06/05, 17:23:36",True,"2019/06/05, 17:23:36",3681.0,11300382.0,2,The VirtualService redirects HTTP requests on port 80 to port 443 where the corresponding DestinationRule then performs the TLS origination.
Jaeger,56462471,56202494,2,"2019/06/05, 17:23:36",True,"2019/06/05, 17:23:36",3681.0,11300382.0,2,"Unlike the previous ServiceEntry, this time the protocol on port 443 is HTTP, instead of HTTPS, because clients will only send HTTP requests and Istio will upgrade the connection to HTTPS."
Jaeger,56462471,56202494,2,"2019/06/05, 17:23:36",True,"2019/06/05, 17:23:36",3681.0,11300382.0,2,I hope it helps.
Jaeger,54367053,54078937,0,"2019/01/25, 16:16:32",True,"2019/01/25, 16:16:32",13313.0,524946.0,2,"Note that tracing data (spans) are not the same as ""metrics"", although there could be some overlap in some cases."
Jaeger,54367053,54078937,0,"2019/01/25, 16:16:32",True,"2019/01/25, 16:16:32",13313.0,524946.0,2,"I recommend the following blog post on what is the purpose of each, including logging:"
Jaeger,54367053,54078937,0,"2019/01/25, 16:16:32",True,"2019/01/25, 16:16:32",13313.0,524946.0,2,https://peter.bourgon.org/blog/2017/02/21/metrics-tracing-and-logging.html
Jaeger,54367053,54078937,0,"2019/01/25, 16:16:32",True,"2019/01/25, 16:16:32",13313.0,524946.0,2,"That said, there is the OpenTracing library mentioned in the blog post you linked, called  opentracing-contrib/java-metrics ."
Jaeger,54367053,54078937,0,"2019/01/25, 16:16:32",True,"2019/01/25, 16:16:32",13313.0,524946.0,2,It allows you to pick specific spans and record them as data points (metrics).
Jaeger,54367053,54078937,0,"2019/01/25, 16:16:32",True,"2019/01/25, 16:16:32",13313.0,524946.0,2,"It works as a decorator of a concrete tracer, so, your spans would reach a concrete backend like Jaeger and, additionally, create data points based on the configured spans."
Jaeger,54367053,54078937,0,"2019/01/25, 16:16:32",True,"2019/01/25, 16:16:32",13313.0,524946.0,2,"The data points are then reported via  Micrometer , which can be configured to expose this data in Prometheus format."
Jaeger,54367053,54078937,0,"2019/01/25, 16:16:32",True,"2019/01/25, 16:16:32",13313.0,524946.0,2,"The problem is, I haven't found any consistent solution to do this, all the examples that I have found so far are outdated, deprecated or lacks documentation."
Jaeger,54367053,54078937,0,"2019/01/25, 16:16:32",True,"2019/01/25, 16:16:32",13313.0,524946.0,2,"Please, open an issue on the  java-metrics  repository with the problems you are facing."
Jaeger,53600710,53095177,1,"2018/12/03, 21:38:34",True,"2018/12/03, 21:38:34",467.0,1404502.0,1,Just to close this question out for the solution to the problem in my instance.
Jaeger,53600710,53095177,1,"2018/12/03, 21:38:34",True,"2018/12/03, 21:38:34",467.0,1404502.0,1,The mistake in configuration started all the way back in the Kubernetes cluster initialisation.
Jaeger,53600710,53095177,1,"2018/12/03, 21:38:34",True,"2018/12/03, 21:38:34",467.0,1404502.0,1,I had applied:
Jaeger,53600710,53095177,1,"2018/12/03, 21:38:34",True,"2018/12/03, 21:38:34",467.0,1404502.0,1,the pod-network-cidr using the same address range as the local LAN on which the Kubernetes installation was deployed i.e.
Jaeger,53600710,53095177,1,"2018/12/03, 21:38:34",True,"2018/12/03, 21:38:34",467.0,1404502.0,1,the desktop for the Ubuntu host used the same IP subnet as what I'd assigned the container network.
Jaeger,53600710,53095177,1,"2018/12/03, 21:38:34",True,"2018/12/03, 21:38:34",467.0,1404502.0,1,"For the most part, everything operated fine as detailed above, until the Istio proxy was trying to route packets from an external load-balancer IP address to an internal IP address which happened to be on the same subnet."
Jaeger,53600710,53095177,1,"2018/12/03, 21:38:34",True,"2018/12/03, 21:38:34",467.0,1404502.0,1,Project Calico with Kubernetes seemed to be able to cope with it as that's effectively Layer 3/4 policy but Istio had a problem with it a L7 (even though it was sitting on Calico underneath).
Jaeger,53600710,53095177,1,"2018/12/03, 21:38:34",True,"2018/12/03, 21:38:34",467.0,1404502.0,1,The solution was to tear down my entire Kubernetes deployment.
Jaeger,53600710,53095177,1,"2018/12/03, 21:38:34",True,"2018/12/03, 21:38:34",467.0,1404502.0,1,I was paranoid and went so far as to uninstall Kubernetes and deploy again and redeploy with a pod network in the 172 range which wasn't anything to do with my local lan.
Jaeger,53600710,53095177,1,"2018/12/03, 21:38:34",True,"2018/12/03, 21:38:34",467.0,1404502.0,1,I also made the same changes in the Project Calico configuration file to match pod networks.
Jaeger,53600710,53095177,1,"2018/12/03, 21:38:34",True,"2018/12/03, 21:38:34",467.0,1404502.0,1,"After that change, everything worked as expected."
Jaeger,53600710,53095177,1,"2018/12/03, 21:38:34",True,"2018/12/03, 21:38:34",467.0,1404502.0,1,I suspect that in a more public configuration where your cluster was directly attached to a BGP router as opposed to using MetalLB with an L2 configuration as a subset of your LAN wouldn't exhibit this issue either.
Jaeger,53600710,53095177,1,"2018/12/03, 21:38:34",True,"2018/12/03, 21:38:34",467.0,1404502.0,1,I've documented it more in this post:
Jaeger,53600710,53095177,1,"2018/12/03, 21:38:34",True,"2018/12/03, 21:38:34",467.0,1404502.0,1,"Microservices: .Net, Linux, Kubernetes and Istio make a powerful combination"
Jaeger,17618211,17616323,0,"2013/07/12, 18:12:11",False,"2013/07/12, 18:12:11",67.0,1278255.0,0,"Actually, there is no error."
Jaeger,17618211,17616323,0,"2013/07/12, 18:12:11",False,"2013/07/12, 18:12:11",67.0,1278255.0,0,The code was changing the color of a yellow texture to a red tint inline.
Jaeger,12318996,12318639,3,"2012/09/07, 16:23:31",True,"2012/09/07, 16:23:31",7480.0,1566232.0,1,Try this : (if you can gives us all the variants of the url it would be better)
Jaeger,54075526,54061611,0,"2019/01/07, 15:41:12",True,"2019/01/07, 15:41:12",213.0,7933630.0,2,"Unlike Jaeger, LightStep is a commercial SaaS offering."
Jaeger,54075526,54061611,0,"2019/01/07, 15:41:12",True,"2019/01/07, 15:41:12",213.0,7933630.0,2,"If you wanted to try out their service, you'd need to contact their sales team."
Jaeger,49580258,49571999,0,"2018/03/30, 22:27:46",False,"2018/03/30, 22:27:46",1.0,9573945.0,0,Managed to create the desired capturing groups:
Jaeger,49580258,49571999,0,"2018/03/30, 22:27:46",False,"2018/03/30, 22:27:46",1.0,9573945.0,0,"Then I could write out the files, it looks correct as for these few occurences."
InspectIT,54902800,52150469,0,"2019/02/27, 12:03:53",False,"2019/02/27, 12:03:53",479.0,715518.0,0,"You need to add instrumentation rules for your application to ""dig deeper""."
InspectIT,54902800,52150469,0,"2019/02/27, 12:03:53",False,"2019/02/27, 12:03:53",479.0,715518.0,0,The  doFilter  and  service  methods are instrumented by default as part of the HTTP instrumentation profile.
InspectIT,54902800,52150469,0,"2019/02/27, 12:03:53",False,"2019/02/27, 12:03:53",479.0,715518.0,0,"In addition to the HTTP profile, by default inspectIT instruments SQL statement executions, other entry points into your application (like JMS), the places where external HTTP/JMS calls are done and Java Executors."
InspectIT,54902800,52150469,0,"2019/02/27, 12:03:53",False,"2019/02/27, 12:03:53",479.0,715518.0,0,"There are some other common instrumentation profiles (Hibernate, SQL parameters, Spring, etc), but you need to enable them by yourself."
InspectIT,54902800,52150469,0,"2019/02/27, 12:03:53",False,"2019/02/27, 12:03:53",479.0,715518.0,0,"Usually, you would start with the default settings and then, in addition, add instrumentation rules related to your application."
Instana,55646922,55606472,0,"2019/04/12, 10:53:10",False,"2019/04/12, 10:53:10",294930.0,208809.0,3,Disclaimer: I work for Instana.
Instana,55646922,55606472,0,"2019/04/12, 10:53:10",False,"2019/04/12, 10:53:10",294930.0,208809.0,3,There is not much to setup.
Instana,55646922,55606472,0,"2019/04/12, 10:53:10",False,"2019/04/12, 10:53:10",294930.0,208809.0,3,Instana provides out-of-the-box support for Kafka and Zookeeper nodes.
Instana,55646922,55606472,0,"2019/04/12, 10:53:10",False,"2019/04/12, 10:53:10",294930.0,208809.0,3,So all you need to do is to install the Instana agent on the server(s) you want to monitor.
Instana,55646922,55606472,0,"2019/04/12, 10:53:10",False,"2019/04/12, 10:53:10",294930.0,208809.0,3,It will automatically detect your Kafka and Zookeeper installations and start reporting metrics for them to your tenant unit.
Instana,55646922,55606472,0,"2019/04/12, 10:53:10",False,"2019/04/12, 10:53:10",294930.0,208809.0,3,"If you don't have a tenant unit yet, you can register for a free trial at  https://www.instana.com/trial/  or contact Sales."
Instana,55646922,55606472,0,"2019/04/12, 10:53:10",False,"2019/04/12, 10:53:10",294930.0,208809.0,3,"If you need additional help, I suggest to open a ticket at  https://instana.zendesk.com  to get dedicated support."
Instana,57088646,55974537,0,"2019/07/18, 09:53:32",True,"2019/07/18, 09:53:32",465.0,469028.0,3,Instana has a  demo application  that shows to do this.
Instana,57088646,55974537,0,"2019/07/18, 09:53:32",True,"2019/07/18, 09:53:32",465.0,469028.0,3,To summarize the parts that you would need:
Instana,57088646,55974537,0,"2019/07/18, 09:53:32",True,"2019/07/18, 09:53:32",465.0,469028.0,3,The combination of these two steps will make TypeScript aware of the function.
Instana,57088646,55974537,0,"2019/07/18, 09:53:32",True,"2019/07/18, 09:53:32",465.0,469028.0,3,Now you can use  ineum  just like any other global.
Instana,59945966,59921305,1,"2020/01/28, 11:46:22",False,"2020/01/28, 14:25:45",1628.0,1847951.0,2,Instana will use the same protocol to make the sourcemap request.
Instana,59945966,59921305,1,"2020/01/28, 11:46:22",False,"2020/01/28, 14:25:45",1628.0,1847951.0,2,"The documentation example uses http, but it will work with https the same way."
Instana,59945966,59921305,1,"2020/01/28, 11:46:22",False,"2020/01/28, 14:25:45",1628.0,1847951.0,2,The most likely reason for your problem is that the sourcemap is not readable from the public internet.
Instana,59945966,59921305,1,"2020/01/28, 11:46:22",False,"2020/01/28, 14:25:45",1628.0,1847951.0,2,"In your case, the sourcemap file requires http session authentication and redirects to a login page."
Instana,60976067,57292600,0,"2020/04/01, 19:46:26",False,"2020/04/01, 19:46:26",121.0,2001962.0,1,"You could remove the location /nginx_status in that server, and add a new server section like this:"
Instana,58527533,58518001,2,"2019/10/23, 19:43:00",False,"2019/10/23, 19:43:00",51.0,10645824.0,3,"That endpoint requires a POST, it appears you are using GET."
Instana,58527533,58518001,2,"2019/10/23, 19:43:00",False,"2019/10/23, 19:43:00",51.0,10645824.0,3,Hence method not allowed.
Instana,64642758,64613844,0,"2020/11/02, 11:18:54",False,"2020/11/02, 11:18:54",11.0,14562868.0,1,"Instana offers an agent tailored to React native, which simplifies the integration."
Instana,64642758,64613844,0,"2020/11/02, 11:18:54",False,"2020/11/02, 11:18:54",11.0,14562868.0,1,The  React native agent  is different than the one used for website monitoring.
Instana,64642758,64613844,0,"2020/11/02, 11:18:54",False,"2020/11/02, 11:18:54",11.0,14562868.0,1,You can get started with React native monitoring by creating a mobile app within Instana's user interface under  Websites &amp; Mobile Apps -&gt; Mobile Apps .
Instana,64642758,64613844,0,"2020/11/02, 11:18:54",False,"2020/11/02, 11:18:54",11.0,14562868.0,1,For the React native agent you can find  dedicated documentation and installation instructions  on Instana's documentation site.
Instana,64642758,64613844,0,"2020/11/02, 11:18:54",False,"2020/11/02, 11:18:54",11.0,14562868.0,1,"For further questions and support, I suggest leveraging  Instana's support portal ."
Instana,57372845,57354975,0,"2019/08/06, 12:22:05",False,"2019/08/06, 12:22:05",1628.0,1847951.0,1,the Instana repository has been upgraded to support Disco Dingo as well.
Instana,62222959,62075759,0,"2020/06/05, 22:35:52",True,"2020/06/05, 22:35:52",4812.0,4828463.0,1,To whoever removed his/her answer: It was a correct answer.
Instana,62222959,62075759,0,"2020/06/05, 22:35:52",True,"2020/06/05, 22:35:52",4812.0,4828463.0,1,I don't know why you deleted it.
Instana,62222959,62075759,0,"2020/06/05, 22:35:52",True,"2020/06/05, 22:35:52",4812.0,4828463.0,1,"Anyhow, I am posting again in case someone stumbles here."
Instana,62222959,62075759,0,"2020/06/05, 22:35:52",True,"2020/06/05, 22:35:52",4812.0,4828463.0,1,You can control frequency and time by using  INSTANA_AGENT_UPDATES_FREQUENCY  and  INSTANA_AGENT_UPDATES_TIME  environment variables.
Instana,62222959,62075759,0,"2020/06/05, 22:35:52",True,"2020/06/05, 22:35:52",4812.0,4828463.0,1,Updating  mode  via env variable is still unknown at this point.
Instana,62222959,62075759,0,"2020/06/05, 22:35:52",True,"2020/06/05, 22:35:52",4812.0,4828463.0,1,Look at this page for more info:  https://www.instana.com/docs/setup_and_manage/host_agent/on/docker/#updates-and-version-pinning
Instana,64377786,62075759,1,"2020/10/15, 21:37:15",False,"2020/10/19, 07:30:18",11.0,13617981.0,1,"Most agent settings that one may want to change quickly are available as environment variables, see  https://www.instana.com/docs/setup_and_manage/host_agent/on/docker ."
Instana,64377786,62075759,1,"2020/10/15, 21:37:15",False,"2020/10/19, 07:30:18",11.0,13617981.0,1,"For example, setting the mode via environment variable is supported as well with  INSTANA_AGENT_MODE , see e.g.,  https://hub.docker.com/r/instana/agent ."
Instana,64377786,62075759,1,"2020/10/15, 21:37:15",False,"2020/10/19, 07:30:18",11.0,13617981.0,1,The valid values are:
Instana,64377786,62075759,1,"2020/10/15, 21:37:15",False,"2020/10/19, 07:30:18",11.0,13617981.0,1,"On Kubernetes, it is also of course possible to use a ConfigMap to override files in the agent container."
Instana,57736812,57580097,0,"2019/08/31, 12:35:09",True,"2019/08/31, 12:35:09",49.0,10826472.0,0,Solved.
Instana,57736812,57580097,0,"2019/08/31, 12:35:09",True,"2019/08/31, 12:35:09",49.0,10826472.0,0,"Added flags to my run configuration, and increase XMS and XMX twice."
Instana,41056601,41053682,3,"2016/12/09, 10:58:46",False,"2016/12/09, 10:58:46",8454.0,958529.0,0,"pod install  is  cocoapods  command, not part of  ruby  or  gem ."
Instana,41056601,41053682,3,"2016/12/09, 10:58:46",False,"2016/12/09, 10:58:46",8454.0,958529.0,0,The error means there is no  pod  or  install  package in  ruby  package repository.
Instana,41056601,41053682,3,"2016/12/09, 10:58:46",False,"2016/12/09, 10:58:46",8454.0,958529.0,0,"After writing a proper  Podfile  in your Xcode project dir, just run  pod install ."
LightStep,44529216,43796314,1,"2017/06/13, 21:43:41",True,"2017/06/13, 23:47:13",592.0,1359408.0,1,[Disclaimer: I work at  LightStep]
LightStep,44529216,43796314,1,"2017/06/13, 21:43:41",True,"2017/06/13, 23:47:13",592.0,1359408.0,1,Sorry you're having trouble getting Java and Go to play well together.
LightStep,44529216,43796314,1,"2017/06/13, 21:43:41",True,"2017/06/13, 23:47:13",592.0,1359408.0,1,I suspect this is caused by time-correction being enabled in Java but not being used in Go.
LightStep,44529216,43796314,1,"2017/06/13, 21:43:41",True,"2017/06/13, 23:47:13",592.0,1359408.0,1,"You can disable time correction in Java using the  withClockSkewCorrection(boolean clockCorrection)  
option to turn off clockCorrection when passing in options to the LightStep tracer"
LightStep,44529216,43796314,1,"2017/06/13, 21:43:41",True,"2017/06/13, 23:47:13",592.0,1359408.0,1,Here is the updated  README  and a link to the  option code
LightStep,44529216,43796314,1,"2017/06/13, 21:43:41",True,"2017/06/13, 23:47:13",592.0,1359408.0,1,"If you contact us via the [Support] button in LightStep, we should be able to get you sorted out."
LightStep,44529216,43796314,1,"2017/06/13, 21:43:41",True,"2017/06/13, 23:47:13",592.0,1359408.0,1,Please send us a note so that we can confirm that this is solved for you.
LightStep,44529216,43796314,1,"2017/06/13, 21:43:41",True,"2017/06/13, 23:47:13",592.0,1359408.0,1,We'll start monitoring SO more carefully so that we catch these things earlier.
LightStep,44529216,43796314,1,"2017/06/13, 21:43:41",True,"2017/06/13, 23:47:13",592.0,1359408.0,1,Thanks and happy tracing!
LightStep,44529216,43796314,1,"2017/06/13, 21:43:41",True,"2017/06/13, 23:47:13",592.0,1359408.0,1,Will
LightStep,63725459,63724073,2,"2020/09/03, 17:13:18",True,"2020/09/03, 17:13:18",91.0,1548178.0,2,lightstep-opentelemetry-launcher-node  basically bundles the required things for you for easier configuration so this is not an exporter.
LightStep,63725459,63724073,2,"2020/09/03, 17:13:18",True,"2020/09/03, 17:13:18",91.0,1548178.0,2,If you were to simply replace the &quot;LightstepExporter&quot; with &quot;OpenTelemetry Collector Exporter&quot; in your code you can simply do this
LightStep,63725459,63724073,2,"2020/09/03, 17:13:18",True,"2020/09/03, 17:13:18",91.0,1548178.0,2,The default  YOUR_DIGETS_URL  from  lightstep/otel-launcher-node  is  https://ingest.lightstep.com:443/api/v2/otel/trace
LightStep,49949656,49949470,0,"2018/04/21, 00:00:46",False,"2018/04/21, 00:00:46",8336.0,1773866.0,1,"If you go to the latest snapshot documentation (or milestone) and you search for the word OpenTracing, you would get your answer."
LightStep,49949656,49949470,0,"2018/04/21, 00:00:46",False,"2018/04/21, 00:00:46",8336.0,1773866.0,1,It's here  https://cloud.spring.io/spring-cloud-sleuth/single/spring-cloud-sleuth.html#_opentracing
LightStep,49949656,49949470,0,"2018/04/21, 00:00:46",False,"2018/04/21, 00:00:46",8336.0,1773866.0,1,Spring Cloud Sleuth is compatible with OpenTracing.
LightStep,49949656,49949470,0,"2018/04/21, 00:00:46",False,"2018/04/21, 00:00:46",8336.0,1773866.0,1,"If you have OpenTracing on the classpath, we automatically register the OpenTracing Tracer bean."
LightStep,49949656,49949470,0,"2018/04/21, 00:00:46",False,"2018/04/21, 00:00:46",8336.0,1773866.0,1,"If you wish to disable this, set  spring.sleuth.opentracing.enabled  to false"
LightStep,49949656,49949470,0,"2018/04/21, 00:00:46",False,"2018/04/21, 00:00:46",8336.0,1773866.0,1,So it's enough to just have OpenTracing on the classpath and Sleuth will work out of the box
SkyWalking,58586629,58314080,0,"2019/10/28, 08:40:49",True,"2019/10/28, 08:40:49",395.0,9571426.0,0,"Explanation how to set up  skywalking  properly:
 https://github.com/apache/skywalking/issues/3589#issuecomment-543268029"
SkyWalking,62389599,60476903,0,"2020/06/15, 16:48:11",False,"2020/06/15, 16:48:11",6436.0,2628868.0,0,"It is the dashboard default time filter value problem, the time range did not contains data:"
SkyWalking,62389599,60476903,0,"2020/06/15, 16:48:11",False,"2020/06/15, 16:48:11",6436.0,2628868.0,0,change the time start and end to having collection data area.
SkyWalking,62358989,62358639,0,"2020/06/13, 14:19:26",True,"2020/06/13, 14:19:26",6436.0,2628868.0,1,Finally I build the side car image by myself:
SkyWalking,62358989,62358639,0,"2020/06/13, 14:19:26",True,"2020/06/13, 14:19:26",6436.0,2628868.0,1,this is the docker file:
SkyWalking,63516070,63509093,0,"2020/08/21, 06:24:43",True,"2020/08/21, 06:24:43",22293.0,225016.0,3,how to add the jdbc driver jar into the image file?
SkyWalking,63516070,63509093,0,"2020/08/21, 06:24:43",True,"2020/08/21, 06:24:43",22293.0,225016.0,3,One way would be an  initContainer:  and then artificially inject the jdbc driver via  -Xbootclasspath
SkyWalking,63516070,63509093,0,"2020/08/21, 06:24:43",True,"2020/08/21, 06:24:43",22293.0,225016.0,3,"a similar, although slightly riskier way, is to find a path that is already on the classpath of the image, and attempt to volume mount the jar path into that directory"
SkyWalking,63516070,63509093,0,"2020/08/21, 06:24:43",True,"2020/08/21, 06:24:43",22293.0,225016.0,3,"All of this seems kind of moot given that your image looks like one that is custom built, and therefore the correct action is to update the  Dockerfile  for it to download the jar at build time"
SkyWalking,65282846,65259756,0,"2020/12/14, 04:23:10",True,"2020/12/14, 04:23:10",1070.0,5196039.0,0,"I created a lifecycle that performs the delete action after a set time, and then I added this configuration to the skywalking application.yml under  storage.elasticsearch7 :"
SkyWalking,65282846,65259756,0,"2020/12/14, 04:23:10",True,"2020/12/14, 04:23:10",1070.0,5196039.0,0,"SW creates index templates, and now I see that this is part of the template, and indeed the indexes have this sw-policy attached."
SkyWalking,60472081,60465004,0,"2020/03/01, 06:50:19",False,"2020/03/01, 06:50:19",43078.0,78722.0,2,So you can see this more clearly in the output.
SkyWalking,60472081,60465004,0,"2020/03/01, 06:50:19",False,"2020/03/01, 06:50:19",43078.0,78722.0,2,The pod is Running but the Ready flag is false meaning the container is up but is failing the Readiness Probe.
SkyWalking,60463565,60462786,0,"2020/02/29, 11:08:36",True,"2020/02/29, 11:08:36",33843.0,498256.0,2,So you should go throught this document first
SkyWalking,60463565,60462786,0,"2020/02/29, 11:08:36",True,"2020/02/29, 11:08:36",33843.0,498256.0,2,https://kubernetes.io/docs/concepts/storage/volumes/#hostpath
SkyWalking,60463565,60462786,0,"2020/02/29, 11:08:36",True,"2020/02/29, 11:08:36",33843.0,498256.0,2,use  hostPath  as sample
SkyWalking,60463565,60462786,0,"2020/02/29, 11:08:36",True,"2020/02/29, 11:08:36",33843.0,498256.0,2,You need reference it for both init container and normal container.
Stagemonitor,44376892,44347364,0,"2017/06/05, 23:09:57",True,"2017/06/06, 08:46:33",4936.0,1125055.0,1,Most of the metrics stagemonitor collects are not available via JMX.
Stagemonitor,44376892,44347364,0,"2017/06/05, 23:09:57",True,"2017/06/06, 08:46:33",4936.0,1125055.0,1,"For example, response time statistics grouped by the endpoint of your application."
Stagemonitor,44376892,44347364,0,"2017/06/05, 23:09:57",True,"2017/06/06, 08:46:33",4936.0,1125055.0,1,"Also, stagemonitor is much more than just metrics."
Stagemonitor,44376892,44347364,0,"2017/06/05, 23:09:57",True,"2017/06/06, 08:46:33",4936.0,1125055.0,1,"It is also a profiler, you can use to see which methods caused a request to be slow."
Stagemonitor,44376892,44347364,0,"2017/06/05, 23:09:57",True,"2017/06/06, 08:46:33",4936.0,1125055.0,1,"Further more, it can (soon) do distributed tracing which helps you to analyze and debug latency problems in a microservice environment by correlating related requests."
Stagemonitor,44376892,44347364,0,"2017/06/05, 23:09:57",True,"2017/06/06, 08:46:33",4936.0,1125055.0,1,It also offers you a Kibana dashboard you can use to drill into the requests your application serves to find out about causes of errors or latency.
Stagemonitor,44376892,44347364,0,"2017/06/05, 23:09:57",True,"2017/06/06, 08:46:33",4936.0,1125055.0,1,Another use case is lightweight web analytics to identify which devices and operating systems your customers use to access your site.
Stagemonitor,48148146,48147866,0,"2018/01/08, 12:12:45",False,"2018/01/08, 12:12:45",128.0,7445902.0,0,Two possible solutions.
Stagemonitor,48148146,48147866,0,"2018/01/08, 12:12:45",False,"2018/01/08, 12:12:45",128.0,7445902.0,0,"You can have a button 'hide', that will hide the metrics using some javascript code."
Stagemonitor,48148146,48147866,0,"2018/01/08, 12:12:45",False,"2018/01/08, 12:12:45",128.0,7445902.0,0,Or in the same button you do the following:
Stagemonitor,30496430,30493074,0,"2015/05/28, 05:55:25",True,"2015/05/28, 05:55:25",74274.0,160313.0,1,It doesn't appear to be compatible with Grails.
Stagemonitor,30496430,30493074,0,"2015/05/28, 05:55:25",True,"2015/05/28, 05:55:25",74274.0,160313.0,1,If you enable logging
Stagemonitor,30496430,30493074,0,"2015/05/28, 05:55:25",True,"2015/05/28, 05:55:25",74274.0,160313.0,1,you'll see a bunch of error stacktraces that appear to imply that the way they're using Javassist to wire in tracing code isn't compatible with Groovy and/or the AST transformations that Grails uses:
Stagemonitor,38195456,38184669,0,"2016/07/05, 07:59:13",True,"2017/02/13, 14:44:38",3568.0,1679544.0,1,Finally I found out how to disable the browser widget.
Stagemonitor,38195456,38184669,0,"2016/07/05, 07:59:13",True,"2017/02/13, 14:44:38",3568.0,1679544.0,1,Set:
Stagemonitor,38195456,38184669,0,"2016/07/05, 07:59:13",True,"2017/02/13, 14:44:38",3568.0,1679544.0,1,You can see more information about it  here .
Stagemonitor,43811524,43806990,4,"2017/05/05, 21:23:27",False,"2017/05/05, 21:23:27",104968.0,1766831.0,0,MySQL does not provide anything more than how much data each tenant has.
Stagemonitor,43811524,43806990,4,"2017/05/05, 21:23:27",False,"2017/05/05, 21:23:27",104968.0,1766831.0,0,That can be found in  information_schema .
Stagemonitor,43811524,43806990,4,"2017/05/05, 21:23:27",False,"2017/05/05, 21:23:27",104968.0,1766831.0,0,"If you need CPU/IO/etc., you need to set up multiple instances of MySQL in VMs or cgroups and have the OS / VM-manager provide the data."
Stagemonitor,43811524,43806990,4,"2017/05/05, 21:23:27",False,"2017/05/05, 21:23:27",104968.0,1766831.0,0,"This will cost extra RAM, so it may not be worth it."
Stagemonitor,48163586,48162254,0,"2018/01/09, 09:52:48",True,"2018/01/09, 09:52:48",4936.0,1125055.0,1,I'm afraid this is currently not possible.
Stagemonitor,48163586,48162254,0,"2018/01/09, 09:52:48",True,"2018/01/09, 09:52:48",4936.0,1125055.0,1,"However, stagemonitor offers a ""Custom Metrics"" dashboard for Grafana."
Stagemonitor,48163586,48162254,0,"2018/01/09, 09:52:48",True,"2018/01/09, 09:52:48",4936.0,1125055.0,1,"To see the metrics locally, currently the only way is to enable periodic logging of all metrics."
Stagemonitor,26004106,25204392,1,"2014/09/23, 23:23:43",False,"2014/09/23, 23:23:43",4936.0,1125055.0,3,Stagemonitor now features a in browser widget that is automatically injected in your web page.
Stagemonitor,26004106,25204392,1,"2014/09/23, 23:23:43",False,"2014/09/23, 23:23:43",4936.0,1125055.0,3,You don't need any infrastructure or docker for this and the configuration and set up is easy.
Stagemonitor,26004106,25204392,1,"2014/09/23, 23:23:43",False,"2014/09/23, 23:23:43",4936.0,1125055.0,3,"For more information, visit  http://www.stagemonitor.org/ ."
Stagemonitor,26004106,25204392,1,"2014/09/23, 23:23:43",False,"2014/09/23, 23:23:43",4936.0,1125055.0,3,This is how you enable the widget:  https://github.com/stagemonitor/stagemonitor/wiki/Step-2%3A-Log-Only-Monitoring-In-Browser-Widget#in-browser-widget .
Wavefront VMware,63893372,63635003,0,"2020/09/15, 03:17:06",False,"2020/09/15, 03:17:06",5847.0,3390417.0,0,"The problem(s) (as noted in  GEODE-788 ,  GEODE-7665 ,  GEODE-7666 ,  GEODE-7670 ,  GEODE-7672  and  GEODE-7676 ) is, is that GemFire/Geode does not support  Region.clear()  for  PARTITION   Regions  (yet)."
Wavefront VMware,63893372,63635003,0,"2020/09/15, 03:17:06",False,"2020/09/15, 03:17:06",5847.0,3390417.0,0,"When you declare the  @CacheEvent(allEntries = true)  annotation/attribute on your managed application component, for example..."
Wavefront VMware,63893372,63635003,0,"2020/09/15, 03:17:06",False,"2020/09/15, 03:17:06",5847.0,3390417.0,0,This in effect calls  Region.clear()  (see  here ).
Wavefront VMware,63893372,63635003,0,"2020/09/15, 03:17:06",False,"2020/09/15, 03:17:06",5847.0,3390417.0,0,"This behavior works for  REPLICATE  and  LOCAL   Regions , however not for  PARTITION   Regions , given the numerous GemFire/Geode problems."
Wavefront VMware,63893372,63635003,0,"2020/09/15, 03:17:06",False,"2020/09/15, 03:17:06",5847.0,3390417.0,0,"It is currently a WIP, though."
Wavefront VMware,63893372,63635003,0,"2020/09/15, 03:17:06",False,"2020/09/15, 03:17:06",5847.0,3390417.0,0,"There was (partly, still is) an intention in Spring Data for Apache Geode &amp; VMware Tanzu (Pivotal) GemFire to handle cache clear operations."
Wavefront VMware,63893372,63635003,0,"2020/09/15, 03:17:06",False,"2020/09/15, 03:17:06",5847.0,3390417.0,0,https://jira.spring.io/browse/DATAGEODE-265
Wavefront VMware,63893372,63635003,0,"2020/09/15, 03:17:06",False,"2020/09/15, 03:17:06",5847.0,3390417.0,0,"However, this is on hold until the above GEODE JIRA tickets get sorted out."
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201.0,1585136.0,0,The short answer is no.
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201.0,1585136.0,0,"You really, really want to have DNS set up properly."
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201.0,1585136.0,0,Here's the long answer that is more nuanced.
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201.0,1585136.0,0,All requests to your foundation go through the Gorouter.
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201.0,1585136.0,0,"Gorouter will take the incoming request, look at the  Host  header and use that to determine where to send the request."
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201.0,1585136.0,0,This happens the same for system services like CAPI and UAA as it does for apps you deploy to the foundation.
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201.0,1585136.0,0,DNS is a requirement because of the  Host  header.
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201.0,1585136.0,0,A browser trying to access CAPI or an application on your foundation is going to set the  Host  header based on the DNS entry you type into your browser's address bar.
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201.0,1585136.0,0,The cf CLI is going to do the same thing.
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201.0,1585136.0,0,There are some ways to work around this:
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201.0,1585136.0,0,If you are strictly using a client like  curl  where you can set the  Host  header to arbitrary values.
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201.0,1585136.0,0,"In that way, you could set the host header to  api.system_domain  and at the same time connect to the IP address of your foundation."
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201.0,1585136.0,0,That's not a very elegant way to use CF though.
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201.0,1585136.0,0,You can manually set entries in your /etc/hosts` (or similar on Windows).
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201.0,1585136.0,0,This is basically a way to override DNS resolution and supply your own custom IP.
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201.0,1585136.0,0,"You would need to do this for  uaa.system_domain ,  login.system_domain ,  api.system_domain  and any host names you want to use for apps deployed to your foundation, like  my-super-cool-app.apps_domain ."
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201.0,1585136.0,0,These should all point to the IP of the load balancer that's in front of your pool of Gorouters.
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201.0,1585136.0,0,If you add enough entries into  /etc/hosts  you can make the cf CLI work.
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201.0,1585136.0,0,I have done this on occasion to bypass the load balancer layer for troubleshooting purposes.
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201.0,1585136.0,0,"Where this won't work is on systems where you can't edit  /etc/hosts , like customers or external users of software running on your foundation or if you're trying to deploy apps on your foundation that talk to each other using routes on CF (because you can't edit  /etc/hosts  in the container)."
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201.0,1585136.0,0,Like if you have  app-a.apps_domain  and  app-b.apps_domain  and  app-a  needs to talk to  app-b .
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201.0,1585136.0,0,That won't work because you have no DNS resolution for  apps_domain .
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201.0,1585136.0,0,You can probably make app-to-app communication work if you are able to use container-to-container networking and the  apps.internal  domain though.
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201.0,1585136.0,0,The resolution for that domain is provided by Bosh DNS.
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201.0,1585136.0,0,"You have to be aware of this difference though when deploying your apps and map routes on the  apps.internal  domain, as well as setting network policy to allow traffic to flow between the two."
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201.0,1585136.0,0,"Anyway, there might be other hiccups."
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201.0,1585136.0,0,This is just off the top of my head.
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201.0,1585136.0,0,You can see it's a lot better if you can set up DNS.
Wavefront VMware,66634194,66545598,0,"2021/03/15, 09:54:59",False,"2021/03/15, 09:54:59",31.0,7582877.0,0,The most easy way to achieve a portable solution is a service like  xip.io  that will work out of the box.
Wavefront VMware,66634194,66545598,0,"2021/03/15, 09:54:59",False,"2021/03/15, 09:54:59",31.0,7582877.0,0,"I have setup and run a lot of PoCs that way, when wildcard DNS was something that enterprise IT was still oblivious about."
Wavefront VMware,66634194,66545598,0,"2021/03/15, 09:54:59",False,"2021/03/15, 09:54:59",31.0,7582877.0,0,It works like this (excerpt from their site):
Wavefront VMware,66634194,66545598,0,"2021/03/15, 09:54:59",False,"2021/03/15, 09:54:59",31.0,7582877.0,0,What is xip.io?
Wavefront VMware,66634194,66545598,0,"2021/03/15, 09:54:59",False,"2021/03/15, 09:54:59",31.0,7582877.0,0,"xip.io is a magic domain name that provides wildcard DNS
for any IP address."
Wavefront VMware,66634194,66545598,0,"2021/03/15, 09:54:59",False,"2021/03/15, 09:54:59",31.0,7582877.0,0,Say your LAN IP address is 10.0.0.1.
Wavefront VMware,66634194,66545598,0,"2021/03/15, 09:54:59",False,"2021/03/15, 09:54:59",31.0,7582877.0,0,"Using xip.io,"
Wavefront VMware,66634194,66545598,0,"2021/03/15, 09:54:59",False,"2021/03/15, 09:54:59",31.0,7582877.0,0,"mysite.10.0.0.1.xip.io   resolves to   10.0.0.1
foo.bar.10.0.0.1.xip.io   resolves to   10.0.0.1"
Wavefront VMware,66634194,66545598,0,"2021/03/15, 09:54:59",False,"2021/03/15, 09:54:59",31.0,7582877.0,0,...and so on.
Wavefront VMware,66634194,66545598,0,"2021/03/15, 09:54:59",False,"2021/03/15, 09:54:59",31.0,7582877.0,0,"You can use these domains to access virtual
hosts on your development web server from devices on your
local network, like iPads, iPhones, and other computers."
Wavefront VMware,66634194,66545598,0,"2021/03/15, 09:54:59",False,"2021/03/15, 09:54:59",31.0,7582877.0,0,No configuration required!
Wavefront VMware,67007504,67007310,3,"2021/04/08, 18:47:38",False,"2021/04/08, 18:47:38",5028.0,5965430.0,1,It is unclear whether you're using the SCDF tile or the SCDF OSS (via  manfest.yml ) on PCF.
Wavefront VMware,67007504,67007310,3,"2021/04/08, 18:47:38",False,"2021/04/08, 18:47:38",5028.0,5965430.0,1,"Suppose you're using the OSS, AFA."
Wavefront VMware,67007504,67007310,3,"2021/04/08, 18:47:38",False,"2021/04/08, 18:47:38",5028.0,5965430.0,1,"In that case, you are providing the right RMQ service-instance configuration (that you pre-created) in the  manifest.yml , then SCDF would automatically propagate that RMQ service instance and bind it to the apps it is deploying to your ORG/Space."
Wavefront VMware,67007504,67007310,3,"2021/04/08, 18:47:38",False,"2021/04/08, 18:47:38",5028.0,5965430.0,1,You don't need to muck around with connection credentials manually.
Wavefront VMware,67007504,67007310,3,"2021/04/08, 18:47:38",False,"2021/04/08, 18:47:38",5028.0,5965430.0,1,"On the other hand, if you are using the SCDF Tile, the SCDF service broker will auto-create the RMQ SI and automatically bind it to the apps it deploys."
Wavefront VMware,67007504,67007310,3,"2021/04/08, 18:47:38",False,"2021/04/08, 18:47:38",5028.0,5965430.0,1,"In summary, there's no reason to manually pass the connection credentials or pack them as application properties inside your apps."
Wavefront VMware,67007504,67007310,3,"2021/04/08, 18:47:38",False,"2021/04/08, 18:47:38",5028.0,5965430.0,1,You can automate all this provided you're configuring all this correctly.
Wavefront VMware,63456882,63327611,0,"2020/08/17, 21:44:26",False,"2020/08/17, 21:44:26",432.0,6689024.0,0,"&quot; In this case it will wait till the processing completes or it
forcibly reduces the instance count when reached threshold."
Wavefront VMware,63456882,63327611,0,"2020/08/17, 21:44:26",False,"2020/08/17, 21:44:26",432.0,6689024.0,0,&quot;
Wavefront VMware,63456882,63327611,0,"2020/08/17, 21:44:26",False,"2020/08/17, 21:44:26",432.0,6689024.0,0,"Answer: 
No, the App Autoscaler will not force anything, after the decision cycle, it will prepare the instance to be escalated-down (shutdown), so the intention is to avoid lose requests or data during this process."
Wavefront VMware,63456882,63327611,0,"2020/08/17, 21:44:26",False,"2020/08/17, 21:44:26",432.0,6689024.0,0,"Please, take a look into the documentation below, it will help you to understand better the App Autoscaler mechanism."
Wavefront VMware,63456882,63327611,0,"2020/08/17, 21:44:26",False,"2020/08/17, 21:44:26",432.0,6689024.0,0,How App Autoscaler Determines When to Scale:
Wavefront VMware,63456882,63327611,0,"2020/08/17, 21:44:26",False,"2020/08/17, 21:44:26",432.0,6689024.0,0,"Every 35 seconds, App Autoscaler makes a decision about whether to
scale up, scale down, or keep the same number of instances."
Wavefront VMware,63456882,63327611,0,"2020/08/17, 21:44:26",False,"2020/08/17, 21:44:26",432.0,6689024.0,0,"To make a scaling decision, App Autoscaler averages the values of a
given metric for the most recent 120 seconds."
Wavefront VMware,63456882,63327611,0,"2020/08/17, 21:44:26",False,"2020/08/17, 21:44:26",432.0,6689024.0,0,The following diagram provides an example of how App Autoscaler makes scaling decisions:
Wavefront VMware,63456882,63327611,0,"2020/08/17, 21:44:26",False,"2020/08/17, 21:44:26",432.0,6689024.0,0,"Reference: 
 VMWare Tanzu App Autoscaler documentation"
Wavefront VMware,63456882,63327611,0,"2020/08/17, 21:44:26",False,"2020/08/17, 21:44:26",432.0,6689024.0,0,VMWare Tanzu is the former Pivotal Cloud Foundry (PCF).
Wavefront VMware,67134760,63327611,0,"2021/04/17, 07:49:29",False,"2021/04/17, 07:49:29",11.0,15668534.0,1,I have the same question and as far as I understood from  App Container Lifecycle  it’s up to your app to gracefully shutdown but that might not be possible in given 10 seconds as some processes might take longer.
Wavefront VMware,67134760,63327611,0,"2021/04/17, 07:49:29",False,"2021/04/17, 07:49:29",11.0,15668534.0,1,"Shutdown 
CF requests a shutdown of your app instance in the following scenarios:
When a user runs  cf scale , cf stop, cf push, cf delete, or cf restart-app-instance
As a result of a system event, such as the replacement procedure during Diego Cell evacuation or when an app instance stops because of a failed health check probe
To shut down the app, CF sends the app process in the container a SIGTERM."
Wavefront VMware,67134760,63327611,0,"2021/04/17, 07:49:29",False,"2021/04/17, 07:49:29",11.0,15668534.0,1,"By default, the process has ten seconds to shut down gracefully."
Wavefront VMware,67134760,63327611,0,"2021/04/17, 07:49:29",False,"2021/04/17, 07:49:29",11.0,15668534.0,1,"If the process has not exited after ten seconds, CF sends a SIGKILL."
Wavefront VMware,67134760,63327611,0,"2021/04/17, 07:49:29",False,"2021/04/17, 07:49:29",11.0,15668534.0,1,"By default, apps must finish their in-flight jobs within ten seconds of receiving the SIGTERM before CF terminates the app with a SIGKILL."
Wavefront VMware,67134760,63327611,0,"2021/04/17, 07:49:29",False,"2021/04/17, 07:49:29",11.0,15668534.0,1,"For instance, a web app must finish processing existing requests  and stop accepting new requests."
Wavefront VMware,67134760,63327611,0,"2021/04/17, 07:49:29",False,"2021/04/17, 07:49:29",11.0,15668534.0,1,Note: One exception to the cases mentioned above is when monit restarts a crashed Diego Cell rep or Garden server.
Wavefront VMware,67134760,63327611,0,"2021/04/17, 07:49:29",False,"2021/04/17, 07:49:29",11.0,15668534.0,1,"In this case, CF immediately stops the apps that are still running using SIGKILL."
Wavefront VMware,59697948,59047752,0,"2020/01/11, 21:31:36",False,"2020/01/11, 21:31:36",334.0,1147487.0,0,I think there's a workaround for kubernetes versions prior to 1.17.
Wavefront VMware,59697948,59047752,0,"2020/01/11, 21:31:36",False,"2020/01/11, 21:31:36",334.0,1147487.0,0,On  kubernetes version v1.16  you can run Sonobuoy (Sonobuoy version v0.16.1 or higher) with providing the test framework flag:  --allowed-not-ready-nodes=1
Wavefront VMware,59697948,59047752,0,"2020/01/11, 21:31:36",False,"2020/01/11, 21:31:36",334.0,1147487.0,0,And on  kubernetes version prior to v1.16  it was more complicated.
Wavefront VMware,59697948,59047752,0,"2020/01/11, 21:31:36",False,"2020/01/11, 21:31:36",334.0,1147487.0,0,I haven't tested this but according to docs:
Wavefront VMware,64468853,64453467,1,"2020/10/21, 20:36:57",False,"2020/10/21, 20:36:57",1170.0,4462517.0,1,Pivotal Web Services is not the same as Pivotal Cloud Foundry.
Wavefront VMware,64468853,64453467,1,"2020/10/21, 20:36:57",False,"2020/10/21, 20:36:57",1170.0,4462517.0,1,"Pivotal Web Services has been sunset, yes."
Wavefront VMware,64468853,64453467,1,"2020/10/21, 20:36:57",False,"2020/10/21, 20:36:57",1170.0,4462517.0,1,"Tanzu Application Service is VMware's enterprise solution that is, if you want to think about it this way, a self-hosted Pivotal Web Services (this is a gross understatement, but works for this situation)."
Wavefront VMware,64468853,64453467,1,"2020/10/21, 20:36:57",False,"2020/10/21, 20:36:57",1170.0,4462517.0,1,Are you looking to test Cloud Foundry for its suitability?
Wavefront VMware,65097195,64327529,0,"2020/12/01, 21:27:18",False,"2020/12/01, 21:27:18",43.0,13300061.0,0,Add the AzureIdentity and AzureIdentityBinding roles to cluster as mentioned in docs.
Wavefront VMware,65097195,64327529,0,"2020/12/01, 21:27:18",False,"2020/12/01, 21:27:18",43.0,13300061.0,0,"once done, use the selector defined AzureIdentityBinding as label while deploying helm chart."
Wavefront VMware,65097195,64327529,0,"2020/12/01, 21:27:18",False,"2020/12/01, 21:27:18",43.0,13300061.0,0,Check for the actual syntax for podLabels using with --set in helm install command.
Wavefront VMware,65097195,64327529,0,"2020/12/01, 21:27:18",False,"2020/12/01, 21:27:18",43.0,13300061.0,0,Or you can clone the charts and make changes to values.yaml below and install it from local charts.
Wavefront VMware,65097195,64327529,0,"2020/12/01, 21:27:18",False,"2020/12/01, 21:27:18",43.0,13300061.0,0,https://github.com/vmware-tanzu/helm-charts/blob/04615803a7d976ce15a6eeb4b1bd6a1cfb9a02c5/charts/velero/values.yaml#L27
Wavefront VMware,65097195,64327529,0,"2020/12/01, 21:27:18",False,"2020/12/01, 21:27:18",43.0,13300061.0,0,"Just for help:
 https://medium.com/@kimvisscher/using-aad-pod-identity-in-an-aks-cluster-117c08565692"
Wavefront VMware,65372173,64327529,0,"2020/12/19, 18:50:18",False,"2020/12/19, 18:50:18",73.0,3389881.0,0,It seems that you are struggling with how to format the  secretContents  section of the values.yaml file.
Wavefront VMware,65372173,64327529,0,"2020/12/19, 18:50:18",False,"2020/12/19, 18:50:18",73.0,3389881.0,0,"If that is so, take a look at a recent update to the documentation."
Wavefront VMware,65372173,64327529,0,"2020/12/19, 18:50:18",False,"2020/12/19, 18:50:18",73.0,3389881.0,0,It lays out and documents exactly how to format it:
Wavefront VMware,65372173,64327529,0,"2020/12/19, 18:50:18",False,"2020/12/19, 18:50:18",73.0,3389881.0,0,https://github.com/vmware-tanzu/helm-charts/blob/1ea07c7fb9c7ba910ba52801c536a6f8dcee096d/charts/velero/values.yaml#L219-L232
Zipkin,47672314,47670883,2,"2017/12/06, 12:42:50",True,"2017/12/06, 12:42:50",3566.0,1358676.0,11,I found that I need to add a sampler percentage.
Zipkin,47672314,47670883,2,"2017/12/06, 12:42:50",True,"2017/12/06, 12:42:50",3566.0,1358676.0,11,By default zero percentage of the samples are sent and that is why the sleuth was not sending anything to zipkin.
Zipkin,47672314,47670883,2,"2017/12/06, 12:42:50",True,"2017/12/06, 12:42:50",3566.0,1358676.0,11,"when I added  spring.sleuth.sampler.percentage=1.0  in the properties files, it started working."
Zipkin,49838749,47670883,0,"2018/04/15, 08:20:04",False,"2018/04/15, 08:20:04",201.0,7956609.0,2,"If you are exporting all the span data to Zipkin, sampler can be installed  by creating a bean definition in the Spring boot main class"
Zipkin,54384025,47670883,0,"2019/01/27, 02:18:35",False,"2019/01/27, 02:18:35",339.0,1405291.0,9,"For the latest version of cloud dependencies  &lt;version&gt;Finchley.SR2&lt;/version&gt; 
The correct property to send traces to zipkin is:  spring.sleuth.sampler.probability=1.0 
Which has changed from percentage to probability."
Zipkin,42580765,34272334,0,"2017/03/03, 15:55:09",False,"2017/03/03, 15:55:09",1247.0,1232692.0,1,"It is a very long time ago, but it looks like it was moved here:"
Zipkin,42580765,34272334,0,"2017/03/03, 15:55:09",False,"2017/03/03, 15:55:09",1247.0,1232692.0,1,http://zipkin.io/pages/quickstart
Zipkin,57852082,34272334,0,"2019/09/09, 13:21:48",False,"2019/09/09, 13:21:48",3190.0,2987755.0,0,Found multiple language examples at  github .
Zipkin,57852082,34272334,0,"2019/09/09, 13:21:48",False,"2019/09/09, 13:21:48",3190.0,2987755.0,0,"If you need basic setup steps:  https://zipkin.io/ 
Integrated zipkin with spring boot 2 and mysql 
 Steps 
 example 
Here is sample"
Zipkin,49724310,45046528,0,"2018/04/09, 04:06:02",False,"2018/04/09, 04:06:02",765.0,6466540.0,0,Lately I have been trying the same and couldn't find that option in initializer.
Zipkin,49724310,45046528,0,"2018/04/09, 04:06:02",False,"2018/04/09, 04:06:02",765.0,6466540.0,0,I am just posting this if anyone encounters the same issues and lands on this page.
Zipkin,49724310,45046528,0,"2018/04/09, 04:06:02",False,"2018/04/09, 04:06:02",765.0,6466540.0,0,"You can refer below sample GitHub project which is consists of four micro services ( zipkin server, client, rest service, and Eureka ) using Edgware release with latest version of sleuth."
Zipkin,49724310,45046528,0,"2018/04/09, 04:06:02",False,"2018/04/09, 04:06:02",765.0,6466540.0,0, Sample Zipkin Server/Client
Zipkin,50127150,45046528,0,"2018/05/02, 07:24:17",False,"2018/05/02, 07:24:17",1767.0,2498986.0,3,Zipkin Server is not part of Spring initializers.
Zipkin,50127150,45046528,0,"2018/05/02, 07:24:17",False,"2018/05/02, 07:24:17",1767.0,2498986.0,3,You have to use the official release of the Zipkin server
Zipkin,50127150,45046528,0,"2018/05/02, 07:24:17",False,"2018/05/02, 07:24:17",1767.0,2498986.0,3,https://github.com/openzipkin/zipkin#quick-start
Zipkin,50127150,45046528,0,"2018/05/02, 07:24:17",False,"2018/05/02, 07:24:17",1767.0,2498986.0,3,And custom servers are not supported anymore meaning you can't use  @EnableZipkinServer  anymore since 2.7
Zipkin,50127150,45046528,0,"2018/05/02, 07:24:17",False,"2018/05/02, 07:24:17",1767.0,2498986.0,3,https://github.com/openzipkin/zipkin#quick-start
Zipkin,42403962,42402849,1,"2017/02/23, 01:04:02",False,"2017/02/23, 01:04:02",84586.0,1384297.0,3,"I can't explain the error that you got with Dalston.BUILD-SNAPSHOT, but the error with Camden.SR4 is because it's not compatible with Spring Boot 1.5."
Zipkin,42403962,42402849,1,"2017/02/23, 01:04:02",False,"2017/02/23, 01:04:02",84586.0,1384297.0,3,I'd recommend upgrading to Camden.SR5  which is compatible with Spring Boot 1.5 .
Zipkin,47165778,42402849,0,"2017/11/07, 21:14:23",False,"2017/11/07, 21:14:23",349.0,961275.0,1,Even I got this error while setting up my project.
Zipkin,47165778,42402849,0,"2017/11/07, 21:14:23",False,"2017/11/07, 21:14:23",349.0,961275.0,1,I was using Spring boot 1.5.8 with the Brixton.SR6 release.
Zipkin,47165778,42402849,0,"2017/11/07, 21:14:23",False,"2017/11/07, 21:14:23",349.0,961275.0,1,"However, when I consulted the site  http://projects.spring.io/spring-cloud/  I got to know the issue and I updated my dependency to Dalston.SR4 and then the application started working."
Zipkin,39174580,39174579,2,"2016/08/27, 00:02:27",True,"2016/08/27, 00:02:27",473.0,2733462.0,8,"The official documentation was helpful, but I think it didn't include all the dependencies explicitly (at least as of now)."
Zipkin,39174580,39174579,2,"2016/08/27, 00:02:27",True,"2016/08/27, 00:02:27",473.0,2733462.0,8,I had to do some extra research for samples to get all the required dependencies and configuration together.
Zipkin,39174580,39174579,2,"2016/08/27, 00:02:27",True,"2016/08/27, 00:02:27",473.0,2733462.0,8,"I wanted to share it, because I believe it could be helpful for someone else."
Zipkin,39174580,39174579,2,"2016/08/27, 00:02:27",True,"2016/08/27, 00:02:27",473.0,2733462.0,8,Spring Boot version:   1.4.0.RELEASE
Zipkin,39174580,39174579,2,"2016/08/27, 00:02:27",True,"2016/08/27, 00:02:27",473.0,2733462.0,8,Spring Cloud version:   Brixton.SR4
Zipkin,39174580,39174579,2,"2016/08/27, 00:02:27",True,"2016/08/27, 00:02:27",473.0,2733462.0,8,POM:
Zipkin,39174580,39174579,2,"2016/08/27, 00:02:27",True,"2016/08/27, 00:02:27",473.0,2733462.0,8,Java:
Zipkin,39174580,39174579,2,"2016/08/27, 00:02:27",True,"2016/08/27, 00:02:27",473.0,2733462.0,8,application.yml:
Zipkin,39174580,39174579,2,"2016/08/27, 00:02:27",True,"2016/08/27, 00:02:27",473.0,2733462.0,8,References:
Zipkin,39174580,39174579,2,"2016/08/27, 00:02:27",True,"2016/08/27, 00:02:27",473.0,2733462.0,8,https://cloud.spring.io/spring-cloud-sleuth/
Zipkin,63987464,60023762,0,"2020/09/21, 09:54:36",False,"2021/03/27, 13:21:49",1.0,13026705.0,0,"Your application starts in Tomcat Server but Zipkin use another server but i dont know that name , i include this code to spring boot starter web dependecy for ignore tomcat server"
Zipkin,63987464,60023762,0,"2020/09/21, 09:54:36",False,"2021/03/27, 13:21:49",1.0,13026705.0,0,This worked for me try it
Zipkin,59544246,59444729,0,"2019/12/31, 14:36:17",True,"2019/12/31, 19:46:58",3932.0,1371995.0,1,I have tested this with the official  opencensus-node  example at github.
Zipkin,59544246,59444729,0,"2019/12/31, 14:36:17",True,"2019/12/31, 19:46:58",3932.0,1371995.0,1,"Zipkin UI displays 2 spans with same name MyApplication(see image),
  but expected 2 different span names"
Zipkin,59544246,59444729,0,"2019/12/31, 14:36:17",True,"2019/12/31, 19:46:58",3932.0,1371995.0,1,"Just to be clear,  MyApplication  is the service name you set in your app.js, and the span names are those which you selected on the image  /service1 ,  /service1 ,  /external_service_2 ."
Zipkin,59544246,59444729,0,"2019/12/31, 14:36:17",True,"2019/12/31, 19:46:58",3932.0,1371995.0,1,"I think this is the intended behavior, you got one service ( MyApplication ), a root span ( /service1 ) and a child span ( /external_service_2 )."
Zipkin,59544246,59444729,0,"2019/12/31, 14:36:17",True,"2019/12/31, 19:46:58",3932.0,1371995.0,1,If you got multiple services connected to the same Zipkin server then you'll have multiple names for the services.
Zipkin,59544246,59444729,0,"2019/12/31, 14:36:17",True,"2019/12/31, 19:46:58",3932.0,1371995.0,1,From the Zipkin's  documentation :
Zipkin,59544246,59444729,0,"2019/12/31, 14:36:17",True,"2019/12/31, 19:46:58",3932.0,1371995.0,1,Span
Zipkin,59544246,59444729,0,"2019/12/31, 14:36:17",True,"2019/12/31, 19:46:58",3932.0,1371995.0,1,A set of Annotations and BinaryAnnotations that correspond to a particular RPC.
Zipkin,59544246,59444729,0,"2019/12/31, 14:36:17",True,"2019/12/31, 19:46:58",3932.0,1371995.0,1,"Spans contain identifying information such as traceId, spanId, parentId, and RPC name."
Zipkin,59544246,59444729,0,"2019/12/31, 14:36:17",True,"2019/12/31, 19:46:58",3932.0,1371995.0,1,Trace
Zipkin,59544246,59444729,0,"2019/12/31, 14:36:17",True,"2019/12/31, 19:46:58",3932.0,1371995.0,1,A set of spans that share a single root span.
Zipkin,59544246,59444729,0,"2019/12/31, 14:36:17",True,"2019/12/31, 19:46:58",3932.0,1371995.0,1,Traces are built by collecting all Spans that share a traceId.
Zipkin,59544246,59444729,0,"2019/12/31, 14:36:17",True,"2019/12/31, 19:46:58",3932.0,1371995.0,1,The spans are then arranged in a tree based on spanId and parentId thus providing an overview of the path a request takes through the system.
Zipkin,59544246,59444729,0,"2019/12/31, 14:36:17",True,"2019/12/31, 19:46:58",3932.0,1371995.0,1,"As far Zipkin UI displays 2 spans with same name, service dependencies
  page contains one Service only(see image)"
Zipkin,59544246,59444729,0,"2019/12/31, 14:36:17",True,"2019/12/31, 19:46:58",3932.0,1371995.0,1,"Again, this is the intended behavior, since you got only one service and the external request you made goes through it."
Zipkin,59544246,59444729,0,"2019/12/31, 14:36:17",True,"2019/12/31, 19:46:58",3932.0,1371995.0,1,"If you mean the framed names on the first image, at the top it shows only the root span you clicked on the previous screen."
Zipkin,59544246,59444729,0,"2019/12/31, 14:36:17",True,"2019/12/31, 19:46:58",3932.0,1371995.0,1,"However, you can write custom span names after a little change in your code."
Zipkin,59544246,59444729,0,"2019/12/31, 14:36:17",True,"2019/12/31, 19:46:58",3932.0,1371995.0,1,From the  tracing documentation  (with your code):
Zipkin,59544246,59444729,0,"2019/12/31, 14:36:17",True,"2019/12/31, 19:46:58",3932.0,1371995.0,1,"Now you can use  tracer.startRootSpan , I used it in the express sample with a request:"
Zipkin,59544246,59444729,0,"2019/12/31, 14:36:17",True,"2019/12/31, 19:46:58",3932.0,1371995.0,1,A span must be closed.
Zipkin,59544246,59444729,0,"2019/12/31, 14:36:17",True,"2019/12/31, 19:46:58",3932.0,1371995.0,1,"For more information, check the  test file  of the tracer."
Zipkin,38537987,35215821,0,"2016/07/23, 06:56:00",False,"2016/07/23, 06:56:00",762.0,1227937.0,0,The best way to ask for a feature is using github issues.
Zipkin,38537987,35215821,0,"2016/07/23, 06:56:00",False,"2016/07/23, 06:56:00",762.0,1227937.0,0,"To add a new transport such as RabbitMQ, you'd have to affect Brave (reporter) and Zipkin (collector)"
Zipkin,38537987,35215821,0,"2016/07/23, 06:56:00",False,"2016/07/23, 06:56:00",762.0,1227937.0,0,"https://github.com/openzipkin/zipkin/issues 
 https://github.com/openzipkin/brave/issues"
Zipkin,16981952,16981751,0,"2013/06/07, 13:29:08",True,"2018/10/17, 13:15:26",18596.0,69875.0,1,"It's not suitable, Zipkin is about tracing in distributed systems."
Zipkin,16981952,16981751,0,"2013/06/07, 13:29:08",True,"2018/10/17, 13:15:26",18596.0,69875.0,1,"I would say you would want something like a profiler, such as  Visual VM ,  - free and included with the JDK, or  YourKit ."
Zipkin,16981952,16981751,0,"2013/06/07, 13:29:08",True,"2018/10/17, 13:15:26",18596.0,69875.0,1,Other profilers are available.
Zipkin,18345991,16981751,0,"2013/08/21, 01:31:07",False,"2013/08/21, 01:31:07",60071.0,298389.0,1,"No, it  is not suitable at all ."
Zipkin,18345991,16981751,0,"2013/08/21, 01:31:07",False,"2013/08/21, 01:31:07",60071.0,298389.0,1,Why?
Zipkin,18345991,16981751,0,"2013/08/21, 01:31:07",False,"2013/08/21, 01:31:07",60071.0,298389.0,1,"Because the architecture of Zipkin have multiple levels of indirection: you have to send your spans into collector service and even if collector is running on your own machine there is a huge overhead -- imagine you are traveling from two neighbour cities somewhere in Europe and somebody proposes you instead of going directly from A to B, fly from A to New York, USA and back to B."
Zipkin,18345991,16981751,0,"2013/08/21, 01:31:07",False,"2013/08/21, 01:31:07",60071.0,298389.0,1,That is simply ridiculous.
Zipkin,18345991,16981751,0,"2013/08/21, 01:31:07",False,"2013/08/21, 01:31:07",60071.0,298389.0,1,As for the tools that may suit your needs: VisualVM and Yourkit mentioned by @Jonathan are good for looking at average situation in your program -- if you need to carefully inspect low level paths in your program  InTrace  might be a better choice.
Zipkin,31348098,16981751,1,"2015/07/10, 21:43:33",False,"2015/07/10, 21:43:33",17932.0,433344.0,1,Zipkin is foremost intended to provide observability into a complex distributed network of services (aka Microservice Architecture).
Zipkin,31348098,16981751,1,"2015/07/10, 21:43:33",False,"2015/07/10, 21:43:33",17932.0,433344.0,1,It wasn't intended to support just a single application.
Zipkin,31348098,16981751,1,"2015/07/10, 21:43:33",False,"2015/07/10, 21:43:33",17932.0,433344.0,1,"So a profiler can often be a better way to go, particularly if you have an urgent issue to diagnose."
Zipkin,31348098,16981751,1,"2015/07/10, 21:43:33",False,"2015/07/10, 21:43:33",17932.0,433344.0,1,"That said, most profilers will only provide realtime data, where Zipkin persists, allowing for analysis over large datasets."
Zipkin,31348098,16981751,1,"2015/07/10, 21:43:33",False,"2015/07/10, 21:43:33",17932.0,433344.0,1,"If Zipkin is already in your stack, it's not a bad idea to enrich higher level Zipkin spans (ie a complete REST requests) with specific method calls as child-spans, particularly those that do I/O."
Zipkin,31348098,16981751,1,"2015/07/10, 21:43:33",False,"2015/07/10, 21:43:33",17932.0,433344.0,1,This way you can drill down within Zipkin to more quickly determine bottlenecks.
Zipkin,31348098,16981751,1,"2015/07/10, 21:43:33",False,"2015/07/10, 21:43:33",17932.0,433344.0,1,"Otherwise you'd have to first look at the Zipkin span for high-level bottlenecks, then separately look at other logs or run a profiler, which is inefficient."
Zipkin,31348098,16981751,1,"2015/07/10, 21:43:33",False,"2015/07/10, 21:43:33",17932.0,433344.0,1,** If it's already in your stack **
Zipkin,54816556,54262815,1,"2019/02/21, 23:32:10",True,"2019/02/21, 23:32:10",106.0,3956619.0,3,It's hard to tell without more information.
Zipkin,54816556,54262815,1,"2019/02/21, 23:32:10",True,"2019/02/21, 23:32:10",106.0,3956619.0,3,But it can be related to  incompatible libraries .
Zipkin,54816556,54262815,1,"2019/02/21, 23:32:10",True,"2019/02/21, 23:32:10",106.0,3956619.0,3,Can you post your dependencies?
Zipkin,54816556,54262815,1,"2019/02/21, 23:32:10",True,"2019/02/21, 23:32:10",106.0,3956619.0,3,In case you are using  older version  of okhttpclient with  latest  spring cloud:greenwich it can cause this issue.
Zipkin,54816556,54262815,1,"2019/02/21, 23:32:10",True,"2019/02/21, 23:32:10",106.0,3956619.0,3,I'm using  Greenwich.RELEASE  with  okhttpclient:10.2.0  which works without problems
Zipkin,59579899,54262815,0,"2020/01/03, 16:02:23",False,"2020/01/03, 16:02:23",11.0,12647380.0,1,Use the below dependency Management for spring-boot to download the suitable versions for cloud version
Zipkin,59579899,54262815,0,"2020/01/03, 16:02:23",False,"2020/01/03, 16:02:23",11.0,12647380.0,1,"I am using Java 10, cloud.version is  Finchley.SR2  and sprinb-boot:2.2.0 and spring-cloud-starter-openfeign :2.1.2.RELEASE."
Zipkin,59579899,54262815,0,"2020/01/03, 16:02:23",False,"2020/01/03, 16:02:23",11.0,12647380.0,1,and this combination worked for me to fix the issue.
Zipkin,59579899,54262815,0,"2020/01/03, 16:02:23",False,"2020/01/03, 16:02:23",11.0,12647380.0,1,Acctual problem was 10.x.x feign-core  was not working only  and io.github.openfeign:feign-core:jar:9.7.0:compile was working.
Zipkin,62180529,54262815,0,"2020/06/03, 21:51:55",False,"2020/06/03, 21:51:55",116.0,10783374.0,1,"I faced this problem using java 11, springboot 2.3.0.RELEASE, and spring-cloud version Greenwich.RELEASE."
Zipkin,62180529,54262815,0,"2020/06/03, 21:51:55",False,"2020/06/03, 21:51:55",116.0,10783374.0,1,Adding the following dependences saved me:
Zipkin,62180529,54262815,0,"2020/06/03, 21:51:55",False,"2020/06/03, 21:51:55",116.0,10783374.0,1,Hope this helps someone.
Zipkin,49450864,43025795,0,"2018/03/23, 15:33:24",False,"2018/03/23, 15:33:24",485.0,1287376.0,1,It seems to be a timing issue.
Zipkin,49450864,43025795,0,"2018/03/23, 15:33:24",False,"2018/03/23, 15:33:24",485.0,1287376.0,1,"If we add some delay, for instance, between children spans execution like"
Zipkin,49450864,43025795,0,"2018/03/23, 15:33:24",False,"2018/03/23, 15:33:24",485.0,1287376.0,1,In between
Zipkin,49450864,43025795,0,"2018/03/23, 15:33:24",False,"2018/03/23, 15:33:24",485.0,1287376.0,1,Then we get to see spans:
Zipkin,49450864,43025795,0,"2018/03/23, 15:33:24",False,"2018/03/23, 15:33:24",485.0,1287376.0,1,I've faced something like this before when Zipkin dropped spans I was (mistakenly) assigning wrong timestamps to.
Zipkin,49450864,43025795,0,"2018/03/23, 15:33:24",False,"2018/03/23, 15:33:24",485.0,1287376.0,1,"For reference and ease of reproduction: I've setup a  project  for reproducing this issue / ""fix""."
Zipkin,41628945,41508180,0,"2017/01/13, 08:52:38",True,"2017/01/13, 08:52:38",762.0,1227937.0,5,"In zipkin lingo, what you are asking about is often called ""local spans"" or ""local tracing"", basically an operation that neither originated, nor resulted in a remote call."
Zipkin,41628945,41508180,0,"2017/01/13, 08:52:38",True,"2017/01/13, 08:52:38",762.0,1227937.0,5,"I'm not aware of anything at the syscall level, but many tracers support explicit instrumentation of function calls."
Zipkin,41628945,41508180,0,"2017/01/13, 08:52:38",True,"2017/01/13, 08:52:38",762.0,1227937.0,5,"For example, using  py_zipkin 
 
@zipkin_span(service_name='my_service', span_name='some_function')
def some_function(a, b):
    return do_stuff(a, b)"
Zipkin,41628945,41508180,0,"2017/01/13, 08:52:38",True,"2017/01/13, 08:52:38",762.0,1227937.0,5,"Besides explicit instrumentation like this, one could also export data to zipkin."
Zipkin,41628945,41508180,0,"2017/01/13, 08:52:38",True,"2017/01/13, 08:52:38",762.0,1227937.0,5,"For example, one could convert trace data that is made in another tool to  zipkin's json format ."
Zipkin,41628945,41508180,0,"2017/01/13, 08:52:38",True,"2017/01/13, 08:52:38",762.0,1227937.0,5,"This probably doesn't answer your question, but I hope it helps."
Zipkin,65881898,65873809,0,"2021/01/25, 11:19:25",False,"2021/01/25, 11:19:25",4007.0,4255107.0,0,The easiest way to get it working is to use the Micrometer library and configure the micrometer to send this data to the Zipkin server.
Zipkin,65881898,65873809,0,"2021/01/25, 11:19:25",False,"2021/01/25, 11:19:25",4007.0,4255107.0,0,"Enabling metrics using micrometer is very simple, you need to just add a micrometer-core and spring-cloud-starter-zipkin libraries."
Zipkin,65881898,65873809,0,"2021/01/25, 11:19:25",False,"2021/01/25, 11:19:25",4007.0,4255107.0,0,See this tutorial for details about configuration and code  https://www.baeldung.com/tracing-services-with-zipkin
Zipkin,65881898,65873809,0,"2021/01/25, 11:19:25",False,"2021/01/25, 11:19:25",4007.0,4255107.0,0,Micrometer will report consumer/producer metrics to Zipkin
Zipkin,64096381,64094479,0,"2020/09/28, 08:27:01",True,"2020/09/28, 08:27:01",8336.0,1773866.0,2,If you search for zipkin grafana you'll get this as one of the first results  https://grafana.com/docs/grafana/latest/features/datasources/zipkin/
Zipkin,61609752,61570149,1,"2020/05/05, 12:19:05",False,"2020/05/05, 12:19:05",2015.0,1398228.0,0,You need to reload after adding the subsystem:
Zipkin,61636753,61570149,6,"2020/05/06, 16:43:39",False,"2020/05/06, 16:43:39",2015.0,1398228.0,1,This jboss-cli script should enable opentracing before starting the server properly.
Zipkin,61636753,61570149,6,"2020/05/06, 16:43:39",False,"2020/05/06, 16:43:39",2015.0,1398228.0,1,I'm not sure how/when you can execute that with keycloack image
Zipkin,53209830,53208598,0,"2018/11/08, 16:31:04",False,"2018/11/08, 16:31:04",129048.0,1240763.0,1,Connection refused: connect
Zipkin,53209830,53208598,0,"2018/11/08, 16:31:04",False,"2018/11/08, 16:31:04",129048.0,1240763.0,1,"Simply means that RabbitMQ is not running on  localhost:5672  (which is the default if you don't provide a host/port, or addresses, for it in your  application.yml )."
Zipkin,52814922,52759855,2,"2018/10/15, 13:43:25",True,"2018/10/17, 09:29:03",201.0,7956609.0,2,"If service 2 is getting traceId from service 1, you can take the traceId from requestHeader in your java code."
Zipkin,52814922,52759855,2,"2018/10/15, 13:43:25",True,"2018/10/17, 09:29:03",201.0,7956609.0,2,Otherwise sleuth generate a new traceId in service 2.
Zipkin,52814922,52759855,2,"2018/10/15, 13:43:25",True,"2018/10/17, 09:29:03",201.0,7956609.0,2,To get the trace Id In java
Zipkin,52814922,52759855,2,"2018/10/15, 13:43:25",True,"2018/10/17, 09:29:03",201.0,7956609.0,2,Just do
Zipkin,59216676,52759855,0,"2019/12/06, 18:13:45",False,"2019/12/06, 18:13:45",859.0,4513218.0,1,"Hello you can also get the x-b3-traceid header information from the request, I created a Util class for that -   https://gist.github.com/walterwhites/067dd635986e564aafdb5ac559073b0f"
Zipkin,59216676,52759855,0,"2019/12/06, 18:13:45",False,"2019/12/06, 18:13:45",859.0,4513218.0,1,Then in your main class you just need to call
Zipkin,52031368,52029459,0,"2018/08/27, 03:52:08",True,"2018/09/16, 13:54:14",391.0,6452043.0,3,"after many searches, i found that there are a version conflicts between the dependencies."
Zipkin,52031368,52029459,0,"2018/08/27, 03:52:08",True,"2018/09/16, 13:54:14",391.0,6452043.0,3,and thanks for  vladimir-vagaytsev
Zipkin,52031368,52029459,0,"2018/08/27, 03:52:08",True,"2018/09/16, 13:54:14",391.0,6452043.0,3,"so, i see that  spring-cloud-starter-sleuth  imported as a different version."
Zipkin,52031368,52029459,0,"2018/08/27, 03:52:08",True,"2018/09/16, 13:54:14",391.0,6452043.0,3,to fix it i have added   sleuth.version  to properties  in pom.xml like so.
Zipkin,52031368,52029459,0,"2018/08/27, 03:52:08",True,"2018/09/16, 13:54:14",391.0,6452043.0,3,then in dependency management we need to specify the version like so
Zipkin,52031368,52029459,0,"2018/08/27, 03:52:08",True,"2018/09/16, 13:54:14",391.0,6452043.0,3,after then remove unused dependencies build and run.
Zipkin,52338780,52029459,0,"2018/09/14, 23:20:40",False,"2018/09/14, 23:20:40",373.0,1037492.0,-1,This class comes from zipkin-2.
Zipkin,52338780,52029459,0,"2018/09/14, 23:20:40",False,"2018/09/14, 23:20:40",373.0,1037492.0,-1,You can try adding this dependency.
Zipkin,51011690,50027127,0,"2018/06/24, 19:10:20",False,"2018/06/24, 22:07:28",3589.0,5587542.0,2,"Hello seeing your screenshot maybe you are using a spring boot 2.x version, I had the same problem with spring boot 2.0.3 with Finchley.RELEASE."
Zipkin,51011690,50027127,0,"2018/06/24, 19:10:20",False,"2018/06/24, 22:07:28",3589.0,5587542.0,2,"I found that Zipkin custom Server are not any more supported and deprecated for this reason it is not possible to use @EnableZipkinServer in Spring Cloud code and you have the ui but not the server side configured, api endpoint and so on."
Zipkin,51011690,50027127,0,"2018/06/24, 19:10:20",False,"2018/06/24, 22:07:28",3589.0,5587542.0,2,form the Zipkin base code:
Zipkin,51011690,50027127,0,"2018/06/24, 19:10:20",False,"2018/06/24, 22:07:28",3589.0,5587542.0,2,"the code is available on  official repo of Zipkin 
I solve the my problem using the official docker image with a compose"
Zipkin,51011690,50027127,0,"2018/06/24, 19:10:20",False,"2018/06/24, 22:07:28",3589.0,5587542.0,2,How you can see i use the streaming version.
Zipkin,51011690,50027127,0,"2018/06/24, 19:10:20",False,"2018/06/24, 22:07:28",3589.0,5587542.0,2,it for me work
Zipkin,51011690,50027127,0,"2018/06/24, 19:10:20",False,"2018/06/24, 22:07:28",3589.0,5587542.0,2,I hope that is can help you
Zipkin,54355078,50027127,0,"2019/01/24, 22:47:42",False,"2019/01/24, 22:47:42",21.0,9286335.0,0,try with this:
Zipkin,50877783,49676752,0,"2018/06/15, 17:39:40",False,"2018/06/15, 17:39:40",140.0,7980867.0,0,I believe you should be able to as long as you use the fully qualified domain name.
Zipkin,50877783,49676752,0,"2018/06/15, 17:39:40",False,"2018/06/15, 17:39:40",140.0,7980867.0,0,"For instance,  zipkin.mynamespace.svc.cluster.local ."
Zipkin,49182914,49182626,0,"2018/03/08, 23:38:09",False,"2018/03/08, 23:38:09",9818.0,1490322.0,6,Ok we found the problem and also a work around.
Zipkin,49182914,49182626,0,"2018/03/08, 23:38:09",False,"2018/03/08, 23:38:09",9818.0,1490322.0,6,"It looks like all the documentation out there is wrong, at least for the version of Spring Cloud Sleuth we are using."
Zipkin,49182914,49182626,0,"2018/03/08, 23:38:09",False,"2018/03/08, 23:38:09",9818.0,1490322.0,6,The correct property is not  spring.sleuth.sampler.percentage .
Zipkin,49182914,49182626,0,"2018/03/08, 23:38:09",False,"2018/03/08, 23:38:09",9818.0,1490322.0,6,The correct property is  spring.sleuth.sampler.probability
Zipkin,49182914,49182626,0,"2018/03/08, 23:38:09",False,"2018/03/08, 23:38:09",9818.0,1490322.0,6,And here is a workaround we found right before noticing that the property was wrong.
Zipkin,49182914,49182626,0,"2018/03/08, 23:38:09",False,"2018/03/08, 23:38:09",9818.0,1490322.0,6,Here are some official documentation from Spring Cloud that contain the wrong property.
Zipkin,49182914,49182626,0,"2018/03/08, 23:38:09",False,"2018/03/08, 23:38:09",9818.0,1490322.0,6,https://cloud.spring.io/spring-cloud-static/spring-cloud-sleuth/2.0.0.M5/single/spring-cloud-sleuth.html
Zipkin,49182914,49182626,0,"2018/03/08, 23:38:09",False,"2018/03/08, 23:38:09",9818.0,1490322.0,6,https://cloud.spring.io/spring-cloud-sleuth/single/spring-cloud-sleuth.html
Zipkin,49182914,49182626,0,"2018/03/08, 23:38:09",False,"2018/03/08, 23:38:09",9818.0,1490322.0,6,Here is the source code that is being used and it is using  probability  not  percentage .
Zipkin,49182914,49182626,0,"2018/03/08, 23:38:09",False,"2018/03/08, 23:38:09",9818.0,1490322.0,6,https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/sampler/SamplerProperties.java
Zipkin,47777632,47764295,0,"2017/12/12, 18:47:54",True,"2017/12/12, 18:47:54",81.0,2232476.0,1,"Creating custom zipkin servers is an unsupported configuration, but if you must all of the configuration options are documented in the project readme:  https://github.com/openzipkin/zipkin/blob/master/zipkin-server/README.md"
Zipkin,50200855,47764295,0,"2018/05/06, 17:35:24",False,"2018/05/06, 17:35:24",69.0,2629308.0,1,"For the dependencies part, the most important one is  zipkin-autoconfigure-storage-elasticsearch-http , here's an full maven pom.xml example:"
Zipkin,50200855,47764295,0,"2018/05/06, 17:35:24",False,"2018/05/06, 17:35:24",69.0,2629308.0,1,"For the configuration part, you will need the following in you  application.yml :"
Zipkin,64168840,47764295,0,"2020/10/02, 11:59:59",False,"2020/10/02, 11:59:59",511.0,2564032.0,0,I configured zipkin to use ES as a data storage on top of kubernetes.
Zipkin,64168840,47764295,0,"2020/10/02, 11:59:59",False,"2020/10/02, 11:59:59",511.0,2564032.0,0,If it fits your requirement feel free to download and use  https://github.com/handysofts/zipkin-on-kubernetes
Zipkin,47195615,47195614,4,"2017/11/09, 08:50:24",True,"2017/11/09, 08:50:24",3909.0,3067542.0,3,I found that these traces are actually generated by  https://github.com/spring-cloud/spring-cloud-consul/blob/master/spring-cloud-consul-discovery/src/main/java/org/springframework/cloud/consul/discovery/ConsulCatalogWatch.java
Zipkin,47195615,47195614,4,"2017/11/09, 08:50:24",True,"2017/11/09, 08:50:24",3909.0,3067542.0,3,"And since this is a class annotated with @scheduled , this Sleuth aspect applies :"
Zipkin,47195615,47195614,4,"2017/11/09, 08:50:24",True,"2017/11/09, 08:50:24",3909.0,3067542.0,3,https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/instrument/scheduling/TraceSchedulingAspect.java
Zipkin,47195615,47195614,4,"2017/11/09, 08:50:24",True,"2017/11/09, 08:50:24",3909.0,3067542.0,3,"And therefore, the property to control the skipped regexp is not spring.sleuth.instrument.web.skipPattern , but spring.sleuth.instrument."
Zipkin,47195615,47195614,4,"2017/11/09, 08:50:24",True,"2017/11/09, 08:50:24",3909.0,3067542.0,3,scheduled .skip-pattern
Zipkin,42764225,42764069,1,"2017/03/13, 14:56:55",True,"2017/03/13, 17:25:56",8336.0,1773866.0,3,Of course - you have to just provide your own logging format (e.g.
Zipkin,42764225,42764069,1,"2017/03/13, 14:56:55",True,"2017/03/13, 17:25:56",8336.0,1773866.0,3,via   logging.pattern.level  - check  https://docs.spring.io/spring-boot/docs/current/reference/html/boot-features-logging.html  for more info).
Zipkin,42764225,42764069,1,"2017/03/13, 14:56:55",True,"2017/03/13, 17:25:56",8336.0,1773866.0,3,Then you have to register your own  SpanLogger  bean implementation where you will take care of adding the value of a spring profile via MDC  (you can take a look at this as an example  https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/log/Slf4jSpanLogger.java  )
Zipkin,42764225,42764069,1,"2017/03/13, 14:56:55",True,"2017/03/13, 17:25:56",8336.0,1773866.0,3,UPDATE:
Zipkin,42764225,42764069,1,"2017/03/13, 14:56:55",True,"2017/03/13, 17:25:56",8336.0,1773866.0,3,There's another solution for more complex approach that seems much easier than rewriting Sleuth classes.
Zipkin,42764225,42764069,1,"2017/03/13, 14:56:55",True,"2017/03/13, 17:25:56",8336.0,1773866.0,3,You can try the  logback-spring.xml  way like here -  https://github.com/spring-cloud-samples/sleuth-documentation-apps/blob/master/service1/src/main/resources/logback-spring.xml#L5-L11  .
Zipkin,42764225,42764069,1,"2017/03/13, 14:56:55",True,"2017/03/13, 17:25:56",8336.0,1773866.0,3,I'm resolving the application name there so maybe you could do the same with active profiles and won't need to write any code?
Zipkin,40861860,40860284,0,"2016/11/29, 11:14:50",True,"2016/11/29, 11:14:50",42504.0,936832.0,2,"As long as you have the ability to specify VM parameters, you can add the monitoring agent, regardless of the whether the JVM is started as a Windows service."
Zipkin,40861860,40860284,0,"2016/11/29, 11:14:50",True,"2016/11/29, 11:14:50",42504.0,936832.0,2,"For perfino, that VM parameter is"
Zipkin,43888530,40654863,0,"2017/05/10, 12:29:15",False,"2017/05/11, 04:27:33",130.0,1427954.0,3,please ensure config your zipkin sever correctly in your spring boot config file.
Zipkin,43888530,40654863,0,"2017/05/10, 12:29:15",False,"2017/05/11, 04:27:33",130.0,1427954.0,3,just like this:
Zipkin,43888530,40654863,0,"2017/05/10, 12:29:15",False,"2017/05/11, 04:27:33",130.0,1427954.0,3,And add below config in your zipkin client spring boot config file:
Zipkin,39597817,39597545,0,"2016/09/20, 18:15:25",True,"2016/09/20, 18:15:25",8336.0,1773866.0,2,We have a  LazyTraceExecutor  that you can use -  https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/instrument/async/LazyTraceExecutor.java  .
Zipkin,41629061,38738337,0,"2017/01/13, 09:02:26",False,"2017/01/13, 09:02:26",762.0,1227937.0,2,"There are a bunch of ways to answer this, but I'll answer it from the ""one-way"" perspective."
Zipkin,41629061,38738337,0,"2017/01/13, 09:02:26",False,"2017/01/13, 09:02:26",762.0,1227937.0,2,"The short answer though, is I think you have to roll your own right now!"
Zipkin,41629061,38738337,0,"2017/01/13, 09:02:26",False,"2017/01/13, 09:02:26",762.0,1227937.0,2,"While Kafka can be used in many ways, it can be used as a transport for unidirectional single producer single consumer messages."
Zipkin,41629061,38738337,0,"2017/01/13, 09:02:26",False,"2017/01/13, 09:02:26",762.0,1227937.0,2,"This action is similar to normal one-way RPC, where you have a request, but no response."
Zipkin,41629061,38738337,0,"2017/01/13, 09:02:26",False,"2017/01/13, 09:02:26",762.0,1227937.0,2,"In Zipkin, an RPC span is usually request-response."
Zipkin,41629061,38738337,0,"2017/01/13, 09:02:26",False,"2017/01/13, 09:02:26",762.0,1227937.0,2,"For example, you see timing of the client sending to the server, and also the way back to the client."
Zipkin,41629061,38738337,0,"2017/01/13, 09:02:26",False,"2017/01/13, 09:02:26",762.0,1227937.0,2,One-way is where you leave out the other side.
Zipkin,41629061,38738337,0,"2017/01/13, 09:02:26",False,"2017/01/13, 09:02:26",762.0,1227937.0,2,"The span starts with a ""cs"" (client send) and ends with a ""sr"" (server received)."
Zipkin,41629061,38738337,0,"2017/01/13, 09:02:26",False,"2017/01/13, 09:02:26",762.0,1227937.0,2,"Mapping this to Kafka, you would mark client sent when you produce the message and server received when the consumer receives it."
Zipkin,41629061,38738337,0,"2017/01/13, 09:02:26",False,"2017/01/13, 09:02:26",762.0,1227937.0,2,The trick to Kafka is that there is no nice place to stuff the trace context.
Zipkin,41629061,38738337,0,"2017/01/13, 09:02:26",False,"2017/01/13, 09:02:26",762.0,1227937.0,2,"That's because unlike a lot of messaging systems, there are no headers in a Kafka message."
Zipkin,41629061,38738337,0,"2017/01/13, 09:02:26",False,"2017/01/13, 09:02:26",762.0,1227937.0,2,"Without a trace context, you don't know which trace (or span for that matter) you are completing!"
Zipkin,41629061,38738337,0,"2017/01/13, 09:02:26",False,"2017/01/13, 09:02:26",762.0,1227937.0,2,"The ""hack"" approach is to stuff trace identifiers as the message key."
Zipkin,41629061,38738337,0,"2017/01/13, 09:02:26",False,"2017/01/13, 09:02:26",762.0,1227937.0,2,A less hacky way would be to coordinate a body wrapper which you can nest the trace context into.
Zipkin,41629061,38738337,0,"2017/01/13, 09:02:26",False,"2017/01/13, 09:02:26",762.0,1227937.0,2,Here's an example of the former:
Zipkin,41629061,38738337,0,"2017/01/13, 09:02:26",False,"2017/01/13, 09:02:26",762.0,1227937.0,2,https://gist.github.com/adriancole/76d94054b77e3be338bd75424ca8ba30
Zipkin,42805876,38738337,0,"2017/03/15, 11:36:01",False,"2017/03/15, 11:36:01",181.0,6708214.0,0,"I meet the same problem too.Here is my solution, a less hacky way as above said."
Zipkin,42805876,38738337,0,"2017/03/15, 11:36:01",False,"2017/03/15, 11:36:01",181.0,6708214.0,0,"you must implements MyHttpServerRequest and MyRequest.It is easy,you just return something a span need,such as uri,header,method."
Zipkin,42805876,38738337,0,"2017/03/15, 11:36:01",False,"2017/03/15, 11:36:01",181.0,6708214.0,0,"This is a rough and ugly code example,just offer an idea."
Zipkin,66913758,66911879,3,"2021/04/02, 05:29:08",False,"2021/04/02, 05:29:08",66.0,12105655.0,1,server.address=&lt;ip&gt;  does not work?
Zipkin,66913758,66911879,3,"2021/04/02, 05:29:08",False,"2021/04/02, 05:29:08",66.0,12105655.0,1,java -jar zipkin.jar --server.address=192.168.0.7
Zipkin,66913758,66911879,3,"2021/04/02, 05:29:08",False,"2021/04/02, 05:29:08",66.0,12105655.0,1,If it's not working you can add a property file and connects to it when the server starts:
Zipkin,66913758,66911879,3,"2021/04/02, 05:29:08",False,"2021/04/02, 05:29:08",66.0,12105655.0,1,java -jar zipkin.jar --spring.config.location=./application.properties
Zipkin,66913758,66911879,3,"2021/04/02, 05:29:08",False,"2021/04/02, 05:29:08",66.0,12105655.0,1,in application.properties:
Zipkin,66808417,66777772,2,"2021/03/26, 00:23:10",False,"2021/03/26, 00:23:10",46.0,14172753.0,0,"I'm not entirely sure if that's what you mean, but you can use Jeager  https://www.jaegertracing.io/   which checks if trace-id already exist in the invocation metadata and in it generate child trace id."
Zipkin,66808417,66777772,2,"2021/03/26, 00:23:10",False,"2021/03/26, 00:23:10",46.0,14172753.0,0,Based on all trace ids call diagrams are generated
Zipkin,66654542,66517888,1,"2021/03/16, 13:40:16",False,"2021/03/16, 13:40:16",19.0,12338209.0,0,In your command please try the following -Dotel.traces.exporter=zipkin instead of -Dotel.exporter=zipkin
Zipkin,66782647,66517888,1,"2021/03/24, 15:58:38",True,"2021/03/24, 15:58:38",16.0,15470262.0,0,"i had the same issue, but i solved with this jvm arguments:"
Zipkin,66782647,66517888,1,"2021/03/24, 15:58:38",True,"2021/03/24, 15:58:38",16.0,15470262.0,0,"-Dotel.traces.exporter=zipkin -Dotel.metrics.exporter=none -Dotel.exporter.zipkin.endpoint=http://localhost:9411/api/v2/spans
Maybe the error is on zipkin.endpoint, try to write the entire url."
Zipkin,66782647,66517888,1,"2021/03/24, 15:58:38",True,"2021/03/24, 15:58:38",16.0,15470262.0,0,"Regards,
Marco"
Zipkin,66350164,66284701,0,"2021/02/24, 13:40:02",False,"2021/02/24, 13:52:09",11.0,15274583.0,1,I get the same problem and below command did the trick.
Zipkin,66350164,66284701,0,"2021/02/24, 13:40:02",False,"2021/02/24, 13:52:09",11.0,15274583.0,1,I checked the source code.
Zipkin,66350164,66284701,0,"2021/02/24, 13:40:02",False,"2021/02/24, 13:52:09",11.0,15274583.0,1,It looks the property name has been changed:
Zipkin,66350164,66284701,0,"2021/02/24, 13:40:02",False,"2021/02/24, 13:52:09",11.0,15274583.0,1,https://github.com/open-telemetry/opentelemetry-java/blob/14ace1ec32dbb194b8990763beb3ab6935849547/sdk-extensions/autoconfigure/src/main/java/io/opentelemetry/sdk/autoconfigure/TracerProviderConfiguration.java#L43
Zipkin,63969131,63489826,0,"2020/09/19, 16:03:50",True,"2020/09/19, 16:03:50",4433.0,243104.0,3,Getting a handle on the distributed tracing space can be a bit confusing.
Zipkin,63969131,63489826,0,"2020/09/19, 16:03:50",True,"2020/09/19, 16:03:50",4433.0,243104.0,3,Here's a quick summary...
Zipkin,63969131,63489826,0,"2020/09/19, 16:03:50",True,"2020/09/19, 16:03:50",4433.0,243104.0,3,Open Source Tracers
Zipkin,63969131,63489826,0,"2020/09/19, 16:03:50",True,"2020/09/19, 16:03:50",4433.0,243104.0,3,"There are a number of popular open source tracers, which is where Zipkin sits:"
Zipkin,63969131,63489826,0,"2020/09/19, 16:03:50",True,"2020/09/19, 16:03:50",4433.0,243104.0,3,Commercial Tracers
Zipkin,63969131,63489826,0,"2020/09/19, 16:03:50",True,"2020/09/19, 16:03:50",4433.0,243104.0,3,There are also a lot of vendors offering commercial monitoring/observability tools which are either centred around or include distributed tracing:
Zipkin,63969131,63489826,0,"2020/09/19, 16:03:50",True,"2020/09/19, 16:03:50",4433.0,243104.0,3,Standardisation Efforts
Zipkin,63969131,63489826,0,"2020/09/19, 16:03:50",True,"2020/09/19, 16:03:50",4433.0,243104.0,3,Alongside all these products are numerous attempts at creating standards around distributed tracing.
Zipkin,63969131,63489826,0,"2020/09/19, 16:03:50",True,"2020/09/19, 16:03:50",4433.0,243104.0,3,"These typically start by creating a standard API for the trace-recording side of the architecture, and sometimes extend to become prescriptive about the content of traces or even the wire format."
Zipkin,63969131,63489826,0,"2020/09/19, 16:03:50",True,"2020/09/19, 16:03:50",4433.0,243104.0,3,This is where OpenTracing fits in.
Zipkin,63969131,63489826,0,"2020/09/19, 16:03:50",True,"2020/09/19, 16:03:50",4433.0,243104.0,3,"So it is not a tracing solution itself, but an API that can be implemented by the trace recording SDKs of multiple tracers, allowing you to swap between vendors more easily."
Zipkin,63969131,63489826,0,"2020/09/19, 16:03:50",True,"2020/09/19, 16:03:50",4433.0,243104.0,3,The most common standards are:
Zipkin,63969131,63489826,0,"2020/09/19, 16:03:50",True,"2020/09/19, 16:03:50",4433.0,243104.0,3,"Note that the first two in the list have been abandoned, with their contributors joining forces to create the third one together."
Zipkin,63969131,63489826,0,"2020/09/19, 16:03:50",True,"2020/09/19, 16:03:50",4433.0,243104.0,3,[1]
Zipkin,63969131,63489826,0,"2020/09/19, 16:03:50",True,"2020/09/19, 16:03:50",4433.0,243104.0,3,[1]  https://opensource.googleblog.com/2019/05/opentelemetry-merger-of-opencensus-and.html
Zipkin,66499651,63385866,0,"2021/03/05, 23:09:02",True,"2021/03/05, 23:09:02",1301.0,1423685.0,0,"I was using it with default storage which is discouraged in production use, it can handle only small amount of data and can be treated only as a demo version."
Zipkin,66499651,63385866,0,"2021/03/05, 23:09:02",True,"2021/03/05, 23:09:02",1301.0,1423685.0,0,What helped a little was setting
Zipkin,66499651,63385866,0,"2021/03/05, 23:09:02",True,"2021/03/05, 23:09:02",1301.0,1423685.0,0,spring.sleuth.sampler.probability: 0.01
Zipkin,66499651,63385866,0,"2021/03/05, 23:09:02",True,"2021/03/05, 23:09:02",1301.0,1423685.0,0,-- by default it logs all spans.
Zipkin,63672504,63073886,0,"2020/08/31, 17:15:00",False,"2020/08/31, 17:15:00",29.0,9544181.0,1,You should create application.properties file and after that you should add the following
Zipkin,63672504,63073886,0,"2020/08/31, 17:15:00",False,"2020/08/31, 17:15:00",29.0,9544181.0,1,Your application.properties :
Zipkin,63672504,63073886,0,"2020/08/31, 17:15:00",False,"2020/08/31, 17:15:00",29.0,9544181.0,1,Your main class :
Zipkin,63672504,63073886,0,"2020/08/31, 17:15:00",False,"2020/08/31, 17:15:00",29.0,9544181.0,1,Your Pom.xml :
Zipkin,63672504,63073886,0,"2020/08/31, 17:15:00",False,"2020/08/31, 17:15:00",29.0,9544181.0,1,Your zipkin port :
Zipkin,61807946,61800994,1,"2020/05/15, 00:44:40",False,"2020/05/15, 00:44:40",985.0,598932.0,1,"It looks like it is related to  https://github.com/openzipkin/zipkin-js/pull/498 , could you try with zipkin-context-cls@0.19.2-alpha.7 and change  ctxImpl  into  ctxImpl = new CLSContext('zipkin', true); ?"
Zipkin,61862789,61800994,0,"2020/05/18, 08:08:34",True,"2020/05/18, 19:34:48",156911.0,4668.0,0,"The problem ended up not being on Zipkin's end, but instead in how I was instrumenting the express server."
Zipkin,61862789,61800994,0,"2020/05/18, 08:08:34",True,"2020/05/18, 19:34:48",156911.0,4668.0,0,I had added the zipkin middleware  after  my call to  app.get .
Zipkin,61862789,61800994,0,"2020/05/18, 08:08:34",True,"2020/05/18, 19:34:48",156911.0,4668.0,0,Express executes middlwares in order and makes no distinction between middleware for a named route vs. something added via  app.use .
Zipkin,61862789,61800994,0,"2020/05/18, 08:08:34",True,"2020/05/18, 19:34:48",156911.0,4668.0,0,Doing things like this
Zipkin,61862789,61800994,0,"2020/05/18, 08:08:34",True,"2020/05/18, 19:34:48",156911.0,4668.0,0,Gave me the result I was looking for.
Zipkin,61401330,61368689,0,"2020/04/24, 07:46:24",True,"2020/04/24, 07:46:24",21.0,11657025.0,1,"The reason behind error was that i forgot to add the kafka dependency in pom.xml
After adding the dependency, error was gone."
Zipkin,63034460,61208227,0,"2020/07/22, 15:42:19",False,"2020/07/22, 15:42:19",46.0,10967262.0,0,"According to Sleuth documentation, AWS SQS is &quot;natively&quot; supported only on the consumer's side:"
Zipkin,63034460,61208227,0,"2020/07/22, 15:42:19",False,"2020/07/22, 15:42:19",46.0,10967262.0,0,https://docs.spring.io/spring-cloud-sleuth/docs/current-SNAPSHOT/reference/html/#spring-cloud-aws-messaging-sqs
Zipkin,63034460,61208227,0,"2020/07/22, 15:42:19",False,"2020/07/22, 15:42:19",46.0,10967262.0,0,In order to add seamless tracing over AWS SQS I resorted to Brave SQS instrumentation (aka SqsMessageTracing) and had to add another dependency:
Zipkin,63034460,61208227,0,"2020/07/22, 15:42:19",False,"2020/07/22, 15:42:19",46.0,10967262.0,0,and have the following configuration:
Zipkin,63034460,61208227,0,"2020/07/22, 15:42:19",False,"2020/07/22, 15:42:19",46.0,10967262.0,0,This is just because I didn't want to do the SQS producer instrumentation myself nor add the tracing headers programmatically.
Zipkin,63034460,61208227,0,"2020/07/22, 15:42:19",False,"2020/07/22, 15:42:19",46.0,10967262.0,0,Little reference for the Brave instrumentation can be found here:
Zipkin,63034460,61208227,0,"2020/07/22, 15:42:19",False,"2020/07/22, 15:42:19",46.0,10967262.0,0,https://github.com/spring-cloud/spring-cloud-sleuth/issues/1550#issuecomment-589686583
Zipkin,63034460,61208227,0,"2020/07/22, 15:42:19",False,"2020/07/22, 15:42:19",46.0,10967262.0,0,My SQS message producer looks like this:
Zipkin,63034460,61208227,0,"2020/07/22, 15:42:19",False,"2020/07/22, 15:42:19",46.0,10967262.0,0,FINAL NOTE
Zipkin,63034460,61208227,0,"2020/07/22, 15:42:19",False,"2020/07/22, 15:42:19",46.0,10967262.0,0,Not required but I also excluded the
Zipkin,63034460,61208227,0,"2020/07/22, 15:42:19",False,"2020/07/22, 15:42:19",46.0,10967262.0,0,Since it performs an AWS environment configuration scan at application startup which wasn't required for me (and also raised a lengthy error log)
Zipkin,66417136,60631319,0,"2021/03/01, 07:37:26",False,"2021/03/01, 07:37:26",438.0,3404777.0,0,I think you must have found your answer by now.
Zipkin,66417136,60631319,0,"2021/03/01, 07:37:26",False,"2021/03/01, 07:37:26",438.0,3404777.0,0,But I am posting this for future reference.
Zipkin,66417136,60631319,0,"2021/03/01, 07:37:26",False,"2021/03/01, 07:37:26",438.0,3404777.0,0,"Take a look at this  Github issue , it basically explains everything and provides a few workarounds."
Zipkin,58216010,58214695,0,"2019/10/03, 12:18:37",True,"2019/10/03, 12:18:37",381.0,10371480.0,1,According to  this  Spring Cloud Sleuth is the only tracer that supports messaging.
Zipkin,58220753,58214695,0,"2019/10/03, 16:58:34",False,"2019/10/03, 16:58:34",81.0,2232476.0,1,"Brave is the library spring cloud sleuth is built on, therefore you could make it work without sleuth:  https://github.com/openzipkin/brave"
Zipkin,58220753,58214695,0,"2019/10/03, 16:58:34",False,"2019/10/03, 16:58:34",81.0,2232476.0,1,"Just to clarify though, Sleuth doesn't force you to use any of the rest of the spring-cloud components."
Zipkin,58220753,58214695,0,"2019/10/03, 16:58:34",False,"2019/10/03, 16:58:34",81.0,2232476.0,1,"It is  spring-cloud  because it is one of the ""cloud native"" spring technologies"
Zipkin,57911578,57892994,0,"2019/09/12, 20:10:36",True,"2019/09/12, 20:10:36",99.0,7474991.0,2,ok I finally realized whats the mistake that I have done when I was starting my zipkin server with this command
Zipkin,57911578,57892994,0,"2019/09/12, 20:10:36",True,"2019/09/12, 20:10:36",99.0,7474991.0,2,but I was not specifying my Zipkin server where my Kafka is running so when I did
Zipkin,57911578,57892994,0,"2019/09/12, 20:10:36",True,"2019/09/12, 20:10:36",99.0,7474991.0,2,it worked
Zipkin,58040373,57703884,0,"2019/09/21, 15:42:54",False,"2019/09/21, 15:42:54",1477.0,4051158.0,0,"I am new to zipkin and golang, If you want to trace internal process, then you can create span from context"
Zipkin,58040373,57703884,0,"2019/09/21, 15:42:54",False,"2019/09/21, 15:42:54",1477.0,4051158.0,0,"example: say you have api called Login, inside login you might perform database operation or any other operations"
Zipkin,61247710,57180252,0,"2020/04/16, 13:11:53",False,"2020/04/16, 13:11:53",287.0,8631898.0,0,"On my project, we generated the spans manually before sending the events."
Zipkin,61247710,57180252,0,"2020/04/16, 13:11:53",False,"2020/04/16, 13:11:53",287.0,8631898.0,0,"var span = tracing.tracer().nextSpanWithParent(req -&gt; true,
Void.class, ctx.get(Span.class).context());"
Zipkin,61247710,57180252,0,"2020/04/16, 13:11:53",False,"2020/04/16, 13:11:53",287.0,8631898.0,0,span.name(&quot;yourSpanName&quot;).start();
Zipkin,61247710,57180252,0,"2020/04/16, 13:11:53",False,"2020/04/16, 13:11:53",287.0,8631898.0,0,return sendEventPublisher.doOnError(span::error).doOnTerminate(span::finish);
Zipkin,61247710,57180252,0,"2020/04/16, 13:11:53",False,"2020/04/16, 13:11:53",287.0,8631898.0,0,"This way, we also link the span to the publisher lifecycle as we had problems with webflux sharing spans between threads."
Zipkin,61247710,57180252,0,"2020/04/16, 13:11:53",False,"2020/04/16, 13:11:53",287.0,8631898.0,0,"Basically, we create a span and link it to the parent context created by Spring for the request (either from an incoming B3 HTTP header, or generated if absent)."
Zipkin,61247710,57180252,0,"2020/04/16, 13:11:53",False,"2020/04/16, 13:11:53",287.0,8631898.0,0,&quot;ctx&quot; is the subscriber context here.
Zipkin,61247710,57180252,0,"2020/04/16, 13:11:53",False,"2020/04/16, 13:11:53",287.0,8631898.0,0,This also implied to tell sleuth not to generate the spans for async operations in application.properties:
Zipkin,61247710,57180252,0,"2020/04/16, 13:11:53",False,"2020/04/16, 13:11:53",287.0,8631898.0,0,spring.sleuth.async.enabled=false
Zipkin,60081185,55001316,0,"2020/02/05, 19:34:06",True,"2020/02/05, 19:34:06",524.0,746528.0,1,"For basic authentication, the username and password are required to be sent as part of the HTTP Header  Authorization ."
Zipkin,60081185,55001316,0,"2020/02/05, 19:34:06",True,"2020/02/05, 19:34:06",524.0,746528.0,1,"The header value is computed as Base64 encoding of the string  username:password .So if the username is  abcd  and password is  1234 , the header will look something like this (Chatset used: UTF-8)."
Zipkin,60081185,55001316,0,"2020/02/05, 19:34:06",True,"2020/02/05, 19:34:06",524.0,746528.0,1,Authorization: Basic YWJjZDoxMjM0
Zipkin,60081185,55001316,0,"2020/02/05, 19:34:06",True,"2020/02/05, 19:34:06",524.0,746528.0,1,Sleuth cloud project provides  ZipkinRestTemplateCustomizer  to configure the  RestTemplate  used to communicate with the Zipkin server.
Zipkin,60081185,55001316,0,"2020/02/05, 19:34:06",True,"2020/02/05, 19:34:06",524.0,746528.0,1,"Refer to the documentation for the same: 
 https://cloud.spring.io/spring-cloud-sleuth/reference/html/#sending-spans-to-zipkin"
Zipkin,60081185,55001316,0,"2020/02/05, 19:34:06",True,"2020/02/05, 19:34:06",524.0,746528.0,1,Note: Base64 encoding is reversible and hence Basic auth credentials are not secured.
Zipkin,60081185,55001316,0,"2020/02/05, 19:34:06",True,"2020/02/05, 19:34:06",524.0,746528.0,1,HTTPS communication should be used along with Basic Authentication.
Zipkin,63461779,54986635,0,"2020/08/18, 07:12:18",False,"2020/08/18, 07:12:18",3.0,12639023.0,0,I got same problem.
Zipkin,63461779,54986635,0,"2020/08/18, 07:12:18",False,"2020/08/18, 07:12:18",3.0,12639023.0,0,Seem Spring boot  2.1.2.RELEASE  not work with Zipkin.
Zipkin,63461779,54986635,0,"2020/08/18, 07:12:18",False,"2020/08/18, 07:12:18",3.0,12639023.0,0,Please upgrade to Spring boot version &gt;  2.1.2.RELEASE .
Zipkin,52585202,52550644,0,"2018/10/01, 08:47:20",True,"2018/10/01, 08:47:20",568.0,5238035.0,2,"For those who could come across with a same scenario like this,"
Zipkin,52585202,52550644,0,"2018/10/01, 08:47:20",True,"2018/10/01, 08:47:20",568.0,5238035.0,2,github  has given  APIs  to get details on the repository tag set of each project release as a json object ( https://api.github.com/repos/openzipkin/zipkin/tags  ).
Zipkin,52585202,52550644,0,"2018/10/01, 08:47:20",True,"2018/10/01, 08:47:20",568.0,5238035.0,2,So that can be used to get the latest version of zipkin.
Zipkin,52585202,52550644,0,"2018/10/01, 08:47:20",True,"2018/10/01, 08:47:20",568.0,5238035.0,2,"To get the currently running version of my system, zipkin has given an actuator/info end point ( http://localhost:9411/actuator/info )."
Zipkin,51354529,51151547,3,"2018/07/16, 07:19:17",True,"2018/07/16, 07:19:17",1767.0,2498986.0,1,Yes.
Zipkin,51354529,51151547,3,"2018/07/16, 07:19:17",True,"2018/07/16, 07:19:17",1767.0,2498986.0,1,You have to use Brave.
Zipkin,51354529,51151547,3,"2018/07/16, 07:19:17",True,"2018/07/16, 07:19:17",1767.0,2498986.0,1,"In fact, Spring cloud sleuth (V2) uses Brave under the hood."
Zipkin,51354529,51151547,3,"2018/07/16, 07:19:17",True,"2018/07/16, 07:19:17",1767.0,2498986.0,1,Check the brave web-mvc example to get started.
Zipkin,51354529,51151547,3,"2018/07/16, 07:19:17",True,"2018/07/16, 07:19:17",1767.0,2498986.0,1,https://github.com/openzipkin/brave-webmvc-example
Zipkin,51042639,51040315,0,"2018/06/26, 15:11:31",True,"2018/06/26, 15:11:31",1273.0,4035426.0,1,Try to change all properties with this:
Zipkin,51042639,51040315,0,"2018/06/26, 15:11:31",True,"2018/06/26, 15:11:31",1273.0,4035426.0,1,"spring.sleuth.sampler.percentage=1.0 is Edgware
so you need that"
Zipkin,51042639,51040315,0,"2018/06/26, 15:11:31",True,"2018/06/26, 15:11:31",1273.0,4035426.0,1,baseUrl by default is localhost
Zipkin,51073739,50753384,0,"2018/06/28, 05:12:27",True,"2018/06/28, 05:12:27",1728.0,5254103.0,0,"When I change the project root log level to ""debug"", I saw some error report from zipkin."
Zipkin,51073739,50753384,0,"2018/06/28, 05:12:27",True,"2018/06/28, 05:12:27",1728.0,5254103.0,0,Then I realized that the zipkin server I used was very very old.
Zipkin,51073739,50753384,0,"2018/06/28, 05:12:27",True,"2018/06/28, 05:12:27",1728.0,5254103.0,0,And the zipkin API call returned 404.
Zipkin,51073739,50753384,0,"2018/06/28, 05:12:27",True,"2018/06/28, 05:12:27",1728.0,5254103.0,0,When I updated my zipkin server to latest version.
Zipkin,51073739,50753384,0,"2018/06/28, 05:12:27",True,"2018/06/28, 05:12:27",1728.0,5254103.0,0,It worked.
Zipkin,50599592,50599433,4,"2018/05/30, 11:14:23",False,"2018/05/30, 11:14:23",294930.0,208809.0,0,According to  the section titled Cleanup in the Istio docs :
Zipkin,50539945,50533178,1,"2018/05/26, 09:21:55",True,"2018/05/26, 09:21:55",8336.0,1773866.0,1,"However, I'd also like to send log messages to Zipkin (either as new spans or annotations to existing spans)."
Zipkin,50539945,50533178,1,"2018/05/26, 09:21:55",True,"2018/05/26, 09:21:55",8336.0,1773866.0,1,"If I use org.slf4j.Logger to simply LOG.info(""something""), I see the INFO message in console output, with the exportable flag set to true:"
Zipkin,50539945,50533178,1,"2018/05/26, 09:21:55",True,"2018/05/26, 09:21:55",8336.0,1773866.0,1,You can't send logs to Zipkin.
Zipkin,50539945,50533178,1,"2018/05/26, 09:21:55",True,"2018/05/26, 09:21:55",8336.0,1773866.0,1,You can send log statements to ELK.
Zipkin,50539945,50533178,1,"2018/05/26, 09:21:55",True,"2018/05/26, 09:21:55",8336.0,1773866.0,1,"You can check out the sample  https://github.com/marcingrzejszczak/vagrant-elk-box  that has a vagrant box with ELK, uses Sleuth for log correlation and uses ELK to visualize the logs"
Zipkin,50527821,50506432,0,"2018/05/25, 14:08:47",False,"2018/05/25, 14:08:47",201.0,7956609.0,1,To log only request with particular error you can add the log in your exception mapper where you are handling those error.
Zipkin,50527821,50506432,0,"2018/05/25, 14:08:47",False,"2018/05/25, 14:08:47",201.0,7956609.0,1,"To show the log for error response you can set like below,"
Zipkin,50527821,50506432,0,"2018/05/25, 14:08:47",False,"2018/05/25, 14:08:47",201.0,7956609.0,1,and set
Zipkin,50478519,50389884,0,"2018/05/23, 04:20:43",False,"2018/05/23, 04:20:43",201.0,7956609.0,1,You can add the trace id in your logback.xml
Zipkin,50478519,50389884,0,"2018/05/23, 04:20:43",False,"2018/05/23, 04:20:43",201.0,7956609.0,1,"""request_id"":
                {""trace_id"":""%X{X-B3-TraceId}"",""span_id"":""%X{X-B3-SpanId}"",""parent_span_id"":""%X{X-B3-ParentSpanId}""},"
Zipkin,49407790,49396204,0,"2018/03/21, 15:35:06",False,"2018/03/21, 15:35:06",11.0,7472851.0,0,I found the solution I think.
Zipkin,49407790,49396204,0,"2018/03/21, 15:35:06",False,"2018/03/21, 15:35:06",11.0,7472851.0,0,I changed it to this:
Zipkin,49407790,49396204,0,"2018/03/21, 15:35:06",False,"2018/03/21, 15:35:06",11.0,7472851.0,0,RABBIT_URI=amqp://user:password@tracing:5672
Zipkin,48628487,48626548,8,"2018/02/05, 19:47:34",True,"2018/02/05, 19:47:34",8336.0,1773866.0,0,Please use latest snapshots.
Zipkin,48628487,48626548,8,"2018/02/05, 19:47:34",True,"2018/02/05, 19:47:34",8336.0,1773866.0,0,Sleuth in latest snapshots uses brave internally so integration will be extremely simple.
Zipkin,47363721,47343439,2,"2017/11/18, 09:34:23",False,"2017/11/18, 09:34:23",8336.0,1773866.0,3,This feature is available in edgware release train.
Zipkin,47363721,47343439,2,"2017/11/18, 09:34:23",False,"2017/11/18, 09:34:23",8336.0,1773866.0,3,That corresponds to version 1.3.x of sleuth
Zipkin,46289584,46287877,0,"2017/09/19, 02:30:27",False,"2017/09/19, 02:30:27",81.0,2232476.0,0,The Zipkin UI is making an AJAX request to the API in order to retrieve the data that is displayed.
Zipkin,46289584,46287877,0,"2017/09/19, 02:30:27",False,"2017/09/19, 02:30:27",81.0,2232476.0,0,You can find the API definition for zipkin at:
Zipkin,46289584,46287877,0,"2017/09/19, 02:30:27",False,"2017/09/19, 02:30:27",81.0,2232476.0,0,https://github.com/openzipkin/zipkin-api
Zipkin,46289584,46287877,0,"2017/09/19, 02:30:27",False,"2017/09/19, 02:30:27",81.0,2232476.0,0,I believe you are looking for the URL:  http://zipkin.iamplus.xyz/api/v1/traces
Zipkin,46289584,46287877,0,"2017/09/19, 02:30:27",False,"2017/09/19, 02:30:27",81.0,2232476.0,0,From there you will get the traces matching your filter
Zipkin,45632418,45492904,0,"2017/08/11, 13:02:33",False,"2017/08/11, 13:02:33",86.0,7146447.0,0,I have the same config running on my ingress 9.0-beta.11.
Zipkin,45632418,45492904,0,"2017/08/11, 13:02:33",False,"2017/08/11, 13:02:33",86.0,7146447.0,0,I guess it's just a misconfiguration.
Zipkin,45632418,45492904,0,"2017/08/11, 13:02:33",False,"2017/08/11, 13:02:33",86.0,7146447.0,0,First I'll recommend you to not change the template and use the default values and just change when the basic-auth works.
Zipkin,45632418,45492904,0,"2017/08/11, 13:02:33",False,"2017/08/11, 13:02:33",86.0,7146447.0,0,What the logs of ingress show to you?
Zipkin,45632418,45492904,0,"2017/08/11, 13:02:33",False,"2017/08/11, 13:02:33",86.0,7146447.0,0,Did you create the basic-auth file in the same namespace of the ingress resource?
Zipkin,43843859,43790619,0,"2017/05/08, 12:12:55",True,"2017/05/08, 12:12:55",8336.0,1773866.0,1,The issue got fixed -  https://github.com/spring-cloud/spring-cloud-sleuth/issues/585  .
Zipkin,43843859,43790619,0,"2017/05/08, 12:12:55",True,"2017/05/08, 12:12:55",8336.0,1773866.0,1,In the upcoming releases 1.1.5 and 1.2.1 it should work
Zipkin,41723986,40701663,3,"2017/01/18, 18:04:16",True,"2017/01/18, 18:04:16",8336.0,1773866.0,4,Spring Cloud Zipkin Stream is using Spring Cloud Stream underneath.
Zipkin,41723986,40701663,3,"2017/01/18, 18:04:16",True,"2017/01/18, 18:04:16",8336.0,1773866.0,4,You need to provide how do you want to send the spans to Zipkin - thus you need a binder.
Zipkin,41723986,40701663,3,"2017/01/18, 18:04:16",True,"2017/01/18, 18:04:16",8336.0,1773866.0,4,One possible binder is the RabbitMQ binder.
Zipkin,41723986,40701663,3,"2017/01/18, 18:04:16",True,"2017/01/18, 18:04:16",8336.0,1773866.0,4,Check out this:  https://github.com/spring-cloud-samples/sleuth-documentation-apps/blob/master/zipkin-server/build.gradle#L6
Zipkin,37867634,37842256,0,"2016/06/16, 22:15:19",True,"2016/06/16, 22:15:19",911.0,1822028.0,1,It seems that Brave does not support this.
Zipkin,37867634,37842256,0,"2016/06/16, 22:15:19",True,"2016/06/16, 22:15:19",911.0,1822028.0,1,An issue has been reported on their GitHub page.
Zipkin,37867634,37842256,0,"2016/06/16, 22:15:19",True,"2016/06/16, 22:15:19",911.0,1822028.0,1,https://github.com/openzipkin/brave/issues/166
Zipkin,17684615,17017657,0,"2013/07/16, 21:48:17",False,"2019/09/02, 10:04:21",1124.0,819626.0,1,I don't know much about ActiveMQ but you need to pass the zipkin trace information along.
Zipkin,17684615,17017657,0,"2013/07/16, 21:48:17",False,"2019/09/02, 10:04:21",1124.0,819626.0,1,"Review the  ActiveMQ Collector  section in this doc 
 https://github.com/openzipkin/zipkin/blob/master/zipkin-server/README.md"
Zipkin,17684615,17017657,0,"2013/07/16, 21:48:17",False,"2019/09/02, 10:04:21",1124.0,819626.0,1,I just set up Zipkin tracing for a stack that includes RabbitMQ.
Zipkin,17684615,17017657,0,"2013/07/16, 21:48:17",False,"2019/09/02, 10:04:21",1124.0,819626.0,1,"I added the parent_span_id, and span_id to the message header before the message is placed on the queue."
Zipkin,17684615,17017657,0,"2013/07/16, 21:48:17",False,"2019/09/02, 10:04:21",1124.0,819626.0,1,Then the applications that read the messages get the trace information from the header.
Zipkin,17684615,17017657,0,"2013/07/16, 21:48:17",False,"2019/09/02, 10:04:21",1124.0,819626.0,1,"And if you need more help, I recommend jumping on IRC #zipkin."
Zipkin,65962429,65943408,2,"2021/01/30, 00:17:20",False,"2021/01/30, 00:17:20",1693.0,971735.0,1,"I was not able to reproduce your issue with the  spring-cloud-sleuth-sample-zipkin  app (it worked to me), here's what I did:"
Zipkin,65962429,65943408,2,"2021/01/30, 00:17:20",False,"2021/01/30, 00:17:20",1693.0,971735.0,1,A few pointers to troubleshoot this:
Zipkin,65962429,65943408,2,"2021/01/30, 00:17:20",False,"2021/01/30, 00:17:20",1693.0,971735.0,1,Try to make it work using the  sample  and try to bring the working example closer to your app (by adding dependencies) and see what is the difference between the two and where will it break.
Zipkin,65962429,65943408,2,"2021/01/30, 00:17:20",False,"2021/01/30, 00:17:20",1693.0,971735.0,1,If you can create a minimal sample app (e.g.
Zipkin,65962429,65943408,2,"2021/01/30, 00:17:20",False,"2021/01/30, 00:17:20",1693.0,971735.0,1,": based on the zipkin sample) that reproduces the issue, please feel free to create an issue on GH:  https://github.com/spring-cloud/spring-cloud-sleuth  and tag me ( @jonatan-ivanov ), I can take a look."
Zipkin,65966458,65943408,0,"2021/01/30, 11:30:34",False,"2021/01/30, 11:30:34",830.0,7349864.0,1,Finally I found it.
Zipkin,65966458,65943408,0,"2021/01/30, 11:30:34",False,"2021/01/30, 11:30:34",830.0,7349864.0,1,I had 2 problemas
Zipkin,65966458,65943408,0,"2021/01/30, 11:30:34",False,"2021/01/30, 11:30:34",830.0,7349864.0,1,1 - I was using zipkin-slim docker image for my zip container.
Zipkin,65966458,65943408,0,"2021/01/30, 11:30:34",False,"2021/01/30, 11:30:34",830.0,7349864.0,1,This image doesn't contain the rabbitmq collector  rabbitmq collector .
Zipkin,65966458,65943408,0,"2021/01/30, 11:30:34",False,"2021/01/30, 11:30:34",830.0,7349864.0,1,I have replaces by standar zipkin image
Zipkin,65966458,65943408,0,"2021/01/30, 11:30:34",False,"2021/01/30, 11:30:34",830.0,7349864.0,1,"2 - I do not know why, but the connection from sleuth/zipkin to RabbitMQ is not retrying (I will investigate further)."
Zipkin,65966458,65943408,0,"2021/01/30, 11:30:34",False,"2021/01/30, 11:30:34",830.0,7349864.0,1,"So, if I was in a hurry and test very early (when RabbitMQ is not yet available) it fails, and not retries."
Zipkin,65966458,65943408,0,"2021/01/30, 11:30:34",False,"2021/01/30, 11:30:34",830.0,7349864.0,1,My docker-compose relevant sections now are like this:
Zipkin,65966458,65943408,0,"2021/01/30, 11:30:34",False,"2021/01/30, 11:30:34",830.0,7349864.0,1,Thanks again to  Jonatan Ivanov  for helping me!
Zipkin,58554326,58551796,0,"2019/10/25, 10:49:15",True,"2019/10/25, 10:49:15",762.0,1227937.0,1,probably best to have the issue you raised in github vs cross posting.
Zipkin,58554326,58551796,0,"2019/10/25, 10:49:15",True,"2019/10/25, 10:49:15",762.0,1227937.0,1,it is a bug  https://github.com/honeycombio/honeycomb-opentracing-proxy/issues/37
Zipkin,61975479,56923548,0,"2020/05/23, 19:50:21",False,"2020/05/23, 19:50:21",83.0,2702332.0,0,There are 2 entries in mysql zipkin_spans table
Zipkin,61975479,56923548,0,"2020/05/23, 19:50:21",False,"2020/05/23, 19:50:21",83.0,2702332.0,0,Example
Zipkin,61975479,56923548,0,"2020/05/23, 19:50:21",False,"2020/05/23, 19:50:21",83.0,2702332.0,0,32 character hex trace id  5ec92d0240cd9dee0421f4763e9f674f  displayed in zipkin ui corresponds to
Zipkin,61975479,56923548,0,"2020/05/23, 19:50:21",False,"2020/05/23, 19:50:21",83.0,2702332.0,0,trace_id_high = 6830039797584469486 in mysql  (5EC92D0240CD9DEE -  upper 16 hex character)
Zipkin,61975479,56923548,0,"2020/05/23, 19:50:21",False,"2020/05/23, 19:50:21",83.0,2702332.0,0,id = 297787839077115727 in mysql  (421F4763E9F674F -  lower 16 hex charecter)
Zipkin,56032785,56032752,0,"2019/05/08, 05:37:47",False,"2020/08/19, 12:00:44",167.0,486631.0,0,After continue efforts and going throgh core api of spring boot application I got my solution:)
Zipkin,56032785,56032752,0,"2019/05/08, 05:37:47",False,"2020/08/19, 12:00:44",167.0,486631.0,0,Root cause of my issue is below :
Zipkin,56032785,56032752,0,"2019/05/08, 05:37:47",False,"2020/08/19, 12:00:44",167.0,486631.0,0,"MY application using Spring boot RabitMQ integration and due that
zipkin taking 1st prefrance to RabitMQ sender and my trace are ignored
my zipkin server."
Zipkin,56032785,56032752,0,"2019/05/08, 05:37:47",False,"2020/08/19, 12:00:44",167.0,486631.0,0,"So use below configration is anyone has same issue to avoid lot painless efforts , even we are not getting in logs of server root cause of it"
Zipkin,56032785,56032752,0,"2019/05/08, 05:37:47",False,"2020/08/19, 12:00:44",167.0,486631.0,0,*---
Zipkin,53370111,53369694,0,"2018/11/19, 09:32:06",True,"2018/11/19, 09:32:06",463.0,1749786.0,2,I was using Finchley.SR2 train of releases.
Zipkin,53370111,53369694,0,"2018/11/19, 09:32:06",True,"2018/11/19, 09:32:06",463.0,1749786.0,2,"Once I upgraded to the latest Spring Boot and Spring Cloud versions, the issue fixed itself."
Zipkin,53370111,53369694,0,"2018/11/19, 09:32:06",True,"2018/11/19, 09:32:06",463.0,1749786.0,2,I removed the opentracing-spring-cloud-starter dependency and am now just using
Zipkin,52739988,52377663,0,"2018/10/10, 15:12:13",False,"2018/10/10, 15:12:13",201.0,7956609.0,0,Check your configuration file and make sure the baseUrl is given properly here
Zipkin,50298803,50295992,5,"2018/05/11, 22:10:23",False,"2018/05/11, 22:10:23",91301.0,2756547.0,0,OK!
Zipkin,50298803,50295992,5,"2018/05/11, 22:10:23",False,"2018/05/11, 22:10:23",91301.0,2756547.0,0,I see now the problem!
Zipkin,50298803,50295992,5,"2018/05/11, 22:10:23",False,"2018/05/11, 22:10:23",91301.0,2756547.0,0,"So, you say that your HTTP request has these tracing headers:  X-B3-TraceId ,  X-B3-SpanId ,  X-B3-Sampled ,  X-Span-Name ,  X-B3-ParentSpanId ."
Zipkin,50298803,50295992,5,"2018/05/11, 22:10:23",False,"2018/05/11, 22:10:23",91301.0,2756547.0,0,Then you have this code:
Zipkin,50298803,50295992,5,"2018/05/11, 22:10:23",False,"2018/05/11, 22:10:23",91301.0,2756547.0,0,And that's absolutely natural that your tracing header are not transferred to the RabbitMQ: there is just no those headers to send.
Zipkin,50298803,50295992,5,"2018/05/11, 22:10:23",False,"2018/05/11, 22:10:23",91301.0,2756547.0,0,I believe that you can extract those headers in this  @RequestMapping  method and populate them to the AMQP message before sending.
Zipkin,50298803,50295992,5,"2018/05/11, 22:10:23",False,"2018/05/11, 22:10:23",91301.0,2756547.0,0,See  org.springframework.amqp.core.MessageBuilder .
Zipkin,50298803,50295992,5,"2018/05/11, 22:10:23",False,"2018/05/11, 22:10:23",91301.0,2756547.0,0,"I also think that Spring Cloud Sleuth should have some mechanism to obtain those headers, e.g."
Zipkin,50298803,50295992,5,"2018/05/11, 22:10:23",False,"2018/05/11, 22:10:23",91301.0,2756547.0,0,Tracer.currentSpan() :  http://cloud.spring.io/spring-cloud-static/spring-cloud-sleuth/2.0.0.RC1/single/spring-cloud-sleuth.html#_current_span
Zipkin,48063504,47992456,0,"2018/01/02, 17:22:45",False,"2018/01/02, 17:22:45",81.0,2232476.0,0,"Yes, they are both stateless."
Zipkin,48063504,47992456,0,"2018/01/02, 17:22:45",False,"2018/01/02, 17:22:45",81.0,2232476.0,0,You can deploy them using whatever horizontal-scalability construct is available to you.
Zipkin,47630471,47630294,3,"2017/12/04, 11:45:28",True,"2017/12/04, 13:10:19",32952.0,6603816.0,3,"When connecting to the mysql container while using links, you need to use the container name as a hostname."
Zipkin,47630471,47630294,3,"2017/12/04, 11:45:28",True,"2017/12/04, 13:10:19",32952.0,6603816.0,3,Change the connection string to:
Zipkin,47630471,47630294,3,"2017/12/04, 11:45:28",True,"2017/12/04, 13:10:19",32952.0,6603816.0,3,"And when starting the zipkin container, set the env variable:"
Zipkin,45014834,45012331,0,"2017/07/10, 17:32:57",True,"2017/07/10, 17:32:57",8336.0,1773866.0,1,Why are you mocking a span?
Zipkin,45014834,45012331,0,"2017/07/10, 17:32:57",True,"2017/07/10, 17:32:57",8336.0,1773866.0,1,This makes absolutely no sense.
Zipkin,45014834,45012331,0,"2017/07/10, 17:32:57",True,"2017/07/10, 17:32:57",8336.0,1773866.0,1,Also a Span is never a bean.
Zipkin,45014834,45012331,0,"2017/07/10, 17:32:57",True,"2017/07/10, 17:32:57",8336.0,1773866.0,1,You already create a normal span via a builder and you should leave that.
Zipkin,45014834,45012331,0,"2017/07/10, 17:32:57",True,"2017/07/10, 17:32:57",8336.0,1773866.0,1,Assuming that you have set up the Boot context property and  you want to mock out  tracer  bean you should do the following
Zipkin,44636339,44611370,0,"2017/06/19, 20:13:49",False,"2017/06/19, 20:13:49",35.0,440061.0,0,"*sigh, so it turns out, that someone had turned off zipking tracing in a properties file, for no good reason."
Zipkin,44636339,44611370,0,"2017/06/19, 20:13:49",False,"2017/06/19, 20:13:49",35.0,440061.0,0,*sigh
Zipkin,43401515,43394714,2,"2017/04/13, 23:52:30",True,"2017/04/13, 23:52:30",81.0,2232476.0,2,You are trying to run 2 different applications.
Zipkin,43401515,43394714,2,"2017/04/13, 23:52:30",True,"2017/04/13, 23:52:30",81.0,2232476.0,2,To run the  zipkin  application with with ElasticSearch and Kafka you will need to run it with both sets of environment variables:
Zipkin,43401515,43394714,2,"2017/04/13, 23:52:30",True,"2017/04/13, 23:52:30",81.0,2232476.0,2,"Once you have the  zipkin  server running with ES, then you can use your second command to generate the data for the dependency graph view"
Zipkin,41842532,41823536,0,"2017/01/25, 04:43:12",False,"2017/01/25, 04:43:12",762.0,1227937.0,0,"At the moment, there's no replacement for the ""thread binder"" apis."
Zipkin,41842532,41823536,0,"2017/01/25, 04:43:12",False,"2017/01/25, 04:43:12",762.0,1227937.0,0,There will be in the coming months.
Zipkin,41842532,41823536,0,"2017/01/25, 04:43:12",False,"2017/01/25, 04:43:12",762.0,1227937.0,0,This is indeed needed to renovate existing instrumentation.
Zipkin,41842532,41823536,0,"2017/01/25, 04:43:12",False,"2017/01/25, 04:43:12",762.0,1227937.0,0,"Until then, you can re-use thread binders via TracerAdapter or use a different in-process propagation library."
Zipkin,41842532,41823536,0,"2017/01/25, 04:43:12",False,"2017/01/25, 04:43:12",762.0,1227937.0,0,The following link includes a working example  https://github.com/openzipkin/brave/tree/master/brave#upgrading-from-brave-3
Zipkin,66524575,66517644,2,"2021/03/08, 07:07:48",True,"2021/03/08, 07:25:55",1693.0,971735.0,1,"It was removed in Sleuth 3.0, though it seems the docs were not updated, I'm going to update the docs soon."
Zipkin,66524575,66517644,2,"2021/03/08, 07:07:48",True,"2021/03/08, 07:25:55",1693.0,971735.0,1,"To fix the rest with your logs, you can check the logging config  here , the  log integration in the docs  and  this answer ."
Zipkin,66481285,66479206,1,"2021/03/04, 20:52:36",False,"2021/03/04, 20:52:36",1693.0,971735.0,0,This should be done out of the box:  https://docs.spring.io/spring-cloud-sleuth/docs/2.2.7.RELEASE/reference/html/#feign
Zipkin,66481285,66479206,1,"2021/03/04, 20:52:36",False,"2021/03/04, 20:52:36",1693.0,971735.0,0,"You can take a look at the feign sample (you need to go back in the history, currently it is for 3.x):  https://github.com/spring-cloud/spring-cloud-sleuth/tree/master/spring-cloud-sleuth-samples/spring-cloud-sleuth-sample-feign"
Zipkin,66481285,66479206,1,"2021/03/04, 20:52:36",False,"2021/03/04, 20:52:36",1693.0,971735.0,0,"In order to see if propagation works, look into the outgoing request, it should contain the tracing-related headers."
Zipkin,66384663,66383029,4,"2021/02/26, 13:04:31",True,"2021/02/26, 13:04:31",8336.0,1773866.0,2,"You could create your own  SpanHandler  bean that takes the  FinishedSpan , converts into JSON and stores it somewhere on your drive."
Zipkin,66384663,66383029,4,"2021/02/26, 13:04:31",True,"2021/02/26, 13:04:31",8336.0,1773866.0,2,Then you could just iterate over jsons and upload them to the Zipkin server
Zipkin,65231143,65228873,0,"2020/12/10, 10:43:15",True,"2020/12/10, 10:43:15",6331.0,2133695.0,1,"No, the tracing SPI will not be backported to Vert.x 3."
Zipkin,65231143,65228873,0,"2020/12/10, 10:43:15",True,"2020/12/10, 10:43:15",6331.0,2133695.0,1,I would recommend to check out  Migrate from Vert.x 3 to Vert.x 4 :
Zipkin,65231143,65228873,0,"2020/12/10, 10:43:15",True,"2020/12/10, 10:43:15",6331.0,2133695.0,1,"When­ever pos­si­ble, Vert.x 4 APIs have been made avail­able in
Vert.x 3 with a dep­re­ca­tion of the old API, giv­ing the
op­por­tu­nity to im­prove a Vert.x 3 ap­pli­ca­tion with a bet­ter
API while al­low­ing the ap­pli­ca­tion to be ready for a Vert.x 4
mi­gra­tion."
Zipkin,65231143,65228873,0,"2020/12/10, 10:43:15",True,"2020/12/10, 10:43:15",6331.0,2133695.0,1,"In other words, one of the Vert.x 4 goals was to minimize the upgrading effort."
Zipkin,65171650,65169398,0,"2020/12/06, 20:34:26",False,"2020/12/06, 20:34:26",8336.0,1773866.0,0,You should use e.g.
Zipkin,65171650,65169398,0,"2020/12/06, 20:34:26",False,"2020/12/06, 20:34:26",8336.0,1773866.0,0,openzipkin Brave project or Opentelemetry projects directly.
Zipkin,65171650,65169398,0,"2020/12/06, 20:34:26",False,"2020/12/06, 20:34:26",8336.0,1773866.0,0,Sleuth works only with boot based projects
Zipkin,65347921,65064734,0,"2020/12/17, 22:30:30",True,"2020/12/17, 22:30:30",1693.0,971735.0,0,"I think what you call correlationId is in fact the traceId, if you are new to distributed tracing, I highly recommend reading the docs of  spring-cloud-sleuth , the  introduction  section will give you a basic understanding while the  propagation  will tell you well, how your fields are propagated across services."
Zipkin,65347921,65064734,0,"2020/12/17, 22:30:30",True,"2020/12/17, 22:30:30",1693.0,971735.0,0,"I also recommend this talk:  Distributed Tracing: Latency Analysis for Your Microservices - Grzejszczak, Krishna ."
Zipkin,65347921,65064734,0,"2020/12/17, 22:30:30",True,"2020/12/17, 22:30:30",1693.0,971735.0,0,To answer your exact questions:
Zipkin,65347921,65064734,0,"2020/12/17, 22:30:30",True,"2020/12/17, 22:30:30",1693.0,971735.0,0,How the correlation id will be passed to Kafka messages?
Zipkin,65347921,65064734,0,"2020/12/17, 22:30:30",True,"2020/12/17, 22:30:30",1693.0,971735.0,0,"Kafka has headers, I assume the fields are propagated through Kafka headers."
Zipkin,65347921,65064734,0,"2020/12/17, 22:30:30",True,"2020/12/17, 22:30:30",1693.0,971735.0,0,How the correlation id will be passed to Http requests?
Zipkin,65347921,65064734,0,"2020/12/17, 22:30:30",True,"2020/12/17, 22:30:30",1693.0,971735.0,0,Through HTTP Headers.
Zipkin,65347921,65064734,0,"2020/12/17, 22:30:30",True,"2020/12/17, 22:30:30",1693.0,971735.0,0,Is it possible to use existing tracedId from other service?
Zipkin,65347921,65064734,0,"2020/12/17, 22:30:30",True,"2020/12/17, 22:30:30",1693.0,971735.0,0,"Not just possible, Sleuth does this for you out of the box."
Zipkin,65347921,65064734,0,"2020/12/17, 22:30:30",True,"2020/12/17, 22:30:30",1693.0,971735.0,0,If there is a traceId in the incoming request/message/event/etc.
Zipkin,65347921,65064734,0,"2020/12/17, 22:30:30",True,"2020/12/17, 22:30:30",1693.0,971735.0,0,Sleuth will not create a new one but it will use it (see the docs I linked above).
Zipkin,63739507,63737361,2,"2020/09/04, 13:37:27",False,"2020/09/04, 13:37:27",4113.0,12201084.0,0,Your service is expecting following labels on pod:
Zipkin,63739507,63737361,2,"2020/09/04, 13:37:27",False,"2020/09/04, 13:37:27",4113.0,12201084.0,0,Although it looks like you have only one label on zipkin pods:
Zipkin,63739507,63737361,2,"2020/09/04, 13:37:27",False,"2020/09/04, 13:37:27",4113.0,12201084.0,0,"Label selector uses logical AND (&amp;&amp;), and this means that all labels specified must be on pod to match it."
Zipkin,63778418,63737361,0,"2020/09/07, 16:19:45",True,"2020/09/07, 18:08:33",351.0,10196632.0,0,The following worked.
Zipkin,63778418,63737361,0,"2020/09/07, 16:19:45",True,"2020/09/07, 18:08:33",351.0,10196632.0,0,"Sorry, I cannot provide info on all details since I don't know them :( Maybe somebody else can."
Zipkin,63778418,63737361,0,"2020/09/07, 16:19:45",True,"2020/09/07, 18:08:33",351.0,10196632.0,0,deployment.yaml
Zipkin,63778418,63737361,0,"2020/09/07, 16:19:45",True,"2020/09/07, 18:08:33",351.0,10196632.0,0,service.yaml
Zipkin,63778418,63737361,0,"2020/09/07, 16:19:45",True,"2020/09/07, 18:08:33",351.0,10196632.0,0,ingress.yaml
Zipkin,63384808,63384688,0,"2020/08/13, 00:15:25",True,"2020/08/13, 00:15:25",8336.0,1773866.0,1,It's because of sampling.
Zipkin,63384808,63384688,0,"2020/08/13, 00:15:25",True,"2020/08/13, 00:15:25",8336.0,1773866.0,1,Please create a bean of sampler type whose value can be Sampler.ALWAYS or set the probability property to 1.0
Zipkin,63219184,62894286,0,"2020/08/02, 20:39:09",False,"2020/08/02, 20:46:19",11.0,11424150.0,1,Hi I just resolved this issue ..
Zipkin,63219184,62894286,0,"2020/08/02, 20:39:09",False,"2020/08/02, 20:46:19",11.0,11424150.0,1,Step1.verify C:\Programfiles\Err(version) folder including bin folder is created or not.
Zipkin,63219184,62894286,0,"2020/08/02, 20:39:09",False,"2020/08/02, 20:46:19",11.0,11424150.0,1,Otherwise try to download and install Erlang again.
Zipkin,63219184,62894286,0,"2020/08/02, 20:39:09",False,"2020/08/02, 20:46:19",11.0,11424150.0,1,reinstall RabbitMQ and try connecting Zipkin.
Zipkin,63219184,62894286,0,"2020/08/02, 20:39:09",False,"2020/08/02, 20:46:19",11.0,11424150.0,1,Make sure Erlang version and RabbitMQ version is compatible.
Zipkin,63219184,62894286,0,"2020/08/02, 20:39:09",False,"2020/08/02, 20:46:19",11.0,11424150.0,1,Step2.Check ERLANG_HOME is set to proper location in environment variables.
Zipkin,63219184,62894286,0,"2020/08/02, 20:39:09",False,"2020/08/02, 20:46:19",11.0,11424150.0,1,"at this point if RabbitMQ windows installer pointing to any old erlang version installed earlier  try to install RabitMQ windows manually
follow the steps mentioned in below link for manual installation"
Zipkin,63219184,62894286,0,"2020/08/02, 20:39:09",False,"2020/08/02, 20:46:19",11.0,11424150.0,1,https://www.rabbitmq.com/install-windows-manual.html
Zipkin,61710527,61710421,0,"2020/05/10, 13:32:48",True,"2020/05/10, 13:32:48",nan,nan,0,That's an old implementation.
Zipkin,61710527,61710421,0,"2020/05/10, 13:32:48",True,"2020/05/10, 13:32:48",nan,nan,0,Below I have modified your code to work:
Zipkin,61710527,61710421,0,"2020/05/10, 13:32:48",True,"2020/05/10, 13:32:48",nan,nan,0,For more information check this link:  https://gist.github.com/marcingrzejszczak/d3c15a0c11dda71970e42c513c9c0e09
Zipkin,61332491,61198731,0,"2020/04/21, 00:41:04",False,"2020/04/21, 00:41:04",985.0,598932.0,0,From  zipkin docs :
Zipkin,61332491,61198731,0,"2020/04/21, 00:41:04",False,"2020/04/21, 00:41:04",985.0,598932.0,0,There is no support for TTL through this SpanStore.
Zipkin,61332491,61198731,0,"2020/04/21, 00:41:04",False,"2020/04/21, 00:41:04",985.0,598932.0,0,It is recommended instead to use Elastic Curator to remove indices older than the point you are interested in.
Zipkin,62824310,60598519,0,"2020/07/10, 00:55:28",False,"2020/07/16, 00:09:39",1.0,13324871.0,0,"I had the same problem and solved it by:
Open windows task manger and kill all java instances java.exe or javaw.exe"
Zipkin,60213881,60170816,0,"2020/02/13, 19:56:57",False,"2020/02/13, 19:56:57",2576.0,12014434.0,0,I have finally figured out what could be the cause of this issue:
Zipkin,60213881,60170816,0,"2020/02/13, 19:56:57",False,"2020/02/13, 19:56:57",2576.0,12014434.0,0,The install option:
Zipkin,60213881,60170816,0,"2020/02/13, 19:56:57",False,"2020/02/13, 19:56:57",2576.0,12014434.0,0,--set values.tracing.provider=zipkin --set values.global.tracer.zipkin.address
Zipkin,60213881,60170816,0,"2020/02/13, 19:56:57",False,"2020/02/13, 19:56:57",2576.0,12014434.0,0,requires  &lt;zipkin-collector-service&gt;.&lt;zipkin-collector-namespace&gt;:9411  according to  istio  documentation.
Zipkin,60213881,60170816,0,"2020/02/13, 19:56:57",False,"2020/02/13, 19:56:57",2576.0,12014434.0,0,While You have just IP address and port of external server.
Zipkin,60213881,60170816,0,"2020/02/13, 19:56:57",False,"2020/02/13, 19:56:57",2576.0,12014434.0,0,This most likely means that the install option requires existing name that is in istio service mesh registry.
Zipkin,60213881,60170816,0,"2020/02/13, 19:56:57",False,"2020/02/13, 19:56:57",2576.0,12014434.0,0,"So if Your zipkin collector is outside cluster We need to add  ServiceEntry ,  VirtualService  and maybe  DestinationRule  and so the external service can be used within mesh."
Zipkin,60213881,60170816,0,"2020/02/13, 19:56:57",False,"2020/02/13, 19:56:57",2576.0,12014434.0,0,You can follow  istio  documentation to see how to create these objects for external service.
Zipkin,60213881,60170816,0,"2020/02/13, 19:56:57",False,"2020/02/13, 19:56:57",2576.0,12014434.0,0,Here  is another guide.
Zipkin,60213881,60170816,0,"2020/02/13, 19:56:57",False,"2020/02/13, 19:56:57",2576.0,12014434.0,0,After that We need to update the tracer address value with the  VirtualService  as an endpoint.
Zipkin,60213881,60170816,0,"2020/02/13, 19:56:57",False,"2020/02/13, 19:56:57",2576.0,12014434.0,0,Hope this helps.
Zipkin,59877579,59859303,0,"2020/01/23, 13:31:55",True,"2020/01/23, 13:31:55",2576.0,12014434.0,1,By using the following  commands  I was able to generate the manifests using  istioctl  with parameters You mentioned:
Zipkin,59877579,59859303,0,"2020/01/23, 13:31:55",True,"2020/01/23, 13:31:55",2576.0,12014434.0,1,Then compared them to see differences made with those parameter modifications.
Zipkin,59877579,59859303,0,"2020/01/23, 13:31:55",True,"2020/01/23, 13:31:55",2576.0,12014434.0,1,You can try to manually modify those applied settings or apply it to Your cluster.
Zipkin,59877579,59859303,0,"2020/01/23, 13:31:55",True,"2020/01/23, 13:31:55",2576.0,12014434.0,1,Istioctl I used to generate these manifests:
Zipkin,59877579,59859303,0,"2020/01/23, 13:31:55",True,"2020/01/23, 13:31:55",2576.0,12014434.0,1,Hope it helps.
Zipkin,58754849,58619789,0,"2019/11/07, 20:21:08",False,"2019/11/07, 20:21:08",21.0,9936877.0,0,"Still with using 2.2.0 parent, I still face the whitelable error."
Zipkin,58754849,58619789,0,"2019/11/07, 20:21:08",False,"2019/11/07, 20:21:08",21.0,9936877.0,0,I will check on this latter but by changing the pom defination the Zipkin server work
Zipkin,58754849,58619789,0,"2019/11/07, 20:21:08",False,"2019/11/07, 20:21:08",21.0,9936877.0,0,And in zipkinserverapplication we need the @Enablezipkinserver
Zipkin,58755169,58619789,2,"2019/11/07, 20:47:05",False,"2019/11/07, 20:47:05",3589.0,5587542.0,0,Hi if your spring cloud app target is a Spring Boot 2.x base I suggest to do not try to use the @EnableZipkinServer because it is not a raccomanded way as the java doc suggest:
Zipkin,58755169,58619789,2,"2019/11/07, 20:47:05",False,"2019/11/07, 20:47:05",3589.0,5587542.0,0,form the Zipkin base code:
Zipkin,58755169,58619789,2,"2019/11/07, 20:47:05",False,"2019/11/07, 20:47:05",3589.0,5587542.0,0,I spoke for personal experience with spring boot application 2.x family.
Zipkin,58755169,58619789,2,"2019/11/07, 20:47:05",False,"2019/11/07, 20:47:05",3589.0,5587542.0,0,"The my solution,for development, was use a docker compose like below(consider that my application use spring cloud stream in order to push on zipkin the tracing information:"
Zipkin,58755169,58619789,2,"2019/11/07, 20:47:05",False,"2019/11/07, 20:47:05",3589.0,5587542.0,0,On the other hands if your target is a spring boot 1.5.x you can use the legacy embedded zipkin server like below:
Zipkin,58755169,58619789,2,"2019/11/07, 20:47:05",False,"2019/11/07, 20:47:05",3589.0,5587542.0,0,POM:
Zipkin,58755169,58619789,2,"2019/11/07, 20:47:05",False,"2019/11/07, 20:47:05",3589.0,5587542.0,0,Application:
Zipkin,58755169,58619789,2,"2019/11/07, 20:47:05",False,"2019/11/07, 20:47:05",3589.0,5587542.0,0,package it.valeriovaudi.emarket;
Zipkin,58755169,58619789,2,"2019/11/07, 20:47:05",False,"2019/11/07, 20:47:05",3589.0,5587542.0,0,"both solution works for me, spring boot 1.5.x was the base spring boot version for my master thesis and spring boot 2.x version for a my personal distributed system project that I use every day for my persona family budget management."
Zipkin,58755169,58619789,2,"2019/11/07, 20:47:05",False,"2019/11/07, 20:47:05",3589.0,5587542.0,0,I hope that this can help you
Zipkin,58799041,58619789,0,"2019/11/11, 11:50:52",False,"2019/11/11, 11:50:52",21.0,9936877.0,0,For more details can read the document  https://www.baeldung.com/spring-boot-actuators#boot-2x-actuator
Zipkin,58298923,58294974,2,"2019/10/09, 10:18:06",True,"2019/10/09, 10:18:06",2382.0,4497840.0,2,I have a working project with spring cloud stream and zipkin using the following configuration (maybe you should set the sender.type):
Zipkin,58298923,58294974,2,"2019/10/09, 10:18:06",True,"2019/10/09, 10:18:06",2382.0,4497840.0,2,Hope this can help.
Zipkin,58172731,58170335,2,"2019/09/30, 20:39:51",False,"2019/09/30, 20:39:51",81.0,2232476.0,0,"The problem lies in your  ES_HOSTS  variable, from the docs  here :"
Zipkin,58172731,58170335,2,"2019/09/30, 20:39:51",False,"2019/09/30, 20:39:51",81.0,2232476.0,0,So you will need:  ES_HOSTS=http://storage:9200
Zipkin,58181776,58170335,0,"2019/10/01, 12:23:56",False,"2019/10/01, 12:23:56",35.0,9795454.0,0,Finally I have this file:
Zipkin,58181776,58170335,0,"2019/10/01, 12:23:56",False,"2019/10/01, 12:23:56",35.0,9795454.0,0,Main differences are the usage of
Zipkin,58181776,58170335,0,"2019/10/01, 12:23:56",False,"2019/10/01, 12:23:56",35.0,9795454.0,0,"""ES_HOSTS=elasticsearch:9300"""
Zipkin,58181776,58170335,0,"2019/10/01, 12:23:56",False,"2019/10/01, 12:23:56",35.0,9795454.0,0,instead of
Zipkin,58181776,58170335,0,"2019/10/01, 12:23:56",False,"2019/10/01, 12:23:56",35.0,9795454.0,0,"""ES_HOSTS=storage:9300"""
Zipkin,58181776,58170335,0,"2019/10/01, 12:23:56",False,"2019/10/01, 12:23:56",35.0,9795454.0,0,and in the dependencies configuration I add the entrypoint in dependencies:
Zipkin,58181776,58170335,0,"2019/10/01, 12:23:56",False,"2019/10/01, 12:23:56",35.0,9795454.0,0,"entrypoint: crond -f
  This one is really the key to not have the exception when I start docker-compose."
Zipkin,58181776,58170335,0,"2019/10/01, 12:23:56",False,"2019/10/01, 12:23:56",35.0,9795454.0,0,"To solve this issue, I check the this project:  https://github.com/openzipkin/docker-zipkin"
Zipkin,58181776,58170335,0,"2019/10/01, 12:23:56",False,"2019/10/01, 12:23:56",35.0,9795454.0,0,The remaining question is: why do I need to use entrypoint: crond -f
Zipkin,57287189,57284146,0,"2019/07/31, 12:12:05",False,"2019/07/31, 12:12:05",11.0,11602721.0,1,"I found examples from:
 https://github.com/openzipkin/zipkin/tree/master/zipkin-lens/testdata"
Zipkin,57287189,57284146,0,"2019/07/31, 12:12:05",False,"2019/07/31, 12:12:05",11.0,11602721.0,1,It works well.
Zipkin,57232063,57219354,0,"2019/07/27, 15:16:24",False,"2019/07/27, 15:16:24",1.0,11841253.0,0,"Set the datasource setting in the application.yml file of the application as follows,"
Zipkin,57232063,57219354,0,"2019/07/27, 15:16:24",False,"2019/07/27, 15:16:24",1.0,11841253.0,0,You can add the zipkin attribute to POM.xml
Zipkin,57232063,57219354,0,"2019/07/27, 15:16:24",False,"2019/07/27, 15:16:24",1.0,11841253.0,0,Problems can occur due to Spring's auto configuration property.
Zipkin,57232063,57219354,0,"2019/07/27, 15:16:24",False,"2019/07/27, 15:16:24",1.0,11841253.0,0,"Therefore, modify the datasource setting as follows and modify the datasource configuration related source to make the application work normally."
Zipkin,56070984,56070700,5,"2019/05/10, 08:05:46",True,"2019/05/10, 08:05:46",12864.0,1413240.0,0,It looks like the trace is triggering an old-fashioned inverted-lock-order deadlock freezing threads that are attempting to acquire new Connections.
Zipkin,56070984,56070700,5,"2019/05/10, 08:05:46",True,"2019/05/10, 08:05:46",12864.0,1413240.0,0,The last of the three deadlocked threads is  trying to get a lock on some singleton or bean .
Zipkin,56070984,56070700,5,"2019/05/10, 08:05:46",True,"2019/05/10, 08:05:46",12864.0,1413240.0,0,It has already passed through and presumably acquired a lock on a  GenericScope .
Zipkin,56070984,56070700,5,"2019/05/10, 08:05:46",True,"2019/05/10, 08:05:46",12864.0,1413240.0,0,"The other two threads are  trying to acquire a lock on a  GenericScope , which presumably the first thread has."
Zipkin,56070984,56070700,5,"2019/05/10, 08:05:46",True,"2019/05/10, 08:05:46",12864.0,1413240.0,0,An unexpected reentrance from the  zipkin  code into spring is generating a deadlock.
Zipkin,56070984,56070700,5,"2019/05/10, 08:05:46",True,"2019/05/10, 08:05:46",12864.0,1413240.0,0,"c3p0  has a fixed-size thread pool that notices when all its threads (just 3 here,  c3p0 's default) are persistently frozen, then (pretty correctly in this case) declares a deadlock and replaces the blocked threads in hopes of recovering."
Zipkin,56070984,56070700,5,"2019/05/10, 08:05:46",True,"2019/05/10, 08:05:46",12864.0,1413240.0,0,Does c3p0 recover?
Zipkin,56070984,56070700,5,"2019/05/10, 08:05:46",True,"2019/05/10, 08:05:46",12864.0,1413240.0,0,Is this a rare or frequent deadlock?
Zipkin,56070984,56070700,5,"2019/05/10, 08:05:46",True,"2019/05/10, 08:05:46",12864.0,1413240.0,0,"There's not much you can easily do to prevent this deadlock, I think either you'll have to tolerate it or do without the instrumentation."
Zipkin,55801244,55512192,0,"2019/04/23, 00:16:47",False,"2019/04/23, 00:16:47",630.0,10676678.0,2,Application Insights users would also be able to leverage the distributed tracing offered through Zipkin by instrumenting their services using existing libraries.
Zipkin,55801244,55512192,0,"2019/04/23, 00:16:47",False,"2019/04/23, 00:16:47",630.0,10676678.0,2,"To use the Application Insights back-end store, configure your Zipkin server instance to use the Application Insights  plug-in ."
Zipkin,55801244,55512192,0,"2019/04/23, 00:16:47",False,"2019/04/23, 00:16:47",630.0,10676678.0,2,This integration makes monitoring and debugging your overall end-to-end applications much easier.
Zipkin,55801244,55512192,0,"2019/04/23, 00:16:47",False,"2019/04/23, 00:16:47",630.0,10676678.0,2,"Once you have the data in Application Insights, you can always perform  cross-resource log queries  between Application Insights and Log Analytics."
Zipkin,55801244,55512192,0,"2019/04/23, 00:16:47",False,"2019/04/23, 00:16:47",630.0,10676678.0,2,Additional Documentation Reference -
Zipkin,55801244,55512192,0,"2019/04/23, 00:16:47",False,"2019/04/23, 00:16:47",630.0,10676678.0,2,Zipkin to Application Insights Module
Zipkin,55801244,55512192,0,"2019/04/23, 00:16:47",False,"2019/04/23, 00:16:47",630.0,10676678.0,2,Zipkin-Azure
Zipkin,55801244,55512192,0,"2019/04/23, 00:16:47",False,"2019/04/23, 00:16:47",630.0,10676678.0,2,Send Log Data to Azure Monitor with HTTP Data Collector API (public preview)
Zipkin,55801244,55512192,0,"2019/04/23, 00:16:47",False,"2019/04/23, 00:16:47",630.0,10676678.0,2,Hope the above information helps.
Zipkin,55535480,55364711,0,"2019/04/05, 15:32:51",True,"2019/04/05, 15:32:51",8336.0,1773866.0,1,No you can't.
Zipkin,55535480,55364711,0,"2019/04/05, 15:32:51",True,"2019/04/05, 15:32:51",8336.0,1773866.0,1,You can use tools like Elasticsearch Logstash Kibana to visualize it.
Zipkin,55535480,55364711,0,"2019/04/05, 15:32:51",True,"2019/04/05, 15:32:51",8336.0,1773866.0,1,"You can go to my repo  https://github.com/marcingrzejszczak/docker-elk  and run  ./   getReadyForConference.sh , it will start docker containers with the ELK stack, run the apps, curl the request to the apps so that you can then check them in ELK."
Zipkin,54758616,54758615,0,"2019/02/19, 05:51:06",True,"2019/02/19, 05:51:06",3909.0,3067542.0,1,"Alright, so after a few hours struggling, I made some progress, and now the app starts - even though the root cause of the issue is not fully clear to me at this time."
Zipkin,54758616,54758615,0,"2019/02/19, 05:51:06",True,"2019/02/19, 05:51:06",3909.0,3067542.0,1,Below are my findings :
Zipkin,54758616,54758615,0,"2019/02/19, 05:51:06",True,"2019/02/19, 05:51:06",3909.0,3067542.0,1,"one strange thing I noticed : if I change the  sender.type  from  web  to  rabbit , then the application starts with no error."
Zipkin,54758616,54758615,0,"2019/02/19, 05:51:06",True,"2019/02/19, 05:51:06",3909.0,3067542.0,1,"I also found this Spring Boot  issue report , very similar to mine, that was pointing at a JDK bug."
Zipkin,54758616,54758615,0,"2019/02/19, 05:51:06",True,"2019/02/19, 05:51:06",3909.0,3067542.0,1,"And indeed, upgrading from  jdk1.8.0_25  to  jdk1.8.0_201  ."
Zipkin,54758616,54758615,0,"2019/02/19, 05:51:06",True,"2019/02/19, 05:51:06",3909.0,3067542.0,1,"Finally, I also found that if I was using  jdk1.8.0_25  and wasn't providing the  sender.type  at all, then the app was also starting with no issue."
Zipkin,54758616,54758615,0,"2019/02/19, 05:51:06",True,"2019/02/19, 05:51:06",3909.0,3067542.0,1,"For some reason, in the other app that I have and that works, I am able to use  jdk1.8.0_25  and  sender.type: web"
Zipkin,54758616,54758615,0,"2019/02/19, 05:51:06",True,"2019/02/19, 05:51:06",3909.0,3067542.0,1,"If anyone has a methodology to figure out this kind of issue quickly, don't hesitate to add it in the comment or edit this answer."
Zipkin,54766304,54645785,3,"2019/02/19, 14:25:08",True,"2019/02/19, 14:25:08",8336.0,1773866.0,0,It makes perfect sense that it's  null .
Zipkin,54766304,54645785,3,"2019/02/19, 14:25:08",True,"2019/02/19, 14:25:08",8336.0,1773866.0,0,That's because YOU control the way what happens with the caught exception.
Zipkin,54766304,54645785,3,"2019/02/19, 14:25:08",True,"2019/02/19, 14:25:08",8336.0,1773866.0,0,"In your case, nothing, cause you swallow that exception."
Zipkin,54766304,54645785,3,"2019/02/19, 14:25:08",True,"2019/02/19, 14:25:08",8336.0,1773866.0,0,"If you want to do sth better, just add the error tag manually via the  SpanCustomizer ."
Zipkin,54766304,54645785,3,"2019/02/19, 14:25:08",True,"2019/02/19, 14:25:08",8336.0,1773866.0,0,That way you'll add the exception to the given span.
Zipkin,54766304,54645785,3,"2019/02/19, 14:25:08",True,"2019/02/19, 14:25:08",8336.0,1773866.0,0,It will then automatically get closed and reported to Zipkin (you can do sth else than  ex.toString()  of course.
Zipkin,54555743,54555618,0,"2019/02/06, 16:19:12",False,"2019/02/06, 23:57:39",2802.0,2810730.0,1,Zipkin  is a solution for distributed tracing.
Zipkin,54555743,54555618,0,"2019/02/06, 16:19:12",False,"2019/02/06, 23:57:39",2802.0,2810730.0,1,Specifically it allows to track latency problems in distributed system.
Zipkin,54555743,54555618,0,"2019/02/06, 16:19:12",False,"2019/02/06, 23:57:39",2802.0,2810730.0,1,Also it's a greate tool for debugging/investigating problems in your application.
Zipkin,54555743,54555618,0,"2019/02/06, 16:19:12",False,"2019/02/06, 23:57:39",2802.0,2810730.0,1,So by definition it requires to collect successful and failed traces.
Zipkin,54555743,54555618,0,"2019/02/06, 16:19:12",False,"2019/02/06, 23:57:39",2802.0,2810730.0,1,However  traces  have nothing to do with logging.
Zipkin,54555743,54555618,0,"2019/02/06, 16:19:12",False,"2019/02/06, 23:57:39",2802.0,2810730.0,1,"Assuming you mean controlling the logging level of Zipkin server, then you can just set it using  --logging.level.zipkin2=INFO ."
Zipkin,54555923,54555618,0,"2019/02/06, 16:29:05",False,"2019/02/06, 16:29:05",8336.0,1773866.0,0,I don't understand the problem.
Zipkin,54555923,54555618,0,"2019/02/06, 16:29:05",False,"2019/02/06, 16:29:05",8336.0,1773866.0,0,You don't send logs to Zipkin.
Zipkin,54555923,54555618,0,"2019/02/06, 16:29:05",False,"2019/02/06, 16:29:05",8336.0,1773866.0,0,You send spans to Zipkin.
Zipkin,54555923,54555618,0,"2019/02/06, 16:29:05",False,"2019/02/06, 16:29:05",8336.0,1773866.0,0,Zipkin has nothing to do with logs.
Zipkin,54468173,54466528,0,"2019/01/31, 21:45:59",True,"2019/01/31, 21:45:59",2296.0,1575416.0,2,Seems to work once I added the Web package.
Zipkin,54468173,54466528,0,"2019/01/31, 21:45:59",True,"2019/01/31, 21:45:59",2296.0,1575416.0,2,Though I don't recall it being needed previously.
Zipkin,53468275,53428788,0,"2018/11/25, 16:03:42",False,"2018/11/25, 16:03:42",1767.0,2498986.0,0,"Zipkin currently supports four types of backend storage to store spans in-memory, MySQl, ElasticSearch, Cassandra."
Zipkin,53468275,53428788,0,"2018/11/25, 16:03:42",False,"2018/11/25, 16:03:42",1767.0,2498986.0,0,Although for production it is recommended to use ES or Cassandra.
Zipkin,53468275,53428788,0,"2018/11/25, 16:03:42",False,"2018/11/25, 16:03:42",1767.0,2498986.0,0,The other two can be used for learning and understanding.
Zipkin,53468275,53428788,0,"2018/11/25, 16:03:42",False,"2018/11/25, 16:03:42",1767.0,2498986.0,0,Traces stored in the in-memory is ephemeral and won't be available after the restart.
Zipkin,53468275,53428788,0,"2018/11/25, 16:03:42",False,"2018/11/25, 16:03:42",1767.0,2498986.0,0,"In the zipkin UI there is an option to see the trace and download it, which can be used to view at a later point in time."
Zipkin,53468275,53428788,0,"2018/11/25, 16:03:42",False,"2018/11/25, 16:03:42",1767.0,2498986.0,0,If you still have further questions drop in to the zipkin  gitter  channel.
Zipkin,53221841,53218692,0,"2018/11/09, 10:01:09",False,"2018/11/09, 10:01:09",136.0,1433218.0,0,we also use use zipkin but can't query with zipkin as elk.
Zipkin,53221841,53218692,0,"2018/11/09, 10:01:09",False,"2018/11/09, 10:01:09",136.0,1433218.0,0,we can just click on each services which are display on zipkin and get more info as below image.
Zipkin,53236126,53218692,1,"2018/11/10, 07:02:05",False,"2018/11/10, 07:02:05",1767.0,2498986.0,0,Zipkin is not a business transaction tracking system and it should not be used that way because it is not built for this purpose.
Zipkin,53236126,53218692,1,"2018/11/10, 07:02:05",False,"2018/11/10, 07:02:05",1767.0,2498986.0,0,There are other tools which are built specifically to cater the needs to business operations which you must consider.
Zipkin,53236126,53218692,1,"2018/11/10, 07:02:05",False,"2018/11/10, 07:02:05",1767.0,2498986.0,0,P.S.
Zipkin,53236126,53218692,1,"2018/11/10, 07:02:05",False,"2018/11/10, 07:02:05",1767.0,2498986.0,0,I am a Zipkin contributor.
Zipkin,53932587,53218692,0,"2018/12/26, 15:12:50",False,"2018/12/26, 15:12:50",3071.0,2979435.0,0,This is not an answer to how achieve this with zipkin but yes for the whole problem.
Zipkin,53932587,53218692,0,"2018/12/26, 15:12:50",False,"2018/12/26, 15:12:50",3071.0,2979435.0,0,If you have a  transaction that didn't complete it's steps then you probably have two of following issues
Zipkin,53932587,53218692,0,"2018/12/26, 15:12:50",False,"2018/12/26, 15:12:50",3071.0,2979435.0,0,Some microservice failed to deliver the event to the next one and didn't figure it out
Zipkin,53932587,53218692,0,"2018/12/26, 15:12:50",False,"2018/12/26, 15:12:50",3071.0,2979435.0,0,"You have to make sure delivery at least once here, using Kafka you have to wait until message get flushed to the server for example"
Zipkin,53932587,53218692,0,"2018/12/26, 15:12:50",False,"2018/12/26, 15:12:50",3071.0,2979435.0,0,The destiny microservice received the message and is not processing it
Zipkin,53932587,53218692,0,"2018/12/26, 15:12:50",False,"2018/12/26, 15:12:50",3071.0,2979435.0,0,"You have to make sure you application is processing what it's supposed to,  you can monitor the database if the transactions are there or use some tool like LinkedIn burrow to monitor your Kafka message group if you are integrating by using Kafka."
Zipkin,53932587,53218692,0,"2018/12/26, 15:12:50",False,"2018/12/26, 15:12:50",3071.0,2979435.0,0,"Conclusion is, instead to try monitor all the thing once it looks like creating specialist monitors for every step will be more assertive and simple to develop."
Zipkin,52975826,52972425,3,"2018/10/24, 21:33:48",False,"2018/10/24, 21:33:48",8295.0,4550110.0,0,Here is the related issue:
Zipkin,52975826,52972425,3,"2018/10/24, 21:33:48",False,"2018/10/24, 21:33:48",8295.0,4550110.0,0,https://github.com/openzipkin/zipkin/issues/1939
Zipkin,52987258,52972425,0,"2018/10/25, 13:38:05",False,"2018/10/25, 13:38:05",1.0,10552245.0,0,"I opened a issue on the zipkin github, a theme already being treated as a bug."
Zipkin,52987258,52972425,0,"2018/10/25, 13:38:05",False,"2018/10/25, 13:38:05",1.0,10552245.0,0,"Initial thread:
 https://github.com/openzipkin/zipkin/issues/2218#issuecomment-432876510"
Zipkin,52987258,52972425,0,"2018/10/25, 13:38:05",False,"2018/10/25, 13:38:05",1.0,10552245.0,0,"Bug track:
 https://github.com/openzipkin/zipkin/issues/2219"
Zipkin,52987258,52972425,0,"2018/10/25, 13:38:05",False,"2018/10/25, 13:38:05",1.0,10552245.0,0,Tks for all!
Zipkin,52815354,52706088,1,"2018/10/15, 14:07:16",False,"2018/10/15, 14:07:16",201.0,7956609.0,2,You have to use  spring.sleuth.web.skipPattern
Zipkin,52815354,52706088,1,"2018/10/15, 14:07:16",False,"2018/10/15, 14:07:16",201.0,7956609.0,2,sample you will get here  https://www.baeldung.com/tracing-services-with-zipkin
Zipkin,52739617,52424864,0,"2018/10/10, 14:51:01",False,"2018/10/10, 14:51:01",201.0,7956609.0,0,I think to remove the service names from zipkin you have to Re-deploy the zipkin service
Zipkin,52776732,52424864,0,"2018/10/12, 12:41:59",True,"2018/10/12, 12:41:59",131.0,6236211.0,0,You will need to remove the spring.zipkin.base-url property from the corresponding applications to remove it from zipkin server list.
Zipkin,52608036,51661009,0,"2018/10/02, 15:08:15",True,"2018/10/02, 15:08:15",153.0,2999097.0,0,"finally got working after spring verison updated to  5.x 
It already have  Brave Instrument for zipkin trace"
Zipkin,51580557,51578263,2,"2018/07/29, 15:58:11",True,"2018/07/29, 15:58:11",8336.0,1773866.0,2,If you read the docs or any information starting from edgware you would see that we've removed that support.
Zipkin,51580557,51578263,2,"2018/07/29, 15:58:11",True,"2018/07/29, 15:58:11",8336.0,1773866.0,2,You should use native zipkin rabbit / kafka dependencies.
Zipkin,51580557,51578263,2,"2018/07/29, 15:58:11",True,"2018/07/29, 15:58:11",8336.0,1773866.0,2,Everything is there in the docs.
Zipkin,51438611,51438273,1,"2018/07/20, 11:46:33",False,"2018/07/20, 11:46:33",8336.0,1773866.0,1,If it comes from the  @Scheduled  method then you can use  https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/instrument/scheduling/SleuthSchedulingProperties.java#L38  ( spring.sleuth.scheduled.skipPattern ) to find the thread and disable it.
Zipkin,51438611,51438273,1,"2018/07/20, 11:46:33",False,"2018/07/20, 11:46:33",8336.0,1773866.0,1,If you say its name is  async  then it means that it comes from a  TraceRunnable  or  TraceCallable .
Zipkin,51438611,51438273,1,"2018/07/20, 11:46:33",False,"2018/07/20, 11:46:33",8336.0,1773866.0,1,That can be problematic to get rid off.
Zipkin,51438611,51438273,1,"2018/07/20, 11:46:33",False,"2018/07/20, 11:46:33",8336.0,1773866.0,1,You can file an issue in Sleuth to allow  SpanAdjuster  to actually not send spans to Zipkin (by for example returning  null ).
Zipkin,51438611,51438273,1,"2018/07/20, 11:46:33",False,"2018/07/20, 11:46:33",8336.0,1773866.0,1,You can also try to disable async at all  spring.sleuth.async.enabled .
Zipkin,51438611,51438273,1,"2018/07/20, 11:46:33",False,"2018/07/20, 11:46:33",8336.0,1773866.0,1,If you're not using any other features of async that should not interfere.
Zipkin,51354519,51244941,1,"2018/07/16, 07:17:28",False,"2018/07/16, 07:17:28",1767.0,2498986.0,0,Brave will work regardless of the server that you choose to use.
Zipkin,51354519,51244941,1,"2018/07/16, 07:17:28",False,"2018/07/16, 07:17:28",1767.0,2498986.0,0,Remove the jetty configuration from the pom file and use the Tomcat.
Zipkin,51354519,51244941,1,"2018/07/16, 07:17:28",False,"2018/07/16, 07:17:28",1767.0,2498986.0,0,If you still have trouble or want to know more about zipkin/brave connect to the community via the gitter channel.
Zipkin,51354519,51244941,1,"2018/07/16, 07:17:28",False,"2018/07/16, 07:17:28",1767.0,2498986.0,0,P.S.
Zipkin,51354519,51244941,1,"2018/07/16, 07:17:28",False,"2018/07/16, 07:17:28",1767.0,2498986.0,0,I contribute to OpenZipkin (Zipkin)
Zipkin,51141657,51068201,0,"2018/07/02, 21:12:12",False,"2018/07/02, 21:12:12",81.0,2232476.0,0,"The UI cannot be password protected without also password protecting the API endpoints, including the one you would send your spans to."
Zipkin,51141657,51068201,0,"2018/07/02, 21:12:12",False,"2018/07/02, 21:12:12",81.0,2232476.0,0,PS: The  @EnableZipkinServer  annotation has been deprecated
Zipkin,50813959,50813714,8,"2018/06/12, 12:39:34",True,"2018/06/26, 16:13:12",8336.0,1773866.0,1,EDGWARE
Zipkin,50813959,50813714,8,"2018/06/12, 12:39:34",True,"2018/06/26, 16:13:12",8336.0,1773866.0,1,Have you read the documentation?
Zipkin,50813959,50813714,8,"2018/06/12, 12:39:34",True,"2018/06/26, 16:13:12",8336.0,1773866.0,1,If you use Spring Cloud Sleuth in Edgware version if you read the Sleuth section you would find this piece of the documentation  https://cloud.spring.io/spring-cloud-static/Edgware.SR3/single/spring-cloud.html#_custom_sa_tag_in_zipkin
Zipkin,50813959,50813714,8,"2018/06/12, 12:39:34",True,"2018/06/26, 16:13:12",8336.0,1773866.0,1,Let me copy that for you
Zipkin,50813959,50813714,8,"2018/06/12, 12:39:34",True,"2018/06/26, 16:13:12",8336.0,1773866.0,1,"54.5 Custom SA tag in Zipkin Sometimes you want to create a manual Span that will wrap a call to an external service which is not
  instrumented."
Zipkin,50813959,50813714,8,"2018/06/12, 12:39:34",True,"2018/06/26, 16:13:12",8336.0,1773866.0,1,"What you can do is to create a span with the
  peer.service tag that will contain a value of the service that you
  want to call."
Zipkin,50813959,50813714,8,"2018/06/12, 12:39:34",True,"2018/06/26, 16:13:12",8336.0,1773866.0,1,"Below you can see an example of a call to Redis that is
  wrapped in such a span."
Zipkin,50813959,50813714,8,"2018/06/12, 12:39:34",True,"2018/06/26, 16:13:12",8336.0,1773866.0,1,"[Important]   Important Remember not to add both peer.service tag and
  the SA tag!"
Zipkin,50813959,50813714,8,"2018/06/12, 12:39:34",True,"2018/06/26, 16:13:12",8336.0,1773866.0,1,You have to add only peer.service.
Zipkin,50813959,50813714,8,"2018/06/12, 12:39:34",True,"2018/06/26, 16:13:12",8336.0,1773866.0,1,FINCHLEY
Zipkin,50813959,50813714,8,"2018/06/12, 12:39:34",True,"2018/06/26, 16:13:12",8336.0,1773866.0,1,The  SA  tag will not work for Finchley.
Zipkin,50813959,50813714,8,"2018/06/12, 12:39:34",True,"2018/06/26, 16:13:12",8336.0,1773866.0,1,You have to do it in the following manner using the  remoteEndpoint  on the span.
Zipkin,50798776,50796087,4,"2018/06/11, 16:23:06",True,"2018/06/11, 16:39:48",8336.0,1773866.0,1,That was a bug in Spring Cloud Sleuth in Edgware.
Zipkin,50798776,50796087,4,"2018/06/11, 16:23:06",True,"2018/06/11, 16:39:48",8336.0,1773866.0,1,The Stream Kafka Binder in Edgware required explicit passing of headers that should get propagated.
Zipkin,50798776,50796087,4,"2018/06/11, 16:23:06",True,"2018/06/11, 16:39:48",8336.0,1773866.0,1,The side effect of adding  sleuth-stream  on the classpath was exactly that feature.
Zipkin,50798776,50796087,4,"2018/06/11, 16:23:06",True,"2018/06/11, 16:39:48",8336.0,1773866.0,1,By fixing the  https://github.com/spring-cloud/spring-cloud-sleuth/issues/1005  issue we're adding back the missing feature to core.
Zipkin,50798776,50796087,4,"2018/06/11, 16:23:06",True,"2018/06/11, 16:39:48",8336.0,1773866.0,1,This is not ported to Finchley since Stream Kafka Binder in Finchley passes all headers by default.
Zipkin,50798776,50796087,4,"2018/06/11, 16:23:06",True,"2018/06/11, 16:39:48",8336.0,1773866.0,1,The workaround for Edgware is to pass a list of headers in the following manner:
Zipkin,50626312,50599475,0,"2018/05/31, 17:09:32",True,"2018/05/31, 17:09:32",3141.0,553720.0,0,The Istio sidecar proxy (Envoy) generates the first headers.
Zipkin,50626312,50599475,0,"2018/05/31, 17:09:32",True,"2018/05/31, 17:09:32",3141.0,553720.0,0,According to  https://www.envoyproxy.io/docs/envoy/latest/configuration/http_conn_man/headers#x-request-id :
Zipkin,50626312,50599475,0,"2018/05/31, 17:09:32",True,"2018/05/31, 17:09:32",3141.0,553720.0,0,Envoy will generate an x-request-id header for all external origin requests (the header is sanitized).
Zipkin,50626312,50599475,0,"2018/05/31, 17:09:32",True,"2018/05/31, 17:09:32",3141.0,553720.0,0,It will also generate an x-request-id header for internal requests that do not already have one.
Zipkin,50010347,50010083,9,"2018/04/24, 23:34:00",True,"2018/04/24, 23:34:00",8336.0,1773866.0,3,You've mixed almost everything you could have mixed.
Zipkin,50010347,50010083,9,"2018/04/24, 23:34:00",True,"2018/04/24, 23:34:00",8336.0,1773866.0,3,On the app side you're using both the deprecated zipkin server and the deprecated client.
Zipkin,50010347,50010083,9,"2018/04/24, 23:34:00",True,"2018/04/24, 23:34:00",8336.0,1773866.0,3,On the server side you're using deprecated zipkin server.
Zipkin,50010347,50010083,9,"2018/04/24, 23:34:00",True,"2018/04/24, 23:34:00",8336.0,1773866.0,3,My suggestion is that you go through the documentation  https://cloud.spring.io/spring-cloud-static/Edgware.SR3/single/spring-cloud.html#_spring_cloud_sleuth  and read that the  stream servers  are deprecated and you should use the openzipkin zipkin server with rabbitmq support ( https://github.com/openzipkin/zipkin/tree/master/zipkin-collector/rabbitmq ).
Zipkin,50010347,50010083,9,"2018/04/24, 23:34:00",True,"2018/04/24, 23:34:00",8336.0,1773866.0,3,On the consumer side use  https://cloud.spring.io/spring-cloud-static/Edgware.SR3/single/spring-cloud.html#_sleuth_with_zipkin_via_rabbitmq_or_kafka  .
Zipkin,50010347,50010083,9,"2018/04/24, 23:34:00",True,"2018/04/24, 23:34:00",8336.0,1773866.0,3,It really is as simple as that.
Zipkin,50010347,50010083,9,"2018/04/24, 23:34:00",True,"2018/04/24, 23:34:00",8336.0,1773866.0,3,Also don't forget to turn on the sampling percentage to 1.0
Zipkin,53221956,49280873,0,"2018/11/09, 10:11:00",False,"2018/11/09, 10:11:00",136.0,1433218.0,0,"Just add below, it need to be working,"
Zipkin,48982608,48518928,0,"2018/02/26, 08:26:46",False,"2018/02/26, 08:26:46",181.0,6708214.0,0,"Yeah,you should use different libraries for different languages."
Zipkin,48982608,48518928,0,"2018/02/26, 08:26:46",False,"2018/02/26, 08:26:46",181.0,6708214.0,0,"Brave for Java,Zipkin4net for C# and so on."
Zipkin,48982608,48518928,0,"2018/02/26, 08:26:46",False,"2018/02/26, 08:26:46",181.0,6708214.0,0,"For more details,you can visit Zipkin official site:  Zipkin Existing instrumentations ."
Zipkin,48982608,48518928,0,"2018/02/26, 08:26:46",False,"2018/02/26, 08:26:46",181.0,6708214.0,0,Then all you shoud do is following the librarie guide.
Zipkin,48982608,48518928,0,"2018/02/26, 08:26:46",False,"2018/02/26, 08:26:46",181.0,6708214.0,0,Have fun!
Zipkin,48346769,48160588,0,"2018/01/19, 19:46:45",False,"2018/01/19, 19:46:45",1207.0,7376337.0,0,The first request uses v1 of the Zipkin api while the second uses v2 (see  https://github.com/openzipkin/zipkin/issues/1499  for the v2 specification).
Zipkin,48346769,48160588,0,"2018/01/19, 19:46:45",False,"2018/01/19, 19:46:45",1207.0,7376337.0,0,"Spans are broken up by kind (SERVER and CLIENT) instead of having client receive, server receive, client send, and server send annotations (hence why there are more spans)."
Zipkin,47664992,47660900,0,"2017/12/06, 02:38:32",True,"2017/12/06, 02:38:32",8336.0,1773866.0,1,I have a client application with multiple channels as SOURCE/SINK.
Zipkin,47664992,47660900,0,"2017/12/06, 02:38:32",True,"2017/12/06, 02:38:32",8336.0,1773866.0,1,I want to send logs to Zipkin server.
Zipkin,47664992,47660900,0,"2017/12/06, 02:38:32",True,"2017/12/06, 02:38:32",8336.0,1773866.0,1,Zipkin is not a tool to store logs
Zipkin,47664992,47660900,0,"2017/12/06, 02:38:32",True,"2017/12/06, 02:38:32",8336.0,1773866.0,1,"According to my understanding, if spring finds spring cloud stream in classpath, Zipkin client defaults to messaging instead of sending logs through HTTP."
Zipkin,47664992,47660900,0,"2017/12/06, 02:38:32",True,"2017/12/06, 02:38:32",8336.0,1773866.0,1,No - you need the  sleuth-stream  dependency on the client side and the  zipkin-stream  dependency on the server side (which got deprecated and you should start using the inbuilt rabbitmq support from Zipkin).
Zipkin,47664992,47660900,0,"2017/12/06, 02:38:32",True,"2017/12/06, 02:38:32",8336.0,1773866.0,1,At client side: Q1.
Zipkin,47664992,47660900,0,"2017/12/06, 02:38:32",True,"2017/12/06, 02:38:32",8336.0,1773866.0,1,Is there an automatic configuration for zipkin rabbit binding in such scenario?
Zipkin,47664992,47660900,0,"2017/12/06, 02:38:32",True,"2017/12/06, 02:38:32",8336.0,1773866.0,1,"If not, what is default channel name of zipkin SOURCE channel?"
Zipkin,47664992,47660900,0,"2017/12/06, 02:38:32",True,"2017/12/06, 02:38:32",8336.0,1773866.0,1,"Yes, there is."
Zipkin,47664992,47660900,0,"2017/12/06, 02:38:32",True,"2017/12/06, 02:38:32",8336.0,1773866.0,1,The channel is  sleuth
Zipkin,47664992,47660900,0,"2017/12/06, 02:38:32",True,"2017/12/06, 02:38:32",8336.0,1773866.0,1,Q2.
Zipkin,47664992,47660900,0,"2017/12/06, 02:38:32",True,"2017/12/06, 02:38:32",8336.0,1773866.0,1,Do I need to configure defaultSampler to AlwaysSampler()?
Zipkin,47664992,47660900,0,"2017/12/06, 02:38:32",True,"2017/12/06, 02:38:32",8336.0,1773866.0,1,"No, you have the  PercentageBasedSampler  (I'm pretty sure it's written in the docs)."
Zipkin,47664992,47660900,0,"2017/12/06, 02:38:32",True,"2017/12/06, 02:38:32",8336.0,1773866.0,1,You can tweak its values.
Zipkin,47664992,47660900,0,"2017/12/06, 02:38:32",True,"2017/12/06, 02:38:32",8336.0,1773866.0,1,At Server side: Q1.
Zipkin,47664992,47660900,0,"2017/12/06, 02:38:32",True,"2017/12/06, 02:38:32",8336.0,1773866.0,1,Do I need to create Zipkin server as a spring boot application for my use case or can I use the jar retrieved using: wget -O zipkin.jar ' https://search.maven.org/remote_content?g=io.zipkin.java&amp;a=zipkin-server&amp;v=LATEST&amp;c=exec ' ...as stated on  https://zipkin.io/pages/quickstart.html  ?
Zipkin,47664992,47660900,0,"2017/12/06, 02:38:32",True,"2017/12/06, 02:38:32",8336.0,1773866.0,1,You should do the wget.
Zipkin,47664992,47660900,0,"2017/12/06, 02:38:32",True,"2017/12/06, 02:38:32",8336.0,1773866.0,1,If you want to use the legacy stream support then you should create a zipkin server yourself.
Zipkin,47664992,47660900,0,"2017/12/06, 02:38:32",True,"2017/12/06, 02:38:32",8336.0,1773866.0,1,Q2.
Zipkin,47664992,47660900,0,"2017/12/06, 02:38:32",True,"2017/12/06, 02:38:32",8336.0,1773866.0,1,How do I configure zipkin SINK channel to destination?
Zipkin,47664992,47660900,0,"2017/12/06, 02:38:32",True,"2017/12/06, 02:38:32",8336.0,1773866.0,1,If you're using the legacy zipkin stream app then it's automatically configured to point to proper destination.
Zipkin,47664992,47660900,0,"2017/12/06, 02:38:32",True,"2017/12/06, 02:38:32",8336.0,1773866.0,1,You can tweak the destination as you please in the standard way Spring Cloud Stream supports it.
Zipkin,47665630,47655299,2,"2017/12/06, 03:58:02",False,"2017/12/06, 03:58:02",693.0,2901325.0,0,I'm not sure I fully understand your question but I can elaborate a bit on how istio works with respect to tracing:
Zipkin,47665630,47655299,2,"2017/12/06, 03:58:02",False,"2017/12/06, 03:58:02",693.0,2901325.0,0,"Tracing means identifying every span or node that is part of the original request, so typically an Id is generated by the istio-ingress and your application should  propagate it  so each istio-proxy can capture and forward that information to istio-mixer which then lets you use Zipkin or Jaeger to visualize it."
Zipkin,47665630,47655299,2,"2017/12/06, 03:58:02",False,"2017/12/06, 03:58:02",693.0,2901325.0,0,Istio can't know when you make outcalls from your application for which original request it was for unless you do copy the headers.
Zipkin,47665630,47655299,2,"2017/12/06, 03:58:02",False,"2017/12/06, 03:58:02",693.0,2901325.0,0,Does that help/makes sense ?
Zipkin,47592674,47591972,4,"2017/12/01, 13:44:22",False,"2017/12/01, 13:44:22",1176.0,2269535.0,0,"It looks a incompatibility between version in my opinion, something is overridden when you inject the spring-cloud-starter-zipkin dependency"
Zipkin,47592674,47591972,4,"2017/12/01, 13:44:22",False,"2017/12/01, 13:44:22",1176.0,2269535.0,0,What i don't understand from your question is:
Zipkin,47592674,47591972,4,"2017/12/01, 13:44:22",False,"2017/12/01, 13:44:22",1176.0,2269535.0,0,"Do you need this dependency ""spring-cloud-starter-zipkin"", are you using it?"
Zipkin,47592674,47591972,4,"2017/12/01, 13:44:22",False,"2017/12/01, 13:44:22",1176.0,2269535.0,0,"If no obviously just put it out of the pom, if yes, check which version are you using:"
Zipkin,47592674,47591972,4,"2017/12/01, 13:44:22",False,"2017/12/01, 13:44:22",1176.0,2269535.0,0,mvn dependency:tree and try to align spring-cloud-starter-zipkin with the Spring version you are using.
Zipkin,47592674,47591972,4,"2017/12/01, 13:44:22",False,"2017/12/01, 13:44:22",1176.0,2269535.0,0,Playing a bit with the version of your artifacts you will find the solution.
Zipkin,47592674,47591972,4,"2017/12/01, 13:44:22",False,"2017/12/01, 13:44:22",1176.0,2269535.0,0,Hope it helped.
Zipkin,52585722,47026664,0,"2018/10/01, 09:37:07",False,"2020/09/02, 11:35:57",61.0,3548002.0,0,I use &quot;TraceCallable&quot; class from &quot; spring-cloud-sleuth &quot; lib to solve it in my code.
Zipkin,52585722,47026664,0,"2018/10/01, 09:37:07",False,"2020/09/02, 11:35:57",61.0,3548002.0,0,My code example is:
Zipkin,64439116,47026664,0,"2020/10/20, 08:41:48",False,"2020/10/22, 13:48:08",145.0,10936956.0,1,"Attention: This solution does works for logging purposes, but does not work for other Sleuth features like instrumenting RestTemplates to send the tracing headers to other services."
Zipkin,64439116,47026664,0,"2020/10/20, 08:41:48",False,"2020/10/22, 13:48:08",145.0,10936956.0,1,"So unfortunately, this is not a fully working solution."
Zipkin,64439116,47026664,0,"2020/10/20, 08:41:48",False,"2020/10/22, 13:48:08",145.0,10936956.0,1,:(
Zipkin,64439116,47026664,0,"2020/10/20, 08:41:48",False,"2020/10/22, 13:48:08",145.0,10936956.0,1,"Some time after adopting @Baca's solution, I discovered that Kotlin Coroutines offer direct integration with slf4j, which is what Spring Sleuth builds on."
Zipkin,64439116,47026664,0,"2020/10/20, 08:41:48",False,"2020/10/22, 13:48:08",145.0,10936956.0,1,"Sleuth adds properties  X-B3-TraceId ,  traceId ,  X-B3-SpanId , and  spanId  to the thread's MDC."
Zipkin,64439116,47026664,0,"2020/10/20, 08:41:48",False,"2020/10/22, 13:48:08",145.0,10936956.0,1,You can retain the parent thread's MDC for a coroutine with the code shown below.
Zipkin,64439116,47026664,0,"2020/10/20, 08:41:48",False,"2020/10/22, 13:48:08",145.0,10936956.0,1,The coroutine framework will take care of restoring the MDC context on the worker thread whenever the coroutine is executed/resumed.
Zipkin,64439116,47026664,0,"2020/10/20, 08:41:48",False,"2020/10/22, 13:48:08",145.0,10936956.0,1,This is the easiest solution I could discover so far.
Zipkin,64439116,47026664,0,"2020/10/20, 08:41:48",False,"2020/10/22, 13:48:08",145.0,10936956.0,1,:)
Zipkin,64439116,47026664,0,"2020/10/20, 08:41:48",False,"2020/10/22, 13:48:08",145.0,10936956.0,1,The launch method takes an optional CoroutineContext and the coroutine-slf4j integration implements the MDCContext.
Zipkin,64439116,47026664,0,"2020/10/20, 08:41:48",False,"2020/10/22, 13:48:08",145.0,10936956.0,1,This class captures the calling thread's MDC context (creates a copy) and uses that for the coroutine execution.
Zipkin,64439116,47026664,0,"2020/10/20, 08:41:48",False,"2020/10/22, 13:48:08",145.0,10936956.0,1,Add this dependency to your build.gradle:
Zipkin,64439116,47026664,0,"2020/10/20, 08:41:48",False,"2020/10/22, 13:48:08",145.0,10936956.0,1,"Project:  https://github.com/Kotlin/kotlinx.coroutines/tree/master/integration/kotlinx-coroutines-slf4j 
Documentation:  https://kotlin.github.io/kotlinx.coroutines/kotlinx-coroutines-slf4j/index.html"
Zipkin,46454753,46432583,1,"2017/09/27, 21:22:45",False,"2017/09/27, 21:22:45",2661.0,1813696.0,0,With Spring boot  Dalston.SR3  (which uses open zipkin 1.28) you can achieve this by setting property  zipkin.storage.mem.max-spans=xxx  This will limit the number of spans and discard old ones.
Zipkin,46454753,46432583,1,"2017/09/27, 21:22:45",False,"2017/09/27, 21:22:45",2661.0,1813696.0,0,pom.xml
Zipkin,47120862,46164976,1,"2017/11/05, 13:08:17",True,"2017/11/05, 13:08:17",26.0,5772061.0,1,The best way to trace OpenStack project is to use Osprofiler library.
Zipkin,47120862,46164976,1,"2017/11/05, 13:08:17",True,"2017/11/05, 13:08:17",26.0,5772061.0,1,If you just want to understand the workflow or just know about the types of calls being made inside OpenStack then Osprofiler is the best and easiest way to get the trace.
Zipkin,47120862,46164976,1,"2017/11/05, 13:08:17",True,"2017/11/05, 13:08:17",26.0,5772061.0,1,Now Osprofiler is an accepted OpenStack project and the official project to get traces for OpenStack.
Zipkin,47120862,46164976,1,"2017/11/05, 13:08:17",True,"2017/11/05, 13:08:17",26.0,5772061.0,1,"Instead of having to go through the whole code and adding instrumentation points near HTTP request or RPC calls, osprofiler is already integrated in all of the main projects of OpenStack (Nova, Neutron, Keystone, Glance etc..)."
Zipkin,47120862,46164976,1,"2017/11/05, 13:08:17",True,"2017/11/05, 13:08:17",26.0,5772061.0,1,You just have to enable osprofiler in the configuration files of each project in OpenStack to get a trace of that particular project.
Zipkin,47120862,46164976,1,"2017/11/05, 13:08:17",True,"2017/11/05, 13:08:17",26.0,5772061.0,1,You can go through this link -  https://docs.openstack.org/osprofiler/latest/
Zipkin,47120862,46164976,1,"2017/11/05, 13:08:17",True,"2017/11/05, 13:08:17",26.0,5772061.0,1,Enabling of Osprofiler in the configuration files can be done by adding these lines at the end of the configuration file (nova.conf or neutron.conf) :
Zipkin,47120862,46164976,1,"2017/11/05, 13:08:17",True,"2017/11/05, 13:08:17",26.0,5772061.0,1,The connection_string parameter indicates the collector (where the trace information is stored).
Zipkin,47120862,46164976,1,"2017/11/05, 13:08:17",True,"2017/11/05, 13:08:17",26.0,5772061.0,1,By default it uses Ceilometer.
Zipkin,47120862,46164976,1,"2017/11/05, 13:08:17",True,"2017/11/05, 13:08:17",26.0,5772061.0,1,You can actually redirect the trace information to other collectors like Elasticsearch by changing the connection_string parameter in the conf file to the elasticsearch server.
Zipkin,47120862,46164976,1,"2017/11/05, 13:08:17",True,"2017/11/05, 13:08:17",26.0,5772061.0,1,This is by far the easiest way to get a trace in OpenStack with just minimal effort.
Zipkin,45568932,45174992,0,"2017/08/08, 15:53:02",True,"2017/08/08, 15:53:02",45.0,4107097.0,0,After some discussion with Marcin Grzejszczak and Adrien Cole (zipkin and sleuth creators/active developers) I ended up creating a Jersey filter that acts as bridge between sleuth and brave.
Zipkin,45568932,45174992,0,"2017/08/08, 15:53:02",True,"2017/08/08, 15:53:02",45.0,4107097.0,0,"Regarding AMQP integration, added a new @StreamListener with a conditional for zipkin format spans (using headers)."
Zipkin,45568932,45174992,0,"2017/08/08, 15:53:02",True,"2017/08/08, 15:53:02",45.0,4107097.0,0,Sending messages to the sleuth exchange with zipkin format will then be valid and consumed by the listener.
Zipkin,45568932,45174992,0,"2017/08/08, 15:53:02",True,"2017/08/08, 15:53:02",45.0,4107097.0,0,"For javascript (zipkin-js), I ended up creating a new AMQP Logger that sends zipkin spans to a determined exchange."
Zipkin,45568932,45174992,0,"2017/08/08, 15:53:02",True,"2017/08/08, 15:53:02",45.0,4107097.0,0,"If someone ends up reading this and needs more detail, you're welcome to reach out to me."
Zipkin,45048596,45046700,1,"2017/07/12, 07:41:40",False,"2017/07/12, 07:41:40",4434.0,7122593.0,0,Take a look at Sampling interval in the docs :
Zipkin,45048596,45046700,1,"2017/07/12, 07:41:40",False,"2017/07/12, 07:41:40",4434.0,7122593.0,0,In distributed tracing the data volumes can be very high so sampling can be important (you usually don’t need to export all spans to get a good picture of what is happening).
Zipkin,45048596,45046700,1,"2017/07/12, 07:41:40",False,"2017/07/12, 07:41:40",4434.0,7122593.0,0,Spring Cloud Sleuth has a Sampler strategy that you can implement to take control of the sampling algorithm.
Zipkin,45048596,45046700,1,"2017/07/12, 07:41:40",False,"2017/07/12, 07:41:40",4434.0,7122593.0,0,"Samplers do not stop span (correlation) ids from being generated, but they do prevent the tags and events being attached and exported."
Zipkin,45048596,45046700,1,"2017/07/12, 07:41:40",False,"2017/07/12, 07:41:40",4434.0,7122593.0,0,"By default you get a strategy that continues to trace if a span is already active, but new ones are always marked as non-exportable."
Zipkin,45048596,45046700,1,"2017/07/12, 07:41:40",False,"2017/07/12, 07:41:40",4434.0,7122593.0,0,"If all your apps run with this sampler you will see traces in logs, but not in any remote store."
Zipkin,45048596,45046700,1,"2017/07/12, 07:41:40",False,"2017/07/12, 07:41:40",4434.0,7122593.0,0,"For testing the default is often enough, and it probably is all you need if you are only using the logs (e.g."
Zipkin,45048596,45046700,1,"2017/07/12, 07:41:40",False,"2017/07/12, 07:41:40",4434.0,7122593.0,0,with an ELK aggregator).
Zipkin,45048596,45046700,1,"2017/07/12, 07:41:40",False,"2017/07/12, 07:41:40",4434.0,7122593.0,0,"If you are exporting span data to Zipkin or Spring Cloud Stream, there is also an AlwaysSampler that exports everything and a PercentageBasedSampler that samples a fixed fraction of spans."
Zipkin,45048596,45046700,1,"2017/07/12, 07:41:40",False,"2017/07/12, 07:41:40",4434.0,7122593.0,0,http://cloud.spring.io/spring-cloud-sleuth/spring-cloud-sleuth.html#_sampling
Zipkin,44395594,44382633,1,"2017/06/06, 19:47:13",False,"2017/06/06, 19:47:13",37971.0,1237575.0,0,You are blending Zipkin autoconfigure version 1.2 with Zipkin 1.26.
Zipkin,44395594,44382633,1,"2017/06/06, 19:47:13",False,"2017/06/06, 19:47:13",37971.0,1237575.0,0,This results in a version missmatch.
Zipkin,44403836,44382633,0,"2017/06/07, 07:58:45",False,"2017/06/07, 07:58:45",1.0,7608262.0,0,The problem is casued by two reasons.
Zipkin,44403836,44382633,0,"2017/06/07, 07:58:45",False,"2017/06/07, 07:58:45",1.0,7608262.0,0,First:just as Rafael Winterhalter said the version dismatched; second: you are lacking dependency for zipkin.
Zipkin,44403836,44382633,0,"2017/06/07, 07:58:45",False,"2017/06/07, 07:58:45",1.0,7608262.0,0,"Finally i fixed the problem by adding the whole 'io.zipkin.java:zipkin"" library."
Zipkin,44403836,44382633,0,"2017/06/07, 07:58:45",False,"2017/06/07, 07:58:45",1.0,7608262.0,0,Here is my final pom file with the storage type of elasticsearch:
Zipkin,44193861,44147933,0,"2017/05/26, 07:52:41",False,"2017/05/26, 07:52:41",1.0,7983801.0,0,Appears that a sleuth span is not the same as a Zipkin span.
Zipkin,44193861,44147933,0,"2017/05/26, 07:52:41",False,"2017/05/26, 07:52:41",1.0,7983801.0,0,"Hence, in the above code there is no way to instantiate a default tracer with zipkin span reporter."
Zipkin,44193861,44147933,0,"2017/05/26, 07:52:41",False,"2017/05/26, 07:52:41",1.0,7983801.0,0,I converted the sleuth span into a zipkin span and then reported it to zipkin.
Zipkin,44193861,44147933,0,"2017/05/26, 07:52:41",False,"2017/05/26, 07:52:41",1.0,7983801.0,0,The class to convert it is available in spring-cloud-sleuth-stream.
Zipkin,44193861,44147933,0,"2017/05/26, 07:52:41",False,"2017/05/26, 07:52:41",1.0,7983801.0,0,I used pretty much the same class with some tweaks.
Zipkin,44062121,44027882,0,"2017/05/19, 08:32:21",False,"2017/05/19, 08:32:21",8336.0,1773866.0,2,Here you have a very basic example of Sleuth &amp; HTTP communication.
Zipkin,44062121,44027882,0,"2017/05/19, 08:32:21",False,"2017/05/19, 08:32:21",8336.0,1773866.0,2,https://github.com/openzipkin/sleuth-webmvc-example  You can set your dependencies in a similar manner and everything should work fine.
Zipkin,44062121,44027882,0,"2017/05/19, 08:32:21",False,"2017/05/19, 08:32:21",8336.0,1773866.0,2,In your example you've got Stream but I don't think you're using it so it's better to remove it.
Zipkin,45815790,44027882,0,"2017/08/22, 13:58:17",False,"2017/08/22, 13:58:17",253.0,5751473.0,0,As M.Deinum said remove  stream  and  stream-rabbit  dependencies what if you do not need some AMQP server to store the trace message.
Zipkin,45815790,44027882,0,"2017/08/22, 13:58:17",False,"2017/08/22, 13:58:17",253.0,5751473.0,0,or
Zipkin,45815790,44027882,0,"2017/08/22, 13:58:17",False,"2017/08/22, 13:58:17",253.0,5751473.0,0,"config the AMQP(rabbitMQ in your code) from application-configuration(both) and add  zipkin-stream  &amp;  stream-rabbit  in  zipkin-server  side, so this time your app( zipkin-client ) will not direct connect with  zipkin-server  
and it will be:"
Zipkin,47431400,43913962,0,"2017/11/22, 11:49:20",False,"2017/11/22, 11:49:20",215.0,3702774.0,1,You may define all needed params via ENV options.
Zipkin,47431400,43913962,0,"2017/11/22, 11:49:20",False,"2017/11/22, 11:49:20",215.0,3702774.0,1,Here is a cmd for running zipkin in docker:
Zipkin,47431400,43913962,0,"2017/11/22, 11:49:20",False,"2017/11/22, 11:49:20",215.0,3702774.0,1,All these params can be defined in Deployment (see  Expose Pod Information to Containers Through Environment Variables )
Zipkin,48982440,43782954,0,"2018/02/26, 08:10:42",False,"2018/02/26, 08:10:42",181.0,6708214.0,1,1.You should check if your Zipkin Server is on.
Zipkin,48982440,43782954,0,"2018/02/26, 08:10:42",False,"2018/02/26, 08:10:42",181.0,6708214.0,1,2.You should check if the Span transfering is async.
Zipkin,48982440,43782954,0,"2018/02/26, 08:10:42",False,"2018/02/26, 08:10:42",181.0,6708214.0,1,"In HTTP,Zipkin uses in-band transfer,all the information carried in HTTP headers.The cost time of generating Span is about 200 nanosecond."
Zipkin,43153293,43151368,0,"2017/04/01, 08:46:41",True,"2017/04/01, 08:46:41",762.0,1227937.0,1,"The TL;DR; is that  B3 propagation  was initially designed for fixed size data: carrying data ancillary to tracing isn't in scope, and for this reason any solution that extends B3 in such a fashion wouldn't be compatible with existing code."
Zipkin,43153293,43151368,0,"2017/04/01, 08:46:41",True,"2017/04/01, 08:46:41",762.0,1227937.0,1,"So, that means any solution like this will be an extension which means custom handling in the  instrumented apps  which are the things passing headers around."
Zipkin,43153293,43151368,0,"2017/04/01, 08:46:41",True,"2017/04/01, 08:46:41",762.0,1227937.0,1,The server won't care as it never sees these headers anyway.
Zipkin,43153293,43151368,0,"2017/04/01, 08:46:41",True,"2017/04/01, 08:46:41",762.0,1227937.0,1,Ways people usually integrate other things like flags with zipkin is to add a tag aka binary annotation including its value (usually in the root span).
Zipkin,43153293,43151368,0,"2017/04/01, 08:46:41",True,"2017/04/01, 08:46:41",762.0,1227937.0,1,"This would allow you to query or retrieve these offline, but it doesn't address in-flight lookups from applications."
Zipkin,43153293,43151368,0,"2017/04/01, 08:46:41",True,"2017/04/01, 08:46:41",762.0,1227937.0,1,"Let's say that instead of using an intermediary like linkerd, or a platform-specific propagated context, we want to dispatch the responsibility to the tracing layer."
Zipkin,43153293,43151368,0,"2017/04/01, 08:46:41",True,"2017/04/01, 08:46:41",762.0,1227937.0,1,"Firstly, what sort of data could work alright?"
Zipkin,43153293,43151368,0,"2017/04/01, 08:46:41",True,"2017/04/01, 08:46:41",762.0,1227937.0,1,The easiest is something set-once (like zipkin's trace id).
Zipkin,43153293,43151368,0,"2017/04/01, 08:46:41",True,"2017/04/01, 08:46:41",762.0,1227937.0,1,Anything set and propagated without mutating it is the least mechanics.
Zipkin,43153293,43151368,0,"2017/04/01, 08:46:41",True,"2017/04/01, 08:46:41",762.0,1227937.0,1,"Next in difficulty is appending new entries mid-stream, and most difficult is mutating/merging entries."
Zipkin,43153293,43151368,0,"2017/04/01, 08:46:41",True,"2017/04/01, 08:46:41",762.0,1227937.0,1,Let's assume this is for inbound flags which never change through the request/trace tree.
Zipkin,43153293,43151368,0,"2017/04/01, 08:46:41",True,"2017/04/01, 08:46:41",762.0,1227937.0,1,"We see a header when processing trace data, we store it and forward it downstream."
Zipkin,43153293,43151368,0,"2017/04/01, 08:46:41",True,"2017/04/01, 08:46:41",762.0,1227937.0,1,"If this value doesn't need to be read by the tracing system, it is easiest, as it is largely a transport/propagation concern."
Zipkin,43153293,43151368,0,"2017/04/01, 08:46:41",True,"2017/04/01, 08:46:41",762.0,1227937.0,1,"For example, maybe other middleware read that header and it is only a ""side job"" we are adding to the tracer to remember certain things to pass along."
Zipkin,43153293,43151368,0,"2017/04/01, 08:46:41",True,"2017/04/01, 08:46:41",762.0,1227937.0,1,"If this was done in a single header, it would be less code than a pattern in each of the places this would be to added."
Zipkin,43153293,43151368,0,"2017/04/01, 08:46:41",True,"2017/04/01, 08:46:41",762.0,1227937.0,1,"It would be even less code if the flags could be encoded in a number, however unrealistic that may be."
Zipkin,43153293,43151368,0,"2017/04/01, 08:46:41",True,"2017/04/01, 08:46:41",762.0,1227937.0,1,"There are libraries with apis to manipulate the propagated context manually, for example,  ""baggage"" from brownsys  and OpenTracing (of which some libraries support zipkin)."
Zipkin,43153293,43151368,0,"2017/04/01, 08:46:41",True,"2017/04/01, 08:46:41",762.0,1227937.0,1,"The former aims to be a generic layer for any instrumentation (ex monitoring, chargeback, tracing etc) and the latter is specific to tracing."
Zipkin,43153293,43151368,0,"2017/04/01, 08:46:41",True,"2017/04/01, 08:46:41",762.0,1227937.0,1,OpenTracing has defines abstract types like  injector and extractor  which could be customized to carry other fields.
Zipkin,43153293,43151368,0,"2017/04/01, 08:46:41",True,"2017/04/01, 08:46:41",762.0,1227937.0,1,"However, you still would need a concrete implementation (which knows your header format etc) in order to do this."
Zipkin,43153293,43151368,0,"2017/04/01, 08:46:41",True,"2017/04/01, 08:46:41",762.0,1227937.0,1,"Unless you want applications to read this data, it would need to be a secret detail of that implementation (specifically the trace context)."
Zipkin,43153293,43151368,0,"2017/04/01, 08:46:41",True,"2017/04/01, 08:46:41",762.0,1227937.0,1,"Certain zipkin-specific libraries like Spring Cloud Sleuth and Brave have means to  customize how headers are parsed , to support variants of B3 or new or site-specific trace formats."
Zipkin,43153293,43151368,0,"2017/04/01, 08:46:41",True,"2017/04/01, 08:46:41",762.0,1227937.0,1,"Not all support this at the moment, but I would expect this type of feature to become more common."
Zipkin,43153293,43151368,0,"2017/04/01, 08:46:41",True,"2017/04/01, 08:46:41",762.0,1227937.0,1,This means you may need to do some surgery in order to support all platforms you may need to support.
Zipkin,43153293,43151368,0,"2017/04/01, 08:46:41",True,"2017/04/01, 08:46:41",762.0,1227937.0,1,"So long story short is that there are some libraries which are pluggable with regards to propagation, and those will be easiest to modify to support this use case."
Zipkin,43153293,43151368,0,"2017/04/01, 08:46:41",True,"2017/04/01, 08:46:41",762.0,1227937.0,1,Some code will be needed regardless as B3 doesn't currently define an expression like this.
Zipkin,43106901,43095404,0,"2017/03/30, 04:24:13",True,"2017/03/30, 04:24:13",762.0,1227937.0,0,In general it is better to use the zipkin's http variant of Elasticsearch as it cannot conflict with Spring Boot's elasticsearch library versions.
Zipkin,43106901,43095404,0,"2017/03/30, 04:24:13",True,"2017/03/30, 04:24:13",762.0,1227937.0,0,I would set everything in zipkin's group id to latest (currently 1.21.0 which is Spring Boot 1.4.x) and use zipkin-autoconfigure-storage-elasticsearch-http (plus.. the one you are using  will be dropped )
Zipkin,43106901,43095404,0,"2017/03/30, 04:24:13",True,"2017/03/30, 04:24:13",762.0,1227937.0,0,Make sure your es hosts is specified in url syntax ex.
Zipkin,43106901,43095404,0,"2017/03/30, 04:24:13",True,"2017/03/30, 04:24:13",762.0,1227937.0,0,http://host1:9200
Zipkin,42901789,42884672,0,"2017/03/20, 12:55:54",False,"2017/03/20, 12:55:54",37971.0,1237575.0,0,Zipkin generates traces and communicates them back to a Zipkin server.
Zipkin,42901789,42884672,0,"2017/03/20, 12:55:54",False,"2017/03/20, 12:55:54",37971.0,1237575.0,0,The latter action is typically performed via HTTP but Zipkin is agnostic to how a span is started and ended.
Zipkin,42901789,42884672,0,"2017/03/20, 12:55:54",False,"2017/03/20, 12:55:54",37971.0,1237575.0,0,"If you want to measure the execution time of a single method, you would normally create a local span that is nested inside a server span."
Zipkin,42901789,42884672,0,"2017/03/20, 12:55:54",False,"2017/03/20, 12:55:54",37971.0,1237575.0,0,Zipkin is a distributrd tracing tool for discovering machine-to-machine interaction which is why spans often cover HTTP.
Zipkin,42901789,42884672,0,"2017/03/20, 12:55:54",False,"2017/03/20, 12:55:54",37971.0,1237575.0,0,"If you want to measure execution time of a method, a tool like metrics might be more suited."
Zipkin,41798515,40954585,1,"2017/01/23, 04:38:41",True,"2017/01/23, 04:38:41",762.0,1227937.0,1,This was an issue with MySQL 5.7 and more recently resolved.
Zipkin,41798515,40954585,1,"2017/01/23, 04:38:41",True,"2017/01/23, 04:38:41",762.0,1227937.0,1,You can try latest Zipkin.
Zipkin,39601988,39600581,2,"2016/09/20, 22:06:46",True,"2016/09/20, 22:06:46",8336.0,1773866.0,1,You'd have to implement your own ZipkinSpanReporter that would look more or less like  https://github.com/spring-cloud/spring-cloud-sleuth/blob/v1.0.8.RELEASE/spring-cloud-sleuth-zipkin/src/main/java/org/springframework/cloud/sleuth/zipkin/HttpZipkinSpanReporter.java  .
Zipkin,39601988,39600581,2,"2016/09/20, 22:06:46",True,"2016/09/20, 22:06:46",8336.0,1773866.0,1,In the next version of Sleuth you will be able to register a bean of ZipkinSpanReporter that can you a custom version of a publisher -  https://github.com/spring-cloud/spring-cloud-sleuth/blob/1.0.x/spring-cloud-sleuth-zipkin/src/main/java/org/springframework/cloud/sleuth/zipkin/HttpZipkinSpanReporter.java
Zipkin,38537946,38527466,1,"2016/07/23, 06:47:14",False,"2016/07/23, 06:47:14",762.0,1227937.0,1,Instrumenting a library is something that sometimes folks have to do for one reason or another.
Zipkin,38537946,38527466,1,"2016/07/23, 06:47:14",False,"2016/07/23, 06:47:14",762.0,1227937.0,1,"There are several tracer libraries in Java but the salient points about creating a tracer are either on the website, or issues on the website."
Zipkin,38537946,38527466,1,"2016/07/23, 06:47:14",False,"2016/07/23, 06:47:14",762.0,1227937.0,1,"http://zipkin.io/pages/instrumenting.html 
 https://github.com/openzipkin/openzipkin.github.io/issues/11"
Zipkin,38537946,38527466,1,"2016/07/23, 06:47:14",False,"2016/07/23, 06:47:14",762.0,1227937.0,1,OpenTracing also has some nice fundamentals to look at  http://opentracing.io/
Zipkin,38537946,38527466,1,"2016/07/23, 06:47:14",False,"2016/07/23, 06:47:14",762.0,1227937.0,1,"This isn't a one-answer type of question, in my experience, as you'll learn you'll need to address other things that tracer libraries address out-of-box."
Zipkin,38537946,38527466,1,"2016/07/23, 06:47:14",False,"2016/07/23, 06:47:14",762.0,1227937.0,1,"For that reason I'd highly suggest joining gitter so that you
can have a dialogue through your journey  https://gitter.im/openzipkin/zipkin"
Zipkin,37799358,37757200,0,"2016/06/14, 00:23:23",False,"2016/07/26, 21:58:32",13.0,3491416.0,1,I was recording wrong annotation i.e client instead of server.
Zipkin,37799358,37757200,0,"2016/06/14, 00:23:23",False,"2016/07/26, 21:58:32",13.0,3491416.0,1,Just a simple change did the trick.
Zipkin,37799358,37757200,0,"2016/06/14, 00:23:23",False,"2016/07/26, 21:58:32",13.0,3491416.0,1,"Trace.traceService(""Function1"",""Test"")"
Zipkin,37799358,37757200,0,"2016/06/14, 00:23:23",False,"2016/07/26, 21:58:32",13.0,3491416.0,1,Sample working Zipkin example:  https://gist.github.com/AkhilJ876/3e38757c28d43924f296dd2d147c0bd9#file-zipkintracing_example-L34
Zipkin,37648120,37459261,0,"2016/06/06, 03:30:28",False,"2016/06/06, 16:49:03",91.0,3623286.0,1,"Same problem here with the docker images using Cassandra (1.40.1, 1.40.2, 1.1.4)."
Zipkin,37648120,37459261,0,"2016/06/06, 03:30:28",False,"2016/06/06, 16:49:03",91.0,3623286.0,1,This is a problem specific to using Cassandra as the storage tier.
Zipkin,37648120,37459261,0,"2016/06/06, 03:30:28",False,"2016/06/06, 16:49:03",91.0,3623286.0,1,Mysql and the in-memory storage generate the dependency graph on-demand as expected.
Zipkin,37648120,37459261,0,"2016/06/06, 03:30:28",False,"2016/06/06, 16:49:03",91.0,3623286.0,1,There are references to the following project to generate the Cassandra graph data for the UI to display.
Zipkin,37648120,37459261,0,"2016/06/06, 03:30:28",False,"2016/06/06, 16:49:03",91.0,3623286.0,1,This looks to be superseded by ongoing work mentioned here
Zipkin,58025941,37459261,0,"2019/09/20, 12:50:09",False,"2019/09/20, 13:20:39",1592.0,7251967.0,0,"If storage type is other than inline storage, for zipkin dependencies graph you have to start separate cron job/scheduler which reads the storage database and builds the graph."
Zipkin,58025941,37459261,0,"2019/09/20, 12:50:09",False,"2019/09/20, 13:20:39",1592.0,7251967.0,0,Because zipkin dependencies is separate spark job .
Zipkin,58025941,37459261,0,"2019/09/20, 12:50:09",False,"2019/09/20, 13:20:39",1592.0,7251967.0,0,For reference :  https://github.com/openzipkin/docker-zipkin-dependencies
Zipkin,58025941,37459261,0,"2019/09/20, 12:50:09",False,"2019/09/20, 13:20:39",1592.0,7251967.0,0,I have used zipkin with elastic search as storage type.
Zipkin,58025941,37459261,0,"2019/09/20, 12:50:09",False,"2019/09/20, 13:20:39",1592.0,7251967.0,0,I will share the steps for setting up the zipkin dependencies with elastic search and cron job for the same:
Zipkin,58025941,37459261,0,"2019/09/20, 12:50:09",False,"2019/09/20, 13:20:39",1592.0,7251967.0,0,"Other solution is to start a separate service and run the cron job
  using docker"
Zipkin,58025941,37459261,0,"2019/09/20, 12:50:09",False,"2019/09/20, 13:20:39",1592.0,7251967.0,0,"Steps to get the latest zipkin-dependencies jar try running given
  command on teminal"
Zipkin,58025941,37459261,0,"2019/09/20, 12:50:09",False,"2019/09/20, 13:20:39",1592.0,7251967.0,0,you will get jar file at above mention directory
Zipkin,58025941,37459261,0,"2019/09/20, 12:50:09",False,"2019/09/20, 13:20:39",1592.0,7251967.0,0,Dockerfile
Zipkin,58025941,37459261,0,"2019/09/20, 12:50:09",False,"2019/09/20, 13:20:39",1592.0,7251967.0,0,entry.sh
Zipkin,58025941,37459261,0,"2019/09/20, 12:50:09",False,"2019/09/20, 13:20:39",1592.0,7251967.0,0,script.sh
Zipkin,58025941,37459261,0,"2019/09/20, 12:50:09",False,"2019/09/20, 13:20:39",1592.0,7251967.0,0,crontab.txt
Zipkin,21582785,21551629,3,"2014/02/05, 18:27:05",True,"2014/02/05, 18:27:05",26.0,2070089.0,1,This is due to not having an instance of the query server running.
Zipkin,21582785,21551629,3,"2014/02/05, 18:27:05",True,"2014/02/05, 18:27:05",26.0,2070089.0,1,I'm in the middle of a re-write that'll simplify all of this.
Zipkin,21582785,21551629,3,"2014/02/05, 18:27:05",True,"2014/02/05, 18:27:05",26.0,2070089.0,1,"Until then, you need to spin up a query server."
Zipkin,61514513,61514326,0,"2020/04/30, 04:37:29",True,"2020/04/30, 04:37:29",396.0,4641689.0,1,I would say you can have fluentd in multiple namespaces and Elasticsearch in one namespace and fluentd can discover Elasticsearch via K8s internal DNS A/AAAA record e.g.
Zipkin,61514513,61514326,0,"2020/04/30, 04:37:29",True,"2020/04/30, 04:37:29",396.0,4641689.0,1,elasticsearch.${namespace}.svc.cluster.local .
Zipkin,61514513,61514326,0,"2020/04/30, 04:37:29",True,"2020/04/30, 04:37:29",396.0,4641689.0,1,"I don't have any link to the best practice, but I would show you a practice I saw from the community."
Zipkin,61514513,61514326,0,"2020/04/30, 04:37:29",True,"2020/04/30, 04:37:29",396.0,4641689.0,1,"If you are not familiar with configuring K8s cluster, I recommend to deploy ELK by Helm."
Zipkin,61514513,61514326,0,"2020/04/30, 04:37:29",True,"2020/04/30, 04:37:29",396.0,4641689.0,1,It will save you a lot of time and give you enough configuration options.
Zipkin,61514513,61514326,0,"2020/04/30, 04:37:29",True,"2020/04/30, 04:37:29",396.0,4641689.0,1,https://github.com/helm/charts/tree/master/stable/elastic-stack .
Zipkin,61514513,61514326,0,"2020/04/30, 04:37:29",True,"2020/04/30, 04:37:29",396.0,4641689.0,1,"Install your ELK helm release on a  separate  namespace, for example:  logging ."
Zipkin,61514513,61514326,0,"2020/04/30, 04:37:29",True,"2020/04/30, 04:37:29",396.0,4641689.0,1,Install fluentd in any namespaces in your cluster and configure elasticsearch host  https://github.com/helm/charts/tree/master/stable/fluentd-elasticsearch
Zipkin,57200810,57143690,0,"2019/07/25, 14:23:59",False,"2019/07/25, 14:23:59",9.0,4762878.0,1,"I used an Aspect and from the returned ResponseEntity object, decided whether or not to programatically add an error tag to span."
Zipkin,57200810,57143690,0,"2019/07/25, 14:23:59",False,"2019/07/25, 14:23:59",9.0,4762878.0,1,"With this tag, zipkin will identify and highlight the trace in red colour."
Zipkin,57200810,57143690,0,"2019/07/25, 14:23:59",False,"2019/07/25, 14:23:59",9.0,4762878.0,1,Below is the code snippet to add error tag to span.
Zipkin,48765679,48668426,0,"2018/02/13, 13:23:09",False,"2018/02/13, 13:23:09",681.0,4255878.0,0,i think i found a suitable way to do this.
Zipkin,48765679,48668426,0,"2018/02/13, 13:23:09",False,"2018/02/13, 13:23:09",681.0,4255878.0,0,After further thinking my idea went into the direction of using annotations and aspects for intercepting HTTP requests from/to thrifts http client which seems to be quite some work.
Zipkin,48765679,48668426,0,"2018/02/13, 13:23:09",False,"2018/02/13, 13:23:09",681.0,4255878.0,0,"after further search, i found this library for spring-boot serving exactly my needs:
 https://github.com/aatarasoff/spring-thrift-starter"
Zipkin,58889653,58889547,0,"2019/11/16, 12:16:29",False,"2019/11/16, 12:16:29",1.0,12382349.0,0,I dont kown can you see my pic and I put my code:
Zipkin,58889653,58889547,0,"2019/11/16, 12:16:29",False,"2019/11/16, 12:16:29",1.0,12382349.0,0,pom.xml:
Zipkin,58889653,58889547,0,"2019/11/16, 12:16:29",False,"2019/11/16, 12:16:29",1.0,12382349.0,0,ZipkinApplication.java:
Zipkin,58889653,58889547,0,"2019/11/16, 12:16:29",False,"2019/11/16, 12:16:29",1.0,12382349.0,0,The error:
Zipkin,53692930,53692437,3,"2018/12/09, 15:41:18",False,"2018/12/09, 15:41:18",81.0,2232476.0,0,I can say your YAML has some bad indentation and things are not in the right sections even.
Zipkin,53692930,53692437,3,"2018/12/09, 15:41:18",False,"2018/12/09, 15:41:18",81.0,2232476.0,0,"Otherwise though, you are trying to run Zipkin in an unsupported configuration."
Zipkin,53692930,53692437,3,"2018/12/09, 15:41:18",False,"2018/12/09, 15:41:18",81.0,2232476.0,0,Please check out our quickstart documentation:  https://zipkin.io/pages/quickstart.html
Zipkin,53694198,53692437,3,"2018/12/09, 18:13:01",True,"2018/12/13, 16:58:29",5963.0,3625215.0,1,There are 2 approaches to this
Zipkin,53694198,53692437,3,"2018/12/09, 18:13:01",True,"2018/12/13, 16:58:29",5963.0,3625215.0,1,Looking at your yml file you have added
Zipkin,53694198,53692437,3,"2018/12/09, 18:13:01",True,"2018/12/13, 16:58:29",5963.0,3625215.0,1,which means your approach is 2.
Zipkin,53694198,53692437,3,"2018/12/09, 18:13:01",True,"2018/12/13, 16:58:29",5963.0,3625215.0,1,"But then in your pom, you have added  zipkin-server  and  zipkin-autoconfigure-ui  dependencies which is not required."
Zipkin,53694198,53692437,3,"2018/12/09, 18:13:01",True,"2018/12/13, 16:58:29",5963.0,3625215.0,1,I will try to separate both setups
Zipkin,53694198,53692437,3,"2018/12/09, 18:13:01",True,"2018/12/13, 16:58:29",5963.0,3625215.0,1,1.
Zipkin,53694198,53692437,3,"2018/12/09, 18:13:01",True,"2018/12/13, 16:58:29",5963.0,3625215.0,1,To Start Zipkin server with SpringBootApplication
Zipkin,53694198,53692437,3,"2018/12/09, 18:13:01",True,"2018/12/13, 16:58:29",5963.0,3625215.0,1,pom.xml
Zipkin,53694198,53692437,3,"2018/12/09, 18:13:01",True,"2018/12/13, 16:58:29",5963.0,3625215.0,1,application.properties
Zipkin,53694198,53692437,3,"2018/12/09, 18:13:01",True,"2018/12/13, 16:58:29",5963.0,3625215.0,1,Application.java
Zipkin,53694198,53692437,3,"2018/12/09, 18:13:01",True,"2018/12/13, 16:58:29",5963.0,3625215.0,1,2.
Zipkin,53694198,53692437,3,"2018/12/09, 18:13:01",True,"2018/12/13, 16:58:29",5963.0,3625215.0,1,To Start Zipkin server as a standalone and use SpringBootApplication as Zipkin Client
Zipkin,53694198,53692437,3,"2018/12/09, 18:13:01",True,"2018/12/13, 16:58:29",5963.0,3625215.0,1,Start Zipkin server
Zipkin,53694198,53692437,3,"2018/12/09, 18:13:01",True,"2018/12/13, 16:58:29",5963.0,3625215.0,1,pom.xml
Zipkin,53694198,53692437,3,"2018/12/09, 18:13:01",True,"2018/12/13, 16:58:29",5963.0,3625215.0,1,application.properties
Zipkin,53694198,53692437,3,"2018/12/09, 18:13:01",True,"2018/12/13, 16:58:29",5963.0,3625215.0,1,Edit 1:
Zipkin,53694198,53692437,3,"2018/12/09, 18:13:01",True,"2018/12/13, 16:58:29",5963.0,3625215.0,1,@EnableZipkinServer  is deprecated and unsupported as per Brian Devins's comment.
Zipkin,53694198,53692437,3,"2018/12/09, 18:13:01",True,"2018/12/13, 16:58:29",5963.0,3625215.0,1,"So, please go through the  doc  for more detail info."
Zipkin,50837359,50836516,3,"2018/06/13, 15:31:21",False,"2018/06/13, 15:31:21",8336.0,1773866.0,0,You're using an ancient version of Spring CLoud.
Zipkin,50837359,50836516,3,"2018/06/13, 15:31:21",False,"2018/06/13, 15:31:21",8336.0,1773866.0,0,Please upgrade to latest Edgware.
Zipkin,50837359,50836516,3,"2018/06/13, 15:31:21",False,"2018/06/13, 15:31:21",8336.0,1773866.0,0,The RxJava support is very basic so we suggest that you use Project Reactor.
Zipkin,50837359,50836516,3,"2018/06/13, 15:31:21",False,"2018/06/13, 15:31:21",8336.0,1773866.0,0,To do that just migrate to Finchley and it should work out of the box with WebFlux.
Zipkin,50268263,50267380,6,"2018/05/10, 11:01:52",False,"2018/05/10, 11:01:52",8336.0,1773866.0,1,"You're using an ancient version of Sleuth, can you please upgrade?"
Zipkin,50268263,50267380,6,"2018/05/10, 11:01:52",False,"2018/05/10, 11:01:52",8336.0,1773866.0,1,Why do you provide Zipkin's version manually?
Zipkin,50268263,50267380,6,"2018/05/10, 11:01:52",False,"2018/05/10, 11:01:52",8336.0,1773866.0,1,Also as far as I see you're using the Sleuth's Zipkin server (that is deprecated in Edgware and removed in Finchley).
Zipkin,50268263,50267380,6,"2018/05/10, 11:01:52",False,"2018/05/10, 11:01:52",8336.0,1773866.0,1,My suggestion is that you stop using the Sleuth's Stream server (you can read more about this here  https://cloud.spring.io/spring-cloud-static/Edgware.SR3/single/spring-cloud.html#_sleuth_with_zipkin_via_rabbitmq_or_kafka ).
Zipkin,50268263,50267380,6,"2018/05/10, 11:01:52",False,"2018/05/10, 11:01:52",8336.0,1773866.0,1,1) In order not to pick versions by yourself it’s much better if you add the dependency management via the Spring BOM
Zipkin,50268263,50267380,6,"2018/05/10, 11:01:52",False,"2018/05/10, 11:01:52",8336.0,1773866.0,1,2) Add the dependency to spring-cloud-starter-zipkin - that way all dependent dependencies will be downloaded
Zipkin,50268263,50267380,6,"2018/05/10, 11:01:52",False,"2018/05/10, 11:01:52",8336.0,1773866.0,1,"3) To automatically configure rabbit, simply add the spring-rabbit dependency"
Zipkin,50203216,50203215,0,"2018/05/06, 21:33:41",True,"2018/05/06, 21:33:41",93.0,6816012.0,0,"the simplest solution i have found for solving broken ui for zipkin thru gateway
is by changing following property of zipkin-server-shared.yml file inside zipkin server"
Zipkin,50203216,50203215,0,"2018/05/06, 21:33:41",True,"2018/05/06, 21:33:41",93.0,6816012.0,0,"zipkin:
  ui:
   base-path: /zipkin
 
change above property to"
Zipkin,50203216,50203215,0,"2018/05/06, 21:33:41",True,"2018/05/06, 21:33:41",93.0,6816012.0,0,"zipkin:
  ui:
   base-path: /api/tracing/zipkin"
Zipkin,50203216,50203215,0,"2018/05/06, 21:33:41",True,"2018/05/06, 21:33:41",93.0,6816012.0,0,"and change ur zuul path to following
 zuul.routes.zipkin.path=/api/tracing/*"
Zipkin,50203216,50203215,0,"2018/05/06, 21:33:41",True,"2018/05/06, 21:33:41",93.0,6816012.0,0,and than access zipkin using follwing url
Zipkin,50203216,50203215,0,"2018/05/06, 21:33:41",True,"2018/05/06, 21:33:41",93.0,6816012.0,0,https://gatewayhost:port/api/tracing/zipkin/
Zipkin,50203216,50203215,0,"2018/05/06, 21:33:41",True,"2018/05/06, 21:33:41",93.0,6816012.0,0,"give attention to small details in config and dont forget to put trailing ""/"" after zipkin  in url"
Zipkin,46093745,46092526,2,"2017/09/07, 13:12:45",False,"2017/09/07, 13:12:45",8336.0,1773866.0,0,It has nothing to do with Spring Cloud Sleuth or Zipkin.
Zipkin,46093745,46092526,2,"2017/09/07, 13:12:45",False,"2017/09/07, 13:12:45",8336.0,1773866.0,0,@SpringBootApplication automatically does @ComponentScan so all @RestController classes will get registered as beans if they are in the same package as your @SpringBootApplication annotated class or if it's in the child packages.
Zipkin,46093745,46092526,2,"2017/09/07, 13:12:45",False,"2017/09/07, 13:12:45",8336.0,1773866.0,0,Please read and try to understand how Spring Boot works by reading this chapter of the docs -  https://docs.spring.io/spring-boot/docs/current/reference/html/using-boot-using-springbootapplication-annotation.html
Zipkin,59260266,48857181,0,"2019/12/10, 06:31:59",False,"2019/12/10, 06:31:59",907.0,3941521.0,0,Object.keys(headers).filter((key) =&gt; TRACING_HEADERS.includes(key)).map((key) =&gt; headers[key])  returns an  array .
Zipkin,59260266,48857181,0,"2019/12/10, 06:31:59",False,"2019/12/10, 06:31:59",907.0,3941521.0,0,What you want is:
Zipkin,59260266,48857181,0,"2019/12/10, 06:31:59",False,"2019/12/10, 06:31:59",907.0,3941521.0,0,I'm pretty sure this isn't an istio / distributed tracing issue ;-)
Zipkin,65147869,48857181,0,"2020/12/04, 19:17:32",False,"2020/12/04, 19:17:32",36.0,4216591.0,0,b3-propagation of x-b3-parentspanid ( https://github.com/openzipkin/b3-propagation ) can be configured in your application.yml by adding:
Zipkin,60284574,60190181,1,"2020/02/18, 17:41:23",False,"2020/02/18, 17:41:23",593.0,2750758.0,0,Details of error (Java stack trace) would be really useful here.
Zipkin,60284574,60190181,1,"2020/02/18, 17:41:23",False,"2020/02/18, 17:41:23",593.0,2750758.0,0,"By error message I assume, you are using  qpid JMS client , that is performing check of message properties' names."
Zipkin,60284574,60190181,1,"2020/02/18, 17:41:23",False,"2020/02/18, 17:41:23",593.0,2750758.0,0,"These names can contain only characters, that are valid  Java identifier characters ."
Zipkin,60284574,60190181,1,"2020/02/18, 17:41:23",False,"2020/02/18, 17:41:23",593.0,2750758.0,0,"In string 'queue-name' there is a '-' character, that is not Java identifier."
Zipkin,60284574,60190181,1,"2020/02/18, 17:41:23",False,"2020/02/18, 17:41:23",593.0,2750758.0,0,"To fix, you need to change 'queue-name' into something with valid characters, for example 'queue_name' (with underscore), or 'queueName' (camel case)."
Zipkin,60330998,60190181,0,"2020/02/21, 03:40:33",False,"2020/02/21, 03:40:33",16250.0,8381946.0,4,Section 3.5.1 of the JMS 2 specification states this about message properties:
Zipkin,60330998,60190181,0,"2020/02/21, 03:40:33",False,"2020/02/21, 03:40:33",16250.0,8381946.0,4,Property names must obey the rules for a message selector identifier.
Zipkin,60330998,60190181,0,"2020/02/21, 03:40:33",False,"2020/02/21, 03:40:33",16250.0,8381946.0,4,"See
  Section 3.8 “Message selection” for more information."
Zipkin,60330998,60190181,0,"2020/02/21, 03:40:33",False,"2020/02/21, 03:40:33",16250.0,8381946.0,4,"In regards to identifiers, section 3.8.1.1 states, in part:"
Zipkin,60330998,60190181,0,"2020/02/21, 03:40:33",False,"2020/02/21, 03:40:33",16250.0,8381946.0,4,An identifier is an unlimited-length character sequence that must begin with a Java identifier start character; all following characters must be Java identifier part characters.
Zipkin,60330998,60190181,0,"2020/02/21, 03:40:33",False,"2020/02/21, 03:40:33",16250.0,8381946.0,4,An identifier start character is any character for which the method  Character.isJavaIdentifierStart  returns  true .
Zipkin,60330998,60190181,0,"2020/02/21, 03:40:33",False,"2020/02/21, 03:40:33",16250.0,8381946.0,4,This includes '_' and '$'.
Zipkin,60330998,60190181,0,"2020/02/21, 03:40:33",False,"2020/02/21, 03:40:33",16250.0,8381946.0,4,An identifier part character is any character for which the method   Character.isJavaIdentifierPart  returns  true .
Zipkin,60330998,60190181,0,"2020/02/21, 03:40:33",False,"2020/02/21, 03:40:33",16250.0,8381946.0,4,If you pass the character  -  into either  Character.isJavaIdentifierStart  or  Character.isJavaIdentifierPart  the return value is  false .
Zipkin,60330998,60190181,0,"2020/02/21, 03:40:33",False,"2020/02/21, 03:40:33",16250.0,8381946.0,4,"In other words,  the  -  character in the name of a message property violates the JMS specification  and therefore will cause an error."
Zipkin,60392406,60190181,0,"2020/02/25, 12:26:42",True,"2020/02/25, 12:26:42",98.0,11377504.0,1,From the error message its obvious that you are using qpid JMS client for communication through queues.
Zipkin,60392406,60190181,0,"2020/02/25, 12:26:42",True,"2020/02/25, 12:26:42",98.0,11377504.0,1,qpid client won’t allow any keys which violates java variable naming convention e.g.
Zipkin,60392406,60190181,0,"2020/02/25, 12:26:42",True,"2020/02/25, 12:26:42",98.0,11377504.0,1,"you won’t be able to send x-request-id in a queue’s header
which qpid jms client is consuming as it’ll throw error."
Zipkin,60392406,60190181,0,"2020/02/25, 12:26:42",True,"2020/02/25, 12:26:42",98.0,11377504.0,1,You need to take care of istio/zipkin to not to add certain headers (id you don’t need them actually) with the queue when its trying to communicate on azure bus.
Zipkin,60392406,60190181,0,"2020/02/25, 12:26:42",True,"2020/02/25, 12:26:42",98.0,11377504.0,1,So you have to disable the istio/zipkin libraries  to intercept the request for queues so that request to/from queue can be made without headers.
Zipkin,60392406,60190181,0,"2020/02/25, 12:26:42",True,"2020/02/25, 12:26:42",98.0,11377504.0,1,This will fix the issue.
Zipkin,57331090,57319678,1,"2019/08/02, 20:28:15",False,"2019/08/02, 20:28:15",103.0,4522858.0,2,"It the application.properties file for each eureka client ,  I added/changed"
Zipkin,57331090,57319678,1,"2019/08/02, 20:28:15",False,"2019/08/02, 20:28:15",103.0,4522858.0,2,------------------ client
Zipkin,57331090,57319678,1,"2019/08/02, 20:28:15",False,"2019/08/02, 20:28:15",103.0,4522858.0,2,-------------------- eureka server application.property--------------------
Zipkin,59930544,57319678,0,"2020/01/27, 13:47:06",False,"2020/09/28, 22:28:46",121.0,5897631.0,12,I was facing a similar issue where the eureka server was registering the services at host.docker.internal instead of localhost.
Zipkin,59930544,57319678,0,"2020/01/27, 13:47:06",False,"2020/09/28, 22:28:46",121.0,5897631.0,12,The issue in my case was an altered host file at location C:\Windows\System32\Drivers\etc\hosts.
Zipkin,59930544,57319678,0,"2020/01/27, 13:47:06",False,"2020/09/28, 22:28:46",121.0,5897631.0,12,I deleted all the lines in the host file and saved it using npp with admin privilege.
Zipkin,59930544,57319678,0,"2020/01/27, 13:47:06",False,"2020/09/28, 22:28:46",121.0,5897631.0,12,Restart the server post this change.
Zipkin,59930544,57319678,0,"2020/01/27, 13:47:06",False,"2020/09/28, 22:28:46",121.0,5897631.0,12,Looks like 'Docker Desktop' was changing the hostfile.
Zipkin,61323578,57319678,1,"2020/04/20, 16:24:19",False,"2020/04/20, 16:24:19",21.0,2579322.0,1,"""message"": ""Connection refused: no further information: host.docker.internal in eureka gateway error"
Zipkin,61323578,57319678,1,"2020/04/20, 16:24:19",False,"2020/04/20, 16:24:19",21.0,2579322.0,1,Resolution:
Zipkin,61323578,57319678,1,"2020/04/20, 16:24:19",False,"2020/04/20, 16:24:19",21.0,2579322.0,1,"check ping host.docker.internal
response is some ip addresses apart form local host i,e 127.0.0.1
remove the C:\Windows\System32\Drivers\etc\hosts.file entries , make it empty"
Zipkin,61323578,57319678,1,"2020/04/20, 16:24:19",False,"2020/04/20, 16:24:19",21.0,2579322.0,1,then restart eureka and your microservice instance.
Zipkin,61323578,57319678,1,"2020/04/20, 16:24:19",False,"2020/04/20, 16:24:19",21.0,2579322.0,1,also will find the message like below in the log this ensures you are registered in eureka
Zipkin,61323578,57319678,1,"2020/04/20, 16:24:19",False,"2020/04/20, 16:24:19",21.0,2579322.0,1,"DiscoveryClient_BEER-SERVICE/DESKTOP-G2AIGG1:beer-service:
splitting the above log message which denotes discovery client 
BEER-SERVICE is my service and 
DESKTOP-G2AIGG1 is my pc name
beer-service is the service registered."
Zipkin,63259763,57319678,1,"2020/08/05, 09:43:08",False,"2020/08/05, 09:43:08",3449.0,387417.0,0,"I was also facing the same issue, when I was loadbalancing my restTemplate."
Zipkin,63259763,57319678,1,"2020/08/05, 09:43:08",False,"2020/08/05, 09:43:08",3449.0,387417.0,0,Something like this
Zipkin,63259763,57319678,1,"2020/08/05, 09:43:08",False,"2020/08/05, 09:43:08",3449.0,387417.0,0,This is because of the ribbon client.
Zipkin,63259763,57319678,1,"2020/08/05, 09:43:08",False,"2020/08/05, 09:43:08",3449.0,387417.0,0,"So, without making any changes in the host file, when i deleted this code and made use of  RestTemplateBuilder  to get restTemplate, everything was working fine."
Zipkin,63259763,57319678,1,"2020/08/05, 09:43:08",False,"2020/08/05, 09:43:08",3449.0,387417.0,0,Code Example:
Zipkin,63259763,57319678,1,"2020/08/05, 09:43:08",False,"2020/08/05, 09:43:08",3449.0,387417.0,0,You can try this approach as well.
Zipkin,63283687,57319678,0,"2020/08/06, 15:30:53",True,"2020/08/06, 15:30:53",103.0,4522858.0,1,Thanks for the tip on the host file on windows
Zipkin,63283687,57319678,0,"2020/08/06, 15:30:53",True,"2020/08/06, 15:30:53",103.0,4522858.0,1,I found that docker adds aliases  in the host file for host.docker.internal and gateway.docker.internal.
Zipkin,63283687,57319678,0,"2020/08/06, 15:30:53",True,"2020/08/06, 15:30:53",103.0,4522858.0,1,I am guessing that Eureka does a host lookup from the IP and host.docker.internal is returned.
Zipkin,63283687,57319678,0,"2020/08/06, 15:30:53",True,"2020/08/06, 15:30:53",103.0,4522858.0,1,"I am not an expert at hosts files, but I added an alias for my actual host name to my IP (note: we use static ip's)."
Zipkin,63283687,57319678,0,"2020/08/06, 15:30:53",True,"2020/08/06, 15:30:53",103.0,4522858.0,1,"After doing this, docker did not change my host file on reboot and the reverse lookup of the ip-&gt;host now returns my machine name instead of host.docker.internal"
Zipkin,66593805,57319678,0,"2021/03/12, 05:34:31",False,"2021/03/12, 05:34:31",11.0,13366251.0,1,"Solution for Window 10:
You don't have to remove all the lines from hosts files."
Zipkin,66593805,57319678,0,"2021/03/12, 05:34:31",False,"2021/03/12, 05:34:31",11.0,13366251.0,1,"Just comment this if exists (#192.168.1.4 host.docker.internal) (as we use this when playing with docker)
And paste this (127.0.0.1   host.docker.internal)
It worked for me."
Zipkin,42982623,42982050,4,"2017/03/23, 19:08:47",True,"2017/03/23, 19:08:47",8336.0,1773866.0,1,You can use the new Dalston feature of using annotations on Spring Data repositories.
Zipkin,42982623,42982050,4,"2017/03/23, 19:08:47",True,"2017/03/23, 19:08:47",8336.0,1773866.0,1,You can check out this for more info  http://cloud.spring.io/spring-cloud-sleuth/spring-cloud-sleuth.html#_managing_spans_with_annotations
Zipkin,49212158,49211771,2,"2018/03/10, 19:47:36",True,"2018/03/10, 19:53:21",6010.0,5210117.0,5,This is really strange because you are using latest relase and in the GitHub spring-cloud-sleuth depends to  &lt;brave.version&gt;4.17.2&lt;/brave.version&gt; .
Zipkin,49212158,49211771,2,"2018/03/10, 19:47:36",True,"2018/03/10, 19:53:21",6010.0,5210117.0,5,And I think 4.16.3-SNAPSHOT version is not exists in the maven repo.
Zipkin,49212158,49211771,2,"2018/03/10, 19:47:36",True,"2018/03/10, 19:53:21",6010.0,5210117.0,5,(just checked 2.0.0.M8 depends to this version)
Zipkin,49212158,49211771,2,"2018/03/10, 19:47:36",True,"2018/03/10, 19:53:21",6010.0,5210117.0,5,If you change to  &lt;sleuth.version&gt;2.0.0.M7&lt;/sleuth.version&gt;  it does find the required dependencies.
Zipkin,49212158,49211771,2,"2018/03/10, 19:47:36",True,"2018/03/10, 19:53:21",6010.0,5210117.0,5,https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/pom.xml
Zipkin,49212538,49211771,1,"2018/03/10, 20:24:25",False,"2018/03/10, 22:18:03",8336.0,1773866.0,2,The M8 for sleuth was broken.
Zipkin,49212538,49211771,1,"2018/03/10, 20:24:25",False,"2018/03/10, 22:18:03",8336.0,1773866.0,2,That issue will be fixed in M9.
Zipkin,49212538,49211771,1,"2018/03/10, 20:24:25",False,"2018/03/10, 22:18:03",8336.0,1773866.0,2,You can use M8 but you have to explicitly change the brave version to some release one.
Zipkin,43232907,43230619,1,"2017/04/05, 16:39:23",True,"2017/04/05, 16:39:23",399.0,6921715.0,13,"For 1, 2, 3 it was because I was doing a new RestTemplate."
Zipkin,43232907,43230619,1,"2017/04/05, 16:39:23",True,"2017/04/05, 16:39:23",399.0,6921715.0,13,The doc says :
Zipkin,43232907,43230619,1,"2017/04/05, 16:39:23",True,"2017/04/05, 16:39:23",399.0,6921715.0,13,You have to register RestTemplate as a bean so that the interceptors will get injected.
Zipkin,43232907,43230619,1,"2017/04/05, 16:39:23",True,"2017/04/05, 16:39:23",399.0,6921715.0,13,If you create a RestTemplate instance with a new keyword then the instrumentation WILL NOT work.
Zipkin,43232907,43230619,1,"2017/04/05, 16:39:23",True,"2017/04/05, 16:39:23",399.0,6921715.0,13,"So RTFM for myself, and this solved my 3 first problems :"
Zipkin,30204797,23032944,4,"2015/05/13, 05:49:26",False,"2015/05/13, 05:49:26",15576.0,677561.0,1,The first step to good searching in elasticsearch is to create fields from your data.
Zipkin,30204797,23032944,4,"2015/05/13, 05:49:26",False,"2015/05/13, 05:49:26",15576.0,677561.0,1,"With logs, logstash is the proper tool."
Zipkin,30204797,23032944,4,"2015/05/13, 05:49:26",False,"2015/05/13, 05:49:26",15576.0,677561.0,1,The grok{} filter uses patterns (existing or user-defined regexps) to split the input into fields.
Zipkin,30204797,23032944,4,"2015/05/13, 05:49:26",False,"2015/05/13, 05:49:26",15576.0,677561.0,1,You would need to make sure that it was mapped to an integer (e.g.
Zipkin,30204797,23032944,4,"2015/05/13, 05:49:26",False,"2015/05/13, 05:49:26",15576.0,677561.0,1,%{INT:duration:int} in your pattern).
Zipkin,30204797,23032944,4,"2015/05/13, 05:49:26",False,"2015/05/13, 05:49:26",15576.0,677561.0,1,"You could then query elasticsearch for ""duration: 1000"" to get the results."
Zipkin,30204797,23032944,4,"2015/05/13, 05:49:26",False,"2015/05/13, 05:49:26",15576.0,677561.0,1,"Elasticsearch uses the lucene query engine, so you can find sample queries based on that."
Zipkin,19034042,19032203,5,"2013/09/26, 19:49:25",False,"2013/09/26, 20:56:50",15073.0,14032.0,8,Zipkin is the best solution.
Zipkin,19034042,19032203,5,"2013/09/26, 19:49:25",False,"2013/09/26, 20:56:50",15073.0,14032.0,8,--zipkin developer
Zipkin,19034042,19032203,5,"2013/09/26, 19:49:25",False,"2013/09/26, 20:56:50",15073.0,14032.0,8,"EDIT  - Ok ok, here's a serious answer:"
Zipkin,19034042,19032203,5,"2013/09/26, 19:49:25",False,"2013/09/26, 20:56:50",15073.0,14032.0,8,Zipkin is a distributed tracing system developed by Twitter because our service-oriented-architecture is so goddamned big that it's often hard to understand WTF is happening in any given request.
Zipkin,19034042,19032203,5,"2013/09/26, 19:49:25",False,"2013/09/26, 20:56:50",15073.0,14032.0,8,"Seriously, here's a visualization in Zipkin of all the services dependencies at twitter:"
Zipkin,19034042,19032203,5,"2013/09/26, 19:49:25",False,"2013/09/26, 20:56:50",15073.0,14032.0,8,Is your platform this intense?
Zipkin,19034042,19032203,5,"2013/09/26, 19:49:25",False,"2013/09/26, 20:56:50",15073.0,14032.0,8,You should use zipkin.
Zipkin,19034042,19032203,5,"2013/09/26, 19:49:25",False,"2013/09/26, 20:56:50",15073.0,14032.0,8,Did I mention it's one of the best scaling systems I've ever seen?
Zipkin,19034042,19032203,5,"2013/09/26, 19:49:25",False,"2013/09/26, 20:56:50",15073.0,14032.0,8,"It has zero problem keeping up with twitter-level load, and that might be important to you if you're that big."
Zipkin,19034042,19032203,5,"2013/09/26, 19:49:25",False,"2013/09/26, 20:56:50",15073.0,14032.0,8,What's that you say?
Zipkin,19034042,19032203,5,"2013/09/26, 19:49:25",False,"2013/09/26, 20:56:50",15073.0,14032.0,8,You're not as big as twitter?
Zipkin,19034042,19032203,5,"2013/09/26, 19:49:25",False,"2013/09/26, 20:56:50",15073.0,14032.0,8,"You only have three services: a web frontend, some kind of middleware, and your database backend?"
Zipkin,19034042,19032203,5,"2013/09/26, 19:49:25",False,"2013/09/26, 20:56:50",15073.0,14032.0,8,Maybe zipkin is a bit overkill for you.
Zipkin,19034042,19032203,5,"2013/09/26, 19:49:25",False,"2013/09/26, 20:56:50",15073.0,14032.0,8,"We've done some work to make it a bit easier to setup, but really my job isn't to make zipkin easy for you, it's to make zipkin awesome for Twitter."
Zipkin,19034042,19032203,5,"2013/09/26, 19:49:25",False,"2013/09/26, 20:56:50",15073.0,14032.0,8,"Still, if you plan on scaling scala, the twitter stack with Finagle etc is insanely good."
Zipkin,19034042,19032203,5,"2013/09/26, 19:49:25",False,"2013/09/26, 20:56:50",15073.0,14032.0,8,Don't let all the evangelists from Typesafe fool you.
Zipkin,19034042,19032203,5,"2013/09/26, 19:49:25",False,"2013/09/26, 20:56:50",15073.0,14032.0,8,Their stack has some serious deficiencies when you try to deploy it in massive-scale architectures.
Zipkin,19034042,19032203,5,"2013/09/26, 19:49:25",False,"2013/09/26, 20:56:50",15073.0,14032.0,8,"But again, our job isn't to tell you how good our stack is, or even help you use it."
Zipkin,19034042,19032203,5,"2013/09/26, 19:49:25",False,"2013/09/26, 20:56:50",15073.0,14032.0,8,It's to make our stack awesome.
Zipkin,56529683,56525260,0,"2019/06/10, 18:54:49",False,"2019/06/10, 18:54:49",525.0,4504053.0,10,"You can add the following setting on your properties key to disable zipkin,  source ."
Zipkin,56529683,56525260,0,"2019/06/10, 18:54:49",False,"2019/06/10, 18:54:49",525.0,4504053.0,10,"Better yet, create separate development properties (like  application-dev.properties ) to avoid changing above setting everytime you want to run in your machine:  https://stackoverflow.com/a/34846351/4504053"
Zipkin,47010922,47008485,4,"2017/10/30, 10:38:04",False,"2017/10/30, 10:38:04",8336.0,1773866.0,2,Most likely your code is broken.
Zipkin,47010922,47008485,4,"2017/10/30, 10:38:04",False,"2017/10/30, 10:38:04",8336.0,1773866.0,2,You can check out the  https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-zipkin/src/main/java/org/springframework/cloud/sleuth/zipkin/ZipkinAutoConfiguration.java  class where for Edgware we've added load balanced zipkin server resolution.
Zipkin,47139081,47008485,1,"2017/11/06, 16:23:51",True,"2017/11/06, 16:23:51",1260.0,4864870.0,0,Is it correct that you are using the code example from the baeldung tutorial?
Zipkin,47139081,47008485,1,"2017/11/06, 16:23:51",True,"2017/11/06, 16:23:51",1260.0,4864870.0,0,( http://www.baeldung.com/tracing-services-with-zipkin  - 3.2.
Zipkin,47139081,47008485,1,"2017/11/06, 16:23:51",True,"2017/11/06, 16:23:51",1260.0,4864870.0,0,Spring Config)
Zipkin,47139081,47008485,1,"2017/11/06, 16:23:51",True,"2017/11/06, 16:23:51",1260.0,4864870.0,0,I think there is a mistake with line 34 and 35 (the closing curly brace).
Zipkin,47139081,47008485,1,"2017/11/06, 16:23:51",True,"2017/11/06, 16:23:51",1260.0,4864870.0,0,I've fixed the problem by modifing the method like this:
Zipkin,47139081,47008485,1,"2017/11/06, 16:23:51",True,"2017/11/06, 16:23:51",1260.0,4864870.0,0,Maybe this helps someone or maybe someone from baeldung reads this and could verify and correct the code example.
Zipkin,47139081,47008485,1,"2017/11/06, 16:23:51",True,"2017/11/06, 16:23:51",1260.0,4864870.0,0,;)
Zipkin,44664880,44643259,3,"2017/06/21, 03:09:57",True,"2017/06/23, 08:31:36",nan,nan,4,Problem solved.
Zipkin,44664880,44643259,3,"2017/06/21, 03:09:57",True,"2017/06/23, 08:31:36",nan,nan,4,tracer.withSpanInScope(clientSpan)  would do the work.
Zipkin,44664880,44643259,3,"2017/06/21, 03:09:57",True,"2017/06/23, 08:31:36",nan,nan,4,"Note that,  withSpanInScope(...)  has not been called before sending messages ."
Zipkin,40358966,39870535,0,"2016/11/01, 13:16:38",False,"2016/11/01, 13:16:38",762.0,1227937.0,1,"Some people use zipkin to identify dead services, but probably metrics/stats would be the better route if you are trying to break down and report by thrift method."
Zipkin,19022596,19022191,0,"2013/09/26, 11:03:01",False,"2013/09/26, 11:03:01",4556.0,1097599.0,5,Well before starting digging into JVM stuff or setting up all the infrastructure needed by Zipkin you could simply start by measuring some application-level metrics.
Zipkin,19022596,19022191,0,"2013/09/26, 11:03:01",False,"2013/09/26, 11:03:01",4556.0,1097599.0,5,You could try the library  metrics  via this  scala api .
Zipkin,19022596,19022191,0,"2013/09/26, 11:03:01",False,"2013/09/26, 11:03:01",4556.0,1097599.0,5,Basically you manually set up counters and gauges at specific points of your application that will help you diagnose your bottleneck problem.
Zipkin,66205972,66167917,5,"2021/02/15, 12:01:11",True,"2021/03/01, 10:16:15",8336.0,1773866.0,2,The problem might be related to the fact that you're creating the Feign builder manually via  Feign.builder()  factory method.
Zipkin,66205972,66167917,5,"2021/02/15, 12:01:11",True,"2021/03/01, 10:16:15",8336.0,1773866.0,2,We're unable to instrument that call.
Zipkin,66205972,66167917,5,"2021/02/15, 12:01:11",True,"2021/03/01, 10:16:15",8336.0,1773866.0,2,You should create a bean (via  SleuthFeignBuilder.builder ) and inject that into your code.
Zipkin,62871239,62870927,19,"2020/07/13, 10:38:42",True,"2020/07/13, 10:38:42",26220.0,927493.0,3,Dependencies are resolved before plugins are executed.
Zipkin,62871239,62870927,19,"2020/07/13, 10:38:42",True,"2020/07/13, 10:38:42",26220.0,927493.0,3,So the properties you read with the properties-maven-plugin are not available in the  &lt;dependencies&gt;  section.
Zipkin,62871239,62870927,19,"2020/07/13, 10:38:42",True,"2020/07/13, 10:38:42",26220.0,927493.0,3,"If you want to set a dependency version by property, this property must be set inside the POM, on the command line or in the  settings.xml ."
Zipkin,62871339,62870927,0,"2020/07/13, 10:45:40",False,"2020/07/13, 10:53:28",1709.0,8484783.0,-1,"You are using an old version of the plugin ( 1.0-alpha-2 ), update it to the  latest   1.0.0 ."
Zipkin,62871339,62870927,0,"2020/07/13, 10:45:40",False,"2020/07/13, 10:53:28",1709.0,8484783.0,-1,Then make sure that the file  version.properties  is in the folder  C:\Workspace .
Zipkin,62871339,62870927,0,"2020/07/13, 10:45:40",False,"2020/07/13, 10:53:28",1709.0,8484783.0,-1,"Anyway, with the latest version of the plugin you should get a proper error message if it can't find the file."
Zipkin,62871339,62870927,0,"2020/07/13, 10:45:40",False,"2020/07/13, 10:53:28",1709.0,8484783.0,-1,One more suggestion:  spring-cloud-starter-zipkin  belongs to the  org.springframework.cloud  group which follows another version.
Zipkin,62871339,62870927,0,"2020/07/13, 10:45:40",False,"2020/07/13, 10:53:28",1709.0,8484783.0,-1,The suggested way to declare that dependency is like the following:
Zipkin,62756345,62674846,2,"2020/07/06, 15:42:41",False,"2020/07/06, 15:42:41",50.0,11866103.0,1,I was able to setup OpenCensus in GCP (in an instance on the project) by following the steps  mentioned here .
Zipkin,62756345,62674846,2,"2020/07/06, 15:42:41",False,"2020/07/06, 15:42:41",50.0,11866103.0,1,"To set it up quickly, here are the commands I ran in a brand new Ubuntu instance"
Zipkin,59925933,59924890,2,"2020/01/27, 08:15:42",False,"2020/01/27, 08:20:48",26898.0,1192728.0,0,You should use  egress-gateway .
Zipkin,59925933,59924890,2,"2020/01/27, 08:15:42",False,"2020/01/27, 08:20:48",26898.0,1192728.0,0,"When all external calls go to the gateway, istio can get the metadata and does some tracing works."
Zipkin,59925933,59924890,2,"2020/01/27, 08:15:42",False,"2020/01/27, 08:20:48",26898.0,1192728.0,0,There are many advantages when using ingress/egress gateway:
Zipkin,59985374,59924890,0,"2020/01/30, 13:47:03",False,"2020/01/30, 13:47:03",5749.0,11977760.0,0,Based on  envoy documentation  it doesn't support https tracing.
Zipkin,59985374,59924890,0,"2020/01/30, 13:47:03",False,"2020/01/30, 13:47:03",5749.0,11977760.0,0,The tracing configuration specifies global settings for the HTTP tracer used by Envoy.
Zipkin,59985374,59924890,0,"2020/01/30, 13:47:03",False,"2020/01/30, 13:47:03",5749.0,11977760.0,0,The configuration is defined by the Bootstrap tracing field.
Zipkin,59985374,59924890,0,"2020/01/30, 13:47:03",False,"2020/01/30, 13:47:03",5749.0,11977760.0,0,"Envoy may support other tracers in the future, but right now the HTTP tracer is the only one supported."
Zipkin,59985374,59924890,0,"2020/01/30, 13:47:03",False,"2020/01/30, 13:47:03",5749.0,11977760.0,0,And this post on  stackoverflow
Zipkin,59985374,59924890,0,"2020/01/30, 13:47:03",False,"2020/01/30, 13:47:03",5749.0,11977760.0,0,"HTTPS (HTTP over SSL) sends all HTTP content over a SSL tunel, so HTTP content and headers are encrypted as well."
Zipkin,59985374,59924890,0,"2020/01/30, 13:47:03",False,"2020/01/30, 13:47:03",5749.0,11977760.0,0,"I have even tried to reproduce that, but like in your case zipkin worked only for http."
Zipkin,59985374,59924890,0,"2020/01/30, 13:47:03",False,"2020/01/30, 13:47:03",5749.0,11977760.0,0,Based on that I would say it's not possible to use zipkin to track https.
Zipkin,58562527,58562126,2,"2019/10/25, 19:38:58",False,"2019/10/25, 19:38:58",6038.0,11032044.0,0,It's because you haven't mentioned the  host  here:
Zipkin,58581451,58562126,1,"2019/10/27, 18:52:20",False,"2019/10/27, 18:52:20",1017.0,6713869.0,0,"First, previous answer is wrong, you don't need to specify  host  it is not mandatory unless you want to set up a DNS."
Zipkin,58581451,58562126,1,"2019/10/27, 18:52:20",False,"2019/10/27, 18:52:20",1017.0,6713869.0,0,"Second, the backend  zipkin  requires the  /zipkin  URI to respond right?"
Zipkin,58581451,58562126,1,"2019/10/27, 18:52:20",False,"2019/10/27, 18:52:20",1017.0,6713869.0,0,"If this is the case, then the rewrite annotation is removing the URI."
Zipkin,58581451,58562126,1,"2019/10/27, 18:52:20",False,"2019/10/27, 18:52:20",1017.0,6713869.0,0,So you would need to change your yaml like this to pass  /zipkin  to your backend.
Zipkin,58591046,58562126,0,"2019/10/28, 14:35:42",False,"2019/10/28, 14:35:42",2049.0,11207414.0,0,Just to clarify the OP problem.
Zipkin,58591046,58562126,0,"2019/10/28, 14:35:42",False,"2019/10/28, 14:35:42",2049.0,11207414.0,0,There are different  ingress Controllers
Zipkin,58591046,58562126,0,"2019/10/28, 14:35:42",False,"2019/10/28, 14:35:42",2049.0,11207414.0,0,Note:
Zipkin,58591046,58562126,0,"2019/10/28, 14:35:42",False,"2019/10/28, 14:35:42",2049.0,11207414.0,0,"When you create an ingress, you should annotate each ingress with the appropriate ingress.class to indicate which ingress controller should be used if more than one exists within your cluster."
Zipkin,58591046,58562126,0,"2019/10/28, 14:35:42",False,"2019/10/28, 14:35:42",2049.0,11207414.0,0,"If you do not define a class, your cloud provider may use a default ingress controller."
Zipkin,58591046,58562126,0,"2019/10/28, 14:35:42",False,"2019/10/28, 14:35:42",2049.0,11207414.0,0,"Ideally, all ingress controllers should fulfill this specification, but the various ingress controllers operate slightly differently."
Zipkin,58591046,58562126,0,"2019/10/28, 14:35:42",False,"2019/10/28, 14:35:42",2049.0,11207414.0,0,Using this annotation:
Zipkin,58591046,58562126,0,"2019/10/28, 14:35:42",False,"2019/10/28, 14:35:42",2049.0,11207414.0,0,It looks like you are using NGINX Ingress Controller provided by nginxinc.
Zipkin,58591046,58562126,0,"2019/10/28, 14:35:42",False,"2019/10/28, 14:35:42",2049.0,11207414.0,0,You can find more information about  Rewrites Support  for NGINX Ingress Controller provided by  nginxinc here .
Zipkin,58591046,58562126,0,"2019/10/28, 14:35:42",False,"2019/10/28, 14:35:42",2049.0,11207414.0,0,example:
Zipkin,58591046,58562126,0,"2019/10/28, 14:35:42",False,"2019/10/28, 14:35:42",2049.0,11207414.0,0,It's different from the kubernetes community at  kubernetes/ingress-nginx repo .
Zipkin,58591046,58562126,0,"2019/10/28, 14:35:42",False,"2019/10/28, 14:35:42",2049.0,11207414.0,0,Different ingress controllers have different configs and annotations.
Zipkin,58591046,58562126,0,"2019/10/28, 14:35:42",False,"2019/10/28, 14:35:42",2049.0,11207414.0,0,So for this example:
Zipkin,58591046,58562126,0,"2019/10/28, 14:35:42",False,"2019/10/28, 14:35:42",2049.0,11207414.0,0,Test it:
Zipkin,57856470,57856249,2,"2019/09/09, 17:52:26",True,"2019/09/10, 12:40:49",8336.0,1773866.0,3,If S1 sends a request to S2 it's the S1 that sets the sampler value and S2 just continues the result.
Zipkin,57856470,57856249,2,"2019/09/09, 17:52:26",True,"2019/09/10, 12:40:49",8336.0,1773866.0,3,"In other words S1 will export to zipkin 100 * 0.1 = 10 requests, and S2 will export 100 * 0.1 = 10 requests."
Zipkin,57856470,57856249,2,"2019/09/09, 17:52:26",True,"2019/09/10, 12:40:49",8336.0,1773866.0,3,S1 makes a decision and S2 will continue it.
Zipkin,55661357,55575721,0,"2019/04/13, 04:59:36",False,"2019/04/13, 04:59:36",123.0,10593981.0,1,"FYI, this worked after generating dummy X-B3-SpanId; it works as long as X-B3-TraceId is unique."
Zipkin,55661357,55575721,0,"2019/04/13, 04:59:36",False,"2019/04/13, 04:59:36",123.0,10593981.0,1,e.g.
Zipkin,59222522,55571957,3,"2019/12/07, 04:36:36",True,"2019/12/07, 05:27:09",26.0,12494357.0,1,"I know this is old but I have just had exactly the same problem, and have just worked out it's being caused by the appmetrics libraries."
Zipkin,59222522,55571957,3,"2019/12/07, 04:36:36",True,"2019/12/07, 05:27:09",26.0,12494357.0,1,Once I figure out how to get it working I'll update this.
Zipkin,59222522,55571957,3,"2019/12/07, 04:36:36",True,"2019/12/07, 05:27:09",26.0,12494357.0,1,EDIT:
Zipkin,59222522,55571957,3,"2019/12/07, 04:36:36",True,"2019/12/07, 05:27:09",26.0,12494357.0,1,OK managed to get it working with appmetrics-dash.
Zipkin,59222522,55571957,3,"2019/12/07, 04:36:36",True,"2019/12/07, 05:27:09",26.0,12494357.0,1,You need to use monitor() instead of attach() and move the monitor to the end of your routes as so.
Zipkin,59222522,55571957,3,"2019/12/07, 04:36:36",True,"2019/12/07, 05:27:09",26.0,12494357.0,1,I have investigated appmetrics-prometheus and it only has an attach() at this stage so can't be used:
Zipkin,55173328,54702777,0,"2019/03/15, 01:07:05",False,"2019/03/15, 01:14:36",nan,nan,1,"Alright, so I'm going to answer this based on what you said here:"
Zipkin,55173328,54702777,0,"2019/03/15, 01:07:05",False,"2019/03/15, 01:14:36",nan,nan,1,Or a better approach  if there aint any support/ plugin for the same.
Zipkin,55173328,54702777,0,"2019/03/15, 01:07:05",False,"2019/03/15, 01:14:36",nan,nan,1,"The way that I do it us through  Prometheus , in combination with  cloudwatch_exporter , and  alertmanager ."
Zipkin,55173328,54702777,0,"2019/03/15, 01:07:05",False,"2019/03/15, 01:14:36",nan,nan,1,"The configuration for  cloudwatch_exporter  to monitor SQS is going to be something like (this is only two metrics, you'll need to add more based on what you're looking to monitor):"
Zipkin,55173328,54702777,0,"2019/03/15, 01:07:05",False,"2019/03/15, 01:14:36",nan,nan,1,"You'll then need to configure prometheus to scrape the  cloudwatch_exporter  endpoint at an interval, for ex what I do:"
Zipkin,55173328,54702777,0,"2019/03/15, 01:07:05",False,"2019/03/15, 01:14:36",nan,nan,1,You would then configure  alertmanager  to alert based on those scraped metrics; I do not alert on those metrics so I cannot give you an example.
Zipkin,55173328,54702777,0,"2019/03/15, 01:07:05",False,"2019/03/15, 01:14:36",nan,nan,1,"But, to give you an idea how of this architecture, a diagram is below:"
Zipkin,55173328,54702777,0,"2019/03/15, 01:07:05",False,"2019/03/15, 01:14:36",nan,nan,1,If you need to use something like  statsd  you can use  statsd_exporter .
Zipkin,55173328,54702777,0,"2019/03/15, 01:07:05",False,"2019/03/15, 01:14:36",nan,nan,1,"And, just in-case you were wondering, yes  Grafana supports prometheus ."
Zipkin,53845941,53765439,0,"2018/12/19, 08:47:57",False,"2018/12/19, 09:03:57",533.0,5736574.0,2,"As there is a bug in Spring AMQP, which will be fixed in Release 2.1.3 
 Issue link"
Zipkin,53845941,53765439,0,"2018/12/19, 08:47:57",False,"2018/12/19, 09:03:57",533.0,5736574.0,2,"For a tempory fix, you can enable retry properties to create advice chain."
Zipkin,53845941,53765439,0,"2018/12/19, 08:47:57",False,"2018/12/19, 09:03:57",533.0,5736574.0,2,Hope this resolves your problem.
Zipkin,53967904,53765439,0,"2018/12/29, 10:21:00",False,"2018/12/29, 10:27:20",53.0,2842281.0,1,"I had this same issue, changing Spring Boot version to 2.1.0.RELEASE did the trick for me."
Zipkin,53967904,53765439,0,"2018/12/29, 10:21:00",False,"2018/12/29, 10:27:20",53.0,2842281.0,1,You should try it too.
Zipkin,53967904,53765439,0,"2018/12/29, 10:21:00",False,"2018/12/29, 10:27:20",53.0,2842281.0,1,There must be something wrong with  RabbitMQ in Spring Boot version 2.1.1.RELEASE.
Zipkin,55292618,53765439,0,"2019/03/22, 05:48:23",False,"2019/03/22, 05:48:23",11.0,9568260.0,1,add build.gradle
Zipkin,55292618,53765439,0,"2019/03/22, 05:48:23",False,"2019/03/22, 05:48:23",11.0,9568260.0,1,apply plugin: 'org.springframework.boot'
Zipkin,55292618,53765439,0,"2019/03/22, 05:48:23",False,"2019/03/22, 05:48:23",11.0,9568260.0,1,"springBootVersion=2.1.3.RELEASE
springCloudVersion=Greenwich.RELEASE"
Zipkin,52505155,52486463,1,"2018/09/25, 22:03:18",False,"2018/09/25, 22:03:18",5434.0,7059.0,0,"Finally got it to work removing  @AutoConfigureAfter ,  @CondtionnalOnBean  and  @ConditionnalOnMissingBean , using instead  @ConditionalOnClass ,  @ConditionnalOnMissingClass  and reproducing other  @Conditionnals  from  TraceAutoConfiguration ."
Zipkin,52505155,52486463,1,"2018/09/25, 22:03:18",False,"2018/09/25, 22:03:18",5434.0,7059.0,0,"Not great, but at least working."
Zipkin,51938178,51931755,0,"2018/08/20, 23:37:41",False,"2018/08/20, 23:37:41",8855.0,9705485.0,1,I think that Christian Posta article you refer to is very good.
Zipkin,51938178,51931755,0,"2018/08/20, 23:37:41",False,"2018/08/20, 23:37:41",8855.0,9705485.0,1,"As he says, you can deal with the most common use-cases with the out of the box Kubernetes solutions for discovery (kub dns), load-balancing (with Services) and edge services/gateway (Ingress)."
Zipkin,51938178,51931755,0,"2018/08/20, 23:37:41",False,"2018/08/20, 23:37:41",8855.0,9705485.0,1,"As Christian also points out, if you need to dynamically discover services by actively querying rather than knowing what you are looking for then Spring Cloud Kubernetes can be better than going directly to Kubernetes Apis."
Zipkin,51938178,51931755,0,"2018/08/20, 23:37:41",False,"2018/08/20, 23:37:41",8855.0,9705485.0,1,If you need to refresh your app from a config change and see it update quickly without going through a rolling update (which would be needed if you were mounting the configmap as a volume) then Spring cloud Kubernetes config client could be of value.
Zipkin,51938178,51931755,0,"2018/08/20, 23:37:41",False,"2018/08/20, 23:37:41",8855.0,9705485.0,1,The ribbon integration could also be of value if you need client-side load-balancing.
Zipkin,51938178,51931755,0,"2018/08/20, 23:37:41",False,"2018/08/20, 23:37:41",8855.0,9705485.0,1,So you could start out without Spring Cloud Kubernetes and add parts of it if and when you find that it would help.
Zipkin,51938178,51931755,0,"2018/08/20, 23:37:41",False,"2018/08/20, 23:37:41",8855.0,9705485.0,1,I think it is better to think of the project as adding extra options and conveniences rather than alternatives to Kubernetes-native solutions.
Zipkin,51938178,51931755,0,"2018/08/20, 23:37:41",False,"2018/08/20, 23:37:41",8855.0,9705485.0,1,It is also worth noting that you can deploy a Netflix stack app to Kubernetes (including using Zuul and eureka) and there isn't necessarily anything wrong with that.
Zipkin,51938178,51931755,0,"2018/08/20, 23:37:41",False,"2018/08/20, 23:37:41",8855.0,9705485.0,1,It has the advantage that you can work with it outside Kubernetes and it might be more convenient for your particular team if it's Java team.
Zipkin,51938178,51931755,0,"2018/08/20, 23:37:41",False,"2018/08/20, 23:37:41",8855.0,9705485.0,1,"The main downside is that the Netflix stack is very tied to Java, whereas Kubernetes is language neutral."
Zipkin,50941229,50940162,3,"2018/06/20, 08:59:58",False,"2018/06/21, 08:11:08",491.0,2389992.0,0,We had this very similar issue with Akka.
Zipkin,50941229,50940162,3,"2018/06/20, 08:59:58",False,"2018/06/21, 08:11:08",491.0,2389992.0,0,We observed huge delay in ask pattern to deliver messages to the target actor on peek load.
Zipkin,50941229,50940162,3,"2018/06/20, 08:59:58",False,"2018/06/21, 08:11:08",491.0,2389992.0,0,Most of these issues are related to heap memory consumption and not because of usages of dispatchers.
Zipkin,50941229,50940162,3,"2018/06/20, 08:59:58",False,"2018/06/21, 08:11:08",491.0,2389992.0,0,Finally we fixed these issues by tuning some of the below configuration and changes.
Zipkin,50941229,50940162,3,"2018/06/20, 08:59:58",False,"2018/06/21, 08:11:08",491.0,2389992.0,0,1) Make sure you stop entities/actors which are no longer required.
Zipkin,50941229,50940162,3,"2018/06/20, 08:59:58",False,"2018/06/21, 08:11:08",491.0,2389992.0,0,If its a persistent actor then you can always bring it back when you need it.
Zipkin,50941229,50940162,3,"2018/06/20, 08:59:58",False,"2018/06/21, 08:11:08",491.0,2389992.0,0,Refer :  https://doc.akka.io/docs/akka/current/cluster-sharding.html#passivation
Zipkin,50941229,50940162,3,"2018/06/20, 08:59:58",False,"2018/06/21, 08:11:08",491.0,2389992.0,0,2) If you are using cluster sharding then check the akka.cluster.sharding.state-store-mode.
Zipkin,50941229,50940162,3,"2018/06/20, 08:59:58",False,"2018/06/21, 08:11:08",491.0,2389992.0,0,By changing this to persistence we gained 50% more TPS.
Zipkin,50941229,50940162,3,"2018/06/20, 08:59:58",False,"2018/06/21, 08:11:08",491.0,2389992.0,0,3) Minimize your log entries (set it to info level).
Zipkin,50941229,50940162,3,"2018/06/20, 08:59:58",False,"2018/06/21, 08:11:08",491.0,2389992.0,0,4) Tune your logs to publish messages frequently to your logging system.
Zipkin,50941229,50940162,3,"2018/06/20, 08:59:58",False,"2018/06/21, 08:11:08",491.0,2389992.0,0,"Update the batch size, batch count and interval accordingly."
Zipkin,50941229,50940162,3,"2018/06/20, 08:59:58",False,"2018/06/21, 08:11:08",491.0,2389992.0,0,So that the memory is freed.
Zipkin,50941229,50940162,3,"2018/06/20, 08:59:58",False,"2018/06/21, 08:11:08",491.0,2389992.0,0,In our case huge heap memory is used for buffering the log messages and send in bulk.
Zipkin,50941229,50940162,3,"2018/06/20, 08:59:58",False,"2018/06/21, 08:11:08",491.0,2389992.0,0,If the interval is more then you may fill your heap memory and that affects the performance (more GC activity required).
Zipkin,50941229,50940162,3,"2018/06/20, 08:59:58",False,"2018/06/21, 08:11:08",491.0,2389992.0,0,5) Run blocking operations on a separate dispatcher.
Zipkin,50941229,50940162,3,"2018/06/20, 08:59:58",False,"2018/06/21, 08:11:08",491.0,2389992.0,0,6) Use custom serializers (protobuf) and avoid JavaSerializer.
Zipkin,50941229,50940162,3,"2018/06/20, 08:59:58",False,"2018/06/21, 08:11:08",491.0,2389992.0,0,7) Add the below JAVA_OPTS to your jar
Zipkin,50941229,50940162,3,"2018/06/20, 08:59:58",False,"2018/06/21, 08:11:08",491.0,2389992.0,0,"export JAVA_OPTS=""$JAVA_OPTS -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -XX:MaxRAMFraction=2 -Djava.security.egd=file:/dev/./urandom"""
Zipkin,50941229,50940162,3,"2018/06/20, 08:59:58",False,"2018/06/21, 08:11:08",491.0,2389992.0,0,The main thing is XX:MaxRAMFraction=2 which will utilize more than 60% of available memory.
Zipkin,50941229,50940162,3,"2018/06/20, 08:59:58",False,"2018/06/21, 08:11:08",491.0,2389992.0,0,"By default its 4 means your application will use only one fourth of the available memory, which might not be sufficient."
Zipkin,50941229,50940162,3,"2018/06/20, 08:59:58",False,"2018/06/21, 08:11:08",491.0,2389992.0,0,Refer :  https://blog.csanchez.org/2017/05/31/running-a-jvm-in-a-container-without-getting-killed/
Zipkin,50941229,50940162,3,"2018/06/20, 08:59:58",False,"2018/06/21, 08:11:08",491.0,2389992.0,0,"Regards,"
Zipkin,50941229,50940162,3,"2018/06/20, 08:59:58",False,"2018/06/21, 08:11:08",491.0,2389992.0,0,Vinoth
Zipkin,50585875,50562296,0,"2018/05/29, 16:41:00",True,"2018/05/30, 12:06:27",2002.0,9524052.0,1,"As @Bal Chua and @Pär Nilsson mentioned, for environmental variables you can use only string variables because Linux environmental variables can be only strings."
Zipkin,50585875,50562296,0,"2018/05/29, 16:41:00",True,"2018/05/30, 12:06:27",2002.0,9524052.0,1,"So, if you use yaml, you need to place value into quotes to force Kubernetes to use string."
Zipkin,50585875,50562296,0,"2018/05/29, 16:41:00",True,"2018/05/30, 12:06:27",2002.0,9524052.0,1,For example:
Zipkin,49881877,49880941,2,"2018/04/17, 18:17:50",True,"2018/07/27, 14:22:52",5965.0,1235935.0,8,"Even when you use Spring Cloud, 100 services do NOT mean 100 servers."
Zipkin,49881877,49880941,2,"2018/04/17, 18:17:50",True,"2018/07/27, 14:22:52",5965.0,1235935.0,8,In Spring Cloud the packaging unit is Spring Boot application and a single server may host many such Spring Boot applications.
Zipkin,49881877,49880941,2,"2018/04/17, 18:17:50",True,"2018/07/27, 14:22:52",5965.0,1235935.0,8,"If you want, you can containerize the Spring Boot applications and other Spring Cloud infrastructure support components."
Zipkin,49881877,49880941,2,"2018/04/17, 18:17:50",True,"2018/07/27, 14:22:52",5965.0,1235935.0,8,But that is not Kubernetes.
Zipkin,49881877,49880941,2,"2018/04/17, 18:17:50",True,"2018/07/27, 14:22:52",5965.0,1235935.0,8,"If you move to Kubernetes, you don't need the infrastructure support services like Zuul, Ribbon etc."
Zipkin,49881877,49880941,2,"2018/04/17, 18:17:50",True,"2018/07/27, 14:22:52",5965.0,1235935.0,8,"because Kubernetes has its own components for service discovery, gateway, load balancer etc."
Zipkin,49881877,49880941,2,"2018/04/17, 18:17:50",True,"2018/07/27, 14:22:52",5965.0,1235935.0,8,"In Kubernetes, the packaging unit is Docker images and one or more Docker containers can be put inside one pod which is the minimal scaling unit."
Zipkin,49881877,49880941,2,"2018/04/17, 18:17:50",True,"2018/07/27, 14:22:52",5965.0,1235935.0,8,"So, Kubernetes has a different set of components to manage the Microservices."
Zipkin,49881877,49880941,2,"2018/04/17, 18:17:50",True,"2018/07/27, 14:22:52",5965.0,1235935.0,8,Kubernetes is a different platform than Spring cloud.
Zipkin,49881877,49880941,2,"2018/04/17, 18:17:50",True,"2018/07/27, 14:22:52",5965.0,1235935.0,8,Both have the same objectives.
Zipkin,49881877,49880941,2,"2018/04/17, 18:17:50",True,"2018/07/27, 14:22:52",5965.0,1235935.0,8,"However, Kubernetes has some additional features like self healing, auto-scaling, rolling updates, compute resource management, deployments etc."
Zipkin,50084521,49880941,0,"2018/04/29, 10:57:09",False,"2018/04/29, 10:57:09",8855.0,9705485.0,2,"Just to add to saptarshi basu's answer, you might want to look at  https://dzone.com/articles/deploying-microservices-spring-cloud-vs-kubernetes  as it walks through the comparison and asks which responsibilities you might want to be handled by which components when using Spring cloud on kubernetes"
Zipkin,49687046,49686743,0,"2018/04/06, 09:52:45",True,"2018/04/06, 09:52:45",8336.0,1773866.0,1,If you're using Sleuth 2.0 you can call on the  Tracer  a method to create a new trace.
Zipkin,49687046,49686743,0,"2018/04/06, 09:52:45",True,"2018/04/06, 09:52:45",8336.0,1773866.0,1,In the older version of sleuth I guess what I'd do is to use an executor that is  NOT  a bean.
Zipkin,49687046,49686743,0,"2018/04/06, 09:52:45",True,"2018/04/06, 09:52:45",8336.0,1773866.0,1,That way you would lose the trace and it would get restarted at some point (by rest template or sth like that).
Zipkin,45369348,45368147,4,"2017/07/28, 12:08:22",False,"2017/07/28, 12:08:22",8336.0,1773866.0,3,Thanks for the kind words!
Zipkin,45369348,45368147,4,"2017/07/28, 12:08:22",False,"2017/07/28, 12:08:22",8336.0,1773866.0,3,In Sleuth Edgware we will support Reactor -  https://github.com/spring-cloud/spring-cloud-sleuth/tree/master/spring-cloud-sleuth-reactor  and in Sleuth Finchley we will support reactor and webflux  https://github.com/spring-cloud/spring-cloud-sleuth/blob/2.0.x/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/instrument/web/TraceWebFluxAutoConfiguration.java .
Zipkin,45369348,45368147,4,"2017/07/28, 12:08:22",False,"2017/07/28, 12:08:22",8336.0,1773866.0,3,In other words it's already possible to use Sleuth in the reactive context.
Zipkin,43982611,43937265,1,"2017/05/15, 17:54:11",False,"2017/05/15, 17:54:11",473.0,2733462.0,1,It seems like you are using  Sleuth with Zipkin via HTTP .
Zipkin,43982611,43937265,1,"2017/05/15, 17:54:11",False,"2017/05/15, 17:54:11",473.0,2733462.0,1,You can try the  Sleuth with Zipkin via Spring Cloud Stream  approach.
Zipkin,43982611,43937265,1,"2017/05/15, 17:54:11",False,"2017/05/15, 17:54:11",473.0,2733462.0,1,"I haven't done the benchmark myself, but it should improve the performance in theory."
Zipkin,43982611,43937265,1,"2017/05/15, 17:54:11",False,"2017/05/15, 17:54:11",473.0,2733462.0,1,Please see the documentation at:  https://cloud.spring.io/spring-cloud-sleuth/spring-cloud-sleuth.html#_sleuth_with_zipkin_via_spring_cloud_stream
Zipkin,44062172,43937265,3,"2017/05/19, 08:36:59",False,"2017/05/19, 08:36:59",8336.0,1773866.0,1,I wonder what kind of a benchmarking method you have picked.
Zipkin,44062172,43937265,3,"2017/05/19, 08:36:59",False,"2017/05/19, 08:36:59",8336.0,1773866.0,1,Which version of Sleuth are you using?
Zipkin,44062172,43937265,3,"2017/05/19, 08:36:59",False,"2017/05/19, 08:36:59",8336.0,1773866.0,1,Also is this one single benchmark that you're doing?
Zipkin,44062172,43937265,3,"2017/05/19, 08:36:59",False,"2017/05/19, 08:36:59",8336.0,1773866.0,1,Is it on your computer?
Zipkin,44062172,43937265,3,"2017/05/19, 08:36:59",False,"2017/05/19, 08:36:59",8336.0,1773866.0,1,Has the JVM gotten heated up?
Zipkin,44062172,43937265,3,"2017/05/19, 08:36:59",False,"2017/05/19, 08:36:59",8336.0,1773866.0,1,Are there any other processes executed?
Zipkin,44062172,43937265,3,"2017/05/19, 08:36:59",False,"2017/05/19, 08:36:59",8336.0,1773866.0,1,Doing benchmarking is not that easy... You can use tools like JMH to do it better.
Zipkin,44062172,43937265,3,"2017/05/19, 08:36:59",False,"2017/05/19, 08:36:59",8336.0,1773866.0,1,BTW try turning off the DEBUG logging level and check the results again.
Zipkin,44062172,43937265,3,"2017/05/19, 08:36:59",False,"2017/05/19, 08:36:59",8336.0,1773866.0,1,We are performing benchmark tests of Sleuth and from what we see when adding Sleuth the latency gets increased by around 20 ms. Definitely not 600 ms.
Zipkin,59895808,43937265,0,"2020/01/24, 13:46:12",True,"2020/01/24, 13:46:12",131.0,9437494.0,2,I belive you had problem with: org.springframework.cloud.sleuth.zipkin.ServerPropertiesEndpointLocator.
Zipkin,59895808,43937265,0,"2020/01/24, 13:46:12",True,"2020/01/24, 13:46:12",131.0,9437494.0,2,It uses InetUtils.findFirstNonLoopbackAddress() to determine instance address.
Zipkin,59895808,43937265,0,"2020/01/24, 13:46:12",True,"2020/01/24, 13:46:12",131.0,9437494.0,2,Method is called synchronously when each span is close (in ZipkinSpanListener#convert).
Zipkin,59895808,43937265,0,"2020/01/24, 13:46:12",True,"2020/01/24, 13:46:12",131.0,9437494.0,2,The workaround is to create custom org.springframework.cloud.sleuth.zipkin.EndpointLocator.
Zipkin,59895808,43937265,0,"2020/01/24, 13:46:12",True,"2020/01/24, 13:46:12",131.0,9437494.0,2,You can use something like that:
Zipkin,59895808,43937265,0,"2020/01/24, 13:46:12",True,"2020/01/24, 13:46:12",131.0,9437494.0,2,And combine it with one of existing EndpointLocators.
Zipkin,59895808,43937265,0,"2020/01/24, 13:46:12",True,"2020/01/24, 13:46:12",131.0,9437494.0,2,You can find them in: org.springframework.cloud.sleuth.zipkin.ZipkinAutoConfiguration.
Zipkin,59895808,43937265,0,"2020/01/24, 13:46:12",True,"2020/01/24, 13:46:12",131.0,9437494.0,2,This issue is already fixed in sleuth 2.X.X.
Zipkin,59895808,43937265,0,"2020/01/24, 13:46:12",True,"2020/01/24, 13:46:12",131.0,9437494.0,2,Where: org.springframework.cloud.sleuth.zipkin2.DefaultEndpointLocator caches server address:
Zipkin,40117668,40090893,1,"2016/10/18, 23:39:29",True,"2016/10/18, 23:39:29",257711.0,526535.0,2,The web app is trying to access  config.json  at root (accessing as  /config.json  vs just  config.json  ) - that is  http://localhost:8001/config.json  .
Zipkin,40117668,40090893,1,"2016/10/18, 23:39:29",True,"2016/10/18, 23:39:29",257711.0,526535.0,2,This would obviously be wrong as it should be  http://localhost:8001/api/v1/proxy/namespaces/default/services/zipkin-http/config.json
Zipkin,40117668,40090893,1,"2016/10/18, 23:39:29",True,"2016/10/18, 23:39:29",257711.0,526535.0,2,There is a very simple solution for this - just run:
Zipkin,40117668,40090893,1,"2016/10/18, 23:39:29",True,"2016/10/18, 23:39:29",257711.0,526535.0,2,Now just go to  http://localhost:9411  and the UI should be up (tried and verified.)
Zipkin,40117668,40090893,1,"2016/10/18, 23:39:29",True,"2016/10/18, 23:39:29",257711.0,526535.0,2,You can get the name of the pod by doing  kubectl get pods
Zipkin,40117668,40090893,1,"2016/10/18, 23:39:29",True,"2016/10/18, 23:39:29",257711.0,526535.0,2,"PS:  kubectl proxy  is generally meant to access the Kubernetes API, and  kube port-forward  is the right tool in this case."
Zipkin,45769658,38019422,0,"2017/08/19, 11:59:32",False,"2017/08/19, 11:59:32",117.0,916394.0,0,I'm not sure this is the right way to do it but this should normally works
Zipkin,61367030,56328934,0,"2020/04/22, 17:01:23",False,"2020/04/22, 17:01:23",2849.0,797243.0,0,I have seen this issue a lot.
Zipkin,61367030,56328934,0,"2020/04/22, 17:01:23",False,"2020/04/22, 17:01:23",2849.0,797243.0,0,"From my experience the most common cause is, that the base64 string was encoded on the commandline using  echo '$mypw' | base64  which will create newlines in the encoded string."
Zipkin,61367030,56328934,0,"2020/04/22, 17:01:23",False,"2020/04/22, 17:01:23",2849.0,797243.0,0,You need to use the  -n  switch to echo:  echo -n '$mypw' | base64 .
Zipkin,52117392,51187274,1,"2018/08/31, 17:16:40",False,"2018/08/31, 18:03:37",1.0,1380632.0,0,What logging framework are you using?
Zipkin,52117392,51187274,1,"2018/08/31, 17:16:40",False,"2018/08/31, 18:03:37",1.0,1380632.0,0,I was using log4j2 in my project.
Zipkin,52117392,51187274,1,"2018/08/31, 17:16:40",False,"2018/08/31, 18:03:37",1.0,1380632.0,0,It started working when I removed log4j2 and left it to the default logging of spring-cloud-sleuth.
Zipkin,48763174,48759442,0,"2018/02/13, 11:15:18",True,"2018/02/13, 11:15:18",8336.0,1773866.0,2,It's a bug -  https://github.com/spring-cloud/spring-cloud-sleuth/issues/855  .
Zipkin,48763174,48759442,0,"2018/02/13, 11:15:18",True,"2018/02/13, 11:15:18",8336.0,1773866.0,2,I've fixed it ATM.
Zipkin,48763174,48759442,0,"2018/02/13, 11:15:18",True,"2018/02/13, 11:15:18",8336.0,1773866.0,2,A workaround is to start it manually either in each method that uses  @NewSpan  by calling  start()  method on current span (that doesn't scale too nicely)
Zipkin,48763174,48759442,0,"2018/02/13, 11:15:18",True,"2018/02/13, 11:15:18",8336.0,1773866.0,2,You can also create a bean of  SpanCreator  (you can check the fixed version here  https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/annotation/DefaultSpanCreator.java )
Zipkin,48763174,48759442,0,"2018/02/13, 11:15:18",True,"2018/02/13, 11:15:18",8336.0,1773866.0,2,Notice the  .start()  at the end of the method.
Zipkin,48679791,48679359,13,"2018/02/08, 09:23:24",False,"2018/02/08, 09:23:24",2367.0,3860531.0,0,Try this
Zipkin,48679791,48679359,13,"2018/02/08, 09:23:24",False,"2018/02/08, 09:23:24",2367.0,3860531.0,0,I hope this will help you.
Zipkin,66064768,66064081,3,"2021/02/05, 16:09:01",True,"2021/02/05, 18:51:05",7249.0,8340997.0,1,According to  Spring Boot Reference Docs  :
Zipkin,66064768,66064081,3,"2021/02/05, 16:09:01",True,"2021/02/05, 18:51:05",7249.0,8340997.0,1,"To enable  /httptrace  in the actuator, then you have to create a bean of   InMemoryHttpTraceRepository  class in the custom  @Configuration  class which provides the trace of the request and response."
Zipkin,66064768,66064081,3,"2021/02/05, 16:09:01",True,"2021/02/05, 18:51:05",7249.0,8340997.0,1,"To enable  /auditevents  in the actuator, then you have to create a bean of  InMemoryAuditEventRepository  class in the custom  @Configuration  class which exposes audit events information."
Zipkin,66064768,66064081,3,"2021/02/05, 16:09:01",True,"2021/02/05, 18:51:05",7249.0,8340997.0,1,"To enable  /integrationgraph  in actuator, you have to add  spring-integration-core dependency  in the pom.xml (as per documentation) :"
Zipkin,66064768,66064081,3,"2021/02/05, 16:09:01",True,"2021/02/05, 18:51:05",7249.0,8340997.0,1,"or if you are having a spring-boot project, then add this :"
Zipkin,66064768,66064081,3,"2021/02/05, 16:09:01",True,"2021/02/05, 18:51:05",7249.0,8340997.0,1,/actuator/sessions  are by-default enabled.
Zipkin,66064768,66064081,3,"2021/02/05, 16:09:01",True,"2021/02/05, 18:51:05",7249.0,8340997.0,1,But still you can add this explicitly to check the behaviour.
Zipkin,66064768,66064081,3,"2021/02/05, 16:09:01",True,"2021/02/05, 18:51:05",7249.0,8340997.0,1,Add this in application.properties.
Zipkin,64439121,64434436,0,"2020/10/20, 08:42:11",True,"2020/10/20, 08:42:11",66.0,12105655.0,1,Thanks Jorg Heymans for the question.
Zipkin,64439121,64434436,0,"2020/10/20, 08:42:11",True,"2020/10/20, 08:42:11",66.0,12105655.0,1,"Yeah, it's a bug and should be fixed by  https://github.com/line/armeria/pull/3120 
Thank you!"
Zipkin,64043957,64040747,1,"2020/09/24, 12:53:37",False,"2020/09/24, 12:53:37",1554.0,3719412.0,1,Ribbon is a client side load balancer which means there is no any other hop in between your client and service.
Zipkin,64043957,64040747,1,"2020/09/24, 12:53:37",False,"2020/09/24, 12:53:37",1554.0,3719412.0,1,Basically you keep and maintain a list of service on your client.
Zipkin,64043957,64040747,1,"2020/09/24, 12:53:37",False,"2020/09/24, 12:53:37",1554.0,3719412.0,1,In AWS load balancer case you need to make another hop in between the client and server.
Zipkin,64043957,64040747,1,"2020/09/24, 12:53:37",False,"2020/09/24, 12:53:37",1554.0,3719412.0,1,Both have advanges and disadvantages.
Zipkin,64043957,64040747,1,"2020/09/24, 12:53:37",False,"2020/09/24, 12:53:37",1554.0,3719412.0,1,Former has the advantage of not having any dependency to any specific external solution.
Zipkin,64043957,64040747,1,"2020/09/24, 12:53:37",False,"2020/09/24, 12:53:37",1554.0,3719412.0,1,Basically with ribbon and service discovery like eureka you can deploy your product to any cloud provider or on-premise setup without additional effort.
Zipkin,64043957,64040747,1,"2020/09/24, 12:53:37",False,"2020/09/24, 12:53:37",1554.0,3719412.0,1,Latter has advantage of not needing an extra component of service discovery or keeping the cache of service list on client.
Zipkin,64043957,64040747,1,"2020/09/24, 12:53:37",False,"2020/09/24, 12:53:37",1554.0,3719412.0,1,But it has that additional hop which might be an issue if you are trying to run an very high-load system.
Zipkin,64043957,64040747,1,"2020/09/24, 12:53:37",False,"2020/09/24, 12:53:37",1554.0,3719412.0,1,Although I don't have much experience with AWS CloudWatch what I know is it helps you to collect logs to a central place from different AWS components.
Zipkin,64043957,64040747,1,"2020/09/24, 12:53:37",False,"2020/09/24, 12:53:37",1554.0,3719412.0,1,And that is what you are trying to do with your solution.
Zipkin,60258813,60258661,0,"2020/02/17, 10:53:24",False,"2020/02/17, 10:53:24",159.0,4502707.0,3,"kubectl exec -it ""pod-name"" -c ""container-name"" -n ""namespace"""
Zipkin,60258813,60258661,0,"2020/02/17, 10:53:24",False,"2020/02/17, 10:53:24",159.0,4502707.0,3,Here only the container name is needed.
Zipkin,60258813,60258661,0,"2020/02/17, 10:53:24",False,"2020/02/17, 10:53:24",159.0,4502707.0,3,In your case it will be:
Zipkin,60258813,60258661,0,"2020/02/17, 10:53:24",False,"2020/02/17, 10:53:24",159.0,4502707.0,3,kubectl exec -it my-api-XXX -c my-api  -- /bin/bash
Zipkin,60258813,60258661,0,"2020/02/17, 10:53:24",False,"2020/02/17, 10:53:24",159.0,4502707.0,3,You can exec to Zipkin because  exec  is taking zipkin as the default container.
Zipkin,56657881,56655528,0,"2019/06/19, 01:35:50",False,"2019/06/19, 01:35:50",694.0,3800106.0,0,It is solved now; all I had to do was port forwarding.
Zipkin,56657881,56655528,0,"2019/06/19, 01:35:50",False,"2019/06/19, 01:35:50",694.0,3800106.0,0,"Thanks,"
Zipkin,56734998,56655528,0,"2019/06/24, 13:49:27",False,"2019/06/24, 13:49:27",2049.0,11207414.0,0,"By default you service is exposed as  ClusterIP , in this case your service will be accessible from within your cluster."
Zipkin,56734998,56655528,0,"2019/06/24, 13:49:27",False,"2019/06/24, 13:49:27",2049.0,11207414.0,0,"You can use port forwarding "" With this connection in place you can use your local workstation to debug your application that is running in the pod "" as described in the answer above."
Zipkin,56734998,56655528,0,"2019/06/24, 13:49:27",False,"2019/06/24, 13:49:27",2049.0,11207414.0,0,"Another approach is to use other  ""service types""  like  NodePort ."
Zipkin,56734998,56655528,0,"2019/06/24, 13:49:27",False,"2019/06/24, 13:49:27",2049.0,11207414.0,0,You can find more information here  Publishing services (ServiceTypes)
Zipkin,55274713,55274572,0,"2019/03/21, 08:15:19",False,"2019/03/21, 08:15:19",3915.0,837717.0,2,"Sleuth will do the same for messaging by using message headers to propagate  span id, trace id  and other relevant information."
Zipkin,55274713,55274572,0,"2019/03/21, 08:15:19",False,"2019/03/21, 08:15:19",3915.0,837717.0,2,It does so by registering special channel interceptor.
Zipkin,54914623,54845174,0,"2019/02/27, 23:15:56",True,"2019/02/27, 23:15:56",504.0,9187876.0,1,"The configuration you're referring to is for the instrumentation of messaging systems, not for sending traces to zipkin using a messaging system."
Zipkin,54914623,54845174,0,"2019/02/27, 23:15:56",True,"2019/02/27, 23:15:56",504.0,9187876.0,1,"You should look at this  auto-configuration , and especially this  sender config ."
Zipkin,54914623,54845174,0,"2019/02/27, 23:15:56",True,"2019/02/27, 23:15:56",504.0,9187876.0,1,What you want to do has also been documented here:  https://cloud.spring.io/spring-cloud-sleuth/2.0.x/single/spring-cloud-sleuth.html#_sleuth_with_zipkin_over_rabbitmq_or_kafka
Zipkin,54914623,54845174,0,"2019/02/27, 23:15:56",True,"2019/02/27, 23:15:56",504.0,9187876.0,1,You should only need to add  spring-cloud-starter-zipkin  and  spring-rabbit  to your dependencies.
Zipkin,54914623,54845174,0,"2019/02/27, 23:15:56",True,"2019/02/27, 23:15:56",504.0,9187876.0,1,"If you want to change the default queue (which is  zipkin ), then you'll need to add  spring.zipkin.rabbitmq.queue  to your properties."
Zipkin,53655726,53653606,3,"2018/12/06, 18:27:44",False,"2018/12/07, 23:02:30",81.0,2232476.0,0,You will need your own  PropagationFactory  implementation.
Zipkin,53655726,53653606,3,"2018/12/06, 18:27:44",False,"2018/12/07, 23:02:30",81.0,2232476.0,0,Here is the default one:  https://github.com/openzipkin/brave/blob/master/brave/src/main/java/brave/propagation/B3Propagation.java
Zipkin,53655726,53653606,3,"2018/12/06, 18:27:44",False,"2018/12/07, 23:02:30",81.0,2232476.0,0,You can create a bean and sleuth should use that instead of this one.
Zipkin,53655726,53653606,3,"2018/12/06, 18:27:44",False,"2018/12/07, 23:02:30",81.0,2232476.0,0,More specifically you will need an implementation with a custom  TraceContext.Extractor&lt;C&gt;  implementation.
Zipkin,53655726,53653606,3,"2018/12/06, 18:27:44",False,"2018/12/07, 23:02:30",81.0,2232476.0,0,"This can then pull the trace ID from your header, and add return the appropriate  TraceContext ."
Zipkin,53655726,53653606,3,"2018/12/06, 18:27:44",False,"2018/12/07, 23:02:30",81.0,2232476.0,0,Then it can pass it along using the normal headers.
Zipkin,53655726,53653606,3,"2018/12/06, 18:27:44",False,"2018/12/07, 23:02:30",81.0,2232476.0,0,If you'd like to use the same correlation header when sending downstream then you will also have to implement  TraceContext.Injector&lt;C&gt; .
Zipkin,53184979,53174880,3,"2018/11/07, 09:16:13",True,"2018/11/07, 09:16:13",8336.0,1773866.0,1,The option is to disable Slf4j integration as you mentioned.
Zipkin,53184979,53174880,3,"2018/11/07, 09:16:13",True,"2018/11/07, 09:16:13",8336.0,1773866.0,1,"When a new span / scope is created, we go through Slf4j to put data in MDC and it takes time unfortunately."
Zipkin,53184979,53174880,3,"2018/11/07, 09:16:13",True,"2018/11/07, 09:16:13",8336.0,1773866.0,1,Disabling that will save it.
Zipkin,51526235,51526164,6,"2018/07/25, 22:21:47",True,"2018/07/26, 03:24:14",91301.0,2756547.0,0,This is indeed possible with the mentioned  executor channel .
Zipkin,51526235,51526164,6,"2018/07/25, 22:21:47",True,"2018/07/26, 03:24:14",91301.0,2756547.0,0,All you recipient flows must really start from the  ExecutorChannel .
Zipkin,51526235,51526164,6,"2018/07/25, 22:21:47",True,"2018/07/26, 03:24:14",91301.0,2756547.0,0,In your case you have to modify all of them to something like this:
Zipkin,51526235,51526164,6,"2018/07/25, 22:21:47",True,"2018/07/26, 03:24:14",91301.0,2756547.0,0,Pay attention to the  IntegrationFlows.from(MessageChannels.executor(taskExexecutor())) .
Zipkin,51526235,51526164,6,"2018/07/25, 22:21:47",True,"2018/07/26, 03:24:14",91301.0,2756547.0,0,That's exactly how you can make each sub-flow async.
Zipkin,51526235,51526164,6,"2018/07/25, 22:21:47",True,"2018/07/26, 03:24:14",91301.0,2756547.0,0,UPDATE
Zipkin,51526235,51526164,6,"2018/07/25, 22:21:47",True,"2018/07/26, 03:24:14",91301.0,2756547.0,0,For the older Spring Integration version without  IntegrationFlow  improvement for the sub-flows we can do like this:
Zipkin,51526235,51526164,6,"2018/07/25, 22:21:47",True,"2018/07/26, 03:24:14",91301.0,2756547.0,0,This is similar to what you show in the comment above.
Zipkin,51442735,51442393,1,"2018/07/20, 15:38:54",False,"2018/07/20, 15:38:54",51827.0,1259109.0,0,"Works for me with 1.4.0.RELEASE (2.0.0.RELEASE isn't out yet, but should be soon)."
Zipkin,51442735,51442393,1,"2018/07/20, 15:38:54",False,"2018/07/20, 15:38:54",51827.0,1259109.0,0,You probably have a bad jar file in your local maven cache (e.g.
Zipkin,51442735,51442393,1,"2018/07/20, 15:38:54",False,"2018/07/20, 15:38:54",51827.0,1259109.0,0,the one that it complains about).
Zipkin,49357777,49326587,1,"2018/03/19, 09:31:43",False,"2018/03/19, 09:31:43",8336.0,1773866.0,0,You have to provide a different logging pattern to make it work with PCF Metrics AFAIR.
Zipkin,49357777,49326587,1,"2018/03/19, 09:31:43",False,"2018/03/19, 09:31:43",8336.0,1773866.0,0,You need the parent span to be present in logs.
Zipkin,49357777,49326587,1,"2018/03/19, 09:31:43",False,"2018/03/19, 09:31:43",8336.0,1773866.0,0,"Set the property  logging.pattern.level: ""%clr(%5p) %clr([${spring.application.name:-},%X{X-B3-TraceId:-},%X{X-B3-SpanId:-},%X{X-B3-ParentSpanId:-},%X{X-Span-Export:-}]){yellow}"" ."
Zipkin,49357777,49326587,1,"2018/03/19, 09:31:43",False,"2018/03/19, 09:31:43",8336.0,1773866.0,0,Check this example:  https://github.com/pivotal-cf/pcf-metrics-trace-example-spring
Zipkin,49903075,49326587,1,"2018/04/18, 18:02:24",False,"2018/04/18, 18:02:24",1.0,9665105.0,0,"PCF metrics does  not support custom spans, it only shows the respomse time distribution span that corresponds to http request routed by goRouter."
Zipkin,49066640,48940831,1,"2018/03/02, 11:51:31",True,"2018/03/02, 11:51:31",8336.0,1773866.0,1,It was a bug that got fixed with this commit -  https://github.com/spring-cloud/spring-cloud-sleuth/commit/d7a0747907f4ab7201f67e7d0c762a324fbe0668  .
Zipkin,49066640,48940831,1,"2018/03/02, 11:51:31",True,"2018/03/02, 11:51:31",8336.0,1773866.0,1,Please check out the latest snapshots
Zipkin,48762766,48756959,9,"2018/02/13, 10:51:34",True,"2018/02/13, 21:29:25",8336.0,1773866.0,2,Why are you setting the values of dependencies manually?
Zipkin,48762766,48756959,9,"2018/02/13, 10:51:34",True,"2018/02/13, 21:29:25",8336.0,1773866.0,2,Please use the Edgware.SR2 BOM.
Zipkin,48762766,48756959,9,"2018/02/13, 10:51:34",True,"2018/02/13, 21:29:25",8336.0,1773866.0,2,"You have to add the kafka dependency, ensure that rabbit is not on the classpath."
Zipkin,48762766,48756959,9,"2018/02/13, 10:51:34",True,"2018/02/13, 21:29:25",8336.0,1773866.0,2,If you have both kafka and rabbit on the classpath you need to set the  spring.zipkin.sender.type=kafka
Zipkin,48762766,48756959,9,"2018/02/13, 10:51:34",True,"2018/02/13, 21:29:25",8336.0,1773866.0,2,UPDATE:
Zipkin,48762766,48756959,9,"2018/02/13, 10:51:34",True,"2018/02/13, 21:29:25",8336.0,1773866.0,2,"As we describe in the documentation, the Sleuth Stream support is deprecated in Edgware and removed in FInchley."
Zipkin,48762766,48756959,9,"2018/02/13, 10:51:34",True,"2018/02/13, 21:29:25",8336.0,1773866.0,2,"If you've decided to go with the new approach of using native Zipkin messaging support, then you have to use the Zipkin Server with Kafka as described here  https://github.com/openzipkin/zipkin/tree/master/zipkin-autoconfigure/collector-kafka10  ."
Zipkin,48762766,48756959,9,"2018/02/13, 10:51:34",True,"2018/02/13, 21:29:25",8336.0,1773866.0,2,Let me copy part of the docs here
Zipkin,48762766,48756959,9,"2018/02/13, 10:51:34",True,"2018/02/13, 21:29:25",8336.0,1773866.0,2,"The following configuration points apply apply when  KAFKA_BOOTSTRAP_SERVERS  or
 zipkin.collector.kafka.bootstrap-servers  is set."
Zipkin,48762766,48756959,9,"2018/02/13, 10:51:34",True,"2018/02/13, 21:29:25",8336.0,1773866.0,2,"They can be configured by setting an environment
variable or by setting a java system property using the  -Dproperty.name=value  command line
argument."
Zipkin,48762766,48756959,9,"2018/02/13, 10:51:34",True,"2018/02/13, 21:29:25",8336.0,1773866.0,2,"Some settings correspond to ""New Consumer Configs"" in
 Kafka documentation ."
Zipkin,48762766,48756959,9,"2018/02/13, 10:51:34",True,"2018/02/13, 21:29:25",8336.0,1773866.0,2,Environment Variable | Property | New Consumer Config | Description
Zipkin,48762766,48756959,9,"2018/02/13, 10:51:34",True,"2018/02/13, 21:29:25",8336.0,1773866.0,2,"KAFKA_BOOTSTRAP_SERVERS  |  zipkin.collector.kafka.bootstrap-servers  | bootstrap.servers | Comma-separated list of brokers, ex."
Zipkin,48762766,48756959,9,"2018/02/13, 10:51:34",True,"2018/02/13, 21:29:25",8336.0,1773866.0,2,127.0.0.1:9092.
Zipkin,48762766,48756959,9,"2018/02/13, 10:51:34",True,"2018/02/13, 21:29:25",8336.0,1773866.0,2,No default
Zipkin,48762766,48756959,9,"2018/02/13, 10:51:34",True,"2018/02/13, 21:29:25",8336.0,1773866.0,2,KAFKA_GROUP_ID  |  zipkin.collector.kafka.group-id  | group.id | The consumer group this process is consuming on behalf of.
Zipkin,48762766,48756959,9,"2018/02/13, 10:51:34",True,"2018/02/13, 21:29:25",8336.0,1773866.0,2,Defaults to  zipkin
Zipkin,48762766,48756959,9,"2018/02/13, 10:51:34",True,"2018/02/13, 21:29:25",8336.0,1773866.0,2,KAFKA_TOPIC  |  zipkin.collector.kafka.topic  | N/A | Comma-separated list of topics that zipkin spans will be consumed from.
Zipkin,48762766,48756959,9,"2018/02/13, 10:51:34",True,"2018/02/13, 21:29:25",8336.0,1773866.0,2,Defaults to  zipkin
Zipkin,48762766,48756959,9,"2018/02/13, 10:51:34",True,"2018/02/13, 21:29:25",8336.0,1773866.0,2,KAFKA_STREAMS  |  zipkin.collector.kafka.streams  | N/A | Count of threads consuming the topic.
Zipkin,48762766,48756959,9,"2018/02/13, 10:51:34",True,"2018/02/13, 21:29:25",8336.0,1773866.0,2,Defaults to  1
Zipkin,51743845,48639074,0,"2018/08/08, 13:00:10",False,"2018/08/08, 13:00:10",26.0,5772061.0,0,OpenStack does not have Zipkin as an inbuilt tracer.
Zipkin,51743845,48639074,0,"2018/08/08, 13:00:10",False,"2018/08/08, 13:00:10",26.0,5772061.0,0,Hence OSProfiler was adopted as a standard project for tracing in OpenStack.
Zipkin,51743845,48639074,0,"2018/08/08, 13:00:10",False,"2018/08/08, 13:00:10",26.0,5772061.0,0,"As far as i can see from the documentation, Nova should have OSProfiler support for Mitaka."
Zipkin,51743845,48639074,0,"2018/08/08, 13:00:10",False,"2018/08/08, 13:00:10",26.0,5772061.0,0,"Although i have not used OSProfiler with Mitaka, I have worked with OSProfiler with Newton and subsequent releases."
Zipkin,51743845,48639074,0,"2018/08/08, 13:00:10",False,"2018/08/08, 13:00:10",26.0,5772061.0,0,You can post the issue that you are facing so that it will be easier to debug.
Zipkin,48216934,48216192,0,"2018/01/12, 00:38:24",True,"2018/01/12, 00:38:24",8336.0,1773866.0,3,"If you're using Edgware release train, just set  spring.zipkin.sender.type=web ."
Zipkin,48216934,48216192,0,"2018/01/12, 00:38:24",True,"2018/01/12, 00:38:24",8336.0,1773866.0,3,That way you force the HTTP based span sending
Zipkin,48170238,48169950,2,"2018/01/09, 16:22:14",True,"2018/01/09, 16:22:14",129048.0,1240763.0,1,"Embedded headers are not pluggable, but you can disable them with  ...producer.header-mode=raw ."
Zipkin,48170238,48169950,2,"2018/01/09, 16:22:14",True,"2018/01/09, 16:22:14",129048.0,1240763.0,1,"With Ditmars (1.3.x) you can use the kafka11 artifact, which supports native headers - you have to override a bunch of dependencies (kafka-clients, SK, SIK and kafka itself if you are using the  KafkaEmbedded  broker for testing."
Zipkin,48170238,48169950,2,"2018/01/09, 16:22:14",True,"2018/01/09, 16:22:14",129048.0,1240763.0,1,See  the relesae notes ).
Zipkin,48170238,48169950,2,"2018/01/09, 16:22:14",True,"2018/01/09, 16:22:14",129048.0,1240763.0,1,There's  a discussion on Gitter  about overriding the versions.
Zipkin,48170238,48169950,2,"2018/01/09, 16:22:14",True,"2018/01/09, 16:22:14",129048.0,1240763.0,1,spring-kafka 1.3.x natively uses 0.11 but 1.3.1 and higher (1.3.2 is current) also supports the 1.0.0 client.
Zipkin,48170238,48169950,2,"2018/01/09, 16:22:14",True,"2018/01/09, 16:22:14",129048.0,1240763.0,1,Elmhurst (2.0) - currently in milestones - uses SK 2.1.0 which natively uses 1.0.0 kafka.
Zipkin,47925684,47925555,1,"2017/12/21, 15:07:02",False,"2017/12/21, 15:07:02",28734.0,1153938.0,1,We implemented this on our microservices platform
Zipkin,47925684,47925555,1,"2017/12/21, 15:07:02",False,"2017/12/21, 15:07:02",28734.0,1153938.0,1,A lot of the logging is done by pushing requests onto a RabbitMQ queue and then getting logstash to consume that.
Zipkin,47925684,47925555,1,"2017/12/21, 15:07:02",False,"2017/12/21, 15:07:02",28734.0,1153938.0,1,Other data is obtained via filebeat transmitting the logs to logstash
Zipkin,47925684,47925555,1,"2017/12/21, 15:07:02",False,"2017/12/21, 15:07:02",28734.0,1153938.0,1,Both the logs and the RabbitMQ data has the id attached so can be correlated
Zipkin,47925684,47925555,1,"2017/12/21, 15:07:02",False,"2017/12/21, 15:07:02",28734.0,1153938.0,1,An alternative approach would be to build instrumentation into each microservice that specifically monitored latency and then record that directly into logstash
Zipkin,47925684,47925555,1,"2017/12/21, 15:07:02",False,"2017/12/21, 15:07:02",28734.0,1153938.0,1,You might like to read  https://medium.com/devopslinks/how-to-monitor-the-sre-golden-signals-1391cadc7524  for a general guide to essential monitoring that is applicable to microservices
Zipkin,46489881,46479182,0,"2017/09/29, 16:30:12",False,"2017/09/29, 16:30:12",691.0,1029971.0,1,I figured out how to disable the bean that was injecting LogbackAccess.
Zipkin,46489881,46479182,0,"2017/09/29, 16:30:12",False,"2017/09/29, 16:30:12",691.0,1029971.0,1,This resolved the issue so that Zipkin is now accepting requests.
Zipkin,46188800,46185813,1,"2017/09/13, 07:21:00",True,"2017/09/13, 07:21:00",12382.0,707451.0,1,"To log internal HTTP request sent from a Node.js server, you can create a Proxy Node.js server and log all requests there using  Morgan ."
Zipkin,46188800,46185813,1,"2017/09/13, 07:21:00",True,"2017/09/13, 07:21:00",12382.0,707451.0,1,"First, define 3 constants (or read from your project config file):"
Zipkin,46188800,46185813,1,"2017/09/13, 07:21:00",True,"2017/09/13, 07:21:00",12382.0,707451.0,1,"Second, When your Node.js server is launched, start the logger server at the same time if  ENABLE_LOGGER  is true."
Zipkin,46188800,46185813,1,"2017/09/13, 07:21:00",True,"2017/09/13, 07:21:00",12382.0,707451.0,1,The logger server only do one thing: log the request and forward it to the real API server using  request  module.
Zipkin,46188800,46185813,1,"2017/09/13, 07:21:00",True,"2017/09/13, 07:21:00",12382.0,707451.0,1,You can use  Morgan  to provide more readable format.
Zipkin,46188800,46185813,1,"2017/09/13, 07:21:00",True,"2017/09/13, 07:21:00",12382.0,707451.0,1,"Third, in your Node.js server, send API request to logger server when  ENABLE_LOGGER  is true, and send API directly to the real server when  ENABLE_LOGGER  is false."
Zipkin,44763002,44762961,4,"2017/06/26, 18:19:01",True,"2017/06/26, 18:19:01",8336.0,1773866.0,1,No - we haven't added any instrumentation around Webservicetemplate.
Zipkin,44763002,44762961,4,"2017/06/26, 18:19:01",True,"2017/06/26, 18:19:01",8336.0,1773866.0,1,You'd have to add an interceptor similar to the one we add for RestTemplate.
Zipkin,44763002,44762961,4,"2017/06/26, 18:19:01",True,"2017/06/26, 18:19:01",8336.0,1773866.0,1,You'd have to pass all the tracing headers to the request so that the other side can properly parse it.
Zipkin,41045080,41040727,0,"2016/12/08, 19:11:10",True,"2016/12/08, 19:11:10",1825.0,499395.0,3,We have an internal OkHttpClient wrapper implementing Call.Factory which adds an initial interceptor:
Zipkin,41045080,41040727,0,"2016/12/08, 19:11:10",True,"2016/12/08, 19:11:10",1825.0,499395.0,3,to solve this problem.
Zipkin,41045080,41040727,0,"2016/12/08, 19:11:10",True,"2016/12/08, 19:11:10",1825.0,499395.0,3,"It is not transparent, however, so may not be good for Brave."
Zipkin,41045080,41040727,0,"2016/12/08, 19:11:10",True,"2016/12/08, 19:11:10",1825.0,499395.0,3,"It works fine, because in practice once a client is configured, you only really use the  Call.Factory  interface :-)"
Zipkin,41045080,41040727,0,"2016/12/08, 19:11:10",True,"2016/12/08, 19:11:10",1825.0,499395.0,3,"The  Ctx  bit is just the context with stuff we want to propagate, we can do it implicit or explicit, hence the extra method to explicitly take it."
Zipkin,40895971,40694955,0,"2016/11/30, 20:49:26",True,"2016/11/30, 20:49:26",3732.0,560814.0,2,Thanks for trying out HTrace!
Zipkin,40895971,40694955,0,"2016/11/30, 20:49:26",True,"2016/11/30, 20:49:26",3732.0,560814.0,2,Sorry that the version issue is such a pain right now.
Zipkin,40895971,40694955,0,"2016/11/30, 20:49:26",True,"2016/11/30, 20:49:26",3732.0,560814.0,2,It is much easier to configure HTrace with the version in cloudera's CDH5.5 distribution of Hadoop and later.
Zipkin,40895971,40694955,0,"2016/11/30, 20:49:26",True,"2016/11/30, 20:49:26",3732.0,560814.0,2,"There is a good description of how to do it here:  http://blog.cloudera.com/blog/2015/12/new-in-cloudera-labs-apache-htrace-incubating/   If you want to stick with an Apache release of the source code rather than a vendor release, try Hadoop 3.0.0-alpha1."
Zipkin,40895971,40694955,0,"2016/11/30, 20:49:26",True,"2016/11/30, 20:49:26",3732.0,560814.0,2,http://hadoop.apache.org/releases.html
Zipkin,40895971,40694955,0,"2016/11/30, 20:49:26",True,"2016/11/30, 20:49:26",3732.0,560814.0,2,The HTrace libraries shippped in Hadoop 2.6 and 2.7 are very old... we never backported HTrace 4.x to those branches.
Zipkin,40895971,40694955,0,"2016/11/30, 20:49:26",True,"2016/11/30, 20:49:26",3732.0,560814.0,2,"They were stability branches, so new features like tracing was out of scope."
Zipkin,40895971,40694955,0,"2016/11/30, 20:49:26",True,"2016/11/30, 20:49:26",3732.0,560814.0,2,"There is some functionality there, but not much."
Zipkin,40895971,40694955,0,"2016/11/30, 20:49:26",True,"2016/11/30, 20:49:26",3732.0,560814.0,2,I recommend using the newer HTrace 4.x library which is actively developed.
Zipkin,40895971,40694955,0,"2016/11/30, 20:49:26",True,"2016/11/30, 20:49:26",3732.0,560814.0,2,"The HTrace 4.x branch also has a stable API, so hopefully breakage will be minimized in the future."
Zipkin,42159646,40694955,0,"2017/02/10, 14:39:46",False,"2017/02/10, 16:03:38",9.0,7545887.0,0,"Exactly, in the code, I see the configuration key's prefix is  dfs.htrace , not the  hadoop.htrace ."
Zipkin,42159646,40694955,0,"2017/02/10, 14:39:46",False,"2017/02/10, 16:03:38",9.0,7545887.0,0,"And in dfsclient, it's  dfs.client.htrace ."
Zipkin,42159646,40694955,0,"2017/02/10, 14:39:46",False,"2017/02/10, 16:03:38",9.0,7545887.0,0,"You can change the prefix to  dfs.htrace , then restart the cluster and it take effect."
Zipkin,42159646,40694955,0,"2017/02/10, 14:39:46",False,"2017/02/10, 16:03:38",9.0,7545887.0,0,The code is in class  org.apache.hadoop.tracing.SpanReceiverHost .
Zipkin,42159646,40694955,0,"2017/02/10, 14:39:46",False,"2017/02/10, 16:03:38",9.0,7545887.0,0,Hope this help!
Zipkin,40534423,40525453,4,"2016/11/10, 20:16:52",False,"2016/11/10, 20:16:52",8336.0,1773866.0,0,The sampling decision is taken for a trace.
Zipkin,40534423,40525453,4,"2016/11/10, 20:16:52",False,"2016/11/10, 20:16:52",8336.0,1773866.0,0,That means that when the first request comes in and the span is created you have to take a decision.
Zipkin,40534423,40525453,4,"2016/11/10, 20:16:52",False,"2016/11/10, 20:16:52",8336.0,1773866.0,0,You don't have any tags / baggage at that point so you must not depend on the contents of tags to take this decision.
Zipkin,40534423,40525453,4,"2016/11/10, 20:16:52",False,"2016/11/10, 20:16:52",8336.0,1773866.0,0,That's a wrong approach.
Zipkin,40534423,40525453,4,"2016/11/10, 20:16:52",False,"2016/11/10, 20:16:52",8336.0,1773866.0,0,You are taking a very custom approach.
Zipkin,40534423,40525453,4,"2016/11/10, 20:16:52",False,"2016/11/10, 20:16:52",8336.0,1773866.0,0,If you want to go that way (which is not recommended) you can create a custom implementation of a  SpanReporter  -  https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/SpanReporter.java#L30  .
Zipkin,40534423,40525453,4,"2016/11/10, 20:16:52",False,"2016/11/10, 20:16:52",8336.0,1773866.0,0,SpanReporter  is the one that is sending spans to zipkin.
Zipkin,40534423,40525453,4,"2016/11/10, 20:16:52",False,"2016/11/10, 20:16:52",8336.0,1773866.0,0,You can create an implementation that will wrap an existing  SpanReporter  implementation and will delegate the execution to it only when some values of tags match.
Zipkin,40534423,40525453,4,"2016/11/10, 20:16:52",False,"2016/11/10, 20:16:52",8336.0,1773866.0,0,But from my perspective it doesn't sound right.
Zipkin,40151801,39862738,0,"2016/10/20, 13:26:02",False,"2016/10/20, 13:26:02",8336.0,1773866.0,5,If I'm not mistaken (and I guess I'm not) no wonder that you're not sending the Spans to Zipkin cause you didn't add the Zipkin dependency.
Zipkin,40151801,39862738,0,"2016/10/20, 13:26:02",False,"2016/10/20, 13:26:02",8336.0,1773866.0,5,Check the  Sleuth with Zipkin via HTTP  section of the docs:  http://cloud.spring.io/spring-cloud-sleuth/spring-cloud-sleuth.html  .
Zipkin,42608836,39862738,1,"2017/03/05, 15:21:05",False,"2017/03/05, 15:21:05",167.0,3814829.0,0,This config worked for me in 1 of my application:
Zipkin,42608836,39862738,1,"2017/03/05, 15:21:05",False,"2017/03/05, 15:21:05",167.0,3814829.0,0,Enabling property might do the trick!
Zipkin,38528610,38523705,2,"2016/07/22, 17:11:03",True,"2016/07/22, 17:11:03",91301.0,2756547.0,1,"First of all the main feature of Spring Integration is  MessageChannel , but it still isn't clear to me why people are missing  .channel()  operator in between endpoint definitions."
Zipkin,38528610,38523705,2,"2016/07/22, 17:11:03",True,"2016/07/22, 17:11:03",91301.0,2756547.0,1,I mean that for your case it must be like:
Zipkin,38528610,38523705,2,"2016/07/22, 17:11:03",True,"2016/07/22, 17:11:03",91301.0,2756547.0,1,Now about your particular problem.
Zipkin,38528610,38523705,2,"2016/07/22, 17:11:03",True,"2016/07/22, 17:11:03",91301.0,2756547.0,1,"Look,  ContentEnricher  ( .enrich() ) is request-reply component:  http://docs.spring.io/spring-integration/reference/html/messaging-transformation-chapter.html#payload-enricher ."
Zipkin,38528610,38523705,2,"2016/07/22, 17:11:03",True,"2016/07/22, 17:11:03",91301.0,2756547.0,1,Therefore it sends request to its  requestChannel  and waits for reply.
Zipkin,38528610,38523705,2,"2016/07/22, 17:11:03",True,"2016/07/22, 17:11:03",91301.0,2756547.0,1,And it is done independently of the  requestChannel  type.
Zipkin,38528610,38523705,2,"2016/07/22, 17:11:03",True,"2016/07/22, 17:11:03",91301.0,2756547.0,1,I raw Java we can demonstrate such a behavior with this code snippet:
Zipkin,38528610,38523705,2,"2016/07/22, 17:11:03",True,"2016/07/22, 17:11:03",91301.0,2756547.0,1,where you should see that  ADD_LINE_ITEM_CHANNEL  as an  ExecutorChannel  doesn't have much value because we are blocked within loop for the reply anyway.
Zipkin,38528610,38523705,2,"2016/07/22, 17:11:03",True,"2016/07/22, 17:11:03",91301.0,2756547.0,1,"A  .split()  does exactly similar loop, but since by default it is with the  DirectChannel , an iteration is done in the same thread."
Zipkin,38528610,38523705,2,"2016/07/22, 17:11:03",True,"2016/07/22, 17:11:03",91301.0,2756547.0,1,Therefore each next item waits for the reply for the previous.
Zipkin,38528610,38523705,2,"2016/07/22, 17:11:03",True,"2016/07/22, 17:11:03",91301.0,2756547.0,1,"That's why you definitely should parallel exactly as an input for the  .enrich() , just after  .split() ."
Zipkin,28202807,28187275,1,"2015/01/28, 23:32:37",True,"2015/01/28, 23:32:37",1050.0,748140.0,1,The Apache Thrift TSocketTransport (almost certainly what you are using) uses TCP on a configurable port.
Zipkin,28202807,28187275,1,"2015/01/28, 23:32:37",True,"2015/01/28, 23:32:37",1050.0,748140.0,1,Cassandra usually uses port 9160 for thrift.
Zipkin,28202807,28187275,1,"2015/01/28, 23:32:37",True,"2015/01/28, 23:32:37",1050.0,748140.0,1,When using Thrift/TCP no HTTP setup is necessary.
Zipkin,28202807,28187275,1,"2015/01/28, 23:32:37",True,"2015/01/28, 23:32:37",1050.0,748140.0,1,Just open 9160 (and any other ports your custom thrift servers may be listening on).
Zipkin,28202807,28187275,1,"2015/01/28, 23:32:37",True,"2015/01/28, 23:32:37",1050.0,748140.0,1,"Though you can use Thrift over HTTP, Thrift is RPC, not REST, so proxy caching will cause problems, the client needs a direct comm channel with the server."
Zipkin,48832878,28187275,0,"2018/02/16, 20:31:16",False,"2018/02/16, 20:31:16",1620.0,1401124.0,0,"If you do need to access a thrift service via a proxy, something like this would work:"
Zipkin,48832878,28187275,0,"2018/02/16, 20:31:16",False,"2018/02/16, 20:31:16",1620.0,1401124.0,0,https://github.com/totally/thrift_goodies/blob/master/transport.py
Zipkin,48832878,28187275,0,"2018/02/16, 20:31:16",False,"2018/02/16, 20:31:16",1620.0,1401124.0,0,You can kill the kerberos stuff if you don't need that.
Zipkin,26486619,26016714,0,"2014/10/21, 15:21:43",True,"2014/10/21, 15:21:43",583.0,1372349.0,0,i get the same messages but still be able to collect messages and view them with the web service.
Zipkin,26486619,26016714,0,"2014/10/21, 15:21:43",True,"2014/10/21, 15:21:43",583.0,1372349.0,0,I dont know why the [error] prefix is in front of it but if you read the chars behind you see INF/DEB and so on...
Zipkin,26486619,26016714,0,"2014/10/21, 15:21:43",True,"2014/10/21, 15:21:43",583.0,1372349.0,0,It stays for INFO and DEBUG.
Zipkin,26486619,26016714,0,"2014/10/21, 15:21:43",True,"2014/10/21, 15:21:43",583.0,1372349.0,0,Greets
Zipkin,62188352,62188289,1,"2020/06/04, 09:39:36",False,"2020/06/04, 09:39:36",4038.0,809423.0,0,"yes, you can have multiple express running in the same node process (thats how clustering works in node as well)"
Zipkin,62188352,62188289,1,"2020/06/04, 09:39:36",False,"2020/06/04, 09:39:36",4038.0,809423.0,0,but you will need to have them running on different ports.
Zipkin,62188352,62188289,1,"2020/06/04, 09:39:36",False,"2020/06/04, 09:39:36",4038.0,809423.0,0,;
Zipkin,62196735,62188289,1,"2020/06/04, 17:08:54",True,"2020/06/04, 17:08:54",501.0,1272149.0,1,It is possible to create two separate tracer providers.
Zipkin,62196735,62188289,1,"2020/06/04, 17:08:54",True,"2020/06/04, 17:08:54",501.0,1272149.0,1,"Only one of them will be the global tracer provider, which the API will use if you call API methods."
Zipkin,62196735,62188289,1,"2020/06/04, 17:08:54",True,"2020/06/04, 17:08:54",501.0,1272149.0,1,"You can't use the plugins in this configuration, which means you will have to manually instrument your application."
Zipkin,62196735,62188289,1,"2020/06/04, 17:08:54",True,"2020/06/04, 17:08:54",501.0,1272149.0,1,"If this is a use-case which is important to you, I suggest you create an issue on the github repo."
Zipkin,61028338,61027513,1,"2020/04/04, 15:50:12",True,"2020/04/04, 15:50:12",1162.0,4383264.0,0,"This indicates that the pod is not ready, hence the service will not add that pod's IP in the list of endpoints."
Zipkin,61028338,61027513,1,"2020/04/04, 15:50:12",True,"2020/04/04, 15:50:12",1162.0,4383264.0,0,Check the readiness probe of your pod by describing it and debug the issue that's making it non-ready.
Zipkin,61028338,61027513,1,"2020/04/04, 15:50:12",True,"2020/04/04, 15:50:12",1162.0,4383264.0,0,"Once this if fixed, you'll start seeing some endpoints populated when you describe the service &amp; that will enable you to access the service by DNS name."
Zipkin,54493070,54492867,3,"2019/02/02, 14:27:06",False,"2019/02/02, 14:27:06",12094.0,1556338.0,2,First specify your own overlay network (see bottom of code below) and use it for your services.
Zipkin,54493070,54492867,3,"2019/02/02, 14:27:06",False,"2019/02/02, 14:27:06",12094.0,1556338.0,2,"Then in your compose file for your other services like ZIpkin, add the  backbone  network to its list."
Zipkin,54493070,54492867,3,"2019/02/02, 14:27:06",False,"2019/02/02, 14:27:06",12094.0,1556338.0,2,Eg:
Zipkin,54493070,54492867,3,"2019/02/02, 14:27:06",False,"2019/02/02, 14:27:06",12094.0,1556338.0,2,"Note that outside of the first compose file, you'll need to prefix the  project  name for your network."
Zipkin,54493070,54492867,3,"2019/02/02, 14:27:06",False,"2019/02/02, 14:27:06",12094.0,1556338.0,2,Unless you set the environment variable  COMPOSE_PROJECT_NAME  it will be the name of the directory that the compose file is in.
Zipkin,54493070,54492867,3,"2019/02/02, 14:27:06",False,"2019/02/02, 14:27:06",12094.0,1556338.0,2,Do a  docker network ls  to find out the full name of the network to use.
Zipkin,54494741,54492867,0,"2019/02/02, 17:56:24",False,"2019/02/02, 17:56:24",1285.0,6022333.0,0,"OK, so I solved my problem using only docker-compose files."
Zipkin,54494741,54492867,0,"2019/02/02, 17:56:24",False,"2019/02/02, 17:56:24",1285.0,6022333.0,0,By Portainer I pasted my second docker-compose file in Stack section (I creaated new Stack):
Zipkin,54494741,54492867,0,"2019/02/02, 17:56:24",False,"2019/02/02, 17:56:24",1285.0,6022333.0,0,Now we should use 'my_name_zipkin' name to communicate with this service.
Zipkin,54494741,54492867,0,"2019/02/02, 17:56:24",False,"2019/02/02, 17:56:24",1285.0,6022333.0,0,Service name is the name we should use to communicate between containers.
Zipkin,54494741,54492867,0,"2019/02/02, 17:56:24",False,"2019/02/02, 17:56:24",1285.0,6022333.0,0,So in properties file I set:
Zipkin,51306145,51305899,0,"2018/07/12, 15:49:09",False,"2018/07/12, 15:49:09",18670.0,340088.0,0,"The slf4j API only takes  String  as the input to the  info ,  debug ,  warn ,  error  messages."
Zipkin,51306145,51305899,0,"2018/07/12, 15:49:09",False,"2018/07/12, 15:49:09",18670.0,340088.0,0,"What you could do is create your own JsonLogger wrapper, which takes a normal  Logger  (maybe wraps around it), which you could include at the top of your classes like:"
Zipkin,51306145,51305899,0,"2018/07/12, 15:49:09",False,"2018/07/12, 15:49:09",18670.0,340088.0,0,private static final JsonLogger logger = new JsonLogger(LoggerFactory.getLogger(MyClass.class)) ;
Zipkin,51306145,51305899,0,"2018/07/12, 15:49:09",False,"2018/07/12, 15:49:09",18670.0,340088.0,0,"You can then use Jackson, GSON or your favourite object to JSON mapper inside your JsonLogger so that you could do what you want."
Zipkin,51306145,51305899,0,"2018/07/12, 15:49:09",False,"2018/07/12, 15:49:09",18670.0,340088.0,0,"It can then offer the  info ,  debug ,  warn ,  error  methods like a normal logger."
Zipkin,51306145,51305899,0,"2018/07/12, 15:49:09",False,"2018/07/12, 15:49:09",18670.0,340088.0,0,You can also create your own  JsonLoggerFactory  which encapsulates this for you so that the line to include in each class is more concise.
Zipkin,21036699,21035841,1,"2014/01/10, 07:20:28",True,"2014/01/10, 07:20:28",136.0,1982812.0,1,"Yes, you can use BAM/CEP for this."
Zipkin,21036699,21035841,1,"2014/01/10, 07:20:28",True,"2014/01/10, 07:20:28",136.0,1982812.0,1,If you need real time monitoring you can use CEP and you can use BAM for batch process.
Zipkin,21036699,21035841,1,"2014/01/10, 07:20:28",True,"2014/01/10, 07:20:28",136.0,1982812.0,1,"From BAM 2.4.0 onwards, CEP features have been added inside BAM also hence you can use BAM and do real time analytics."
Zipkin,21036699,21035841,1,"2014/01/10, 07:20:28",True,"2014/01/10, 07:20:28",136.0,1982812.0,1,What type of services are involved with your scenario?
Zipkin,21036699,21035841,1,"2014/01/10, 07:20:28",True,"2014/01/10, 07:20:28",136.0,1982812.0,1,Depends on this you can use already existing data publisher or write new data publisher for BAM/CEP to publish your request details.
Zipkin,21036699,21035841,1,"2014/01/10, 07:20:28",True,"2014/01/10, 07:20:28",136.0,1982812.0,1,"For example if you are having chain of axis2 webservice calls for a request from client, and you want to monitor where the bottle neck/more time consumed, then you may use the service stats publishing, and monitor the average time take to process the message which will help you to see where the actual delay has been introduced."
Zipkin,21036699,21035841,1,"2014/01/10, 07:20:28",True,"2014/01/10, 07:20:28",136.0,1982812.0,1,For this you can use existing service statistics publisher feature.
Zipkin,21036699,21035841,1,"2014/01/10, 07:20:28",True,"2014/01/10, 07:20:28",136.0,1982812.0,1,"Also BAM will allow you to create your own dashboard to visualize, hence you can customize the dashboard."
Zipkin,21036699,21035841,1,"2014/01/10, 07:20:28",True,"2014/01/10, 07:20:28",136.0,1982812.0,1,Also with BAM 2.4.0 we have introduced notifications feature also which you can define some threshold value and configure to send notification if that cross that threshold value.
Zipkin,66037294,65957785,0,"2021/02/04, 01:32:16",False,"2021/02/04, 01:32:16",3.0,6206060.0,0,"I contributed to Micronaut and submitted a PR, which is now merged."
Zipkin,66037294,65957785,0,"2021/02/04, 01:32:16",False,"2021/02/04, 01:32:16",3.0,6206060.0,0,Pull request
Zipkin,64845200,64818612,0,"2020/11/15, 15:54:52",True,"2020/11/15, 15:54:52",160.0,3244615.0,0,So it was application B which was not passing the header along.
Zipkin,64845200,64818612,0,"2020/11/15, 15:54:52",True,"2020/11/15, 15:54:52",160.0,3244615.0,0,Turns out that the queue uri had a property  targetClient  which was set to 1.
Zipkin,64845200,64818612,0,"2020/11/15, 15:54:52",True,"2020/11/15, 15:54:52",160.0,3244615.0,0,The uri is something like
Zipkin,64845200,64818612,0,"2020/11/15, 15:54:52",True,"2020/11/15, 15:54:52",160.0,3244615.0,0,"Now I am not an IBM MQ expert by far, but the  documentation  states that setting this property to 1 means that  Messages do not contain an MQRFH2 header."
Zipkin,64845200,64818612,0,"2020/11/15, 15:54:52",True,"2020/11/15, 15:54:52",160.0,3244615.0,0,"I toggled it to 0 and voila, all spans fall into place."
Zipkin,64215147,64214980,6,"2020/10/05, 22:40:51",False,"2020/10/05, 22:40:51",425.0,8214791.0,0,You must tell the containers the network &quot;foo_network&quot;.
Zipkin,64215147,64214980,6,"2020/10/05, 22:40:51",False,"2020/10/05, 22:40:51",425.0,8214791.0,0,The External flag says that the containers are not accessible from outside.
Zipkin,64215147,64214980,6,"2020/10/05, 22:40:51",False,"2020/10/05, 22:40:51",425.0,8214791.0,0,"Of course you don't have to bet, but I thought as an example it might be quite good."
Zipkin,64215147,64214980,6,"2020/10/05, 22:40:51",False,"2020/10/05, 22:40:51",425.0,8214791.0,0,And because of the &quot;links&quot; function look here  Link
Zipkin,63661339,63331441,0,"2020/08/30, 22:28:14",False,"2020/08/30, 22:28:14",11.0,6284144.0,0,I think I found the problem.
Zipkin,63661339,63331441,0,"2020/08/30, 22:28:14",False,"2020/08/30, 22:28:14",11.0,6284144.0,0,Than I used the Exceutor in my code:
Zipkin,63661339,63331441,0,"2020/08/30, 22:28:14",False,"2020/08/30, 22:28:14",11.0,6284144.0,0,With this configuration the service was not starting correctly.
Zipkin,63661339,63331441,0,"2020/08/30, 22:28:14",False,"2020/08/30, 22:28:14",11.0,6284144.0,0,Now the @Bean(&quot;threadPoolTaskExecutor&quot;) configuration is removed and I'm using only @Async.
Zipkin,63661339,63331441,0,"2020/08/30, 22:28:14",False,"2020/08/30, 22:28:14",11.0,6284144.0,0,But why it's not working with Spring Boot Starter 2.3.x?
Zipkin,63661339,63331441,0,"2020/08/30, 22:28:14",False,"2020/08/30, 22:28:14",11.0,6284144.0,0,And there was no error message in  the log.
Zipkin,61571674,61571538,0,"2020/05/03, 11:35:32",True,"2020/05/03, 11:35:32",nan,nan,1,"Zipkin is a Spring-Boot-based project, the @EnableZipkinServer is not a Spring Cloud annotation."
Zipkin,61571674,61571538,0,"2020/05/03, 11:35:32",True,"2020/05/03, 11:35:32",nan,nan,1,It’s an annotation that’spart of the Zipkin project.
Zipkin,61571674,61571538,0,"2020/05/03, 11:35:32",True,"2020/05/03, 11:35:32",nan,nan,1,"This often confuses people who are new to the Spring Cloud Sleuth and Zipkin, because the Spring Cloud team did write the @EnableZipkinStreamServer annotation as part of Spring Cloud Sleuth."
Zipkin,61571674,61571538,0,"2020/05/03, 11:35:32",True,"2020/05/03, 11:35:32",nan,nan,1,The @EnableZipkinStreamServer annotation simplifies the use of Zipkin with RabbitMQ and Kafka.
Zipkin,61571674,61571538,0,"2020/05/03, 11:35:32",True,"2020/05/03, 11:35:32",nan,nan,1,Advantages of  @EnableZipkinServer is simplicity in setup.
Zipkin,61571674,61571538,0,"2020/05/03, 11:35:32",True,"2020/05/03, 11:35:32",nan,nan,1,With the @EnableZipkinStream server you need to set up and configure the services being traced and the Zipkin server to publish/listen to RabbitMQ or Kafka for tracing data.The advantage of the @EnableZipkinStreamServer annotation is that you can continue to collect trace data even if the Zipkin server is unavailable.
Zipkin,61571674,61571538,0,"2020/05/03, 11:35:32",True,"2020/05/03, 11:35:32",nan,nan,1,This is because the trace messages will accumulate the trace data on a message queue until the Zipkin server is available for processing the records.
Zipkin,61571674,61571538,0,"2020/05/03, 11:35:32",True,"2020/05/03, 11:35:32",nan,nan,1,"If you use the @EnableZipkinServer annotation and the Zipkin server is unavailable,the trace data that would have been sent by the service(s) to Zipkin will be lost."
Zipkin,60987725,60974758,2,"2020/04/02, 11:56:59",False,"2020/04/02, 11:56:59",8336.0,1773866.0,0,"Please don't use field injection, use constructor injection."
Zipkin,60987725,60974758,2,"2020/04/02, 11:56:59",False,"2020/04/02, 11:56:59",8336.0,1773866.0,0,Also new span there doesn't make sense cause you already have a new span created by the framework.
Zipkin,59405789,59405521,1,"2019/12/19, 10:24:24",False,"2019/12/19, 10:24:24",8336.0,1773866.0,1,Your versions are wrong.
Zipkin,59405789,59405521,1,"2019/12/19, 10:24:24",False,"2019/12/19, 10:24:24",8336.0,1773866.0,1,"Please don't set the versions by yourself, please use the Spring Cloud BOM (spring-cloud-dependencies) dependency management like presented below"
Zipkin,59405789,59405521,1,"2019/12/19, 10:24:24",False,"2019/12/19, 10:24:24",8336.0,1773866.0,1,Also - it's enough for you to add the starters.
Zipkin,59405789,59405521,1,"2019/12/19, 10:24:24",False,"2019/12/19, 10:24:24",8336.0,1773866.0,1,"You've added a starter in a given version and then you've added the core dependency in another one, that makes no sense."
Zipkin,59405789,59405521,1,"2019/12/19, 10:24:24",False,"2019/12/19, 10:24:24",8336.0,1773866.0,1,Last thing - versions 1.x are deprecated and no longer maintained.
Zipkin,59405789,59405521,1,"2019/12/19, 10:24:24",False,"2019/12/19, 10:24:24",8336.0,1773866.0,1,The current version is 2.2.0.RELEASE and release train version is Hoxton.RELEASE
Zipkin,58357995,58349450,1,"2019/10/12, 23:01:05",False,"2019/10/12, 23:01:05",1164.0,113183.0,0,This can be done using Finagle's  Contexts .
Zipkin,58357995,58349450,1,"2019/10/12, 23:01:05",False,"2019/10/12, 23:01:05",1164.0,113183.0,0,"Contexts give you access to request-scoped state, such as a request’s deadline, throughout the logical life of a request without requiring them to be explicitly passed"
Zipkin,55535572,55090908,0,"2019/04/05, 15:37:34",False,"2019/04/05, 15:37:34",8336.0,1773866.0,0,Unfortunately the best answer to this issue is to upgrade to the latest version of Sleuth where we've migrated to Brave as an internal tracer and fixed a lot of issues.
Zipkin,54445765,54445701,1,"2019/01/30, 19:00:50",False,"2019/01/30, 19:00:50",760.0,4178894.0,0,You can connect your existing container to another network
Zipkin,54001979,53985744,0,"2019/01/02, 08:11:22",False,"2019/01/02, 08:11:22",1.0,10856416.0,-1,The error-code implies an error on the other end - 400-errors are not located on your end.
Zipkin,54001979,53985744,0,"2019/01/02, 08:11:22",False,"2019/01/02, 08:11:22",1.0,10856416.0,-1,Have you tried dumping the response (including headers)?
Zipkin,54001979,53985744,0,"2019/01/02, 08:11:22",False,"2019/01/02, 08:11:22",1.0,10856416.0,-1,"Also, did you try to re-authenticate, perhaps reset cookings, etc?"
Zipkin,54001979,53985744,0,"2019/01/02, 08:11:22",False,"2019/01/02, 08:11:22",1.0,10856416.0,-1,Did you contact the other end?
Zipkin,54001979,53985744,0,"2019/01/02, 08:11:22",False,"2019/01/02, 08:11:22",1.0,10856416.0,-1,How did they respond to it?
Zipkin,53391639,53391503,0,"2018/11/20, 13:06:05",False,"2018/11/20, 13:06:05",26220.0,927493.0,0,Add a dependencyManagement entry for io.zipkin.zipkin2:zipkin:2.7.1
Zipkin,53393882,53391503,0,"2018/11/20, 15:18:12",True,"2018/11/20, 15:18:12",330.0,1270045.0,0,"For me or anybody finding this thread:
Solved it by upgrading from Camden to Edgware which contains 1.3.5 (and resolving everything around that switch)."
Zipkin,53185008,53154813,1,"2018/11/07, 09:18:45",True,"2018/11/07, 09:18:45",8336.0,1773866.0,1,You can create your own custom  SpanAdjuster  that will modify the span name.
Zipkin,53185008,53154813,1,"2018/11/07, 09:18:45",True,"2018/11/07, 09:18:45",8336.0,1773866.0,1,You can also use  FinishedSpanHandler  to operate on finished spans to tweak them.
Zipkin,52481820,52156749,2,"2018/09/24, 17:42:00",False,"2018/09/24, 17:42:00",8336.0,1773866.0,0,"Yes, when you create a span you can set the service name."
Zipkin,52481820,52156749,2,"2018/09/24, 17:42:00",False,"2018/09/24, 17:42:00",8336.0,1773866.0,0,Just call  newSpan.remoteServiceName(...)
Zipkin,52784486,52156749,0,"2018/10/12, 20:33:48",False,"2018/10/12, 20:33:48",2970.0,4296607.0,0,"Taking the input of @MarcinGrzejszczak as reference, I resolved using a custom span:"
Zipkin,52784486,52156749,0,"2018/10/12, 20:33:48",False,"2018/10/12, 20:33:48",2970.0,4296607.0,0,Where  tracer  is an autowired object from  Trace :
Zipkin,52784486,52156749,0,"2018/10/12, 20:33:48",False,"2018/10/12, 20:33:48",2970.0,4296607.0,0,Both classes are in  brave  package
Zipkin,52784486,52156749,0,"2018/10/12, 20:33:48",False,"2018/10/12, 20:33:48",2970.0,4296607.0,0,Result:
Zipkin,52784486,52156749,0,"2018/10/12, 20:33:48",False,"2018/10/12, 20:33:48",2970.0,4296607.0,0,"If you want to take a look at the implementation in more detail, here is the sample:  https://github.com/juanca87/sample-traceability-microservices"
Zipkin,51934542,51934410,2,"2018/08/20, 19:02:24",True,"2018/08/20, 19:02:24",874.0,1753823.0,0,add this under your  section at the end of your pom.xml.
Zipkin,51934542,51934410,2,"2018/08/20, 19:02:24",True,"2018/08/20, 19:02:24",874.0,1753823.0,0,you may need to add for all the dependencies.
Zipkin,52823986,51534836,0,"2018/10/15, 23:04:45",False,"2018/10/15, 23:04:45",17842.0,7862821.0,0,"Maybe, I didn't get your question right, but with almost your  docker-compose.yaml  file:"
Zipkin,52823986,51534836,0,"2018/10/15, 23:04:45",False,"2018/10/15, 23:04:45",17842.0,7862821.0,0,prometheus  metrics are available on  localhost:9411/metrics  both inside container and on host system:
Zipkin,51494483,51444018,0,"2018/07/24, 11:56:29",False,"2018/07/24, 11:56:29",402.0,4298311.0,0,I finally got it working.
Zipkin,51494483,51444018,0,"2018/07/24, 11:56:29",False,"2018/07/24, 11:56:29",402.0,4298311.0,0,I just changed the logstash config file and added:
Zipkin,51494483,51444018,0,"2018/07/24, 11:56:29",False,"2018/07/24, 11:56:29",402.0,4298311.0,0,The filter part was missing earlier.
Zipkin,50922654,50922384,0,"2018/06/19, 10:19:26",False,"2018/06/19, 10:19:26",8336.0,1773866.0,0,"Can you follow the guidelines described here  https://stackoverflow.com/help/how-to-ask  and the next question you ask, ask it with more details?"
Zipkin,50922654,50922384,0,"2018/06/19, 10:19:26",False,"2018/06/19, 10:19:26",8336.0,1773866.0,0,E.g.
Zipkin,50922654,50922384,0,"2018/06/19, 10:19:26",False,"2018/06/19, 10:19:26",8336.0,1773866.0,0,I have no idea how exactly you use Sleuth?
Zipkin,50922654,50922384,0,"2018/06/19, 10:19:26",False,"2018/06/19, 10:19:26",8336.0,1773866.0,0,Anyways I'll try to answer...
Zipkin,50922654,50922384,0,"2018/06/19, 10:19:26",False,"2018/06/19, 10:19:26",8336.0,1773866.0,0,"You can create a  SpanAdjuster  bean, that will analyze the span information (e.g."
Zipkin,50922654,50922384,0,"2018/06/19, 10:19:26",False,"2018/06/19, 10:19:26",8336.0,1773866.0,0,span tags) and basing on that information you will change the sampling decision so as not to send it to Zipkin.
Zipkin,50922654,50922384,0,"2018/06/19, 10:19:26",False,"2018/06/19, 10:19:26",8336.0,1773866.0,0,Another option is to wrap the default span reporter in a similar logic.
Zipkin,50922654,50922384,0,"2018/06/19, 10:19:26",False,"2018/06/19, 10:19:26",8336.0,1773866.0,0,Yet another option is to verify what kind of a thread it is that is creating this span and toggle it off (assuming that it's a  @Scheduled  method) -  https://cloud.spring.io/spring-cloud-static/Finchley.RC2/single/spring-cloud.html#__literal_scheduled_literal_annotated_methods
Zipkin,50709747,50691266,0,"2018/06/06, 01:22:16",False,"2018/06/06, 02:13:47",1.0,9897847.0,-2,name: tcp -  protocol: TCP ?
Zipkin,49899755,49899572,0,"2018/04/18, 15:29:29",True,"2018/04/18, 15:29:29",8336.0,1773866.0,0,"I tried to back to use the stable version(1.3.3) of spring cloud sleuth, but when i use the bom for the project its make conflict in the spring boot version that i am using(2.0)."
Zipkin,49899755,49899572,0,"2018/04/18, 15:29:29",True,"2018/04/18, 15:29:29",8336.0,1773866.0,0,Its compactible with the spring boot version 2?
Zipkin,49899755,49899572,0,"2018/04/18, 15:29:29",True,"2018/04/18, 15:29:29",8336.0,1773866.0,0,You can't use the Sleuth 1.3 with Boot 2.0.
Zipkin,49899755,49899572,0,"2018/04/18, 15:29:29",True,"2018/04/18, 15:29:29",8336.0,1773866.0,0,"I am using the spring cloud sleuth to make trace of services on my company, but i have a version of tracing on others services that is not compactible with the opentracing headers, so i want to change the headers of http messages to make the new services compactibles with the current tracing headers that i have in the others components."
Zipkin,49899755,49899572,0,"2018/04/18, 15:29:29",True,"2018/04/18, 15:29:29",8336.0,1773866.0,0,"Yeah, that's the Brave change."
Zipkin,49899755,49899572,0,"2018/04/18, 15:29:29",True,"2018/04/18, 15:29:29",8336.0,1773866.0,0,For http you can define your own parses.
Zipkin,49899755,49899572,0,"2018/04/18, 15:29:29",True,"2018/04/18, 15:29:29",8336.0,1773866.0,0,https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/instrument/web/TraceHttpAutoConfiguration.java#L57-L68  .
Zipkin,49899755,49899572,0,"2018/04/18, 15:29:29",True,"2018/04/18, 15:29:29",8336.0,1773866.0,0,"
@Autowired HttpClientParser clientParser;
    @Autowired HttpServerParser serverParser;
    @Autowired @ClientSampler HttpSampler clientSampler;
    @Autowired(required = false) @ServerSampler HttpSampler serverSampler;"
Zipkin,49899755,49899572,0,"2018/04/18, 15:29:29",True,"2018/04/18, 15:29:29",8336.0,1773866.0,0,These are the samplers that you can register.
Zipkin,49899755,49899572,0,"2018/04/18, 15:29:29",True,"2018/04/18, 15:29:29",8336.0,1773866.0,0,For messaging you'd have to create your own version of the global channel interceptor - like the one we define here -  https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/instrument/messaging/TraceSpringIntegrationAutoConfiguration.java#L49-L53  .
Zipkin,49899755,49899572,0,"2018/04/18, 15:29:29",True,"2018/04/18, 15:29:29",8336.0,1773866.0,0,"If that's not acceptable for you, go ahead and file an issue in Sleuth so we can discuss it there."
Zipkin,49402195,49393630,0,"2018/03/21, 11:13:30",True,"2018/03/21, 11:13:30",762.0,1227937.0,0,There's no concept of a trace finishing in zipkin.
Zipkin,49402195,49393630,0,"2018/03/21, 11:13:30",True,"2018/03/21, 11:13:30",762.0,1227937.0,0,A span within a trace can start and finish.
Zipkin,49402195,49393630,0,"2018/03/21, 11:13:30",True,"2018/03/21, 11:13:30",762.0,1227937.0,0,"We don't start and stop spans on different hosts, so unfinished spans probably are accidental."
Zipkin,49402195,49393630,0,"2018/03/21, 11:13:30",True,"2018/03/21, 11:13:30",762.0,1227937.0,0,You can chat more here if you like  https://gitter.im/spring-cloud/spring-cloud-sleuth
Zipkin,49242542,49241122,0,"2018/03/12, 20:53:14",False,"2018/03/12, 20:53:14",8336.0,1773866.0,1,"In Sleuth  1.3.x  you can create a custom  SpanReporter  that, before sending a span to Zipkin, would analyze the URL and would not report that span."
Zipkin,49242542,49241122,0,"2018/03/12, 20:53:14",False,"2018/03/12, 20:53:14",8336.0,1773866.0,1,In Sleuth  2.0.x  you can create a custom  HttpSampler  for the client side (with name  sleuthClientSampler )
Zipkin,49202659,49201687,1,"2018/03/09, 23:47:19",True,"2018/03/09, 23:47:19",13548.0,33404.0,1,Turns out the issue was a corrupt Maven package.
Zipkin,49202659,49201687,1,"2018/03/09, 23:47:19",True,"2018/03/09, 23:47:19",13548.0,33404.0,1,Deleting my  .m2\repository  folder and running  mvn spring-boot:run  to downloaded dependencies and run my app resolved the issue.
Zipkin,48900919,48893982,0,"2018/02/21, 10:12:42",True,"2018/02/21, 10:12:42",8336.0,1773866.0,0,Please read this page from Zipkin -  https://zipkin.io/pages/instrumenting.html  .
Zipkin,48900919,48893982,0,"2018/02/21, 10:12:42",True,"2018/02/21, 10:12:42",8336.0,1773866.0,0,It's all written there how things should work.
Zipkin,48900919,48893982,0,"2018/02/21, 10:12:42",True,"2018/02/21, 10:12:42",8336.0,1773866.0,0,HTTP Tracing
Zipkin,48900919,48893982,0,"2018/02/21, 10:12:42",True,"2018/02/21, 10:12:42",8336.0,1773866.0,0,HTTP headers are used to pass along trace information.
Zipkin,48900919,48893982,0,"2018/02/21, 10:12:42",True,"2018/02/21, 10:12:42",8336.0,1773866.0,0,The B3 portion of the header is so named for the original name of Zipkin: BigBrotherBird.
Zipkin,48900919,48893982,0,"2018/02/21, 10:12:42",True,"2018/02/21, 10:12:42",8336.0,1773866.0,0,Also please check the B3 specification page -  https://github.com/openzipkin/b3-propagation
Zipkin,48874125,48873756,13,"2018/02/19, 23:02:35",True,"2018/02/19, 23:02:35",8336.0,1773866.0,0,people are scared about the overhead which sleuth can have by adding @NewSpan on the methods.
Zipkin,48874125,48873756,13,"2018/02/19, 23:02:35",True,"2018/02/19, 23:02:35",8336.0,1773866.0,0,Do they have any information about the overhead?
Zipkin,48874125,48873756,13,"2018/02/19, 23:02:35",True,"2018/02/19, 23:02:35",8336.0,1773866.0,0,Have they turned it on and the application started to lag significantly?
Zipkin,48874125,48873756,13,"2018/02/19, 23:02:35",True,"2018/02/19, 23:02:35",8336.0,1773866.0,0,What are they scared of?
Zipkin,48874125,48873756,13,"2018/02/19, 23:02:35",True,"2018/02/19, 23:02:35",8336.0,1773866.0,0,Is this a high-frequency trading application that you're doing where every microsecond counts?
Zipkin,48874125,48873756,13,"2018/02/19, 23:02:35",True,"2018/02/19, 23:02:35",8336.0,1773866.0,0,I need to decide on runtime whether the Trace should be added or not (Not talking about exporting).
Zipkin,48874125,48873756,13,"2018/02/19, 23:02:35",True,"2018/02/19, 23:02:35",8336.0,1773866.0,0,Like for actuator trace is not getting added at all.
Zipkin,48874125,48873756,13,"2018/02/19, 23:02:35",True,"2018/02/19, 23:02:35",8336.0,1773866.0,0,I assume this will have no overhead on the application.
Zipkin,48874125,48873756,13,"2018/02/19, 23:02:35",True,"2018/02/19, 23:02:35",8336.0,1773866.0,0,Putting X-B3-Sampled = 0 is not exporting but adding tracing information.
Zipkin,48874125,48873756,13,"2018/02/19, 23:02:35",True,"2018/02/19, 23:02:35",8336.0,1773866.0,0,Something like skipPattern property but at runtime.
Zipkin,48874125,48873756,13,"2018/02/19, 23:02:35",True,"2018/02/19, 23:02:35",8336.0,1773866.0,0,I don't think that's possible.
Zipkin,48874125,48873756,13,"2018/02/19, 23:02:35",True,"2018/02/19, 23:02:35",8336.0,1773866.0,0,"The instrumentation is set up by adding interceptors, aspects etc."
Zipkin,48874125,48873756,13,"2018/02/19, 23:02:35",True,"2018/02/19, 23:02:35",8336.0,1773866.0,0,They are started upon application initialization.
Zipkin,48874125,48873756,13,"2018/02/19, 23:02:35",True,"2018/02/19, 23:02:35",8336.0,1773866.0,0,Always export the trace if service exceeds a certain threshold or in case of Exception.
Zipkin,48874125,48873756,13,"2018/02/19, 23:02:35",True,"2018/02/19, 23:02:35",8336.0,1773866.0,0,With the new Brave tracer instrumentation (Sleuth 2.0.0) you will be able to do it in a much easier way.
Zipkin,48874125,48873756,13,"2018/02/19, 23:02:35",True,"2018/02/19, 23:02:35",8336.0,1773866.0,0,"Prior to this version you would have to implement your own version of a  SpanReporter  that verifies the tags (if it contains an  error  tag), and if that's the case send it to zipkin, otherwise not."
Zipkin,48874125,48873756,13,"2018/02/19, 23:02:35",True,"2018/02/19, 23:02:35",8336.0,1773866.0,0,If I am not exporting Spans to zipkin then will there be any overhead by tracing information?
Zipkin,48874125,48873756,13,"2018/02/19, 23:02:35",True,"2018/02/19, 23:02:35",8336.0,1773866.0,0,"Yes, there is cause you need to pass tracing data."
Zipkin,48874125,48873756,13,"2018/02/19, 23:02:35",True,"2018/02/19, 23:02:35",8336.0,1773866.0,0,"However, the overhead is small."
Zipkin,48006082,47934052,2,"2017/12/28, 12:49:29",False,"2017/12/28, 12:49:29",8336.0,1773866.0,0,The code you've provided is not related to Sleuth but opentracing.
Zipkin,48006082,47934052,2,"2017/12/28, 12:49:29",False,"2017/12/28, 12:49:29",8336.0,1773866.0,0,"In Sleuth you would call  Tracer.createSpan(""name"")  and that way a child span od your current trace would be created."
Zipkin,48013962,47934052,0,"2017/12/28, 22:29:13",False,"2017/12/28, 22:29:13",21.0,6203524.0,0,I've also managed to get it working by using just the cloud trace api by doing this before I create a span.
Zipkin,48013962,47934052,0,"2017/12/28, 22:29:13",False,"2017/12/28, 22:29:13",21.0,6203524.0,0,Not sure if there is a negative of doing this.
Zipkin,42800431,42499468,0,"2017/03/15, 05:08:37",True,"2017/03/15, 05:08:37",91.0,5460458.0,0,I succeeded to send gcp's trace api in php client via REST.
Zipkin,42800431,42499468,0,"2017/03/15, 05:08:37",True,"2017/03/15, 05:08:37",91.0,5460458.0,0,"It can see trace set by php client parameters , but my endpoint for trace api has stopped though I don't know why.Maybe ,it is not still supported well because the document have many ambiguous expression so, I realized watching server response by BigQuery with fluentd and DataStudio and it seem best solution because auto span can be set by table name with yyyymmdd and we can watch arbitrary metrics with custom query or calculation field."
Zipkin,41441905,41441661,1,"2017/01/03, 12:56:57",True,"2017/01/03, 12:56:57",762.0,1227937.0,2,Zipkin's connection to cassandra is independent from the normal spring setup.
Zipkin,41441905,41441661,1,"2017/01/03, 12:56:57",True,"2017/01/03, 12:56:57",762.0,1227937.0,2,We use some very specific setup.
Zipkin,41441905,41441661,1,"2017/01/03, 12:56:57",True,"2017/01/03, 12:56:57",762.0,1227937.0,2,you'll want to set properties in the namespace of zipkin.storage.cassandra
Zipkin,41441905,41441661,1,"2017/01/03, 12:56:57",True,"2017/01/03, 12:56:57",762.0,1227937.0,2,https://github.com/openzipkin/zipkin/blob/master/zipkin-server/src/main/resources/zipkin-server-shared.yml#L40
Zipkin,66896684,66766936,0,"2021/04/01, 02:45:08",True,"2021/04/01, 02:45:08",1693.0,971735.0,0,"As the documentation suggests , you need to create a  ProducerFactory  bean if you want to use your own  KafkaTemplate :"
Zipkin,66704380,66687298,1,"2021/03/19, 09:57:02",False,"2021/03/19, 09:57:02",2568.0,8225898.0,0,The Spring Cloud project is moving to their own solutions.
Zipkin,66704380,66687298,1,"2021/03/19, 09:57:02",False,"2021/03/19, 09:57:02",2568.0,8225898.0,0,"Ribbon is replaced by the Spring Cloud Load Balancer, Hysterix by the Spring Cloud Circuit Breaker, Zuul by the Spring Cloud Gateway."
Zipkin,66704380,66687298,1,"2021/03/19, 09:57:02",False,"2021/03/19, 09:57:02",2568.0,8225898.0,0,"This is a good read, including examples, about this topic:  https://piotrminkowski.com/2020/05/01/a-new-era-of-spring-cloud/"
Zipkin,66481119,66475457,0,"2021/03/04, 20:41:58",False,"2021/03/04, 20:41:58",1693.0,971735.0,0,Sleuth does this for you by default in 3.x too:  https://docs.spring.io/spring-cloud-sleuth/docs/current/reference/htmlsingle/#features-log-integration
Zipkin,66481119,66475457,0,"2021/03/04, 20:41:58",False,"2021/03/04, 20:41:58",1693.0,971735.0,0,You can break this functionality by misconfiguring your log pattern or  logging.pattern.level  or your classpath.
Zipkin,66481119,66475457,0,"2021/03/04, 20:41:58",False,"2021/03/04, 20:41:58",1693.0,971735.0,0,"What I would suggest is going to  https://start.spring.io , generate a new project using sleuth and web/webflux, write a single controller and check the logs (do not create any log config file, just leave everything on default)."
Zipkin,65718763,65669868,4,"2021/01/14, 14:14:20",True,"2021/01/14, 14:14:20",2294.0,10347794.0,1,Let me add to this thread my three bits.
Zipkin,65718763,65669868,4,"2021/01/14, 14:14:20",True,"2021/01/14, 14:14:20",2294.0,10347794.0,1,"Speaking of Envoy, yes, when attached to your application it adds a lot of useful features from observability bucket, e.g."
Zipkin,65718763,65669868,4,"2021/01/14, 14:14:20",True,"2021/01/14, 14:14:20",2294.0,10347794.0,1,network level statistics and tracing.
Zipkin,65718763,65669868,4,"2021/01/14, 14:14:20",True,"2021/01/14, 14:14:20",2294.0,10347794.0,1,"Here is the question, have you considered running your legacy apps inside service mesh, like Istio ?."
Zipkin,65718763,65669868,4,"2021/01/14, 14:14:20",True,"2021/01/14, 14:14:20",2294.0,10347794.0,1,Istio simplifies deployment and configuration of Envoy for you.
Zipkin,65718763,65669868,4,"2021/01/14, 14:14:20",True,"2021/01/14, 14:14:20",2294.0,10347794.0,1,"It injects sidecar container (istio-proxy, in fact Envoy instance) to your Pod application, and gives you these extra features like a set of service metrics out of the box*."
Zipkin,65718763,65669868,4,"2021/01/14, 14:14:20",True,"2021/01/14, 14:14:20",2294.0,10347794.0,1,"Example: Stats produced by Envoy in Prometheus format, like  istio_request_bytes  are visualized in Kiali Metrics dashboard for inbound traffic as  request_size  (check screenshot)"
Zipkin,65718763,65669868,4,"2021/01/14, 14:14:20",True,"2021/01/14, 14:14:20",2294.0,10347794.0,1,"*as mentioned by @David Kruk, you still needs to have Prometheus server deployed in your cluster to be able to pull these metrics to Kiali dashboards."
Zipkin,65718763,65669868,4,"2021/01/14, 14:14:20",True,"2021/01/14, 14:14:20",2294.0,10347794.0,1,You can learn more about Istio  here .
Zipkin,65718763,65669868,4,"2021/01/14, 14:14:20",True,"2021/01/14, 14:14:20",2294.0,10347794.0,1,There is also a dedicated section on how to  visualize metrics  collected by Istio (e.g.
Zipkin,65718763,65669868,4,"2021/01/14, 14:14:20",True,"2021/01/14, 14:14:20",2294.0,10347794.0,1,request size).
Zipkin,65602072,65600807,5,"2021/01/06, 21:17:20",False,"2021/01/06, 21:17:20",1693.0,971735.0,1,"You can use a  TagValueResolver  or a  MessageSpanCustomizer , see the docs for the details."
Zipkin,63699177,63699141,4,"2020/09/02, 08:23:29",False,"2020/09/02, 08:23:29",6381.0,2528083.0,0,Better approach will be using Kafka instead of Redis.
Zipkin,63699177,63699141,4,"2020/09/02, 08:23:29",False,"2020/09/02, 08:23:29",6381.0,2528083.0,0,"Create a topic for every microservice &amp; keep moving the packet from
one topic to another after processing."
Zipkin,63699177,63699141,4,"2020/09/02, 08:23:29",False,"2020/09/02, 08:23:29",6381.0,2528083.0,0,"Keep appending the results to same object and keep moving it down the line, untill every micro-service has processed it."
Zipkin,61648502,61646918,2,"2020/05/07, 04:50:02",False,"2020/05/07, 04:50:02",43078.0,78722.0,0,"What you have is correct, your issue is likely not DNS."
Zipkin,61648502,61646918,2,"2020/05/07, 04:50:02",False,"2020/05/07, 04:50:02",43078.0,78722.0,0,You can confirm by doing just a DNS lookup and comparing that to the IP of the Service.
Zipkin,59963724,59867690,1,"2020/01/29, 11:09:14",False,"2020/01/29, 11:09:14",7644.0,259167.0,0,The  doubleName  method is private.
Zipkin,59963724,59867690,1,"2020/01/29, 11:09:14",False,"2020/01/29, 11:09:14",7644.0,259167.0,0,Micronaut cannot apply AOP annotations (like  ContinueSpan  to private methods.
Zipkin,61001724,59867690,0,"2020/04/03, 00:47:14",False,"2020/04/03, 00:47:14",289.0,45693.0,0,"Are these methods on a bean ( Singleton , etc.)?"
Zipkin,61001724,59867690,0,"2020/04/03, 00:47:14",False,"2020/04/03, 00:47:14",289.0,45693.0,0,I have found the span annotations only get applied on beans properly.
Zipkin,61001724,59867690,0,"2020/04/03, 00:47:14",False,"2020/04/03, 00:47:14",289.0,45693.0,0,I had to refactor some of my code to create beans from  Factory s or such.
Zipkin,58272901,58272315,0,"2019/10/07, 18:41:12",True,"2019/10/07, 18:41:12",8336.0,1773866.0,0,You've passed in the  B3Propagation.FACTORY  as the implementation of the propagation factory so you're explicitly stating that you want the default B3 headers.
Zipkin,58272901,58272315,0,"2019/10/07, 18:41:12",True,"2019/10/07, 18:41:12",8336.0,1773866.0,0,You've said that you want some other field that is alphanumeric to be also propagated.
Zipkin,58272901,58272315,0,"2019/10/07, 18:41:12",True,"2019/10/07, 18:41:12",8336.0,1773866.0,0,"Then in a log parsing tool you can define that you want to use your custom field as the trace id, but it doesn't mean that the deafult X-B3-TraceId field will be changed."
Zipkin,58272901,58272315,0,"2019/10/07, 18:41:12",True,"2019/10/07, 18:41:12",8336.0,1773866.0,0,"If you want to use your custom field as trace id that Sleuth understands, you need to change the logging format and implement a different propagation factory bean."
Zipkin,58436544,58272315,0,"2019/10/17, 18:56:45",False,"2019/10/17, 19:03:55",3.0,9052240.0,0,"One  of the  way which   worked for me is 
using ExtraFieldPropagation
and adding those   keys in sleuth  properties  under   propagation-keys
and  whitelisted-keys"
Zipkin,58436544,58272315,0,"2019/10/17, 18:56:45",False,"2019/10/17, 19:03:55",3.0,9052240.0,0,"sample code 
  '  @Autowired Tracer tracer;"
Zipkin,56349083,56293312,0,"2019/05/28, 22:43:26",False,"2019/05/28, 22:43:26",129048.0,1240763.0,0,"With Edgware (SCSt Ditmars), you have to specify which headers will be transferred."
Zipkin,56349083,56293312,0,"2019/05/28, 22:43:26",False,"2019/05/28, 22:43:26",129048.0,1240763.0,0,See Kafka Binder Properties .
Zipkin,56349083,56293312,0,"2019/05/28, 22:43:26",False,"2019/05/28, 22:43:26",129048.0,1240763.0,0,This is because Edgware was based on Kafka before it supported headers natively and we encode the headers into the payload.
Zipkin,56349083,56293312,0,"2019/05/28, 22:43:26",False,"2019/05/28, 22:43:26",129048.0,1240763.0,0,spring.cloud.stream.kafka.binder.headers
Zipkin,56349083,56293312,0,"2019/05/28, 22:43:26",False,"2019/05/28, 22:43:26",129048.0,1240763.0,0,The list of custom headers that will be transported by the binder.
Zipkin,56349083,56293312,0,"2019/05/28, 22:43:26",False,"2019/05/28, 22:43:26",129048.0,1240763.0,0,Default: empty.
Zipkin,56349083,56293312,0,"2019/05/28, 22:43:26",False,"2019/05/28, 22:43:26",129048.0,1240763.0,0,You should also be sure to upgrade spring-kafka to 1.3.9.RELEASE and kafka-clients to 0.11.0.2.
Zipkin,56349083,56293312,0,"2019/05/28, 22:43:26",False,"2019/05/28, 22:43:26",129048.0,1240763.0,0,"Preferably, though, upgrade to Finchley or Greemwich."
Zipkin,56349083,56293312,0,"2019/05/28, 22:43:26",False,"2019/05/28, 22:43:26",129048.0,1240763.0,0,Those versions support headers natively.
Zipkin,55039063,55036782,0,"2019/03/07, 10:20:10",False,"2019/03/07, 10:20:10",181.0,5436378.0,0,"Well, without seeing any code, I could only give you a sample of how you should achieve this."
Zipkin,55039063,55036782,0,"2019/03/07, 10:20:10",False,"2019/03/07, 10:20:10",181.0,5436378.0,0,"So an http call, for example if you use node-fetch or axios will return a promise."
Zipkin,55039063,55036782,0,"2019/03/07, 10:20:10",False,"2019/03/07, 10:20:10",181.0,5436378.0,0,"To wait for promises paralelly, you can do the following:"
Zipkin,55039063,55036782,0,"2019/03/07, 10:20:10",False,"2019/03/07, 10:20:10",181.0,5436378.0,0,"Note that I use fetch API here, provided in node by the node-fetch package."
Zipkin,55039063,55036782,0,"2019/03/07, 10:20:10",False,"2019/03/07, 10:20:10",181.0,5436378.0,0,Fetch returns a  Promise .
Zipkin,55039063,55036782,0,"2019/03/07, 10:20:10",False,"2019/03/07, 10:20:10",181.0,5436378.0,0,Then I call  Promise.all(promises)  where  promises  is a  Promise  array.
Zipkin,55039063,55036782,0,"2019/03/07, 10:20:10",False,"2019/03/07, 10:20:10",181.0,5436378.0,0,You can then do whatever you would like to do with the 3 responses and your requests were made paralelly.
Zipkin,55039063,55036782,0,"2019/03/07, 10:20:10",False,"2019/03/07, 10:20:10",181.0,5436378.0,0,"Hope this helps, good luck!"
Zipkin,54945201,54943755,7,"2019/03/01, 15:00:07",False,"2019/03/01, 15:00:07",26220.0,927493.0,0,You have
Zipkin,54945201,54943755,7,"2019/03/01, 15:00:07",False,"2019/03/01, 15:00:07",26220.0,927493.0,0,This means that there is a  &lt;dependencyManagement&gt;  entry in your POM or your Parent POM that sets the version to  2.2.0 .
Zipkin,51854323,51851541,5,"2018/08/15, 10:07:09",False,"2018/08/22, 18:07:28",8336.0,1773866.0,0,You can have s custom span reporter that before sending spans to zipkin will dump the span as a json structure to logs.
Zipkin,51854323,51851541,5,"2018/08/15, 10:07:09",False,"2018/08/22, 18:07:28",8336.0,1773866.0,0,UPDATE:
Zipkin,51854323,51851541,5,"2018/08/15, 10:07:09",False,"2018/08/22, 18:07:28",8336.0,1773866.0,0,"With this PR merged  https://github.com/spring-cloud/spring-cloud-sleuth/pull/1068 , in 2.1.0 you'll have an easy way to implement your own MDC entries"
Zipkin,48660632,48659538,2,"2018/02/07, 11:49:39",True,"2018/02/07, 11:49:39",8336.0,1773866.0,1,Usage of Sleuth Stream is deprecated.
Zipkin,48660632,48659538,2,"2018/02/07, 11:49:39",True,"2018/02/07, 11:49:39",8336.0,1773866.0,1,"Please use the  zipkin  starter, add the  Kafka  dependency and set things as presented here  https://cloud.spring.io/spring-cloud-static/Edgware.SR1/multi/multi__introduction.html#_sleuth_with_zipkin_via_rabbitmq_or_kafka"
Zipkin,48140158,48137375,1,"2018/01/07, 20:14:38",False,"2018/01/07, 20:14:38",111.0,2639742.0,1,"fran, in Edgware.RELEASE the  &lt;artifactId&gt;spring-cloud-sleuth-zipkin&lt;/artifactId&gt;  will resolve Zipkin 2 dependencies try using  &lt;artifactId&gt; spring-cloud-starter-zipkin-legacy&lt;/artifactId&gt;  instead"
Zipkin,52525191,48137375,0,"2018/09/26, 22:55:47",False,"2018/09/26, 22:55:47",1.0,7391489.0,0,"To define the primary connection factory for RabbitMQ in XML files, you can do something like this:"
Zipkin,47297460,47293445,0,"2017/11/15, 02:20:26",False,"2017/11/15, 02:20:26",67.0,8942125.0,0,"I do not have much knowledge with hysterix, but if you are trying to pass some contextual info like trace IDs around, then io.grpc.Context is the correct class to use."
Zipkin,47297460,47293445,0,"2017/11/15, 02:20:26",False,"2017/11/15, 02:20:26",67.0,8942125.0,0,You would need to call  context.withValue  to create a new context with the traceID.
Zipkin,47297460,47293445,0,"2017/11/15, 02:20:26",False,"2017/11/15, 02:20:26",67.0,8942125.0,0,"In the places where you want the data, you need to attach the context."
Zipkin,47297460,47293445,0,"2017/11/15, 02:20:26",False,"2017/11/15, 02:20:26",67.0,8942125.0,0,"Also be sure to detach the context when done, which I do not see happening in your snippet."
Zipkin,47352178,47293445,6,"2017/11/17, 15:52:06",False,"2017/11/17, 15:52:06",58.0,2174913.0,0,You need to use ...
Zipkin,47352178,47293445,6,"2017/11/17, 15:52:06",False,"2017/11/17, 15:52:06",58.0,2174913.0,0,HystrixPlugins.getInstance().registerConcurrencyStrategy(...)
Zipkin,47352178,47293445,6,"2017/11/17, 15:52:06",False,"2017/11/17, 15:52:06",58.0,2174913.0,0,... to register a custom  HystrixConcurrencyStrategy  that uses your own  Callable  ...
Zipkin,47352178,47293445,6,"2017/11/17, 15:52:06",False,"2017/11/17, 15:52:06",58.0,2174913.0,0,... that applies context preservation around the circuit ...
Zipkin,47352178,47293445,6,"2017/11/17, 15:52:06",False,"2017/11/17, 15:52:06",58.0,2174913.0,0,... via is a helper class capable of preserving the Zipkin context ...
Zipkin,47352178,47293445,6,"2017/11/17, 15:52:06",False,"2017/11/17, 15:52:06",58.0,2174913.0,0,... and allowing an easy method of adding other contexts you may wish to preserve e.g.
Zipkin,47352178,47293445,6,"2017/11/17, 15:52:06",False,"2017/11/17, 15:52:06",58.0,2174913.0,0,"MDC, SecurityContext etc ..."
Zipkin,42395388,42386925,3,"2017/02/22, 17:14:57",False,"2017/02/22, 17:14:57",129048.0,1240763.0,0,This is a guess.
Zipkin,42395388,42386925,3,"2017/02/22, 17:14:57",False,"2017/02/22, 17:14:57",129048.0,1240763.0,0,Kafka has no concept of message headers (where spans are stored).
Zipkin,42395388,42386925,3,"2017/02/22, 17:14:57",False,"2017/02/22, 17:14:57",129048.0,1240763.0,0,SCSt therefore has to embed message headers in the payload.
Zipkin,42395388,42386925,3,"2017/02/22, 17:14:57",False,"2017/02/22, 17:14:57",129048.0,1240763.0,0,The current version requires you to &quot;opt-in&quot; which headers you want transported in this way.
Zipkin,42395388,42386925,3,"2017/02/22, 17:14:57",False,"2017/02/22, 17:14:57",129048.0,1240763.0,0,Documentation here .
Zipkin,42395388,42386925,3,"2017/02/22, 17:14:57",False,"2017/02/22, 17:14:57",129048.0,1240763.0,0,spring.cloud.stream.kafka.binder.headers
Zipkin,42395388,42386925,3,"2017/02/22, 17:14:57",False,"2017/02/22, 17:14:57",129048.0,1240763.0,0,The list of custom headers that will be transported by the binder.
Zipkin,42395388,42386925,3,"2017/02/22, 17:14:57",False,"2017/02/22, 17:14:57",129048.0,1240763.0,0,Default: empty.
Zipkin,42395388,42386925,3,"2017/02/22, 17:14:57",False,"2017/02/22, 17:14:57",129048.0,1240763.0,0,"Unfortunately, patterns are not currently supported, you have to list the headers individually."
Zipkin,42395388,42386925,3,"2017/02/22, 17:14:57",False,"2017/02/22, 17:14:57",129048.0,1240763.0,0,We are  considering adding support for patterns  and/or transporting all headers by default.
Zipkin,42409200,42386925,1,"2017/02/23, 09:11:02",False,"2017/02/23, 09:11:02",31.0,6503007.0,0,"Finally, I found 2 issues related with my applications."
Zipkin,42409200,42386925,1,"2017/02/23, 09:11:02",False,"2017/02/23, 09:11:02",31.0,6503007.0,0,1.
Zipkin,42409200,42386925,1,"2017/02/23, 09:11:02",False,"2017/02/23, 09:11:02",31.0,6503007.0,0,The application with @EnalbeZipkinStreamServer could not be traced.
Zipkin,42409200,42386925,1,"2017/02/23, 09:11:02",False,"2017/02/23, 09:11:02",31.0,6503007.0,0,This looks like a by design.
Zipkin,42409200,42386925,1,"2017/02/23, 09:11:02",False,"2017/02/23, 09:11:02",31.0,6503007.0,0,2.
Zipkin,42409200,42386925,1,"2017/02/23, 09:11:02",False,"2017/02/23, 09:11:02",31.0,6503007.0,0,"If kafka is used as the binder, the applications should specify the headers as the following:"
Zipkin,52815924,41086850,0,"2018/10/15, 14:37:50",False,"2018/10/15, 17:52:20",1.0,10507091.0,0,"I'm not sure this is what you are expecting, you can add this dependancy in  pom.xml  if you are using maven:"
Zipkin,52815924,41086850,0,"2018/10/15, 14:37:50",False,"2018/10/15, 17:52:20",1.0,10507091.0,0,and a  AlwaysSampler @Bean  in your  SpringBootApplication  class
Zipkin,52815924,41086850,0,"2018/10/15, 14:37:50",False,"2018/10/15, 17:52:20",1.0,10507091.0,0,This will help you to sample your inputs in zipkin all time.
Zipkin,37706728,37642783,0,"2016/06/08, 18:23:49",True,"2016/06/08, 18:23:49",8336.0,1773866.0,1,The best thing to do would be to show your sample project.
Zipkin,37706728,37642783,0,"2016/06/08, 18:23:49",True,"2016/06/08, 18:23:49",8336.0,1773866.0,1,Another is to check if you don't have a custom logback.xml or any other type of logging configuration that breaks the current set up (most likely you do cause I can see that the pattern is different).
Zipkin,37608801,37608464,0,"2016/06/03, 10:47:09",True,"2016/06/03, 10:47:09",8336.0,1773866.0,2,So you have an ip and port of your app so that could give you a hint.
Zipkin,37608801,37608464,0,"2016/06/03, 10:47:09",True,"2016/06/03, 10:47:09",8336.0,1773866.0,2,"Also if you want a custom span of yours to have that information, then it's enough to add a custom tag to it."
Zipkin,37608801,37608464,0,"2016/06/03, 10:47:09",True,"2016/06/03, 10:47:09",8336.0,1773866.0,2,"Actually you can always call the  tracer.addTag(""key"", ""value"")  to put the additional information that you need."
Zipkin,32429851,32362058,1,"2015/09/07, 04:04:28",False,"2015/09/07, 04:04:28",154.0,2023873.0,0,"I am not sure about the  dcat  distribution itself, but your error may be because you have:"
Zipkin,32429851,32362058,1,"2015/09/07, 04:04:28",False,"2015/09/07, 04:04:28",154.0,2023873.0,0,"however when you specify  p  in the model, it is a three dimensional array  p[j,k,i]."
Zipkin,32429851,32362058,1,"2015/09/07, 04:04:28",False,"2015/09/07, 04:04:28",154.0,2023873.0,0,I think you need:
Zipkin,32429851,32362058,1,"2015/09/07, 04:04:28",False,"2015/09/07, 04:04:28",154.0,2023873.0,0,"Note, the  i  is the last index for  p , and the two commas."
Zipkin,32429851,32362058,1,"2015/09/07, 04:04:28",False,"2015/09/07, 04:04:28",154.0,2023873.0,0,Hope his helps...
Zipkin,59356961,59356270,2,"2019/12/16, 14:47:29",False,"2019/12/16, 14:47:29",8336.0,1773866.0,1,You can use spring cloud sleuth.
Zipkin,59356961,59356270,2,"2019/12/16, 14:47:29",False,"2019/12/16, 14:47:29",8336.0,1773866.0,1,Please check the documentation for examples of using elk stack to harvest the logs.
Zipkin,59356961,59356270,2,"2019/12/16, 14:47:29",False,"2019/12/16, 14:47:29",8336.0,1773866.0,1,"The zipkin server can be fetched as a standalone jar, you don't need to create your custom version"
Zipkin,52852525,52772025,0,"2018/10/17, 13:19:16",True,"2018/10/17, 13:19:16",71.0,7561583.0,1,It is indeed a cluster problem.
Zipkin,52852525,52772025,0,"2018/10/17, 13:19:16",True,"2018/10/17, 13:19:16",71.0,7561583.0,1,There is a problem with the  __consumer_offsets  topic data of kafka.
Zipkin,52852525,52772025,0,"2018/10/17, 13:19:16",True,"2018/10/17, 13:19:16",71.0,7561583.0,1,It is good to restart kafka after deleting.
Zipkin,47323502,47318596,0,"2017/11/16, 09:02:10",False,"2017/11/16, 09:02:10",8336.0,1773866.0,-1,I have no knowledge of it being impossible.
Zipkin,47323502,47318596,0,"2017/11/16, 09:02:10",False,"2017/11/16, 09:02:10",8336.0,1773866.0,-1,Maybe you should first try doing it and then asking a question?
Zipkin,47323502,47318596,0,"2017/11/16, 09:02:10",False,"2017/11/16, 09:02:10",8336.0,1773866.0,-1,"Also, if for some reason it turns out you can't use it, then if you just google  zipkin scala  you'll see things like  https://github.com/lloydmeta/zipkin-futures  ,  https://github.com/bizreach/play-zipkin-tracing  etc."
Zipkin,52798597,52798338,0,"2018/10/14, 03:55:06",False,"2018/10/14, 03:55:06",387.0,6905597.0,0,You'll find all of it on GitHub.
Zipkin,52798597,52798338,0,"2018/10/14, 03:55:06",False,"2018/10/14, 03:55:06",387.0,6905597.0,0,Spring Web annotations
Zipkin,52798597,52798338,0,"2018/10/14, 03:55:06",False,"2018/10/14, 03:55:06",387.0,6905597.0,0,Spring framework annotations
Zipkin,52798597,52798338,0,"2018/10/14, 03:55:06",False,"2018/10/14, 03:55:06",387.0,6905597.0,0,More spring framework annotations
Zipkin,46836832,46762291,1,"2017/10/19, 21:47:05",False,"2017/10/19, 21:47:05",2102.0,5486262.0,1,You can use Apache NiFi's built-in provenance capabilities to trace how a given flow went through the system.
Zipkin,46836832,46762291,1,"2017/10/19, 21:47:05",False,"2017/10/19, 21:47:05",2102.0,5486262.0,1,https://nifi.apache.org/docs/nifi-docs/html/user-guide.html#data_provenance
