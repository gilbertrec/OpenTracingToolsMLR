toolname,answer_id,question_id,comment_count,creation_date,is_accepted,last_activity_date,owner_reputation,owner_id,score,sentence,neg,pos,neu,com
AppDynamics,13532221,13524580,1,"2012/11/23, 17:48:01",False,"2012/11/23, 17:48:01",17.0,1847969.0,0,"I suggest to look into Gartners magic quadrant and get dynaTrace since it has negligible overhead , less than 1% in production under load.",0.0,0.0,1.0,0.0
AppDynamics,13570380,13524580,1,"2012/11/26, 19:52:11",True,"2012/11/26, 19:52:11",74.0,1854219.0,5,Full Disclosure: I currently work for AppDynamics.,0.0,0.0,1.0,0.0
AppDynamics,13570380,13524580,1,"2012/11/26, 19:52:11",True,"2012/11/26, 19:52:11",74.0,1854219.0,5,AppDynamics was designed from the ground up for high volume production environments but works equally well in both prod and non-prod.,0.0,0.117,0.883,0.3919
AppDynamics,13570380,13524580,1,"2012/11/26, 19:52:11",True,"2012/11/26, 19:52:11",74.0,1854219.0,5,"It's currently running in production in some of the worlds largest mission critical application environments at Netflix, Exact Target, Edmunds, and many others.",0.095,0.0,0.905,-0.3182
AppDynamics,13570380,13524580,1,"2012/11/26, 19:52:11",True,"2012/11/26, 19:52:11",74.0,1854219.0,5,Here are a few quotes from existing customers…,0.0,0.0,1.0,0.0
AppDynamics,13570380,13524580,1,"2012/11/26, 19:52:11",True,"2012/11/26, 19:52:11",74.0,1854219.0,5,"""It's like a profiler that you can run in production"" -- Leonid Igolnik, Taleo",0.0,0.172,0.828,0.3612
AppDynamics,13570380,13524580,1,"2012/11/26, 19:52:11",True,"2012/11/26, 19:52:11",74.0,1854219.0,5,"""We found that the overhead was negligible"" -- Jacob Marcus, Care.com",0.0,0.0,1.0,0.0
AppDynamics,13570380,13524580,1,"2012/11/26, 19:52:11",True,"2012/11/26, 19:52:11",74.0,1854219.0,5,"""We wanted a monitoring solution that wouldn't impact our production runway"" -- John Martin, Edmunds",0.0,0.15,0.85,0.3182
AppDynamics,13570380,13524580,1,"2012/11/26, 19:52:11",True,"2012/11/26, 19:52:11",74.0,1854219.0,5,AppDynamics overhead is extremely low but I suggest you test it and see for yourself.,0.115,0.0,0.885,-0.177
AppDynamics,13570380,13524580,1,"2012/11/26, 19:52:11",True,"2012/11/26, 19:52:11",74.0,1854219.0,5,You can download and use it for free from the AppDynamics website.,0.0,0.231,0.769,0.5106
AppDynamics,13570380,13524580,1,"2012/11/26, 19:52:11",True,"2012/11/26, 19:52:11",74.0,1854219.0,5,Good luck in your search for the right APM tool.,0.0,0.424,0.576,0.7096
AppDynamics,13806782,13524580,0,"2012/12/10, 20:18:05",False,"2012/12/10, 20:18:05",207.0,153728.0,1,Yes it will if the application is sensitive to extra GC cycles caused by call stack sampling.,0.0,0.144,0.856,0.4019
AppDynamics,13806782,13524580,0,"2012/12/10, 20:18:05",False,"2012/12/10, 20:18:05",207.0,153728.0,1,The impact will depend on the number of threads and typical call stack depth.,0.0,0.091,0.909,0.0772
AppDynamics,13806782,13524580,0,"2012/12/10, 20:18:05",False,"2012/12/10, 20:18:05",207.0,153728.0,1,"This is not specific to AppDynamics, other call stack sampling solutions such as NewRelic and VisualVM Sampler will have a similar impact.",0.0,0.078,0.922,0.1779
AppDynamics,13806782,13524580,0,"2012/12/10, 20:18:05",False,"2012/12/10, 20:18:05",207.0,153728.0,1,http://www.jinspired.com/wp-content/uploads/2011/11/JXInsightOpenCore_Good_APM_vs_AppDynamics_Bad_APM.pdf,0.0,0.0,1.0,0.0
AppDynamics,13806782,13524580,0,"2012/12/10, 20:18:05",False,"2012/12/10, 20:18:05",207.0,153728.0,1,http://www.jinspired.com/wp-content/uploads/2011/11/JXInsightOpenCore_vs_AppDynamics.pdf,0.0,0.0,1.0,0.0
AppDynamics,13820531,13524580,0,"2012/12/11, 14:32:58",False,"2012/12/11, 14:32:58",207.0,153728.0,4,There are a number of assumptions made by a vendor but the following are most common:,0.0,0.081,0.919,0.0387
AppDynamics,13820531,13524580,0,"2012/12/11, 14:32:58",False,"2012/12/11, 14:32:58",207.0,153728.0,4,TRANSLATION: We assume you have a slow performing database backend.,0.0,0.0,1.0,0.0
AppDynamics,13820531,13524580,0,"2012/12/11, 14:32:58",False,"2012/12/11, 14:32:58",207.0,153728.0,4,TRANSLATION: We assume that you already know your performance hotspots.,0.0,0.0,1.0,0.0
AppDynamics,13820531,13524580,0,"2012/12/11, 14:32:58",False,"2012/12/11, 14:32:58",207.0,153728.0,4,TRANSLATION: We assume that you will not notice tricks used to hide our overhead.,0.113,0.091,0.796,-0.0849
AppDynamics,13820531,13524580,0,"2012/12/11, 14:32:58",False,"2012/12/11, 14:32:58",207.0,153728.0,4,TRANSLATION: We assume that you know little about performance engineering.,0.0,0.0,1.0,0.0
AppDynamics,13820531,13524580,0,"2012/12/11, 14:32:58",False,"2012/12/11, 14:32:58",207.0,153728.0,4,And my favorite (5) is the restriction within a vendors software license on the publication of benchmark results.,0.104,0.149,0.746,0.2263
AppDynamics,13820531,13524580,0,"2012/12/11, 14:32:58",False,"2012/12/11, 14:32:58",207.0,153728.0,4,TRANSLATION: We assume that you blindly accept our claims – unquestionably.,0.0,0.224,0.776,0.3818
AppDynamics,19518863,13524580,0,"2013/10/22, 16:10:28",False,"2013/10/22, 16:10:28",3374.0,2508411.0,1,"Appdynamics will not slow down your system significant, I was on a usermeeting and they said that they always try to be under 2% cpu usage, thats nothing compared to what you get from them.",0.0,0.053,0.947,0.2023
AppDynamics,19518863,13524580,0,"2013/10/22, 16:10:28",False,"2013/10/22, 16:10:28",3374.0,2508411.0,1,"They are working with samples per time, so if you have 10 requests per second or 100, they will still take the same amout of your cpu / bandwith / whatever.",0.0,0.0,1.0,0.0
AppDynamics,17416710,17412297,0,"2013/07/02, 05:56:59",True,"2013/07/02, 05:56:59",415.0,621964.0,6,The way that these products generally work is by doing bytecode injection / function interposition / monkey-patching on commonly used libraries and methods.,0.0,0.0,1.0,0.0
AppDynamics,17416710,17412297,0,"2013/07/02, 05:56:59",True,"2013/07/02, 05:56:59",415.0,621964.0,6,"For instance, you might hook into JDBC query methods, servlet base classes, and HTTP client libraries.",0.0,0.0,1.0,0.0
AppDynamics,17416710,17412297,0,"2013/07/02, 05:56:59",True,"2013/07/02, 05:56:59",415.0,621964.0,6,"When a request enters the application, track all the important methods/calls it makes, and log them in some way.",0.0,0.096,0.904,0.2023
AppDynamics,17416710,17412297,0,"2013/07/02, 05:56:59",True,"2013/07/02, 05:56:59",415.0,621964.0,6,"Take the data and crunch it into analytics, charts, and alerts.",0.0,0.0,1.0,0.0
AppDynamics,17416710,17412297,0,"2013/07/02, 05:56:59",True,"2013/07/02, 05:56:59",415.0,621964.0,6,"On top of that, you can start to add in statistical profiling or other options.",0.0,0.114,0.886,0.2023
AppDynamics,17416710,17412297,0,"2013/07/02, 05:56:59",True,"2013/07/02, 05:56:59",415.0,621964.0,6,The tricky things are tracking requests across process boundaries and dealing with the volume of performance data you'll gather.,0.082,0.0,0.918,-0.1531
AppDynamics,17416710,17412297,0,"2013/07/02, 05:56:59",True,"2013/07/02, 05:56:59",415.0,621964.0,6,(I work on this problem at AppNeta),0.342,0.0,0.658,-0.481
AppDynamics,17416710,17412297,0,"2013/07/02, 05:56:59",True,"2013/07/02, 05:56:59",415.0,621964.0,6,"One thing to check out is Twitter Zipkin ( https://github.com/twitter/zipkin ), doesn't support much and pretty early-stage but interesting project.",0.07,0.243,0.687,0.615
AppDynamics,17418261,17412297,0,"2013/07/02, 08:52:04",False,"2013/07/02, 08:52:04",101.0,1792169.0,10,Both AppDynamics and New Relic use Standard BCI to monitor the common interfaces (entry and exit points) developers use to build applications (e.g.,0.0,0.0,1.0,0.0
AppDynamics,17418261,17412297,0,"2013/07/02, 08:52:04",False,"2013/07/02, 08:52:04",101.0,1792169.0,10,"Servlet, struts, SOAP, JMS, JDBC, ...).",0.0,0.0,1.0,0.0
AppDynamics,17418261,17412297,0,"2013/07/02, 08:52:04",False,"2013/07/02, 08:52:04",101.0,1792169.0,10,This provides a basic skeleton of code execution (call graphs) with timing information which represents less than 5% of code that is executed.,0.0,0.0,1.0,0.0
AppDynamics,17418261,17412297,0,"2013/07/02, 08:52:04",False,"2013/07/02, 08:52:04",101.0,1792169.0,10,The secret is to then uncover the timing of the remaining 95% code execution during slowdowns without incurring too much overhead in a production JVM.,0.0,0.0,1.0,0.0
AppDynamics,17418261,17412297,0,"2013/07/02, 08:52:04",False,"2013/07/02, 08:52:04",101.0,1792169.0,10,AppDynamics uses a combination of in-memory agent analytics and Java API calls to then extract the remaining code execution in real-time.,0.0,0.0,1.0,0.0
AppDynamics,17418261,17412297,0,"2013/07/02, 08:52:04",False,"2013/07/02, 08:52:04",101.0,1792169.0,10,This means no custom instrumentation is required or explicit declaration of what classes/methods you want the monitoring solution to instrument.,0.096,0.158,0.746,0.1027
AppDynamics,17418261,17412297,0,"2013/07/02, 08:52:04",False,"2013/07/02, 08:52:04",101.0,1792169.0,10,AppDynamics data collection is very different to that of New Relic.,0.0,0.0,1.0,0.0
AppDynamics,17418261,17412297,0,"2013/07/02, 08:52:04",False,"2013/07/02, 08:52:04",101.0,1792169.0,10,"For example, with AppDynamics you can get a complete distributed call graph across multiple JVMs for a specific user request, rather than say an aggregate of requests.",0.0,0.0,1.0,0.0
AppDynamics,17418261,17412297,0,"2013/07/02, 08:52:04",False,"2013/07/02, 08:52:04",101.0,1792169.0,10,"BCI is a commodity these days, the difference is in the analytics and algorithms used by vendors that trigger diagnostics/call graph information so you end up with the right visibility at the right time to solve problems.",0.07,0.047,0.883,-0.2263
AppDynamics,17418261,17412297,0,"2013/07/02, 08:52:04",False,"2013/07/02, 08:52:04",101.0,1792169.0,10,Steve.,0.0,0.0,1.0,0.0
AppDynamics,31063347,31001196,2,"2015/06/26, 04:13:54",True,"2015/06/26, 04:13:54",86.0,5051173.0,7,That error means that gradle is unable to resolve the dependency on  com.appdynamics:appdynamics-runtime .,0.166,0.16,0.675,-0.0258
AppDynamics,31063347,31001196,2,"2015/06/26, 04:13:54",True,"2015/06/26, 04:13:54",86.0,5051173.0,7,The easiest way to fix this problem is to use the AppDynamics libraries from maven central rather than the  adeum-maven-repo  directory.,0.125,0.112,0.762,-0.0836
AppDynamics,31063347,31001196,2,"2015/06/26, 04:13:54",True,"2015/06/26, 04:13:54",86.0,5051173.0,7,You can do that by editing your top level gradle file to look like this:,0.0,0.249,0.751,0.5106
AppDynamics,31063347,31001196,2,"2015/06/26, 04:13:54",True,"2015/06/26, 04:13:54",86.0,5051173.0,7,Then your project-level gradle file would look like:,0.0,0.263,0.737,0.3612
AppDynamics,31063347,31001196,2,"2015/06/26, 04:13:54",True,"2015/06/26, 04:13:54",86.0,5051173.0,7,"Note that I have removed the references to  adeum-maven-repo , and changed the version numbers on the AppDynamics artifacts to refer to them as they exist in maven central.",0.0,0.0,1.0,0.0
AppDynamics,31063347,31001196,2,"2015/06/26, 04:13:54",True,"2015/06/26, 04:13:54",86.0,5051173.0,7,"Once you've done this, you no longer need  adeum-maven-repo  in your project, since gradle is now downloading these dependencies automatically.",0.104,0.0,0.896,-0.296
AppDynamics,58801626,56138542,0,"2019/11/11, 14:45:04",False,"2019/11/11, 14:45:04",26.0,12355048.0,0,"JAVA_OPTS=""$JAVA_OPTS -Djboss.modules.system.pkgs=org.jboss.byteman,com.singularity,org""
If you do not initialize the JVM, the installation throws a ""class not found"" exception.",0.0,0.0,1.0,0.0
AppDynamics,58801626,56138542,0,"2019/11/11, 14:45:04",False,"2019/11/11, 14:45:04",26.0,12355048.0,0,"-Djava.util.logging.manager=org.jboss.logmanager.LogManager -Xbootclasspath/p:
/jboss-logmanager-.jar",0.0,0.0,1.0,0.0
AppDynamics,58801626,56138542,0,"2019/11/11, 14:45:04",False,"2019/11/11, 14:45:04",26.0,12355048.0,0,"JDK9 and above, -Xbootclasspath/p option has been removed; use -Xbootclasspath/a instead.",0.0,0.0,1.0,0.0
AppDynamics,58801626,56138542,0,"2019/11/11, 14:45:04",False,"2019/11/11, 14:45:04",26.0,12355048.0,0,"-Djava.util.logging.manager=org.jboss.logmanager.LogManager -Xbootclasspath/a:
/jboss-logmanager-.jar",0.0,0.0,1.0,0.0
AppDynamics,58801626,56138542,0,"2019/11/11, 14:45:04",False,"2019/11/11, 14:45:04",26.0,12355048.0,0,https://docs.appdynamics.com/display/PRO45/JBoss+and+Wildfly+Startup+Settings#JBossandWildflyStartupSettings-InitializetheJVM,0.0,0.0,1.0,0.0
AppDynamics,16171037,16161856,0,"2013/04/23, 16:48:17",False,"2013/04/23, 16:48:17",11.0,2047636.0,1,A great place to ask this question would be on the AppDynamics discussion forums so that AppDynamics support can answer you directly...  http://appsphere.appdynamics.com/t5/Discussions/ct-p/Discussions,0.0,0.261,0.739,0.7943
AppDynamics,16171037,16161856,0,"2013/04/23, 16:48:17",False,"2013/04/23, 16:48:17",11.0,2047636.0,1,I'm guessing there is a permissions issue somewhere.,0.0,0.0,1.0,0.0
AppDynamics,18347965,16161856,0,"2013/08/21, 05:35:52",False,"2013/08/21, 05:35:52",17506.0,524588.0,1,"You should copy all files to run the agent, not just javaagent.jar.",0.0,0.0,1.0,0.0
AppDynamics,18347965,16161856,0,"2013/08/21, 05:35:52",False,"2013/08/21, 05:35:52",17506.0,524588.0,1,This is a thread about it.,0.0,0.0,1.0,0.0
AppDynamics,18347965,16161856,0,"2013/08/21, 05:35:52",False,"2013/08/21, 05:35:52",17506.0,524588.0,1,http://appsphere.appdynamics.com/t5/Lite-for-Java/Keep-getting-a-quot-Invalid-Agent-Installation-Directory-quot-on/td-p/1151,0.0,0.0,1.0,0.0
AppDynamics,26749167,25891322,0,"2014/11/05, 05:23:23",True,"2014/11/05, 05:23:23",5502.0,376340.0,4,The definition can be found in AppDynamics docs:  Slow and Stalled Transactions,0.141,0.0,0.859,-0.2023
AppDynamics,26749167,25891322,0,"2014/11/05, 05:23:23",True,"2014/11/05, 05:23:23",5502.0,376340.0,4,"By default AppDynamics considers a slow transaction one that lasts longer than 3 times the standard deviation for the last two hours, a very slow transaction 4 times the standard deviation for the last two hours, and a stalled transaction one that lasts longer than 300 deviations above the average for the last two hours.",0.035,0.0,0.965,-0.2023
AppDynamics,26749167,25891322,0,"2014/11/05, 05:23:23",True,"2014/11/05, 05:23:23",5502.0,376340.0,4,"Of course, you can modify the default rules by providing your own:  Configure Thresholds",0.0,0.0,1.0,0.0
AppDynamics,29123098,29122786,0,"2015/03/18, 15:14:41",False,"2015/03/18, 15:14:41",505.0,4382794.0,0,"Even it is not the desired behaviour, if filtering by URL and specifying only createNewActivity worked for me (as long as there are no other REST URLs matching this).",0.129,0.0,0.871,-0.4614
AppDynamics,33141659,33134494,1,"2015/10/15, 09:47:16",True,"2015/10/15, 09:52:42",42504.0,936832.0,2,"Generally, monitoring tools cannot record method-level data continuously, because they have to operate at a much lower level of overhead compared to profiling tools.",0.091,0.0,0.909,-0.296
AppDynamics,33141659,33134494,1,"2015/10/15, 09:47:16",True,"2015/10/15, 09:52:42",42504.0,936832.0,2,"They focus on ""business transactions"" that show you high-level performance measurements with associated semantic information, such as the processing of an order in your web shop.",0.0,0.0,1.0,0.0
AppDynamics,33141659,33134494,1,"2015/10/15, 09:47:16",True,"2015/10/15, 09:52:42",42504.0,936832.0,2,Method level data only comes in when these business transactions are too slow.,0.0,0.0,1.0,0.0
AppDynamics,33141659,33134494,1,"2015/10/15, 09:47:16",True,"2015/10/15, 09:52:42",42504.0,936832.0,2,The monitoring tool will then start sampling the executing thread and show you a call tree or hot spots.,0.0,0.0,1.0,0.0
AppDynamics,33141659,33134494,1,"2015/10/15, 09:47:16",True,"2015/10/15, 09:52:42",42504.0,936832.0,2,"However, you will not get this information for the entire VM for a continuous interval like you're used to from a profiler.",0.0,0.116,0.884,0.3612
AppDynamics,33141659,33134494,1,"2015/10/15, 09:47:16",True,"2015/10/15, 09:52:42",42504.0,936832.0,2,"You mentioned JProfiler, so if you are already familiar with that tool, you might be interested in  perfino  as a monitoring solution.",0.0,0.208,0.792,0.6124
AppDynamics,33141659,33134494,1,"2015/10/15, 09:47:16",True,"2015/10/15, 09:52:42",42504.0,936832.0,2,It shows you samples on the method level and has cross-over functionality into profiling with the native JVMTI interface.,0.0,0.0,1.0,0.0
AppDynamics,33141659,33134494,1,"2015/10/15, 09:47:16",True,"2015/10/15, 09:52:42",42504.0,936832.0,2,It allows you to do  full sampling of the entire JVM  for a selected amount of time and look at the results in the JProfiler GUI.,0.0,0.0,1.0,0.0
AppDynamics,33141659,33134494,1,"2015/10/15, 09:47:16",True,"2015/10/15, 09:52:42",42504.0,936832.0,2,Disclaimer: My company develops JProfiler and perfino.,0.0,0.0,1.0,0.0
AppDynamics,42376943,41130233,1,"2017/02/21, 22:06:15",True,"2017/02/21, 22:06:15",207.0,992280.0,-7,Information I got from another website.,0.0,0.0,1.0,0.0
AppDynamics,42376943,41130233,1,"2017/02/21, 22:06:15",True,"2017/02/21, 22:06:15",207.0,992280.0,-7,Application Insights (AI) is a very simplistic APM tool today.,0.0,0.0,1.0,0.0
AppDynamics,42376943,41130233,1,"2017/02/21, 22:06:15",True,"2017/02/21, 22:06:15",207.0,992280.0,-7,"It does do transaction tracing, it has very limited code and runtime diagnostics, and it doesn’t go very deep into analyzing the browser performance.",0.087,0.0,0.913,-0.2944
AppDynamics,42376943,41130233,1,"2017/02/21, 22:06:15",True,"2017/02/21, 22:06:15",207.0,992280.0,-7,Application Insights is much more like Google Analytics than like a typical APM tool.,0.0,0.337,0.663,0.6794
AppDynamics,47278626,47278327,1,"2017/11/14, 07:56:39",True,"2017/11/14, 07:56:39",22298.0,7005159.0,1,We could install the AppDynamics extension use Azure portal or Kudu tool( https://functionAppname.scm.azurewebsites.net/ ).,0.0,0.0,1.0,0.0
AppDynamics,47278626,47278327,1,"2017/11/14, 07:56:39",True,"2017/11/14, 07:56:39",22298.0,7005159.0,1,Azure Portal:,0.0,0.0,1.0,0.0
AppDynamics,47278626,47278327,1,"2017/11/14, 07:56:39",True,"2017/11/14, 07:56:39",22298.0,7005159.0,1,Kudu UI,0.0,0.0,1.0,0.0
AppDynamics,47278626,47278327,1,"2017/11/14, 07:56:39",True,"2017/11/14, 07:56:39",22298.0,7005159.0,1,After installation,0.0,0.0,1.0,0.0
AppDynamics,48631895,48631700,0,"2018/02/05, 23:36:59",False,"2018/02/05, 23:36:59",1274.0,7716468.0,0,I have used Crashlytics and Google Analytics together without any issue.,0.0,0.0,1.0,0.0
AppDynamics,48631895,48631700,0,"2018/02/05, 23:36:59",False,"2018/02/05, 23:36:59",1274.0,7716468.0,0,All the logging is done in a background process so I don't think you will notice any speed degradation but the app is technically doing more work so there is some sort of performance hit.,0.064,0.0,0.936,-0.296
AppDynamics,48631895,48631700,0,"2018/02/05, 23:36:59",False,"2018/02/05, 23:36:59",1274.0,7716468.0,0,I haven't seen any issues with the crashlog.,0.0,0.0,1.0,0.0
AppDynamics,48631895,48631700,0,"2018/02/05, 23:36:59",False,"2018/02/05, 23:36:59",1274.0,7716468.0,0,Analytics libraries just write the crashlogs to a file and then send them the next time the user opens your app.,0.0,0.0,1.0,0.0
AppDynamics,48631895,48631700,0,"2018/02/05, 23:36:59",False,"2018/02/05, 23:36:59",1274.0,7716468.0,0,They are not affecting how the actually crashes are handled by the operating system so there shouldn't be any issue with them conflicting.,0.109,0.0,0.891,-0.4019
AppDynamics,48631903,48631700,0,"2018/02/05, 23:37:27",True,"2018/02/05, 23:53:27",12972.0,541786.0,4,First  : I would strongly suggest removing the unused one (or the one you don't prefer) from your code.,0.0,0.116,0.884,0.2732
AppDynamics,48631903,48631700,0,"2018/02/05, 23:37:27",True,"2018/02/05, 23:53:27",12972.0,541786.0,4,"For reasons, like : 
1.",0.0,0.455,0.545,0.3612
AppDynamics,48631903,48631700,0,"2018/02/05, 23:37:27",True,"2018/02/05, 23:53:27",12972.0,541786.0,4,It will increase project size which in turn will increase your bundle size.,0.0,0.295,0.705,0.5574
AppDynamics,48631903,48631700,0,"2018/02/05, 23:37:27",True,"2018/02/05, 23:53:27",12972.0,541786.0,4,2.,0.0,0.0,1.0,0.0
AppDynamics,48631903,48631700,0,"2018/02/05, 23:37:27",True,"2018/02/05, 23:53:27",12972.0,541786.0,4,Messy code.,0.714,0.0,0.286,-0.3612
AppDynamics,48631903,48631700,0,"2018/02/05, 23:37:27",True,"2018/02/05, 23:53:27",12972.0,541786.0,4,3.,0.0,0.0,1.0,0.0
AppDynamics,48631903,48631700,0,"2018/02/05, 23:37:27",True,"2018/02/05, 23:53:27",12972.0,541786.0,4,There is no point checking two different analytics.,0.239,0.0,0.761,-0.296
AppDynamics,48631903,48631700,0,"2018/02/05, 23:37:27",True,"2018/02/05, 23:53:27",12972.0,541786.0,4,4.,0.0,0.0,1.0,0.0
AppDynamics,48631903,48631700,0,"2018/02/05, 23:37:27",True,"2018/02/05, 23:53:27",12972.0,541786.0,4,"While third person is understanding the code, he would waste his time in understanding which will lead to confusion.",0.227,0.0,0.773,-0.6124
AppDynamics,48631903,48631700,0,"2018/02/05, 23:37:27",True,"2018/02/05, 23:53:27",12972.0,541786.0,4,I might be missing other reasons.,0.355,0.0,0.645,-0.296
AppDynamics,48631903,48631700,0,"2018/02/05, 23:37:27",True,"2018/02/05, 23:53:27",12972.0,541786.0,4,"Second  : To answer your question, it should work fine.",0.0,0.184,0.816,0.2023
AppDynamics,48631903,48631700,0,"2018/02/05, 23:37:27",True,"2018/02/05, 23:53:27",12972.0,541786.0,4,"I did the same in one of my projects, where initially I was using  Hockey Crash reporting .",0.162,0.0,0.838,-0.4019
AppDynamics,48631903,48631700,0,"2018/02/05, 23:37:27",True,"2018/02/05, 23:53:27",12972.0,541786.0,4,But then client asked to use  Crashlytics .,0.0,0.0,1.0,0.0
AppDynamics,48631903,48631700,0,"2018/02/05, 23:37:27",True,"2018/02/05, 23:53:27",12972.0,541786.0,4,I didn't remove Hockey SDK immediately.,0.0,0.0,1.0,0.0
AppDynamics,48631903,48631700,0,"2018/02/05, 23:37:27",True,"2018/02/05, 23:53:27",12972.0,541786.0,4,"Though this worked fine and both reported the issues, but soon I removed Hockey SDK from the code.",0.0,0.08,0.92,0.1027
AppDynamics,53074197,52994676,0,"2018/10/31, 01:18:54",True,"2018/10/31, 01:18:54",1182.0,1622880.0,2,There are integration tools available between influxdb/AppDynamics and grafana/AppDynamics.,0.0,0.0,1.0,0.0
AppDynamics,53074197,52994676,0,"2018/10/31, 01:18:54",True,"2018/10/31, 01:18:54",1182.0,1622880.0,2,https://github.com/Appdynamics/MetricMover,0.0,0.0,1.0,0.0
AppDynamics,53074197,52994676,0,"2018/10/31, 01:18:54",True,"2018/10/31, 01:18:54",1182.0,1622880.0,2,https://grafana.com/plugins/dlopes7-appdynamics-datasource/installation ).,0.0,0.0,1.0,0.0
AppDynamics,53074197,52994676,0,"2018/10/31, 01:18:54",True,"2018/10/31, 01:18:54",1182.0,1622880.0,2,There's nothing that integrates between Prometheus and AppDynamics at the moment,0.0,0.0,1.0,0.0
AppDynamics,53074197,52994676,0,"2018/10/31, 01:18:54",True,"2018/10/31, 01:18:54",1182.0,1622880.0,2,"I'm not sure there will be one going forward, seeing how they are competing in the same space from different vantage points (Open Source vs Enterprise)",0.073,0.0,0.927,-0.2411
AppDynamics,58803167,53397807,1,"2019/11/11, 16:20:49",False,"2019/11/11, 17:18:30",26.0,12355048.0,0,"If you want to monitor only your worker processes of IIS and Standalone Service, You can use CLR Crash event on your policy configuration.",0.104,0.05,0.846,-0.34
AppDynamics,58803167,53397807,1,"2019/11/11, 16:20:49",False,"2019/11/11, 17:18:30",26.0,12355048.0,0,AppDynamics automaticaly creates CLR Crash Events If your IIS or Standalone Service are crashed.,0.161,0.125,0.714,-0.1531
AppDynamics,58803167,53397807,1,"2019/11/11, 16:20:49",False,"2019/11/11, 17:18:30",26.0,12355048.0,0,"You can find the details of CLR Crash Events:
 https://docs.appdynamics.com/display/PRO45/Monitor+CLR+Crashes",0.231,0.0,0.769,-0.4019
AppDynamics,58803167,53397807,1,"2019/11/11, 16:20:49",False,"2019/11/11, 17:18:30",26.0,12355048.0,0,"Also, Sample policy configuration: 
 Policy Configuration Screen",0.0,0.0,1.0,0.0
AppDynamics,65946975,65946794,2,"2021/01/29, 01:58:56",False,"2021/01/29, 01:58:56",483.0,4895267.0,0,AppDynamics have a fork of contrib project  here  where there is an exporter but it isn't clear if it has been finished,0.122,0.0,0.878,-0.4168
AppDynamics,66421315,65946794,0,"2021/03/01, 13:29:48",True,"2021/03/01, 13:29:48",3170.0,1237595.0,1,there is a Beta program now running for using AppDynamics with the OpenTelemetry Collector.,0.0,0.0,1.0,0.0
AppDynamics,66421315,65946794,0,"2021/03/01, 13:29:48",True,"2021/03/01, 13:29:48",3170.0,1237595.0,1,More details are available in the public docs:  https://docs.appdynamics.com/display/PRO21/Ingest+OpenTelemetry+Trace+Data,0.0,0.0,1.0,0.0
AppDynamics,13182788,13182423,0,"2012/11/01, 19:52:02",True,"2012/11/01, 19:52:02",49920.0,32453.0,1,"Looks like you create the metric, then edit a dashboard, then click on a widget -  add metric -  (browse, but choose ""Individual Nodes"" instead of JMX, then select your metric.",0.0,0.117,0.883,0.3182
AppDynamics,13182788,13182423,0,"2012/11/01, 19:52:02",True,"2012/11/01, 19:52:02",49920.0,32453.0,1,voila.,0.0,0.0,1.0,0.0
AppDynamics,24808336,24797525,2,"2014/07/17, 18:58:39",False,"2014/07/17, 18:58:39",11.0,2047636.0,0,The best place for you to get this question answered would be on the AppDynamics community discussion boards.,0.0,0.198,0.802,0.6369
AppDynamics,24808336,24797525,2,"2014/07/17, 18:58:39",False,"2014/07/17, 18:58:39",11.0,2047636.0,0,Here's a link for you...  http://community.appdynamics.com/t5/Forums-Community-AppDynamics/ct-p/Discussions,0.0,0.0,1.0,0.0
AppDynamics,24808336,24797525,2,"2014/07/17, 18:58:39",False,"2014/07/17, 18:58:39",11.0,2047636.0,0,The AppDynamics documentation site is also a great resource and you don't even need a login to access them...  http://docs.appdynamics.com/,0.0,0.194,0.806,0.6249
AppDynamics,24809185,24797525,3,"2014/07/17, 19:45:22",False,"2014/07/17, 19:45:22",66.0,3850108.0,0,"Installation instructions for the controller can be found in:
 http://docs.appdynamics.com/display/PRO14S/Install+the+Controller 
Assuming you are using App Server Agent for Java, installation instructions for that and the machine agent can be found in the above site on the Left Nav table of contents.",0.0,0.0,1.0,0.0
AppDynamics,24809185,24797525,3,"2014/07/17, 19:45:22",False,"2014/07/17, 19:45:22",66.0,3850108.0,0,"The controller is the management server, the app server agent monitors the JVM and the machine agent collects system metrics (such as CPU, Memory, Disk &amp; Network).",0.0,0.0,1.0,0.0
AppDynamics,24809185,24797525,3,"2014/07/17, 19:45:22",False,"2014/07/17, 19:45:22",66.0,3850108.0,0,Very minimal configuration is required.,0.0,0.0,1.0,0.0
AppDynamics,30294344,29858961,0,"2015/05/18, 05:39:08",True,"2015/05/18, 05:39:08",16.0,4910351.0,0,"From what I understand from their documentation, AppD do not have a way to capture heap dumps.",0.162,0.0,0.838,-0.4019
AppDynamics,30294344,29858961,0,"2015/05/18, 05:39:08",True,"2015/05/18, 05:39:08",16.0,4910351.0,0,They suggest using Memory Leak detection feature in such scenarios.,0.211,0.0,0.789,-0.34
AppDynamics,30294344,29858961,0,"2015/05/18, 05:39:08",True,"2015/05/18, 05:39:08",16.0,4910351.0,0,"On a different note, I know we can get thread dumps which maybe helpful in some cases (Agents -  Request Agent Log Files)",0.115,0.119,0.766,0.0258
AppDynamics,31398932,29858961,0,"2015/07/14, 09:12:20",False,"2015/07/14, 09:12:20",51.0,3269857.0,3,Heap dumps in appdynamics can be taken for JRockit JVM by the below method(Note : This does not work for IBM JVM),0.119,0.0,0.881,-0.4019
AppDynamics,31398932,29858961,0,"2015/07/14, 09:12:20",False,"2015/07/14, 09:12:20",51.0,3269857.0,3,p0 – Path to generate heap dump(/path/dump.hprof),0.0,0.0,1.0,0.0
AppDynamics,31398932,29858961,0,"2015/07/14, 09:12:20",False,"2015/07/14, 09:12:20",51.0,3269857.0,3,p1 -  True – GC before Heap dump ; False - No GC before heap dump,0.407,0.154,0.44,-0.5574
AppDynamics,31398932,29858961,0,"2015/07/14, 09:12:20",False,"2015/07/14, 09:12:20",51.0,3269857.0,3,Note : If you want heap dump to be generated in the case of out of memory give,0.138,0.069,0.794,-0.3182
AppDynamics,31398932,29858961,0,"2015/07/14, 09:12:20",False,"2015/07/14, 09:12:20",51.0,3269857.0,3,p0 : HeapDumpOnOutOfMemoryError,0.0,0.0,1.0,0.0
AppDynamics,31398932,29858961,0,"2015/07/14, 09:12:20",False,"2015/07/14, 09:12:20",51.0,3269857.0,3,Also note that these values will be lost on JVM restart.,0.164,0.193,0.643,0.1027
AppDynamics,33780274,33210833,1,"2015/11/18, 14:36:17",True,"2015/11/18, 14:36:17",66.0,1355536.0,3,"Data retrieval by APM tools is done in several ways, each one with its pros and cons",0.0,0.0,1.0,0.0
AppDynamics,33780274,33210833,1,"2015/11/18, 14:36:17",True,"2015/11/18, 14:36:17",66.0,1355536.0,3,"Bytecode injection  (for both Java and .NET) is one technique, which is somewhat intrusive but allows you to get data from places the application owner (or even 3rd party frameworks) did not intend to allow.",0.052,0.092,0.856,0.3718
AppDynamics,33780274,33210833,1,"2015/11/18, 14:36:17",True,"2015/11/18, 14:36:17",66.0,1355536.0,3,"Native function interception  is similar to bytecode injection, but allows you to intercept unmanaged code",0.0,0.0,1.0,0.0
AppDynamics,33780274,33210833,1,"2015/11/18, 14:36:17",True,"2015/11/18, 14:36:17",66.0,1355536.0,3,Application plugins  - some applications (e.g.,0.0,0.0,1.0,0.0
AppDynamics,33780274,33210833,1,"2015/11/18, 14:36:17",True,"2015/11/18, 14:36:17",66.0,1355536.0,3,"Apache, IIS) give access to monitoring and application information via a well-documented APIs and plugin architecture",0.0,0.0,1.0,0.0
AppDynamics,33780274,33210833,1,"2015/11/18, 14:36:17",True,"2015/11/18, 14:36:17",66.0,1355536.0,3,Network sniffing  allows you to see all the communication to/from the monitored machine,0.0,0.0,1.0,0.0
AppDynamics,33780274,33210833,1,"2015/11/18, 14:36:17",True,"2015/11/18, 14:36:17",66.0,1355536.0,3,"OS specific un/documented APIs  - just like application plugins, but for the Windows/*nix",0.0,0.137,0.863,0.1901
AppDynamics,33780274,33210833,1,"2015/11/18, 14:36:17",True,"2015/11/18, 14:36:17",66.0,1355536.0,3,"Disclaimer : I work for Correlsense, provider of APM software SharePath, which uses all of the above methods to give you complete end-to-end transaction visibility.",0.0,0.0,1.0,0.0
AppDynamics,42301899,41933094,0,"2017/02/17, 17:40:40",False,"2017/02/17, 17:48:24",4877.0,1140237.0,0,I've tried add multi dex without giving any minimum or maximum of method count per dex file wise.I've tried with simply just adding multidex and able to build.And Yes!!,0.063,0.102,0.835,0.3067
AppDynamics,42301899,41933094,0,"2017/02/17, 17:40:40",False,"2017/02/17, 17:48:24",4877.0,1140237.0,0,I am able to build app too.,0.0,0.0,1.0,0.0
AppDynamics,42301899,41933094,0,"2017/02/17, 17:40:40",False,"2017/02/17, 17:48:24",4877.0,1140237.0,0,major change is in  afterEvaluate  &amp;  incremental true  in  dexoption .,0.0,0.237,0.763,0.4215
AppDynamics,42301899,41933094,0,"2017/02/17, 17:40:40",False,"2017/02/17, 17:48:24",4877.0,1140237.0,0,build.gradle,0.0,0.0,1.0,0.0
AppDynamics,42301899,41933094,0,"2017/02/17, 17:40:40",False,"2017/02/17, 17:48:24",4877.0,1140237.0,0,application's parent gradle,0.0,0.0,1.0,0.0
AppDynamics,42301899,41933094,0,"2017/02/17, 17:40:40",False,"2017/02/17, 17:48:24",4877.0,1140237.0,0,If above thing have still issue just check your dependecies hierarchy if any other extra dependecies are added (Based on your  build.gradle    packagingOptions  there should be some other dependecies there).Not sure but it may possible because of internal library conflicts its not proceeding further to create dexfile or build.,0.063,0.08,0.857,-0.0258
AppDynamics,42301899,41933094,0,"2017/02/17, 17:40:40",False,"2017/02/17, 17:48:24",4877.0,1140237.0,0,Let me know if anything,0.0,0.0,1.0,0.0
AppDynamics,43318354,43315501,1,"2017/04/10, 11:34:41",True,"2017/04/10, 11:34:41",324.0,7220665.0,3,You can use the REST API of the Controller described here  https://docs.appdynamics.com/display/PRO42/Using+the+Controller+APIs,0.0,0.0,1.0,0.0
AppDynamics,43318354,43315501,1,"2017/04/10, 11:34:41",True,"2017/04/10, 11:34:41",324.0,7220665.0,3,"To access the REST API Browser, in a Web browser, go to:",0.0,0.0,1.0,0.0
AppDynamics,43318354,43315501,1,"2017/04/10, 11:34:41",True,"2017/04/10, 11:34:41",324.0,7220665.0,3,this will give you a nice swagger UI description of the available resources.,0.0,0.203,0.797,0.4215
AppDynamics,43318354,43315501,1,"2017/04/10, 11:34:41",True,"2017/04/10, 11:34:41",324.0,7220665.0,3,Alternatively you can create reports directly out of the box in AppDynamics via the  Dashboards &amp; Reports  section.,0.0,0.11,0.89,0.2732
AppDynamics,44308592,44283136,0,"2017/06/01, 16:15:26",False,"2017/06/01, 20:52:24",324.0,7220665.0,1,"First you need to setup the AppDynamics Controller (should be hosted on a separate  machine), then you need java agents on the machine with the application.",0.0,0.0,1.0,0.0
AppDynamics,44308592,44283136,0,"2017/06/01, 16:15:26",False,"2017/06/01, 20:52:24",324.0,7220665.0,1,"You need to wire the application with the Java agent, e.g.",0.0,0.0,1.0,0.0
AppDynamics,44308592,44283136,0,"2017/06/01, 16:15:26",False,"2017/06/01, 20:52:24",324.0,7220665.0,1,like this,0.0,0.714,0.286,0.3612
AppDynamics,44308592,44283136,0,"2017/06/01, 16:15:26",False,"2017/06/01, 20:52:24",324.0,7220665.0,1,Now the Java agent sends application information to the controller.,0.0,0.0,1.0,0.0
AppDynamics,44308592,44283136,0,"2017/06/01, 16:15:26",False,"2017/06/01, 20:52:24",324.0,7220665.0,1,Take a look at the documentation.,0.0,0.0,1.0,0.0
AppDynamics,44308592,44283136,0,"2017/06/01, 16:15:26",False,"2017/06/01, 20:52:24",324.0,7220665.0,1,https://docs.appdynamics.com/display/PRO43/Getting+Started,0.0,0.0,1.0,0.0
AppDynamics,44308592,44283136,0,"2017/06/01, 16:15:26",False,"2017/06/01, 20:52:24",324.0,7220665.0,1,The application will automatically be available in AppDynamics and you can see the dashboard.,0.0,0.0,1.0,0.0
AppDynamics,44338258,44283136,0,"2017/06/03, 01:27:31",False,"2017/06/03, 01:27:31",68.0,1394813.0,0,"You don't need to host the controller, you can get a SAAS account here.",0.0,0.0,1.0,0.0
AppDynamics,44338258,44283136,0,"2017/06/03, 01:27:31",False,"2017/06/03, 01:27:31",68.0,1394813.0,0,https://www.appdynamics.com/free-trial/  .,0.0,0.0,1.0,0.0
AppDynamics,44338258,44283136,0,"2017/06/03, 01:27:31",False,"2017/06/03, 01:27:31",68.0,1394813.0,0,Once you get a SAAS account lookup the account name and account key in the welcome email and use the instructions in the email to add JVM args to your app as shown.,0.0,0.088,0.912,0.4588
AppDynamics,44338258,44283136,0,"2017/06/03, 01:27:31",False,"2017/06/03, 01:27:31",68.0,1394813.0,0,Don't forget to set the account name and account access key args: -Dappdynamics.agent.accountName -Dappdynamics.agent.accountAccessKey .,0.0,0.114,0.886,0.1695
AppDynamics,49007036,49004859,0,"2018/02/27, 12:58:41",False,"2018/02/27, 12:58:41",239035.0,2711488.0,2,See  4.3.x Documentation⇒POJO Entry Points⇒Monitor Java Interface Static and Default Methods :,0.0,0.0,1.0,0.0
AppDynamics,49007036,49004859,0,"2018/02/27, 12:58:41",False,"2018/02/27, 12:58:41",239035.0,2711488.0,2,"Note that another Java language feature introduced in Java 8, lambda method interfaces, are not supported by the AppDynamics Java Agent.",0.089,0.0,0.911,-0.2411
AppDynamics,49007036,49004859,0,"2018/02/27, 12:58:41",False,"2018/02/27, 12:58:41",239035.0,2711488.0,2,It’s possible that this is due to technical difficulties with  JDK-8145964  as you suspect.,0.268,0.0,0.732,-0.5267
AppDynamics,49007036,49004859,0,"2018/02/27, 12:58:41",False,"2018/02/27, 12:58:41",239035.0,2711488.0,2,But I’d also point out that this kind of Instrumentation would be questionable.,0.189,0.0,0.811,-0.4215
AppDynamics,49007036,49004859,0,"2018/02/27, 12:58:41",False,"2018/02/27, 12:58:41",239035.0,2711488.0,2,"It’s not this JRE generated class that implements any specific behavior, it’s the invoked target method.",0.0,0.0,1.0,0.0
AppDynamics,49029706,49004859,1,"2018/02/28, 14:33:14",False,"2018/02/28, 14:33:14",474.0,4682632.0,0,Support for Lambda expressions has been in the product since 4.1 which shipped in 2015 I believe.,0.0,0.153,0.847,0.4019
AppDynamics,49029706,49004859,1,"2018/02/28, 14:33:14",False,"2018/02/28, 14:33:14",474.0,4682632.0,0,That being said we are always enhancing support.,0.0,0.278,0.722,0.4019
AppDynamics,49029706,49004859,1,"2018/02/28, 14:33:14",False,"2018/02/28, 14:33:14",474.0,4682632.0,0,These do have some limitations after initialization of the classes using them (dynamic instrumentation limits).,0.0,0.0,1.0,0.0
AppDynamics,49029706,49004859,1,"2018/02/28, 14:33:14",False,"2018/02/28, 14:33:14",474.0,4682632.0,0,"The product should support them, we have added some additional capabilities and features for Lambda expressions in our next major release.",0.0,0.119,0.881,0.4019
AppDynamics,49029706,49004859,1,"2018/02/28, 14:33:14",False,"2018/02/28, 14:33:14",474.0,4682632.0,0,Have you tried to contact help @ appdynamics.com,0.0,0.31,0.69,0.4019
AppDynamics,49033394,49004859,0,"2018/02/28, 17:45:13",False,"2018/02/28, 17:45:13",9111.0,4647853.0,0,"Looks like it was not mentioned in release notes, but instead  Support Advisory 56039  was raised.",0.0,0.275,0.725,0.6486
AppDynamics,49033394,49004859,0,"2018/02/28, 17:45:13",False,"2018/02/28, 17:45:13",9111.0,4647853.0,0,They indeed mention JDK-8145964 as a reason for removing the support.,0.0,0.231,0.769,0.4019
AppDynamics,50713654,50713335,2,"2018/06/06, 09:34:40",False,"2018/06/06, 09:34:40",44.0,2480398.0,1,I got this too...,0.0,0.0,1.0,0.0
AppDynamics,50713654,50713335,2,"2018/06/06, 09:34:40",False,"2018/06/06, 09:34:40",44.0,2480398.0,1,I was running from command-line as a non-root user:,0.0,0.0,1.0,0.0
AppDynamics,50713654,50713335,2,"2018/06/06, 09:34:40",False,"2018/06/06, 09:34:40",44.0,2480398.0,1,I added the shell expand(-x) switch and log to the command(s) like so:,0.0,0.185,0.815,0.3612
AppDynamics,50713654,50713335,2,"2018/06/06, 09:34:40",False,"2018/06/06, 09:34:40",44.0,2480398.0,1,"If we tail the last bit of that log you get, this response in debug mode:",0.0,0.0,1.0,0.0
AppDynamics,50713654,50713335,2,"2018/06/06, 09:34:40",False,"2018/06/06, 09:34:40",44.0,2480398.0,1,and the script checkLibaio.sh isn't left there... so you cannot figure it out easily.,0.0,0.156,0.844,0.34
AppDynamics,50713654,50713335,2,"2018/06/06, 09:34:40",False,"2018/06/06, 09:34:40",44.0,2480398.0,1,I also have a RedHat variant with the packages installed:,0.0,0.0,1.0,0.0
AppDynamics,50713654,50713335,2,"2018/06/06, 09:34:40",False,"2018/06/06, 09:34:40",44.0,2480398.0,1,"Strangely enough I have one VM from the same image that will install the distribution just fine, and one that will not, so on the broken install (where I really want to install this).",0.151,0.092,0.758,-0.4889
AppDynamics,50713654,50713335,2,"2018/06/06, 09:34:40",False,"2018/06/06, 09:34:40",44.0,2480398.0,1,"I ran another command from the expanded view of the install.log, which was a really long JVM command line.",0.0,0.0,1.0,0.0
AppDynamics,50713654,50713335,2,"2018/06/06, 09:34:40",False,"2018/06/06, 09:34:40",44.0,2480398.0,1,Anyways I got it to work and then made a looping script to retrieve the file (Because AppD for some reason removes the check script before you can look at it).,0.0,0.0,1.0,0.0
AppDynamics,50713654,50713335,2,"2018/06/06, 09:34:40",False,"2018/06/06, 09:34:40",44.0,2480398.0,1,The script is as follows:,0.0,0.0,1.0,0.0
AppDynamics,50713654,50713335,2,"2018/06/06, 09:34:40",False,"2018/06/06, 09:34:40",44.0,2480398.0,1,I you run this script like me on the faulty platform what you will discover is that your version of Linux has both:,0.093,0.101,0.806,0.0516
AppDynamics,50713654,50713335,2,"2018/06/06, 09:34:40",False,"2018/06/06, 09:34:40",44.0,2480398.0,1,and,0.0,0.0,1.0,0.0
AppDynamics,50713654,50713335,2,"2018/06/06, 09:34:40",False,"2018/06/06, 09:34:40",44.0,2480398.0,1,installed.,0.0,0.0,1.0,0.0
AppDynamics,50713654,50713335,2,"2018/06/06, 09:34:40",False,"2018/06/06, 09:34:40",44.0,2480398.0,1,To work around this you should temporarily make a name change to one of these two package manager executables so it cannot be found (by your shell environment).,0.0,0.0,1.0,0.0
AppDynamics,50713654,50713335,2,"2018/06/06, 09:34:40",False,"2018/06/06, 09:34:40",44.0,2480398.0,1,Most common here will be that you are running a RedHat variant where someone chose to install dpkg (For who knows what reason).,0.0,0.0,1.0,0.0
AppDynamics,50713654,50713335,2,"2018/06/06, 09:34:40",False,"2018/06/06, 09:34:40",44.0,2480398.0,1,If so desired remove that package and the install should be successful.,0.0,0.382,0.618,0.7346
AppDynamics,53539438,53537738,0,"2018/11/29, 14:47:54",False,"2018/11/29, 14:47:54",6049.0,3156333.0,2,As stated on the AppD docs regarding  Kubernetes and AppDynamics APM,0.0,0.0,1.0,0.0
AppDynamics,53539438,53537738,0,"2018/11/29, 14:47:54",False,"2018/11/29, 14:47:54",6049.0,3156333.0,2,Install a Standalone Machine Agent (1) in a Kubernetes node.,0.0,0.0,1.0,0.0
AppDynamics,53539438,53537738,0,"2018/11/29, 14:47:54",False,"2018/11/29, 14:47:54",6049.0,3156333.0,2,Install an APM Agent (2) inside each container in a pod you want to monitor.,0.0,0.091,0.909,0.0772
AppDynamics,53539438,53537738,0,"2018/11/29, 14:47:54",False,"2018/11/29, 14:47:54",6049.0,3156333.0,2,"The Standalone Machine Agent then collects hardware metrics for each monitored container, as well as Machine and Server metrics for the host (3), and forwards the metrics to the Controller.",0.0,0.068,0.932,0.2732
AppDynamics,53539438,53537738,0,"2018/11/29, 14:47:54",False,"2018/11/29, 14:47:54",6049.0,3156333.0,2,ContainerID and UniqueHostID can be taken from  /proc/self/cgroup,0.0,0.0,1.0,0.0
AppDynamics,53539438,53537738,0,"2018/11/29, 14:47:54",False,"2018/11/29, 14:47:54",6049.0,3156333.0,2,ContainerID  cat /proc/self/cgroup | awk -F '/' '{print $NF}'  | head -n 1,0.0,0.0,1.0,0.0
AppDynamics,53539438,53537738,0,"2018/11/29, 14:47:54",False,"2018/11/29, 14:47:54",6049.0,3156333.0,2,UniqueHostID  sed -rn '1s#.,0.0,0.0,1.0,0.0
AppDynamics,53539438,53537738,0,"2018/11/29, 14:47:54",False,"2018/11/29, 14:47:54",6049.0,3156333.0,2,*/##; 1s/(.{12}).,0.0,0.0,1.0,0.0
AppDynamics,53539438,53537738,0,"2018/11/29, 14:47:54",False,"2018/11/29, 14:47:54",6049.0,3156333.0,2,*/\1/p' /proc/self/cgroup,0.0,0.0,1.0,0.0
AppDynamics,54083177,54046191,0,"2019/01/08, 01:13:19",True,"2019/01/08, 01:13:19",474.0,4682632.0,2,Thanks for the reply to my question.,0.0,0.326,0.674,0.4404
AppDynamics,54083177,54046191,0,"2019/01/08, 01:13:19",True,"2019/01/08, 01:13:19",474.0,4682632.0,2,The .NET agent in most APM tools works the same way which leverages the profiler APIs in the .NET SDK and allows for data collection and also callbacks and other interception.,0.0,0.0,1.0,0.0
AppDynamics,54083177,54046191,0,"2019/01/08, 01:13:19",True,"2019/01/08, 01:13:19",474.0,4682632.0,2,"Most of the tools also use performance counter data, and other sources aside from inside the .NET runtime.",0.0,0.0,1.0,0.0
AppDynamics,54083177,54046191,0,"2019/01/08, 01:13:19",True,"2019/01/08, 01:13:19",474.0,4682632.0,2,This allows you to do several things similar to Java in terms of data collection.,0.0,0.0,1.0,0.0
AppDynamics,54083177,54046191,0,"2019/01/08, 01:13:19",True,"2019/01/08, 01:13:19",474.0,4682632.0,2,ref:  https://docs.microsoft.com/en-us/visualstudio/profiling/walkthrough-using-profiler-apis?view=vs-2017,0.0,0.0,1.0,0.0
AppDynamics,54083177,54046191,0,"2019/01/08, 01:13:19",True,"2019/01/08, 01:13:19",474.0,4682632.0,2,http://www.blong.com/conferences/dcon2003/internals/profiling.htm,0.0,0.0,1.0,0.0
AppDynamics,55150379,55048208,0,"2019/03/13, 22:07:48",True,"2019/03/13, 22:07:48",41.0,2625630.0,0,"The solution was adding ""appdynamics"" to the ""externals"" in the Webpack configuration:  https://webpack.js.org/configuration/externals/",0.0,0.161,0.839,0.3182
AppDynamics,55150379,55048208,0,"2019/03/13, 22:07:48",True,"2019/03/13, 22:07:48",41.0,2625630.0,0,This allows AppDynamics to use the default Node.js require import.,0.0,0.0,1.0,0.0
AppDynamics,58210199,58208802,0,"2019/10/03, 01:37:31",True,"2019/10/03, 01:37:31",955.0,284480.0,1,The most important thing to understand about this error is the meaning of this line:,0.172,0.115,0.714,-0.2575
AppDynamics,58210199,58208802,0,"2019/10/03, 01:37:31",True,"2019/10/03, 01:37:31",955.0,284480.0,1,Caused by: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target,0.0,0.0,1.0,0.0
AppDynamics,58210199,58208802,0,"2019/10/03, 01:37:31",True,"2019/10/03, 01:37:31",955.0,284480.0,1,"SSL certificates work by establishing a certificate chain, or a hierarchy of trust.",0.0,0.248,0.752,0.5106
AppDynamics,58210199,58208802,0,"2019/10/03, 01:37:31",True,"2019/10/03, 01:37:31",955.0,284480.0,1,"For example, if I go to  https://www.google.com  and look at their cert, this is what I see:",0.0,0.0,1.0,0.0
AppDynamics,58210199,58208802,0,"2019/10/03, 01:37:31",True,"2019/10/03, 01:37:31",955.0,284480.0,1,"There is the google cert, which sits on their servers/CDN, then an intermediate cert which also sits on their servers/CDN, then a trusted root CA cert which is in the  client  keystore and is implicitly trusted.",0.0,0.158,0.842,0.7351
AppDynamics,58210199,58208802,0,"2019/10/03, 01:37:31",True,"2019/10/03, 01:37:31",955.0,284480.0,1,"So when someone browses to google, b/c they have the root CA cert and have trusted it, the browser (client) will trust that the server is actually who they say they are and will establish a secure connection to the site.",0.0,0.192,0.808,0.8316
AppDynamics,58210199,58208802,0,"2019/10/03, 01:37:31",True,"2019/10/03, 01:37:31",955.0,284480.0,1,"So getting back to your error, whatever CA issued the server cert being used by RabbitMQ, the monitor is not recognizing it as trusted.",0.097,0.112,0.791,0.1027
AppDynamics,58210199,58208802,0,"2019/10/03, 01:37:31",True,"2019/10/03, 01:37:31",955.0,284480.0,1,"To troubleshoot this error, here are the things to do:",0.242,0.139,0.619,-0.3237
AppDynamics,58210199,58208802,0,"2019/10/03, 01:37:31",True,"2019/10/03, 01:37:31",955.0,284480.0,1,There are a couple other gotchas to watch out for:,0.0,0.0,1.0,0.0
AppDynamics,66101093,63647198,0,"2021/02/08, 13:51:32",False,"2021/02/08, 13:51:32",3170.0,1237595.0,0,This issue was solved here:  https://community.appdynamics.com/t5/Java-Java-Agent-Installation-JVM/monitor-URL-page-availability/td-p/40904,0.0,0.296,0.704,0.2732
AppDynamics,66101093,63647198,0,"2021/02/08, 13:51:32",False,"2021/02/08, 13:51:32",3170.0,1237595.0,0,Summary:,0.0,0.0,1.0,0.0
AppDynamics,13198174,13198172,0,"2012/11/02, 17:21:28",True,"2018/05/25, 01:03:27",49920.0,32453.0,0,"Turns out if you go to configure -  instrumentation -  JMX tab then voila, you can now delete metrics and modify/edit them.",0.0,0.0,1.0,0.0
AppDynamics,13198174,13198172,0,"2012/11/02, 17:21:28",True,"2018/05/25, 01:03:27",49920.0,32453.0,0,But nowhere else.,0.0,0.0,1.0,0.0
AppDynamics,13198174,13198172,0,"2012/11/02, 17:21:28",True,"2018/05/25, 01:03:27",49920.0,32453.0,0,Odd.,1.0,0.0,0.0,-0.3182
AppDynamics,18914810,18895293,0,"2013/09/20, 13:44:18",False,"2013/09/20, 13:44:18",3.0,2795488.0,0,I've instrumented  org.apache.ode.bpel.engine.BpelProcess.handleJobDetails and it is showing me transactions going through Carbon out to the backends.,0.0,0.0,1.0,0.0
AppDynamics,19523960,19518737,4,"2013/10/22, 19:58:09",False,"2013/10/22, 19:58:09",1.0,2908075.0,0,It is a bit hard to completely answer your question and solve the issue with the provided information.,0.077,0.099,0.824,0.1027
AppDynamics,19523960,19518737,4,"2013/10/22, 19:58:09",False,"2013/10/22, 19:58:09",1.0,2908075.0,0,"However, I hope my questions below help you to get on the right track.",0.0,0.337,0.663,0.6808
AppDynamics,19523960,19518737,4,"2013/10/22, 19:58:09",False,"2013/10/22, 19:58:09",1.0,2908075.0,0,1.),0.0,0.0,1.0,0.0
AppDynamics,19523960,19518737,4,"2013/10/22, 19:58:09",False,"2013/10/22, 19:58:09",1.0,2908075.0,0,"After making the configuration change, did you also restart the AppDynamics.AgentCoordinator_WindowsService?",0.0,0.0,1.0,0.0
AppDynamics,19523960,19518737,4,"2013/10/22, 19:58:09",False,"2013/10/22, 19:58:09",1.0,2908075.0,0,Without restarting it the new configuration will not be applied to the agent itself.,0.0,0.0,1.0,0.0
AppDynamics,19523960,19518737,4,"2013/10/22, 19:58:09",False,"2013/10/22, 19:58:09",1.0,2908075.0,0,2.),0.0,0.0,1.0,0.0
AppDynamics,19523960,19518737,4,"2013/10/22, 19:58:09",False,"2013/10/22, 19:58:09",1.0,2908075.0,0,"Also important is your windows service hosting any OOTB support entry point like WCF, ASP.NET MVC4 WebAPI, web services etc.?",0.0,0.292,0.708,0.7184
AppDynamics,19523960,19518737,4,"2013/10/22, 19:58:09",False,"2013/10/22, 19:58:09",1.0,2908075.0,0,"If not, you need to setup a custom entry point.",0.0,0.0,1.0,0.0
AppDynamics,19523960,19518737,4,"2013/10/22, 19:58:09",False,"2013/10/22, 19:58:09",1.0,2908075.0,0,If you  check out the AppDynamics documentation and search for'POCO Entry Points' you should get onto the right track,0.0,0.0,1.0,0.0
AppDynamics,19523960,19518737,4,"2013/10/22, 19:58:09",False,"2013/10/22, 19:58:09",1.0,2908075.0,0,3.),0.0,0.0,1.0,0.0
AppDynamics,19523960,19518737,4,"2013/10/22, 19:58:09",False,"2013/10/22, 19:58:09",1.0,2908075.0,0,"In case No.1 &amp; No.2 did not do the trick, could you please attach the config.xml file for review?",0.0,0.169,0.831,0.3502
AppDynamics,19523960,19518737,4,"2013/10/22, 19:58:09",False,"2013/10/22, 19:58:09",1.0,2908075.0,0,Or directly reach out to the AppDynamics customer success team.,0.0,0.375,0.625,0.5859
AppDynamics,19523960,19518737,4,"2013/10/22, 19:58:09",False,"2013/10/22, 19:58:09",1.0,2908075.0,0,"Kind regards,
Theo",0.0,0.63,0.37,0.5267
AppDynamics,19523960,19518737,4,"2013/10/22, 19:58:09",False,"2013/10/22, 19:58:09",1.0,2908075.0,0,Disclaimer: I work for AppDynamics as part of the Customer Success team.,0.0,0.27,0.73,0.5719
AppDynamics,19524167,19518737,3,"2013/10/22, 20:08:31",False,"2013/10/22, 20:08:31",1.0,2908075.0,0,Just to add to you answer and to clarify a little.,0.0,0.0,1.0,0.0
AppDynamics,19524167,19518737,3,"2013/10/22, 20:08:31",False,"2013/10/22, 20:08:31",1.0,2908075.0,0,"Until release 3.7.7 the .NET agent from AppDynamics used the web.config (for IIS based application), App.config for standalone and windows services or the global.config plus some environment variables to configure the agent.",0.0,0.0,1.0,0.0
AppDynamics,19524167,19518737,3,"2013/10/22, 20:08:31",False,"2013/10/22, 20:08:31",1.0,2908075.0,0,With the 3.7.8 or later release we replaced this with a cleaner truly singly configuration file approach.,0.0,0.247,0.753,0.5574
AppDynamics,19524167,19518737,3,"2013/10/22, 20:08:31",False,"2013/10/22, 20:08:31",1.0,2908075.0,0,The configuration file is called config.xml and located in the %ProgramData%\AppDynamics... directory.,0.0,0.0,1.0,0.0
AppDynamics,19524167,19518737,3,"2013/10/22, 20:08:31",False,"2013/10/22, 20:08:31",1.0,2908075.0,0,For any version after 3.7.8 all settings have to be in the config.xml.,0.0,0.0,1.0,0.0
AppDynamics,19599763,19518737,0,"2013/10/26, 00:32:59",False,"2013/10/26, 00:32:59",11.0,2047636.0,0,You really should take this up with AppDynamics support by filing a ticket or posting in the support forums...  http://www.appdynamics.com/support/#helptab,0.0,0.241,0.759,0.6597
AppDynamics,28696479,28692916,2,"2015/02/24, 14:58:43",False,"2015/02/24, 14:58:43",72.0,3641322.0,-1,There are many things you should analyse.,0.0,0.0,1.0,0.0
AppDynamics,28696479,28692916,2,"2015/02/24, 14:58:43",False,"2015/02/24, 14:58:43",72.0,3641322.0,-1,It depends on what your LR portal uses most.,0.0,0.0,1.0,0.0
AppDynamics,28696479,28692916,2,"2015/02/24, 14:58:43",False,"2015/02/24, 14:58:43",72.0,3641322.0,-1,"You may want to analyse web content , user, groups, calnder, theme related services.",0.0,0.098,0.902,0.0772
AppDynamics,30730071,30706344,0,"2015/06/09, 13:57:33",False,"2015/06/09, 13:57:33",474.0,4682632.0,0,I would ask support on this if you can't get it to work.,0.0,0.197,0.803,0.4019
AppDynamics,30730071,30706344,0,"2015/06/09, 13:57:33",False,"2015/06/09, 13:57:33",474.0,4682632.0,0,Here is the configuration for custom exit and entry points in the product:  https://docs.appdynamics.com/display/PRO14S/Configure+Custom+Exit+Points,0.0,0.0,1.0,0.0
AppDynamics,31233300,31219797,0,"2015/07/05, 20:54:05",False,"2015/07/05, 20:54:05",474.0,4682632.0,0,"Love to help you out here, but there are no details about the error, please email help@appdynamics.com for assistance.",0.229,0.267,0.505,0.0129
AppDynamics,31233300,31219797,0,"2015/07/05, 20:54:05",False,"2015/07/05, 20:54:05",474.0,4682632.0,0,"I assume this is C#, but wouldn't know based on this.",0.0,0.0,1.0,0.0
AppDynamics,31233300,31219797,0,"2015/07/05, 20:54:05",False,"2015/07/05, 20:54:05",474.0,4682632.0,0,AppDynamics supports many languages and technologies.,0.0,0.333,0.667,0.3612
AppDynamics,31309166,31219797,0,"2015/07/09, 08:49:40",True,"2015/07/09, 08:49:40",117.0,2384622.0,1,For whom it might be of interest I have found a workaround and more details about this issue.,0.0,0.167,0.833,0.4588
AppDynamics,31309166,31219797,0,"2015/07/09, 08:49:40",True,"2015/07/09, 08:49:40",117.0,2384622.0,1,This occurs only in the following scenario:,0.0,0.0,1.0,0.0
AppDynamics,31309166,31219797,0,"2015/07/09, 08:49:40",True,"2015/07/09, 08:49:40",117.0,2384622.0,1,If you use the MsBuild 14 that is installed along Microsoft Visual Studio 2015 RC then this issue does not occur anymore.,0.0,0.0,1.0,0.0
AppDynamics,31309166,31219797,0,"2015/07/09, 08:49:40",True,"2015/07/09, 08:49:40",117.0,2384622.0,1,From my first findings there is an issue in ServiceStack's way of caching the endpoints and wrapping the execute method using Linq but I don't understand why this happens only when AppDynamics agent is installed.,0.0,0.0,1.0,0.0
AppDynamics,31309166,31219797,0,"2015/07/09, 08:49:40",True,"2015/07/09, 08:49:40",117.0,2384622.0,1,@mythz if you want to dig deeper into this issue I'm available to help but with the above solution everything is ok.,0.0,0.327,0.673,0.775
AppDynamics,31309166,31219797,0,"2015/07/09, 08:49:40",True,"2015/07/09, 08:49:40",117.0,2384622.0,1,Hope this helps,0.0,0.846,0.154,0.6705
AppDynamics,38459237,38454851,0,"2016/07/19, 15:55:16",False,"2016/07/19, 15:55:16",474.0,4682632.0,0,"There are lots of APIs available out of the box, you can find the docs at  https://docs.appdynamics.com/  not everything has an API.",0.0,0.0,1.0,0.0
AppDynamics,38459237,38454851,0,"2016/07/19, 15:55:16",False,"2016/07/19, 15:55:16",474.0,4682632.0,0,You should find the user management as part of the configuration API here :  https://docs.appdynamics.com/display/PRO42/Configuration+API,0.0,0.0,1.0,0.0
AppDynamics,39897499,39540605,0,"2016/10/06, 16:34:01",False,"2016/10/06, 16:34:01",1247.0,1232692.0,1,They do different things.,0.0,0.0,1.0,0.0
AppDynamics,39897499,39540605,0,"2016/10/06, 16:34:01",False,"2016/10/06, 16:34:01",1247.0,1232692.0,1,ELK will give you log aggregation that you can add in other functionality.,0.0,0.0,1.0,0.0
AppDynamics,39897499,39540605,0,"2016/10/06, 16:34:01",False,"2016/10/06, 16:34:01",1247.0,1232692.0,1,Appdynamics is great for real time monitoring and profiling.,0.0,0.339,0.661,0.6249
AppDynamics,39897499,39540605,0,"2016/10/06, 16:34:01",False,"2016/10/06, 16:34:01",1247.0,1232692.0,1,I think it depends on what you're going for.,0.0,0.0,1.0,0.0
AppDynamics,39897499,39540605,0,"2016/10/06, 16:34:01",False,"2016/10/06, 16:34:01",1247.0,1232692.0,1,Logging a distributed system and capturing error messages in one place might be very helpful with ELK.,0.136,0.156,0.707,0.101
AppDynamics,39897499,39540605,0,"2016/10/06, 16:34:01",False,"2016/10/06, 16:34:01",1247.0,1232692.0,1,"Not just that, but ELK can be used in a number of other ways.",0.0,0.108,0.892,0.1154
AppDynamics,39897499,39540605,0,"2016/10/06, 16:34:01",False,"2016/10/06, 16:34:01",1247.0,1232692.0,1,Elasticsearch can be used stand alone as a search engine or data cache.,0.154,0.0,0.846,-0.25
AppDynamics,39897499,39540605,0,"2016/10/06, 16:34:01",False,"2016/10/06, 16:34:01",1247.0,1232692.0,1,TL;DR  It depends on what you're doing.,0.0,0.0,1.0,0.0
AppDynamics,39897499,39540605,0,"2016/10/06, 16:34:01",False,"2016/10/06, 16:34:01",1247.0,1232692.0,1,Maybe yes...maybe no...,0.0,0.0,1.0,0.0
AppDynamics,39715544,39652512,0,"2016/09/27, 06:32:35",True,"2016/09/27, 06:32:35",506.0,5006681.0,0,"On that particular server, we have the .NET agent running as well.",0.0,0.16,0.84,0.2732
AppDynamics,39715544,39652512,0,"2016/09/27, 06:32:35",True,"2016/09/27, 06:32:35",506.0,5006681.0,0,"After shutting off the .NET agent, we were no longer having this issue with 1-2 hour gaps in the metric browser.",0.099,0.0,0.901,-0.296
AppDynamics,39715544,39652512,0,"2016/09/27, 06:32:35",True,"2016/09/27, 06:32:35",506.0,5006681.0,0,"Apparently, there is some conflict in having multiple machine agents installed on the same server.",0.141,0.0,0.859,-0.3182
AppDynamics,40455823,40452664,0,"2016/11/07, 01:40:56",False,"2016/11/07, 01:40:56",5633.0,4033292.0,0,You need both unit tests and integration tests.,0.0,0.0,1.0,0.0
AppDynamics,40455823,40452664,0,"2016/11/07, 01:40:56",False,"2016/11/07, 01:40:56",5633.0,4033292.0,0,"Unit tests should not use in database or File, ect.",0.0,0.0,1.0,0.0
AppDynamics,40455823,40452664,0,"2016/11/07, 01:40:56",False,"2016/11/07, 01:40:56",5633.0,4033292.0,0,I like to use Spring profiles for my tests.,0.0,0.263,0.737,0.3612
AppDynamics,40455823,40452664,0,"2016/11/07, 01:40:56",False,"2016/11/07, 01:40:56",5633.0,4033292.0,0,"For instance, if I have a profile called integeration_test.",0.0,0.0,1.0,0.0
AppDynamics,40455823,40452664,0,"2016/11/07, 01:40:56",False,"2016/11/07, 01:40:56",5633.0,4033292.0,0,"(I'm using xml) then in your context do something like:  &lt;beans profile=""test""&gt;TODO &lt;/beans&gt;  And configure your data-source in there.",0.0,0.122,0.878,0.3612
AppDynamics,40455823,40452664,0,"2016/11/07, 01:40:56",False,"2016/11/07, 01:40:56",5633.0,4033292.0,0,"I know there are ways to rollback all your transactions after running a test, but I like this better.",0.0,0.358,0.642,0.8322
AppDynamics,40455823,40452664,0,"2016/11/07, 01:40:56",False,"2016/11/07, 01:40:56",5633.0,4033292.0,0,"Just don't delete everything in your real database haha, could even put some safety code in the clearDatabase to make sure that doesn't happen.",0.0,0.278,0.722,0.7964
AppDynamics,40455823,40452664,0,"2016/11/07, 01:40:56",False,"2016/11/07, 01:40:56",5633.0,4033292.0,0,"For performance testing you will really need to figure out what you want to achieve, and what is meaningful to display.",0.0,0.159,0.841,0.3818
AppDynamics,40455823,40452664,0,"2016/11/07, 01:40:56",False,"2016/11/07, 01:40:56",5633.0,4033292.0,0,"If you have a specific question about performance testing you can ask that, otherwise it is too broader topic.",0.0,0.0,1.0,0.0
AppDynamics,40455823,40452664,0,"2016/11/07, 01:40:56",False,"2016/11/07, 01:40:56",5633.0,4033292.0,0,Maybe you can make a mini-webapp which does performance testing for you and has the results exposed as URL requests for displaying in HTML.,0.056,0.0,0.944,-0.0772
AppDynamics,40455823,40452664,0,"2016/11/07, 01:40:56",False,"2016/11/07, 01:40:56",5633.0,4033292.0,0,Really just depends on how much effort you are willing to spend on it and what you want to test.,0.0,0.064,0.936,0.0772
AppDynamics,64280827,40452664,0,"2020/10/09, 16:13:06",False,"2020/10/09, 16:13:06",556.0,1086679.0,0,Once the agent is attached you can use the  AppDynamics Database Queries Window,0.0,0.0,1.0,0.0
AppDynamics,41757511,41755184,1,"2017/01/20, 08:38:17",True,"2017/01/20, 08:38:17",2681.0,506813.0,1,"Reflection (inherently  TypeVisitor  and  TypeToken  classes) is always costly in Java, try not using it.",0.091,0.0,0.909,-0.1027
AppDynamics,41757511,41755184,1,"2017/01/20, 08:38:17",True,"2017/01/20, 08:38:17",2681.0,506813.0,1,Rendering time seems OK.,0.0,0.494,0.506,0.4466
AppDynamics,41757511,41755184,1,"2017/01/20, 08:38:17",True,"2017/01/20, 08:38:17",2681.0,506813.0,1,"There can be thousand reasons for high latency in an application, but you only gave this much information so that's about the best answer you can get.",0.0,0.182,0.818,0.7783
AppDynamics,41773005,41764607,2,"2017/01/20, 23:58:40",False,"2017/01/20, 23:58:40",474.0,4682632.0,1,That's the beauty of APM is you don't need to deal with logging to get performance data.,0.0,0.192,0.808,0.5859
AppDynamics,41773005,41764607,2,"2017/01/20, 23:58:40",False,"2017/01/20, 23:58:40",474.0,4682632.0,1,"APM tools instrument the applications themselves regardless of what the code does (logging, generating metrics, etc).",0.0,0.0,1.0,0.0
AppDynamics,41773005,41764607,2,"2017/01/20, 23:58:40",False,"2017/01/20, 23:58:40",474.0,4682632.0,1,"AppDynamics can collect log data, and provide similar capabilities to what a log analytics tool can do, but it's of less value than the transaction tracing and instrumentation you get.",0.0,0.087,0.913,0.394
AppDynamics,41773005,41764607,2,"2017/01/20, 23:58:40",False,"2017/01/20, 23:58:40",474.0,4682632.0,1,"Your application has to be built in a supported language (Java, .NET, PHP, Python, C++, Node.js) and if it's web or mobile based you can also collect client side performance data and unify between both frontend and backend.",0.0,0.06,0.94,0.3182
AppDynamics,41773005,41764607,2,"2017/01/20, 23:58:40",False,"2017/01/20, 23:58:40",474.0,4682632.0,1,If you have questions just reach out and I can answer them for you.,0.0,0.084,0.916,0.0258
AppDynamics,41773005,41764607,2,"2017/01/20, 23:58:40",False,"2017/01/20, 23:58:40",474.0,4682632.0,1,Good luck!,0.0,1.0,0.0,0.7345
AppDynamics,42998075,41764607,0,"2017/03/24, 13:14:03",False,"2017/03/24, 13:14:03",324.0,7220665.0,1,You basically need the AppDynamics Controller and a AppDynamics Machine-Agent which will be installed on the machine to monitor.,0.0,0.0,1.0,0.0
AppDynamics,42998075,41764607,0,"2017/03/24, 13:14:03",False,"2017/03/24, 13:14:03",324.0,7220665.0,1,In the Machine-Agent configuration you set the URI of the controller and the agent starts to report machine metrics to the controller.,0.0,0.0,1.0,0.0
AppDynamics,42998075,41764607,0,"2017/03/24, 13:14:03",False,"2017/03/24, 13:14:03",324.0,7220665.0,1,"Then you can configure alarms, see metrics, create dashboards, etc.",0.172,0.172,0.656,0.0
AppDynamics,42998075,41764607,0,"2017/03/24, 13:14:03",False,"2017/03/24, 13:14:03",324.0,7220665.0,1,"I can deliver you more information if you want, but as Jonah Kowall said, take a look at the documentation as well  AppDynamics Machine Agent Doc",0.0,0.147,0.853,0.4215
AppDynamics,43040838,42274447,0,"2017/03/27, 10:58:07",False,"2017/03/27, 10:58:07",324.0,7220665.0,1,"Typically, you don't need to change anything in your code to capture JMX metrics except your Java Beans have to fulfill the Management Beans (MBeans) requirements you have to enable JMX monitoring in each Java process that should be monitored and the monitored system runs on Java 1.5 or later.",0.0,0.056,0.944,0.4404
AppDynamics,43040838,42274447,0,"2017/03/27, 10:58:07",False,"2017/03/27, 10:58:07",324.0,7220665.0,1,See also  here  and  here .,0.0,0.0,1.0,0.0
AppDynamics,43040838,42274447,0,"2017/03/27, 10:58:07",False,"2017/03/27, 10:58:07",324.0,7220665.0,1,Then you can navigate to  Tiers &amp; Nodes -&gt; Select a tier -&gt; JMX tab,0.0,0.0,1.0,0.0
AppDynamics,42996563,42361990,0,"2017/03/24, 12:05:58",True,"2017/06/07, 09:23:41",324.0,7220665.0,2,"I am not exactly sure what the problem is, but you can share a Dashboard and open this shared Dashboard URL and make the Browser fullscreen.",0.112,0.216,0.672,0.6242
AppDynamics,42996563,42361990,0,"2017/03/24, 12:05:58",True,"2017/06/07, 09:23:41",324.0,7220665.0,2,Thats how we do it and it works perfectly (on AppDynamics controller version 4.2).,0.0,0.244,0.756,0.6369
AppDynamics,42996563,42361990,0,"2017/03/24, 12:05:58",True,"2017/06/07, 09:23:41",324.0,7220665.0,2,We use the 'revolver tab' add-on for the browser to switch between desktops.,0.0,0.0,1.0,0.0
AppDynamics,52864552,42719255,0,"2018/10/18, 01:37:57",False,"2018/10/18, 01:37:57",1.0,10521093.0,0,Try adding the line below in the crx-quickstart/conf/sling.properties:,0.0,0.0,1.0,0.0
AppDynamics,52864552,42719255,0,"2018/10/18, 01:37:57",False,"2018/10/18, 01:37:57",1.0,10521093.0,0,"org.osgi.framework.bootdelegation=com.singularity.*,com.yourkit.",0.0,0.0,1.0,0.0
AppDynamics,52864552,42719255,0,"2018/10/18, 01:37:57",False,"2018/10/18, 01:37:57",1.0,10521093.0,0,"*, ${org.apache.sling.launcher.bootdelegation}",0.0,0.0,1.0,0.0
AppDynamics,42982843,42950752,0,"2017/03/23, 19:21:05",True,"2017/03/23, 19:21:05",474.0,4682632.0,1,You can build multiple baselines within AppDynamics if you'd like to.,0.0,0.2,0.8,0.3612
AppDynamics,42982843,42950752,0,"2017/03/23, 19:21:05",True,"2017/03/23, 19:21:05",474.0,4682632.0,1,The thresholds should be auto calculated off deviation from baseline.,0.0,0.0,1.0,0.0
AppDynamics,42982843,42950752,0,"2017/03/23, 19:21:05",True,"2017/03/23, 19:21:05",474.0,4682632.0,1,This makes it so you don't need to configure them manually.,0.0,0.0,1.0,0.0
AppDynamics,42982843,42950752,0,"2017/03/23, 19:21:05",True,"2017/03/23, 19:21:05",474.0,4682632.0,1,"If you want to do SLA tracking, Business iQ (analytics) can do this very well.",0.0,0.221,0.779,0.4005
AppDynamics,42982843,42950752,0,"2017/03/23, 19:21:05",True,"2017/03/23, 19:21:05",474.0,4682632.0,1,We also are building additional features around SLA use cases we can share.,0.0,0.155,0.845,0.296
AppDynamics,42982843,42950752,0,"2017/03/23, 19:21:05",True,"2017/03/23, 19:21:05",474.0,4682632.0,1,Feel free to email me or support for a hand.,0.0,0.605,0.395,0.8481
AppDynamics,43535736,43496426,0,"2017/04/21, 09:47:35",False,"2017/04/21, 09:47:35",324.0,7220665.0,0,"I think everything you need is described  here  
Basically you need to do the following:",0.0,0.0,1.0,0.0
AppDynamics,43535736,43496426,0,"2017/04/21, 09:47:35",False,"2017/04/21, 09:47:35",324.0,7220665.0,0,probably in your case it is,0.0,0.0,1.0,0.0
AppDynamics,43535736,43496426,0,"2017/04/21, 09:47:35",False,"2017/04/21, 09:47:35",324.0,7220665.0,0,via CURL it looks like this,0.0,0.333,0.667,0.3612
AppDynamics,43535736,43496426,0,"2017/04/21, 09:47:35",False,"2017/04/21, 09:47:35",324.0,7220665.0,0,You can find more information about the AppDynamics API  here,0.0,0.0,1.0,0.0
AppDynamics,44338324,44269450,0,"2017/06/03, 01:35:15",True,"2017/06/03, 01:35:15",68.0,1394813.0,1,"Inside the widget settings make sure you select the ""Stack Areas or Columns"" checkbox.",0.0,0.15,0.85,0.3182
AppDynamics,44338324,44269450,0,"2017/06/03, 01:35:15",True,"2017/06/03, 01:35:15",68.0,1394813.0,1,This works in every version of AppD I've used including 4.3.0.2.,0.0,0.0,1.0,0.0
AppDynamics,44411981,44391470,1,"2017/06/07, 14:55:46",True,"2017/06/07, 14:55:46",474.0,4682632.0,1,"SQL 2005 is supported, but this was a bug which was introduced in version 4.3.0.",0.0,0.113,0.887,0.1655
AppDynamics,44411981,44391470,1,"2017/06/07, 14:55:46",True,"2017/06/07, 14:55:46",474.0,4682632.0,1,There is currently a diagnostic patch for this issue for supported customers.,0.0,0.187,0.813,0.3182
AppDynamics,44411981,44391470,1,"2017/06/07, 14:55:46",True,"2017/06/07, 14:55:46",474.0,4682632.0,1,The fix should be in the next patch level once we isolate the issue.,0.122,0.0,0.878,-0.2023
AppDynamics,44411981,44391470,1,"2017/06/07, 14:55:46",True,"2017/06/07, 14:55:46",474.0,4682632.0,1,If you'd like support just email help@appdynamics.com and they can assist.,0.0,0.366,0.634,0.6369
AppDynamics,44411981,44391470,1,"2017/06/07, 14:55:46",True,"2017/06/07, 14:55:46",474.0,4682632.0,1,Thanks.,0.0,1.0,0.0,0.4404
AppDynamics,44606002,44569260,1,"2017/06/17, 17:54:45",False,"2017/06/17, 17:54:45",474.0,4682632.0,1,In order to capture this data follow these steps:,0.0,0.0,1.0,0.0
AppDynamics,44606002,44569260,1,"2017/06/17, 17:54:45",False,"2017/06/17, 17:54:45",474.0,4682632.0,1,"Pointer, you can also adjust the time periods in the metric browser to adjust the time.",0.0,0.0,1.0,0.0
AppDynamics,44606002,44569260,1,"2017/06/17, 17:54:45",False,"2017/06/17, 17:54:45",474.0,4682632.0,1,Hope this is helpful.,0.0,0.74,0.26,0.6908
AppDynamics,45979323,44776999,0,"2017/08/31, 13:33:52",False,"2017/08/31, 13:33:52",676.0,7761024.0,0,Couple of things:,0.0,0.0,1.0,0.0
AppDynamics,45979323,44776999,0,"2017/08/31, 13:33:52",False,"2017/08/31, 13:33:52",676.0,7761024.0,0,I think the general URL format for app dynamics applications are (notice the '#'):,0.0,0.149,0.851,0.2732
AppDynamics,45979323,44776999,0,"2017/08/31, 13:33:52",False,"2017/08/31, 13:33:52",676.0,7761024.0,0,"Also, I think the requests.get method needs an additional parameter for the 'account'.",0.0,0.0,1.0,0.0
AppDynamics,45979323,44776999,0,"2017/08/31, 13:33:52",False,"2017/08/31, 13:33:52",676.0,7761024.0,0,"For instance, my auth format looks like:",0.0,0.294,0.706,0.3612
AppDynamics,45979323,44776999,0,"2017/08/31, 13:33:52",False,"2017/08/31, 13:33:52",676.0,7761024.0,0,I am able to get a right response code back with this config.,0.0,0.0,1.0,0.0
AppDynamics,45979323,44776999,0,"2017/08/31, 13:33:52",False,"2017/08/31, 13:33:52",676.0,7761024.0,0,Let me know if this works for you.,0.0,0.0,1.0,0.0
AppDynamics,50472163,44776999,0,"2018/05/22, 19:09:38",False,"2018/05/22, 21:41:45",6.0,8195498.0,-1,You could also use native python code for more control:,0.0,0.0,1.0,0.0
AppDynamics,50472163,44776999,0,"2018/05/22, 19:09:38",False,"2018/05/22, 21:41:45",6.0,8195498.0,-1,example:,0.0,0.0,1.0,0.0
AppDynamics,50472163,44776999,0,"2018/05/22, 19:09:38",False,"2018/05/22, 21:41:45",6.0,8195498.0,-1,If you prefer it in JSON simply specify it in the request.,0.0,0.0,1.0,0.0
AppDynamics,44805282,44805162,1,"2017/06/28, 17:23:30",False,"2017/06/28, 17:23:30",333.0,2534221.0,0,"You can get severity information by appending &amp;severities=INFO,WARN,ERROR to your URL.",0.0,0.0,1.0,0.0
AppDynamics,44805282,44805162,1,"2017/06/28, 17:23:30",False,"2017/06/28, 17:23:30",333.0,2534221.0,0,"So your url must be like :  http://:/controller/rest/applications//business-transactions?output=JSON&amp;severities=INFO,WARN,ERROR",0.0,0.294,0.706,0.3612
AppDynamics,45253438,44805162,0,"2017/07/22, 13:24:47",False,"2017/07/22, 13:24:47",11.0,8349426.0,1,Severity is associated with specific events/entities in AppDynamics.,0.0,0.0,1.0,0.0
AppDynamics,45253438,44805162,0,"2017/07/22, 13:24:47",False,"2017/07/22, 13:24:47",11.0,8349426.0,1,Based on your API call I can see that you are trying to retrieve information about Business Transactions (BTs).,0.0,0.0,1.0,0.0
AppDynamics,45253438,44805162,0,"2017/07/22, 13:24:47",False,"2017/07/22, 13:24:47",11.0,8349426.0,1,Severity param is not associated with BTs.,0.0,0.0,1.0,0.0
AppDynamics,45253438,44805162,0,"2017/07/22, 13:24:47",False,"2017/07/22, 13:24:47",11.0,8349426.0,1,e.g.,0.0,0.0,1.0,0.0
AppDynamics,45253438,44805162,0,"2017/07/22, 13:24:47",False,"2017/07/22, 13:24:47",11.0,8349426.0,1,"You can pull Severity for Health rule violations in AppDynamics by making the following API call:
http:///controller/rest/applications//problems/healthrule-violations
Result:",0.167,0.0,0.833,-0.5267
AppDynamics,45253438,44805162,0,"2017/07/22, 13:24:47",False,"2017/07/22, 13:24:47",11.0,8349426.0,1,You can find further information about using AppD controller API in the following documentation pages:,0.0,0.0,1.0,0.0
AppDynamics,45253438,44805162,0,"2017/07/22, 13:24:47",False,"2017/07/22, 13:24:47",11.0,8349426.0,1,https://docs.appdynamics.com/display/PRO42/AppDynamics+APIs,0.0,0.0,1.0,0.0
AppDynamics,45253438,44805162,0,"2017/07/22, 13:24:47",False,"2017/07/22, 13:24:47",11.0,8349426.0,1,https://docs.appdynamics.com/display/PRO42/Alert+and+Respond+API,0.0,0.0,1.0,0.0
AppDynamics,44960915,44939278,0,"2017/07/07, 03:51:06",False,"2017/07/07, 03:51:06",7.0,1858160.0,0,The Issue is resolved.,0.0,0.362,0.638,0.1779
AppDynamics,44960915,44939278,0,"2017/07/07, 03:51:06",False,"2017/07/07, 03:51:06",7.0,1858160.0,0,"The controller-info of Machine Agent need not to have any Application,Node and Tier name.",0.0,0.0,1.0,0.0
AppDynamics,44960915,44939278,0,"2017/07/07, 03:51:06",False,"2017/07/07, 03:51:06",7.0,1858160.0,0,It should include unique host id which should be same as controller-info of JavaAgent.,0.0,0.0,1.0,0.0
AppDynamics,46592166,46591986,0,"2017/10/05, 21:07:15",False,"2017/10/05, 21:07:15",938.0,1640095.0,1,It looks like you can use environment variables to configure the python appdynamics agent as well.,0.0,0.247,0.753,0.5574
AppDynamics,46592166,46591986,0,"2017/10/05, 21:07:15",False,"2017/10/05, 21:07:15",938.0,1640095.0,1,Open up your repl,0.0,0.0,1.0,0.0
AppDynamics,46592166,46591986,0,"2017/10/05, 21:07:15",False,"2017/10/05, 21:07:15",938.0,1640095.0,1,"For the usual configuration values (APP_NAME, TIER_NAME, NODE_NAME, etc) you can configure them via the environment variables.",0.0,0.144,0.856,0.4019
AppDynamics,46592166,46591986,0,"2017/10/05, 21:07:15",False,"2017/10/05, 21:07:15",938.0,1640095.0,1,You just need to prefix them with 'APPD_'.,0.0,0.0,1.0,0.0
AppDynamics,46592166,46591986,0,"2017/10/05, 21:07:15",False,"2017/10/05, 21:07:15",938.0,1640095.0,1,For for APP_NAME it would be:,0.0,0.0,1.0,0.0
AppDynamics,47352287,46591986,0,"2017/11/17, 15:57:08",False,"2017/11/17, 15:57:08",1248.0,2339348.0,0,You can configure the python agent in your code like so:,0.0,0.2,0.8,0.3612
AppDynamics,47352287,46591986,0,"2017/11/17, 15:57:08",False,"2017/11/17, 15:57:08",1248.0,2339348.0,0,"Alternatively, you can pass in the location of your appdynamics.cfg file.",0.0,0.0,1.0,0.0
AppDynamics,47352287,46591986,0,"2017/11/17, 15:57:08",False,"2017/11/17, 15:57:08",1248.0,2339348.0,0,"That is to say, setting environment variables is not enough.",0.0,0.0,1.0,0.0
AppDynamics,47352287,46591986,0,"2017/11/17, 15:57:08",False,"2017/11/17, 15:57:08",1248.0,2339348.0,0,"Then you need to manually start the proxy (after you  appd.init ) by running
 pyagent proxy start",0.0,0.0,1.0,0.0
AppDynamics,47352287,46591986,0,"2017/11/17, 15:57:08",False,"2017/11/17, 15:57:08",1248.0,2339348.0,0,The agent configuration from your code will be automatically used by the proxy.,0.0,0.0,1.0,0.0
AppDynamics,47352287,46591986,0,"2017/11/17, 15:57:08",False,"2017/11/17, 15:57:08",1248.0,2339348.0,0,For a full list of config keys see the  setting docs,0.0,0.0,1.0,0.0
AppDynamics,53704958,46591986,0,"2018/12/10, 13:41:43",False,"2018/12/10, 13:41:43",13.0,4981560.0,0,I managed to define just environment variables without changing application code.,0.0,0.0,1.0,0.0
AppDynamics,53704958,46591986,0,"2018/12/10, 13:41:43",False,"2018/12/10, 13:41:43",13.0,4981560.0,0,Note that variable name for controller host is APPD_CONTROLLER_HOST.,0.0,0.0,1.0,0.0
AppDynamics,53704958,46591986,0,"2018/12/10, 13:41:43",False,"2018/12/10, 13:41:43",13.0,4981560.0,0,You can also pass command line parameters to the process.,0.0,0.0,1.0,0.0
AppDynamics,47149088,47129599,0,"2017/11/07, 04:33:37",False,"2017/11/07, 04:33:37",474.0,4682632.0,0,"Yes, it does transaction tracking for every intra-component call across languages.",0.0,0.213,0.787,0.4019
AppDynamics,47149088,47129599,0,"2017/11/07, 04:33:37",False,"2017/11/07, 04:33:37",474.0,4682632.0,0,Without code changes.,0.0,0.0,1.0,0.0
AppDynamics,47149088,47129599,0,"2017/11/07, 04:33:37",False,"2017/11/07, 04:33:37",474.0,4682632.0,0,"When there is a slow transaction detail down to code level will show up in snapshots, this is what you'd use for diagnostics.",0.0,0.0,1.0,0.0
AppDynamics,47149088,47129599,0,"2017/11/07, 04:33:37",False,"2017/11/07, 04:33:37",474.0,4682632.0,0,Depending on your language you can also configure data collectors which do runtime instrumentation of custom data from the code.,0.0,0.0,1.0,0.0
AppDynamics,47149088,47129599,0,"2017/11/07, 04:33:37",False,"2017/11/07, 04:33:37",474.0,4682632.0,0,"These show up on every call, and you can turn them into metrics too.",0.0,0.0,1.0,0.0
AppDynamics,47149088,47129599,0,"2017/11/07, 04:33:37",False,"2017/11/07, 04:33:37",474.0,4682632.0,0,Once again no code changes.,0.355,0.0,0.645,-0.296
AppDynamics,47149111,47135535,0,"2017/11/07, 04:37:05",False,"2017/11/07, 04:37:05",474.0,4682632.0,3,Zipkin only does tracing.,0.0,0.0,1.0,0.0
AppDynamics,47149111,47135535,0,"2017/11/07, 04:37:05",False,"2017/11/07, 04:37:05",474.0,4682632.0,3,"APM tools like Appdynamics do other monitoring (browser, mobile, database, server, network).",0.0,0.185,0.815,0.3612
AppDynamics,47149111,47135535,0,"2017/11/07, 04:37:05",False,"2017/11/07, 04:37:05",474.0,4682632.0,3,Code-level diagnostics with automated overhead controls and limiters.,0.0,0.0,1.0,0.0
AppDynamics,47149111,47135535,0,"2017/11/07, 04:37:05",False,"2017/11/07, 04:37:05",474.0,4682632.0,3,Don't forget log analytics and transaction analytics.,0.0,0.217,0.783,0.1695
AppDynamics,47149111,47135535,0,"2017/11/07, 04:37:05",False,"2017/11/07, 04:37:05",474.0,4682632.0,3,It also collects metrics.,0.0,0.0,1.0,0.0
AppDynamics,47149111,47135535,0,"2017/11/07, 04:37:05",False,"2017/11/07, 04:37:05",474.0,4682632.0,3,"There is a lot more to APM than just tracing, which is what Zipkin does.",0.0,0.0,1.0,0.0
AppDynamics,47149111,47135535,0,"2017/11/07, 04:37:05",False,"2017/11/07, 04:37:05",474.0,4682632.0,3,"You could do this with a stack of 20 open source tools, but you'd have to deal with disjointed UIs and data models not to mention the work associated with keeping them all working.",0.084,0.0,0.916,-0.4497
AppDynamics,47382243,47378215,1,"2017/11/19, 23:34:58",False,"2017/11/19, 23:34:58",127.0,7526329.0,-2,"It's actually more the other way around - Crittercism does crash logging, network monitoring, and performance timing whereas AppDynamics is more focused on server monitoring.",0.098,0.105,0.797,0.0498
AppDynamics,47398459,47378215,0,"2017/11/20, 20:08:39",True,"2017/11/20, 20:08:39",474.0,4682632.0,0,"Both products essentially do the same thing, if anything AppDynamics has advanced beyond Crittercism (now called Apteligent).",0.0,0.111,0.889,0.25
AppDynamics,47398459,47378215,0,"2017/11/20, 20:08:39",True,"2017/11/20, 20:08:39",474.0,4682632.0,0,"They had the lead when the mAPM market started, but now there is not much innovating happening, especially after they sold the company to VMware earlier this year.",0.0,0.0,1.0,0.0
AppDynamics,47398459,47378215,0,"2017/11/20, 20:08:39",True,"2017/11/20, 20:08:39",474.0,4682632.0,0,"Here are the mRUM docs for AppD:  https://docs.appdynamics.com/display/PRO44/Mobile+Real+User+Monitoring  some of the more advanced features in AppD you will not find in Appteligent would be things like screen recording, breadcrumb/navigation for every click and interaction.",0.0,0.13,0.87,0.5849
AppDynamics,47398459,47378215,0,"2017/11/20, 20:08:39",True,"2017/11/20, 20:08:39",474.0,4682632.0,0,"The most valuable feature, of course, is the measurement outside of a straight mobile use case.",0.0,0.289,0.711,0.6478
AppDynamics,47398459,47378215,0,"2017/11/20, 20:08:39",True,"2017/11/20, 20:08:39",474.0,4682632.0,0,"AppD does measure and ties together the mobile transaction with the backend, server (and network), Docker containers, AWS or other cloud providers.",0.0,0.0,1.0,0.0
AppDynamics,47398459,47378215,0,"2017/11/20, 20:08:39",True,"2017/11/20, 20:08:39",474.0,4682632.0,0,"Additionally, it monitors performance and usage of browsers.",0.0,0.0,1.0,0.0
AppDynamics,47398459,47378215,0,"2017/11/20, 20:08:39",True,"2017/11/20, 20:08:39",474.0,4682632.0,0,"Disclosure: I used to be the lead for these products at Gartner, and have been working at AppDynamics for the last 3 years.",0.0,0.0,1.0,0.0
AppDynamics,50738776,47378215,0,"2018/06/07, 13:19:23",False,"2018/06/07, 13:25:10",2304.0,1162044.0,0,For Apple platforms I recommend you to avoid any third party crash log frameworks and use  Xcode crashes organizer.,0.203,0.216,0.581,0.0772
AppDynamics,52641688,49509430,0,"2018/10/04, 10:52:28",False,"2018/10/04, 10:52:28",21.0,3348861.0,0,It seems that you schould be able to define your custom  Asynchronous Transaction Demarcator  as described in:   https://docs.appdynamics.com/display/PRO44/Asynchronous+Transaction+Demarcators,0.0,0.0,1.0,0.0
AppDynamics,52641688,49509430,0,"2018/10/04, 10:52:28",False,"2018/10/04, 10:52:28",21.0,3348861.0,0,which will point to the last method of Runnable that you passes to the Executor.,0.0,0.0,1.0,0.0
AppDynamics,52641688,49509430,0,"2018/10/04, 10:52:28",False,"2018/10/04, 10:52:28",21.0,3348861.0,0,Then according to the documentation all you need is to attach the Demarcator to your Business Transaction and it will collect the asynchronous call.,0.0,0.0,1.0,0.0
AppDynamics,50270677,50218209,3,"2018/05/10, 13:22:05",False,"2018/05/10, 13:22:05",16918.0,1047788.0,0,The first step is to add a username and password in the etc/users.properties file.,0.0,0.0,1.0,0.0
AppDynamics,50270677,50218209,3,"2018/05/10, 13:22:05",False,"2018/05/10, 13:22:05",16918.0,1047788.0,0,"For most purposes, it is ok to just 
use the default settings provided out of the box.",0.0,0.121,0.879,0.296
AppDynamics,50270677,50218209,3,"2018/05/10, 13:22:05",False,"2018/05/10, 13:22:05",16918.0,1047788.0,0,"For this, just uncomment the following line:",0.0,0.0,1.0,0.0
AppDynamics,50270677,50218209,3,"2018/05/10, 13:22:05",False,"2018/05/10, 13:22:05",16918.0,1047788.0,0,"Then, you must bypass credential checks on BrokeViewMBean by adding it to the whitelist ACL configuration.",0.0,0.0,1.0,0.0
AppDynamics,50270677,50218209,3,"2018/05/10, 13:22:05",False,"2018/05/10, 13:22:05",16918.0,1047788.0,0,You can do so by replacing this line:,0.0,0.0,1.0,0.0
AppDynamics,50270677,50218209,3,"2018/05/10, 13:22:05",False,"2018/05/10, 13:22:05",16918.0,1047788.0,0,with this:,0.0,0.0,1.0,0.0
AppDynamics,50270677,50218209,3,"2018/05/10, 13:22:05",False,"2018/05/10, 13:22:05",16918.0,1047788.0,0,"In addition to being the correct way, it also enables several different configuration options (eg: port, listen address, etc) by just changing the file org.apache.karaf.management.cfg on broker's etc directory.",0.0,0.0,1.0,0.0
AppDynamics,50270677,50218209,3,"2018/05/10, 13:22:05",False,"2018/05/10, 13:22:05",16918.0,1047788.0,0,"Please keep in mind that JMX access is made through a different JMX connector root in this case: it uses  karaf-root  instead of  jmxrmi , which was previously used in the older method.",0.0,0.071,0.929,0.3182
AppDynamics,50270677,50218209,3,"2018/05/10, 13:22:05",False,"2018/05/10, 13:22:05",16918.0,1047788.0,0,"It also uses port 1099 by default, instead of 1616.",0.0,0.0,1.0,0.0
AppDynamics,50270677,50218209,3,"2018/05/10, 13:22:05",False,"2018/05/10, 13:22:05",16918.0,1047788.0,0,"Therefore, the uri should be",0.0,0.0,1.0,0.0
AppDynamics,52869901,52825100,0,"2018/10/18, 11:25:36",False,"2018/10/18, 11:25:36",294930.0,208809.0,3,"From my current knowledge: no, AppDynamics doesn't support OpenTracing yet.",0.358,0.0,0.642,-0.5358
AppDynamics,52869901,52825100,0,"2018/10/18, 11:25:36",False,"2018/10/18, 11:25:36",294930.0,208809.0,3,"Usually, APM vendors have their own OpenTracing tracers build off the official specification and then get them listed at  http://opentracing.io .",0.0,0.0,1.0,0.0
AppDynamics,52869901,52825100,0,"2018/10/18, 11:25:36",False,"2018/10/18, 11:25:36",294930.0,208809.0,3,But as of this writing there is no mention of any AppDynamics Tracers at  https://opentracing.io/docs/supported-tracers/  nor  https://github.com/opentracing-contrib/meta .,0.149,0.0,0.851,-0.4215
AppDynamics,52869901,52825100,0,"2018/10/18, 11:25:36",False,"2018/10/18, 11:25:36",294930.0,208809.0,3,"Full disclosure: I work for Instana, a competitor that does support OpenTracing.",0.0,0.231,0.769,0.4019
AppDynamics,52989775,52825100,0,"2018/10/25, 15:53:20",False,"2018/10/25, 15:53:20",474.0,4682632.0,0,No AppD doesn't support OpenTracing at this time.,0.426,0.0,0.574,-0.5358
AppDynamics,52989775,52825100,0,"2018/10/25, 15:53:20",False,"2018/10/25, 15:53:20",474.0,4682632.0,0,The question is why do you want this functionality when you can already extract custom data from transactions dynamically with most AppDynamics agents?,0.0,0.153,0.847,0.4215
AppDynamics,52989775,52825100,0,"2018/10/25, 15:53:20",False,"2018/10/25, 15:53:20",474.0,4682632.0,0,Do you really want to hardcode your APM tool into your software application?,0.0,0.117,0.883,0.1513
AppDynamics,52989775,52825100,0,"2018/10/25, 15:53:20",False,"2018/10/25, 15:53:20",474.0,4682632.0,0,"AppDynamics is building a unique way to support OpenTracing, which is currently in testing, but the approach is not by hardcoding libraries into the code.",0.0,0.074,0.926,0.2144
AppDynamics,52989775,52825100,0,"2018/10/25, 15:53:20",False,"2018/10/25, 15:53:20",474.0,4682632.0,0,"If you'd like to learn more please reach out to support, or you can contact me directly as I work for AppDynamics.",0.0,0.35,0.65,0.8004
AppDynamics,52989775,52825100,0,"2018/10/25, 15:53:20",False,"2018/10/25, 15:53:20",474.0,4682632.0,0,Thanks.,0.0,1.0,0.0,0.4404
AppDynamics,58802450,54003615,0,"2019/11/11, 15:36:01",False,"2019/11/11, 15:36:01",26.0,12355048.0,0,You need to add:,0.0,0.0,1.0,0.0
AppDynamics,58802450,54003615,0,"2019/11/11, 15:36:01",False,"2019/11/11, 15:36:01",26.0,12355048.0,0,into   wrapper.conf  file.,0.0,0.0,1.0,0.0
AppDynamics,58802450,54003615,0,"2019/11/11, 15:36:01",False,"2019/11/11, 15:36:01",26.0,12355048.0,0,https://docs.appdynamics.com/display/PRO45/Tanuki+Service+Wrapper+Settings,0.0,0.0,1.0,0.0
AppDynamics,58802165,55234879,0,"2019/11/11, 15:18:20",False,"2019/11/11, 15:18:20",26.0,12355048.0,0,You can export the dashboard and you will get the json version of your dashboard.,0.0,0.0,1.0,0.0
AppDynamics,58802165,55234879,0,"2019/11/11, 15:18:20",False,"2019/11/11, 15:18:20",26.0,12355048.0,0,(The export button is located top of the your dashboard page),0.0,0.153,0.847,0.2023
AppDynamics,58802165,55234879,0,"2019/11/11, 15:18:20",False,"2019/11/11, 15:18:20",26.0,12355048.0,0,This json file can be easily editable with any editor and you can replace the Application and/or other properties which contain on of your dashboards.,0.0,0.091,0.909,0.34
AppDynamics,58802165,55234879,0,"2019/11/11, 15:18:20",False,"2019/11/11, 15:18:20",26.0,12355048.0,0,https://docs.appdynamics.com/display/PRO45/Import+and+Export+Custom+Dashboards+and+Templates+Using+the+UI,0.0,0.0,1.0,0.0
AppDynamics,55811490,55332537,1,"2019/04/23, 15:42:30",False,"2019/04/23, 15:42:30",53.0,2309810.0,0,Due to its proprietary nature i don't think the internals are available for anyone to view or discuss.,0.0,0.0,1.0,0.0
AppDynamics,55811490,55332537,1,"2019/04/23, 15:42:30",False,"2019/04/23, 15:42:30",53.0,2309810.0,0,Below link might give an idea of how the product works at a high level.,0.0,0.0,1.0,0.0
AppDynamics,55811490,55332537,1,"2019/04/23, 15:42:30",False,"2019/04/23, 15:42:30",53.0,2309810.0,0,https://www.appdynamics.com/product/how-it-works/,0.0,0.0,1.0,0.0
AppDynamics,55700067,55697936,0,"2019/04/16, 06:18:36",False,"2019/04/16, 06:18:36",11138.0,9132707.0,0,"You are on the right track, but you are not saying that you are having errors or showing the.",0.147,0.0,0.853,-0.4767
AppDynamics,55700067,55697936,0,"2019/04/16, 06:18:36",False,"2019/04/16, 06:18:36",11138.0,9132707.0,0,"Yet, based on your experience to date, I am sure you already know that PowerShell provides cmdlets for working with XML.",0.0,0.108,0.892,0.3182
AppDynamics,55700067,55697936,0,"2019/04/16, 06:18:36",False,"2019/04/16, 06:18:36",11138.0,9132707.0,0,See them using ---,0.0,0.0,1.0,0.0
AppDynamics,55700067,55697936,0,"2019/04/16, 06:18:36",False,"2019/04/16, 06:18:36",11138.0,9132707.0,0,or get other XML modules from the MS PowerShellGallery.com using ---,0.0,0.0,1.0,0.0
AppDynamics,55700067,55697936,0,"2019/04/16, 06:18:36",False,"2019/04/16, 06:18:36",11138.0,9132707.0,0,--- and install the one(s) that fit your needed goals.,0.0,0.217,0.783,0.3612
AppDynamics,55700067,55697936,0,"2019/04/16, 06:18:36",False,"2019/04/16, 06:18:36",11138.0,9132707.0,0,And of course there are lot's of examples and videos on the topic.,0.0,0.0,1.0,0.0
AppDynamics,55700067,55697936,0,"2019/04/16, 06:18:36",False,"2019/04/16, 06:18:36",11138.0,9132707.0,0,"Searching for 'PowerShell working with XML', gives you plenty of hits.",0.0,0.0,1.0,0.0
AppDynamics,55700067,55697936,0,"2019/04/16, 06:18:36",False,"2019/04/16, 06:18:36",11138.0,9132707.0,0,PowerShell Data Basics: XML,0.0,0.0,1.0,0.0
AppDynamics,55700067,55697936,0,"2019/04/16, 06:18:36",False,"2019/04/16, 06:18:36",11138.0,9132707.0,0,"For the most part, what you will find is very similar to what you already have posted.",0.0,0.0,1.0,0.0
AppDynamics,55700067,55697936,0,"2019/04/16, 06:18:36",False,"2019/04/16, 06:18:36",11138.0,9132707.0,0,"Yet, you say you want add nodes / elements, but that is not in your post, so, something like the below should help.",0.0,0.306,0.694,0.8109
AppDynamics,55700067,55697936,0,"2019/04/16, 06:18:36",False,"2019/04/16, 06:18:36",11138.0,9132707.0,0,Or even just using the  WebAdministration module  on the IIS server directly.,0.0,0.0,1.0,0.0
AppDynamics,55709010,55697936,0,"2019/04/16, 16:11:04",False,"2019/04/16, 16:11:04",101.0,2586735.0,0,You should check out the AppDynamics agent installation PowerShell Extension:  https://www.appdynamics.com/community/exchange/extension/dotnet-agent-installation-with-remote-management-powershell-extension/,0.0,0.0,1.0,0.0
AppDynamics,55709010,55697936,0,"2019/04/16, 16:11:04",False,"2019/04/16, 16:11:04",101.0,2586735.0,0,"It should be able to handle what you are trying to do without having to generate the xml manually, check the pdf for advanced options and read about the Add-IISApplicationMonitoring cmdlet",0.0,0.062,0.938,0.25
AppDynamics,55709010,55697936,0,"2019/04/16, 16:11:04",False,"2019/04/16, 16:11:04",101.0,2586735.0,0,What it can’t do is the newer stuff like “multiple business application” and “custom node name” configuration.,0.0,0.135,0.865,0.3612
AppDynamics,55709010,55697936,0,"2019/04/16, 16:11:04",False,"2019/04/16, 16:11:04",101.0,2586735.0,0,(Which you can’t do in the install wizard either),0.0,0.0,1.0,0.0
AppDynamics,55810979,55787843,0,"2019/04/23, 15:12:33",False,"2019/04/23, 15:24:27",53.0,2309810.0,0,"AppDynamics itself powerful enough to provide all sort of information, However if you still want actuator data to be captured and displayed then you may need to use AppD extension.",0.0,0.128,0.872,0.4767
AppDynamics,55810979,55787843,0,"2019/04/23, 15:12:33",False,"2019/04/23, 15:24:27",53.0,2309810.0,0,Please refer below official link to AppDyanmics Exchange.,0.0,0.247,0.753,0.3182
AppDynamics,55810979,55787843,0,"2019/04/23, 15:12:33",False,"2019/04/23, 15:24:27",53.0,2309810.0,0,https://www.appdynamics.com/community/exchange/,0.0,0.0,1.0,0.0
AppDynamics,55810979,55787843,0,"2019/04/23, 15:12:33",False,"2019/04/23, 15:24:27",53.0,2309810.0,0,if the relevant is not available you may need write your own.,0.0,0.0,1.0,0.0
AppDynamics,57925017,57912322,0,"2019/09/13, 16:56:48",False,"2019/09/13, 16:56:48",21.0,5846608.0,0,I found this solution  https://github.com/Appdynamics/flowmap-builder,0.0,0.434,0.566,0.3182
AppDynamics,57925017,57912322,0,"2019/09/13, 16:56:48",False,"2019/09/13, 16:56:48",21.0,5846608.0,0,Seems to be working so far.,0.0,0.0,1.0,0.0
AppDynamics,57925017,57912322,0,"2019/09/13, 16:56:48",False,"2019/09/13, 16:56:48",21.0,5846608.0,0,Doesn't rely (directly) on APIs but it works!,0.0,0.0,1.0,0.0
AppDynamics,58010383,57958059,0,"2019/09/19, 14:50:45",True,"2019/09/19, 14:50:45",41.0,7611518.0,0,For anyone who is looking for an answer here.,0.0,0.0,1.0,0.0
AppDynamics,58010383,57958059,0,"2019/09/19, 14:50:45",True,"2019/09/19, 14:50:45",41.0,7611518.0,0,This is resolved by unchecking the Extended  tracking option for kafka in AppDynamics Console.,0.0,0.116,0.884,0.1779
AppDynamics,58010383,57958059,0,"2019/09/19, 14:50:45",True,"2019/09/19, 14:50:45",41.0,7611518.0,0,The option will be there in Automatic Backend Discovery.,0.0,0.0,1.0,0.0
AppDynamics,58010383,57958059,0,"2019/09/19, 14:50:45",True,"2019/09/19, 14:50:45",41.0,7611518.0,0,Hope this helps.,0.0,0.846,0.154,0.6705
AppDynamics,58800924,57958059,0,"2019/11/11, 13:57:56",False,"2019/11/11, 13:57:56",26.0,12355048.0,0,"Disable default Kafka configuration on ""Backend Detection"" page",0.0,0.0,1.0,0.0
AppDynamics,58800924,57958059,0,"2019/11/11, 13:57:56",False,"2019/11/11, 13:57:56",26.0,12355048.0,0,"Define a new ""Custom Exit Point"" with the last class and method which you see on the call graphs before Kafka exit call",0.0,0.0,1.0,0.0
AppDynamics,58800924,57958059,0,"2019/11/11, 13:57:56",False,"2019/11/11, 13:57:56",26.0,12355048.0,0,"Don't forget to click on ""Is High Volume"" checkbox.",0.0,0.172,0.828,0.1695
AppDynamics,58801167,57977882,0,"2019/11/11, 14:16:20",True,"2019/11/11, 14:16:20",26.0,12355048.0,1,If we take the example of an ECommerce Application:,0.0,0.0,1.0,0.0
AppDynamics,58801167,57977882,0,"2019/11/11, 14:16:20",True,"2019/11/11, 14:16:20",26.0,12355048.0,1,"Business Transactions  are Checkout, Landing Page, Add to Cart etc.",0.0,0.0,1.0,0.0
AppDynamics,58801167,57977882,0,"2019/11/11, 14:16:20",True,"2019/11/11, 14:16:20",26.0,12355048.0,1,which are known by every end user of the application.,0.0,0.0,1.0,0.0
AppDynamics,58801167,57977882,0,"2019/11/11, 14:16:20",True,"2019/11/11, 14:16:20",26.0,12355048.0,1,"These business transactions cover all the method executions, database calls, web service calls etc.",0.0,0.0,1.0,0.0
AppDynamics,58801167,57977882,0,"2019/11/11, 14:16:20",True,"2019/11/11, 14:16:20",26.0,12355048.0,1,Service End Points  are the sub calls(method call or web service call) execute inside of the Business Transactions.,0.0,0.0,1.0,0.0
AppDynamics,58801167,57977882,0,"2019/11/11, 14:16:20",True,"2019/11/11, 14:16:20",26.0,12355048.0,1,"Such as ""Check Inventory"" service which is executed in Checkout and Add to Cart transactions as well.",0.0,0.116,0.884,0.2732
AppDynamics,58801167,57977882,0,"2019/11/11, 14:16:20",True,"2019/11/11, 14:16:20",26.0,12355048.0,1,"Information Points  are the key business or technical metric counts, such as Checkout amount, Add to Cart item count.",0.0,0.0,1.0,0.0
AppDynamics,58801167,57977882,0,"2019/11/11, 14:16:20",True,"2019/11/11, 14:16:20",26.0,12355048.0,1,"Service End Points and Information Points only give you performance metrics but Business Transactions also give you full code visibility with ""call graphs""",0.0,0.0,1.0,0.0
AppDynamics,58801167,57977882,0,"2019/11/11, 14:16:20",True,"2019/11/11, 14:16:20",26.0,12355048.0,1,"Also, there are some limitations like max 200 Business Transaction on the default but you can change these rules.",0.0,0.089,0.911,0.1901
AppDynamics,58801167,57977882,0,"2019/11/11, 14:16:20",True,"2019/11/11, 14:16:20",26.0,12355048.0,1,"While configuring the BTs &amp; SEs, you must focus on the needs of AppDynamics users.",0.0,0.0,1.0,0.0
AppDynamics,58801167,57977882,0,"2019/11/11, 14:16:20",True,"2019/11/11, 14:16:20",26.0,12355048.0,1,"If you configure AppDynamics for mostly business teams, I can use BTs like I described above.",0.0,0.161,0.839,0.3612
AppDynamics,58801167,57977882,0,"2019/11/11, 14:16:20",True,"2019/11/11, 14:16:20",26.0,12355048.0,1,"But If you target the Dev and Ops teams, you can configure your BTs based on method or service calls.",0.0,0.0,1.0,0.0
AppDynamics,58801167,57977882,0,"2019/11/11, 14:16:20",True,"2019/11/11, 14:16:20",26.0,12355048.0,1,There is no only single approach on BT &amp; SE configuration.,0.18,0.0,0.82,-0.296
AppDynamics,58801167,57977882,0,"2019/11/11, 14:16:20",True,"2019/11/11, 14:16:20",26.0,12355048.0,1,You must shape that with the needs of your AppDynamics users.,0.0,0.0,1.0,0.0
AppDynamics,58800607,58644710,2,"2019/11/11, 13:37:12",False,"2019/11/11, 13:55:20",26.0,12355048.0,0,Configuration- Instrumentation- Transaction Detection- Add,0.0,0.0,1.0,0.0
AppDynamics,58800607,58644710,2,"2019/11/11, 13:37:12",False,"2019/11/11, 13:55:20",26.0,12355048.0,0,"On the ""Split Transactions Using Request Data"" section you must choose "" Specific URI Segments ""
Segment Numbers: 1,2,4",0.0,0.0,1.0,0.0
AppDynamics,58800607,58644710,2,"2019/11/11, 13:37:12",False,"2019/11/11, 13:55:20",26.0,12355048.0,0,"In your case transaction name will be ""/data/scenario/job""",0.0,0.0,1.0,0.0
AppDynamics,58800607,58644710,2,"2019/11/11, 13:37:12",False,"2019/11/11, 13:55:20",26.0,12355048.0,0,Sample Configuration:,0.0,0.0,1.0,0.0
AppDynamics,60203721,60082614,0,"2020/02/13, 10:51:30",False,"2020/02/13, 10:51:30",11.0,2131858.0,0,"On analyzing the heap dump taken during system idle state, we only see various WebAppClassLoaders holding instances of different library classes.",0.115,0.0,0.885,-0.3818
AppDynamics,60203721,60082614,0,"2020/02/13, 10:51:30",False,"2020/02/13, 10:51:30",11.0,2131858.0,0,This pattern is also explained in official blogs of APM experts like  Plumbr  and  Datadog  as a sign of healthy JVM where regular GC activity is occurring and they explain that it means none of the objects will stay in memory forever.,0.0,0.118,0.882,0.6369
AppDynamics,60203721,60082614,0,"2020/02/13, 10:51:30",False,"2020/02/13, 10:51:30",11.0,2131858.0,0,From Plumbr blog:,0.0,0.0,1.0,0.0
AppDynamics,60203721,60082614,0,"2020/02/13, 10:51:30",False,"2020/02/13, 10:51:30",11.0,2131858.0,0,Seeing the following pattern is a confirmation that the JVM at question is definitely not leaking memory.,0.0,0.153,0.847,0.4019
AppDynamics,60203721,60082614,0,"2020/02/13, 10:51:30",False,"2020/02/13, 10:51:30",11.0,2131858.0,0,The reason for the double-sawtooth pattern is that the JVM needs to allocate memory on the heap as new objects are created as a part of the normal program execution.,0.0,0.067,0.933,0.25
AppDynamics,60203721,60082614,0,"2020/02/13, 10:51:30",False,"2020/02/13, 10:51:30",11.0,2131858.0,0,Most of these objects are short-lived and quickly become garbage.,0.0,0.0,1.0,0.0
AppDynamics,60203721,60082614,0,"2020/02/13, 10:51:30",False,"2020/02/13, 10:51:30",11.0,2131858.0,0,These short-lived objects are collected by a collector called “Minor GC” and represent the small drops on the sawteeth.,0.0,0.0,1.0,0.0
AppDynamics,60129360,60127980,4,"2020/02/08, 19:36:49",False,"2020/02/09, 10:51:22",2705.0,1501456.0,0,The reason you don't see the browser name in HTTP protocol is because there is no browser.,0.121,0.0,0.879,-0.296
AppDynamics,60129360,60127980,4,"2020/02/08, 19:36:49",False,"2020/02/09, 10:51:22",2705.0,1501456.0,0,The protocol is a transport level protocol which means it simulates the traffic of the browser without running the actual browser.,0.0,0.0,1.0,0.0
AppDynamics,60129360,60127980,4,"2020/02/08, 19:36:49",False,"2020/02/09, 10:51:22",2705.0,1501456.0,0,This allows the protocol to simulate many more virtual users than a client level protocol such as TruClient.,0.0,0.0,1.0,0.0
AppDynamics,60129360,60127980,4,"2020/02/08, 19:36:49",False,"2020/02/09, 10:51:22",2705.0,1501456.0,0,EDIT: There is no dedicated API and you must use the user agent.,0.136,0.185,0.679,0.2023
AppDynamics,60129360,60127980,4,"2020/02/08, 19:36:49",False,"2020/02/09, 10:51:22",2705.0,1501456.0,0,Please refer to this article for more details:  https://www.appdynamics.com/blog/engineering/how-to-use-appdynamics-with-loadrunner-for-load-testing/,0.0,0.223,0.777,0.3182
AppDynamics,62775806,61730657,0,"2020/07/07, 15:55:48",False,"2020/07/07, 15:55:48",1.0,11862547.0,0,I'd try setting &quot;Use data from last&quot; to  60 minutes .,0.0,0.0,1.0,0.0
AppDynamics,62775806,61730657,0,"2020/07/07, 15:55:48",False,"2020/07/07, 15:55:48",1.0,11862547.0,0,In Criteria-tab use &quot;Single Metric&quot; with  Sum  of &quot;Calls per Minute&quot; and define your threshold.,0.0,0.0,1.0,0.0
AppDynamics,65580996,63517278,0,"2021/01/05, 16:39:58",False,"2021/01/05, 16:39:58",3170.0,1237595.0,0,The Drop-off rate is the percentage of user sessions which reach the specific page and then end their session (they do not reach any further pages as part of the session and therefore &quot;drop-off&quot; the map).,0.0,0.061,0.939,0.0516
AppDynamics,65580996,63517278,0,"2021/01/05, 16:39:58",False,"2021/01/05, 16:39:58",3170.0,1237595.0,0,This can be seen in the example (image) in the  documentation,0.0,0.0,1.0,0.0
AppDynamics,66421794,65134789,0,"2021/03/01, 14:02:52",False,"2021/03/01, 14:02:52",3170.0,1237595.0,0,Yes - you can do this using ADQL / Analytics searches (assuming you have this licensed).,0.0,0.172,0.828,0.4019
AppDynamics,66421794,65134789,0,"2021/03/01, 14:02:52",False,"2021/03/01, 14:02:52",3170.0,1237595.0,0,Without specifics I can only give you a general guide:,0.0,0.0,1.0,0.0
AppDynamics,66623443,65710328,0,"2021/03/14, 12:32:34",False,"2021/03/20, 18:19:18",3170.0,1237595.0,0,Generally the workflow for sending reports from AppDynamics is as follows:,0.0,0.0,1.0,0.0
AppDynamics,66623443,65710328,0,"2021/03/14, 12:32:34",False,"2021/03/20, 18:19:18",3170.0,1237595.0,0,Note: There is a sample custom dashboard .json available here to get you started:  https://community.appdynamics.com/t5/Knowledge-Base/Sample-Custom-Dashboard-for-Business-Transaction-Report/ta-p/21264,0.0,0.0,1.0,0.0
AppDynamics,66623443,65710328,0,"2021/03/14, 12:32:34",False,"2021/03/20, 18:19:18",3170.0,1237595.0,0,Note: There are already many &quot;Standard Reports&quot; which could make things easier depending on use case.,0.0,0.157,0.843,0.4215
AppDynamics,66623443,65710328,0,"2021/03/14, 12:32:34",False,"2021/03/20, 18:19:18",3170.0,1237595.0,0,"Note: If you instead want to export data and then analyse with your own tooling, then see the docs on Public API's available here:  https://docs.appdynamics.com/display/PRO21/AppDynamics+APIs",0.0,0.051,0.949,0.0772
AppDynamics,66421156,66096667,0,"2021/03/01, 13:19:27",False,"2021/03/01, 13:19:27",3170.0,1237595.0,0,All currently available AppDynamics APIs are documented here:  https://docs.appdynamics.com/display/PRO21/AppDynamics+APIs,0.0,0.0,1.0,0.0
AppDynamics,66421156,66096667,0,"2021/03/01, 13:19:27",False,"2021/03/01, 13:19:27",3170.0,1237595.0,0,The main page includes a summary of what is available.,0.0,0.0,1.0,0.0
AppDynamics,31367542,31358408,0,"2015/07/12, 15:01:07",False,"2015/07/12, 15:01:07",474.0,4682632.0,0,Business transactions should accomplish this.,0.0,0.412,0.588,0.4215
AppDynamics,31367542,31358408,0,"2015/07/12, 15:01:07",False,"2015/07/12, 15:01:07",474.0,4682632.0,0,If you want to report on each web service you can build a report or custom dashboard.,0.0,0.08,0.92,0.0772
AppDynamics,31367542,31358408,0,"2015/07/12, 15:01:07",False,"2015/07/12, 15:01:07",474.0,4682632.0,0,If you need more assistance just email help@appdynamics.com,0.0,0.0,1.0,0.0
AppDynamics,29093613,29083565,0,"2015/03/17, 09:37:21",True,"2015/03/17, 09:37:21",5170.0,1242093.0,0,AFAIK the critical point for AppDynamics (or profilers like that) it is essential to find an entry point.,0.111,0.12,0.769,0.0516
AppDynamics,29093613,29083565,0,"2015/03/17, 09:37:21",True,"2015/03/17, 09:37:21",5170.0,1242093.0,0,"Usually the prefered way is to have an Servlet ""Endpoint"" that starts a threat and can be followed.",0.175,0.0,0.825,-0.5267
AppDynamics,29093613,29083565,0,"2015/03/17, 09:37:21",True,"2015/03/17, 09:37:21",5170.0,1242093.0,0,"For the scenario you are describing this wouldn't work as it's missing the ""trigger"" to start the following.",0.115,0.0,0.885,-0.296
AppDynamics,29093613,29083565,0,"2015/03/17, 09:37:21",True,"2015/03/17, 09:37:21",5170.0,1242093.0,0,Most likely you'll need to build your own app-dynamics monitoring extension for it.,0.0,0.0,1.0,0.0
AppDynamics,29100312,29083565,2,"2015/03/17, 15:27:06",False,"2015/03/17, 15:27:06",26.0,2378795.0,0,By default much of the Apache stuff is excluded.,0.231,0.0,0.769,-0.34
AppDynamics,29100312,29083565,2,"2015/03/17, 15:27:06",False,"2015/03/17, 15:27:06",26.0,2378795.0,0,"Try adding Call Graph Settings (Configure    Instrumentation    Call Graph Settings), to include specific transports, like org.apache.camel.component.file.",0.0,0.143,0.857,0.3612
AppDynamics,29100312,29083565,2,"2015/03/17, 15:27:06",False,"2015/03/17, 15:27:06",26.0,2378795.0,0,* in the Specific sub-packages / classes from the Excluded Packages to be included in call graphs section.,0.138,0.0,0.862,-0.34
AppDynamics,29100312,29083565,2,"2015/03/17, 15:27:06",False,"2015/03/17, 15:27:06",26.0,2378795.0,0,Do not include org.apache.camel.,0.0,0.0,1.0,0.0
AppDynamics,29100312,29083565,2,"2015/03/17, 15:27:06",False,"2015/03/17, 15:27:06",26.0,2378795.0,0,* as it will instrument all the camel code which is very expensive.,0.0,0.0,1.0,0.0
AppDynamics,29100312,29083565,2,"2015/03/17, 15:27:06",False,"2015/03/17, 15:27:06",26.0,2378795.0,0,"You may want to do it at first to detect what you want to watch, but make sure to change it back.",0.0,0.216,0.784,0.5023
AppDynamics,29398487,29083565,1,"2015/04/01, 21:48:07",False,"2015/04/01, 21:48:07",26.0,2378795.0,0,Edit AppServerAgent\conf\app-agent-config.xml:,0.0,0.0,1.0,0.0
AppDynamics,29398487,29083565,1,"2015/04/01, 21:48:07",False,"2015/04/01, 21:48:07",26.0,2378795.0,0,From the Controller web site:,0.0,0.0,1.0,0.0
AppDynamics,29398487,29083565,1,"2015/04/01, 21:48:07",False,"2015/04/01, 21:48:07",26.0,2378795.0,0,"Configure    Instrumentation    Call Graph Settings
Add Always Shown Package/Class: org.apache.camel.",0.0,0.0,1.0,0.0
AppDynamics,29398487,29083565,1,"2015/04/01, 21:48:07",False,"2015/04/01, 21:48:07",26.0,2378795.0,0,*,0.0,0.0,0.0,0.0
AppDynamics,29398487,29083565,1,"2015/04/01, 21:48:07",False,"2015/04/01, 21:48:07",26.0,2378795.0,0,"Servers    App Servers    {tiername}    {nodename}    Agents
App Server Agent
Configure
Use Custom Configuration
find-entry-points: true",0.0,0.167,0.833,0.4215
AppDynamics,29370213,29346979,0,"2015/03/31, 16:51:25",True,"2015/03/31, 16:51:25",602.0,3453371.0,0,Can I ask if your trial is beyond 15 days?,0.0,0.0,1.0,0.0
AppDynamics,29370213,29346979,0,"2015/03/31, 16:51:25",True,"2015/03/31, 16:51:25",602.0,3453371.0,0,According to their site the trial only includes monitoring for one agent beyond that time - that would explain why only your first agent delivers data.,0.0,0.0,1.0,0.0
AppDynamics,29370213,29346979,0,"2015/03/31, 16:51:25",True,"2015/03/31, 16:51:25",602.0,3453371.0,0,"Other tools in that space, e.g: Dynatrace free trial - gives you a bit more time if that's what you need and also allow you to extend the free trial if you need more time",0.0,0.26,0.74,0.8481
AppDynamics,30553280,29346979,0,"2015/05/31, 05:34:32",False,"2015/05/31, 05:34:32",474.0,4682632.0,0,You have gone beyond the 15 day pro trial and you are using the free product.,0.0,0.18,0.82,0.5106
AppDynamics,30553280,29346979,0,"2015/05/31, 05:34:32",False,"2015/05/31, 05:34:32",474.0,4682632.0,0,"If you want to buy the product you can get an extension on the trial, if you do not want to buy the product it will not trace across JVMs.",0.0,0.085,0.915,0.1531
AppDynamics,16543274,14993309,2,"2013/05/14, 15:33:59",True,"2013/05/14, 15:33:59",1970.0,131903.0,15,"Not sure if you already have resolved this, but: SNI is SQL Server Network Interface, and the mentioned method exists in most ADO.NET full call stacks that wait for data from SQL Server.",0.044,0.04,0.916,-0.0338
AppDynamics,16543274,14993309,2,"2013/05/14, 15:33:59",True,"2013/05/14, 15:33:59",1970.0,131903.0,15,"This is regardless of whether the higher-level implementation is EF, raw ADO.NET or whatever.",0.0,0.0,1.0,0.0
AppDynamics,16543274,14993309,2,"2013/05/14, 15:33:59",True,"2013/05/14, 15:33:59",1970.0,131903.0,15,"I'm not sure which metric or signal AppDynamics uses to capture the completion of a stored procedure execution, but you could be seeing this kind of behavior if your stored procedure completes relatively fast, but transmitting the query result from the server to your client takes a while.",0.032,0.0,0.968,-0.1232
AppDynamics,16543274,14993309,2,"2013/05/14, 15:33:59",True,"2013/05/14, 15:33:59",1970.0,131903.0,15,"Without knowing more about your infrastructure, it is very hard to help further.",0.108,0.189,0.703,0.3117
AppDynamics,16543274,14993309,2,"2013/05/14, 15:33:59",True,"2013/05/14, 15:33:59",1970.0,131903.0,15,"If the problem still persists, I would recommend running the same query in SQL Server Management studio with SET STATISTICS TIME ON and ""Include Client Statistics"" switched to on.",0.087,0.08,0.833,-0.0516
AppDynamics,16543274,14993309,2,"2013/05/14, 15:33:59",True,"2013/05/14, 15:33:59",1970.0,131903.0,15,Perhaps those numbers would give you an idea on whether data transfer is actually the problem.,0.153,0.0,0.847,-0.4019
AppDynamics,42025178,14993309,0,"2017/02/03, 15:12:58",False,"2017/02/03, 15:12:58",2131.0,585819.0,1,"In my case it was indeed, as Jouni mentions, very slow transmitting of the query results.",0.0,0.0,1.0,0.0
AppDynamics,42025178,14993309,0,"2017/02/03, 15:12:58",False,"2017/02/03, 15:12:58",2131.0,585819.0,1,I use Automapper to prepare data for sending to client.,0.0,0.0,1.0,0.0
AppDynamics,42025178,14993309,0,"2017/02/03, 15:12:58",False,"2017/02/03, 15:12:58",2131.0,585819.0,1,"So, it's unclear what exact property caused the load but to be sure I've cut all compound ones I don't need to show on client side.",0.147,0.101,0.752,-0.0872
AppDynamics,42025178,14993309,0,"2017/02/03, 15:12:58",False,"2017/02/03, 15:12:58",2131.0,585819.0,1,(I originally needed a collection to show in grid on client side.),0.0,0.0,1.0,0.0
AppDynamics,42025178,14993309,0,"2017/02/03, 15:12:58",False,"2017/02/03, 15:12:58",2131.0,585819.0,1,The execution became very fast.,0.0,0.0,1.0,0.0
AppDynamics,49904327,14993309,0,"2018/04/18, 19:08:53",False,"2018/04/18, 19:08:53",1707.0,2308106.0,0,I've came across similar issue - it turns out that SqlDataReader.Dispose can get stuck for very lengthy time if you break early from large select.,0.08,0.0,0.92,-0.25
AppDynamics,49904327,14993309,0,"2018/04/18, 19:08:53",False,"2018/04/18, 19:08:53",1707.0,2308106.0,0,see:  https://github.com/dotnet/corefx/issues/29181,0.0,0.0,1.0,0.0
AppDynamics,14983064,14330329,3,"2013/02/20, 17:05:14",False,"2016/08/20, 12:41:24",424.0,2091508.0,7,"please see my reply in 
 G1GC long pause with initial-mark",0.0,0.204,0.796,0.3182
AppDynamics,14983064,14330329,3,"2013/02/20, 17:05:14",False,"2016/08/20, 12:41:24",424.0,2091508.0,7,your every setting should has a solid reason to be here...and unfortunately some of them don't have e.g.,0.126,0.084,0.789,-0.2023
AppDynamics,14983064,14330329,3,"2013/02/20, 17:05:14",False,"2016/08/20, 12:41:24",424.0,2091508.0,7,"-XX:+UseBiasedLocking (used for combination of tenured and young generation GCs, but G1GC is capable to handle both) -XX:+DisableExplicitGC (its unpredictable, in my experience its never restrict explicit gc calls)",0.0,0.186,0.814,0.7332
AppDynamics,14983064,14330329,3,"2013/02/20, 17:05:14",False,"2016/08/20, 12:41:24",424.0,2091508.0,7,"please use/tweak accordingly below mentioned settings to get optimum results, I'm giving you baseline to move forward, hope so these settings will work for you:
 Java G1 garbage collection in production",0.0,0.284,0.716,0.8668
AppDynamics,16782833,14330329,1,"2013/05/28, 05:08:52",True,"2013/05/28, 05:08:52",1275.0,1335717.0,5,"We recorded this bug on 1.7._u06 and upgraded to  1.7.0_21-b11  just a couple of days ago and we haven't seen any Full GC's since upgrade, so it seems like this bug was fixed in JVM.",0.0,0.077,0.923,0.4144
AppDynamics,16782833,14330329,1,"2013/05/28, 05:08:52",True,"2013/05/28, 05:08:52",1275.0,1335717.0,5,The code cache memory profiles look much nicer now too in the profiler.,0.0,0.195,0.805,0.4404
AppDynamics,16782833,14330329,1,"2013/05/28, 05:08:52",True,"2013/05/28, 05:08:52",1275.0,1335717.0,5,"In the past, this problem used to be a daily one, one to more times per day.",0.172,0.0,0.828,-0.481
AppDynamics,16782833,14330329,1,"2013/05/28, 05:08:52",True,"2013/05/28, 05:08:52",1275.0,1335717.0,5,"If the situation changes, I will report back.",0.0,0.0,1.0,0.0
AppDynamics,16782833,14330329,1,"2013/05/28, 05:08:52",True,"2013/05/28, 05:08:52",1275.0,1335717.0,5,"Until then, I consider this problem solved with the upgrade.",0.256,0.172,0.573,-0.2558
AppDynamics,28458957,28436858,1,"2015/02/11, 18:07:23",False,"2015/02/11, 18:13:31",49324.0,168493.0,0,"Your query profile shows that the ""Query end"" time is very large.",0.0,0.0,1.0,0.0
AppDynamics,28458957,28436858,1,"2015/02/11, 18:07:23",False,"2015/02/11, 18:13:31",49324.0,168493.0,0,This may be caused by a very (too) large  query cache .,0.0,0.0,1.0,0.0
AppDynamics,28458957,28436858,1,"2015/02/11, 18:07:23",False,"2015/02/11, 18:13:31",49324.0,168493.0,0,"Every time you perform an update statement (INSERT, DELETE, UPDATE), the query cache must be updated (every query that reads from the updated tables is invalidated).",0.0,0.0,1.0,0.0
AppDynamics,28481902,28436858,1,"2015/02/12, 17:54:03",True,"2018/08/13, 11:49:47",525.0,3132445.0,20,I got in touch with RDS engineers from amazon and they gave me the solution.,0.0,0.25,0.75,0.4588
AppDynamics,28481902,28436858,1,"2015/02/12, 17:54:03",True,"2018/08/13, 11:49:47",525.0,3132445.0,20,Such a high latency was due to a very low performing storage type.,0.193,0.0,0.807,-0.3384
AppDynamics,28481902,28436858,1,"2015/02/12, 17:54:03",True,"2018/08/13, 11:49:47",525.0,3132445.0,20,"Indeed, I was using the default 5GB SSD (which they call GP2) which gives 3 IOPS per GB of storage, resulting in 15 IOPS when my application required about 50 IOPS or even more.",0.0,0.0,1.0,0.0
AppDynamics,28481902,28436858,1,"2015/02/12, 17:54:03",True,"2018/08/13, 11:49:47",525.0,3132445.0,20,"Therefore, they suggested me to change the storage type to  Magnetic  which provides 100 IOPS as baseline.",0.0,0.0,1.0,0.0
AppDynamics,28481902,28436858,1,"2015/02/12, 17:54:03",True,"2018/08/13, 11:49:47",525.0,3132445.0,20,"Moreover, I've also been able to decrease the instance type because the bottleneck was only the disk.",0.0,0.0,1.0,0.0
AppDynamics,28481902,28436858,1,"2015/02/12, 17:54:03",True,"2018/08/13, 11:49:47",525.0,3132445.0,20,The migration took about 3h due to the very low performance of the source disk (GP2).,0.138,0.0,0.862,-0.3384
AppDynamics,28481902,28436858,1,"2015/02/12, 17:54:03",True,"2018/08/13, 11:49:47",525.0,3132445.0,20,Hope it may help someone out there!,0.0,0.541,0.459,0.7088
AppDynamics,49953075,35971200,0,"2018/04/21, 09:34:37",True,"2018/04/21, 09:34:37",639.0,812093.0,0,We never were able to properly solve the issue but at some point it vanished.,0.0,0.091,0.909,0.1027
AppDynamics,49953075,35971200,0,"2018/04/21, 09:34:37",True,"2018/04/21, 09:34:37",639.0,812093.0,0,If I'm not wrong one the client change the appdynamics configuration / removed it which seemed to have solved the issue.,0.0,0.205,0.795,0.5653
AppDynamics,28937115,24261110,0,"2015/03/09, 09:38:07",False,"2015/03/09, 09:38:07",10216.0,854207.0,0,did you take a look to this google group ticket issue?,0.0,0.0,1.0,0.0
AppDynamics,28937115,24261110,0,"2015/03/09, 09:38:07",False,"2015/03/09, 09:38:07",10216.0,854207.0,0,https://groups.google.com/forum/#!topic/hazelcast/ivk6hzk2YwA,0.0,0.0,1.0,0.0
AppDynamics,28937115,24261110,0,"2015/03/09, 09:38:07",False,"2015/03/09, 09:38:07",10216.0,854207.0,0,Here in particular looks the reason of the issue.,0.0,0.0,1.0,0.0
AppDynamics,28937115,24261110,0,"2015/03/09, 09:38:07",False,"2015/03/09, 09:38:07",10216.0,854207.0,0,https://github.com/hazelcast/hazelcast/issues/553,0.0,0.0,1.0,0.0
AppDynamics,32718584,32717415,1,"2015/09/22, 16:44:26",False,"2015/09/22, 16:44:26",,,0,Have you tried to Minify your code?,0.0,0.0,1.0,0.0
AppDynamics,32718584,32717415,1,"2015/09/22, 16:44:26",False,"2015/09/22, 16:44:26",,,0,Minifying unneccesary characters from your code without removing any functionality.,0.0,0.0,1.0,0.0
AppDynamics,32718584,32717415,1,"2015/09/22, 16:44:26",False,"2015/09/22, 16:44:26",,,0,This could help speed up the download times.,0.0,0.278,0.722,0.4019
AppDynamics,32718584,32717415,1,"2015/09/22, 16:44:26",False,"2015/09/22, 16:44:26",,,0,Take a look at the following link:  http://www.htmlgoodies.com/beyond/reference/7-tips-to-make-your-websites-load-faster.html,0.0,0.0,1.0,0.0
AppDynamics,32719029,32717415,0,"2015/09/22, 17:02:56",False,"2015/09/22, 17:18:53",55753.0,761202.0,5,Ill take a look at serving assets from webroot,0.267,0.162,0.571,-0.2732
AppDynamics,32719029,32717415,0,"2015/09/22, 17:02:56",False,"2015/09/22, 17:18:53",55753.0,761202.0,5,From the book  (emphasis added):,0.0,0.0,1.0,0.0
AppDynamics,32719029,32717415,0,"2015/09/22, 17:02:56",False,"2015/09/22, 17:18:53",55753.0,761202.0,5,It’s a well known fact that  serving assets through PHP is guaranteed to be slower than serving those assets without invoking PHP .,0.0,0.234,0.766,0.5423
AppDynamics,32719029,32717415,0,"2015/09/22, 17:02:56",False,"2015/09/22, 17:18:53",55753.0,761202.0,5,"And while the core team has taken steps to make plugin and theme asset serving as fast as possible, there may be situations where more performance is required.",0.0,0.085,0.915,0.3612
AppDynamics,32719029,32717415,0,"2015/09/22, 17:02:56",False,"2015/09/22, 17:18:53",55753.0,761202.0,5,In these situations it’s recommended that you either symlink or copy out plugin/theme assets to directories in app/webroot with paths matching those used by CakePHP.,0.0,0.132,0.868,0.3612
AppDynamics,32719029,32717415,0,"2015/09/22, 17:02:56",False,"2015/09/22, 17:18:53",55753.0,761202.0,5,"Depending on many factors, &quot;slower&quot; can be anywhere between barely-noticeable to barely-usable.",0.0,0.0,1.0,0.0
AppDynamics,32719029,32717415,0,"2015/09/22, 17:02:56",False,"2015/09/22, 17:18:53",55753.0,761202.0,5,"This advice is not version specific, and pretty much always applies.",0.0,0.242,0.758,0.4939
AppDynamics,32719029,32717415,0,"2015/09/22, 17:02:56",False,"2015/09/22, 17:18:53",55753.0,761202.0,5,"To make assets load faster, let the webserver take care of them for you.",0.0,0.29,0.71,0.5994
AppDynamics,21035147,20975043,0,"2014/01/10, 04:24:32",False,"2016/07/23, 21:57:52",1025.0,1777388.0,0,"Yes, it is possible for the behaviour of GCs to change over time due to JIT optimisation.",0.0,0.261,0.739,0.6486
AppDynamics,21035147,20975043,0,"2014/01/10, 04:24:32",False,"2016/07/23, 21:57:52",1025.0,1777388.0,0,One example is 'Escape Analysis' which has been enabled by default since Java 6u23.,0.0,0.116,0.884,0.1779
AppDynamics,21035147,20975043,0,"2014/01/10, 04:24:32",False,"2016/07/23, 21:57:52",1025.0,1777388.0,0,This type of optimisation can prevent some objects from being created in the heap and therefore not require garbage collection at all.,0.0,0.231,0.769,0.5719
AppDynamics,21035147,20975043,0,"2014/01/10, 04:24:32",False,"2016/07/23, 21:57:52",1025.0,1777388.0,0,For more information see  Java 7's HotSpot Performance Enhancements .,0.0,0.0,1.0,0.0
AppDynamics,16108561,16090702,0,"2013/04/19, 18:49:57",False,"2013/04/19, 18:49:57",625.0,2298085.0,1,It is really difficult to find the exact cause of your problem without more information.,0.297,0.0,0.703,-0.6697
AppDynamics,16108561,16090702,0,"2013/04/19, 18:49:57",False,"2013/04/19, 18:49:57",625.0,2298085.0,1,"But I can try to answer to your question : 
 Can the OS block the garbage collection ?",0.216,0.0,0.784,-0.5927
AppDynamics,16108561,16090702,0,"2013/04/19, 18:49:57",False,"2013/04/19, 18:49:57",625.0,2298085.0,1,It is very unlikely than your OS blocks the thread garbage collector and let the other threads run.,0.101,0.0,0.899,-0.2263
AppDynamics,16108561,16090702,0,"2013/04/19, 18:49:57",False,"2013/04/19, 18:49:57",625.0,2298085.0,1,You should not investigate that way.,0.0,0.0,1.0,0.0
AppDynamics,16108561,16090702,0,"2013/04/19, 18:49:57",False,"2013/04/19, 18:49:57",625.0,2298085.0,1,Can the OS block the JVM ?,0.367,0.0,0.633,-0.4404
AppDynamics,16108561,16090702,0,"2013/04/19, 18:49:57",False,"2013/04/19, 18:49:57",625.0,2298085.0,1,"Yes it perflecty can and it do it a lot, but so fast than you think that the processes are all running at the same time.",0.0,0.072,0.928,0.2144
AppDynamics,16108561,16090702,0,"2013/04/19, 18:49:57",False,"2013/04/19, 18:49:57",625.0,2298085.0,1,jvm is a process like the other and his under the control of the OS.,0.0,0.161,0.839,0.3612
AppDynamics,16108561,16090702,0,"2013/04/19, 18:49:57",False,"2013/04/19, 18:49:57",625.0,2298085.0,1,You have to check the cpu used by the application when it hangs (with monitoring on the server not in the jvm).,0.0,0.0,1.0,0.0
AppDynamics,16108561,16090702,0,"2013/04/19, 18:49:57",False,"2013/04/19, 18:49:57",625.0,2298085.0,1,If it is very low then I see 2 causes (but there are more) :,0.179,0.0,0.821,-0.3384
AppDynamics,16108770,16090702,0,"2013/04/19, 18:59:28",False,"2013/04/19, 18:59:28",243.0,2096203.0,0,"In theory, YES, it can.",0.0,0.462,0.538,0.5319
AppDynamics,16108770,16090702,0,"2013/04/19, 18:59:28",False,"2013/04/19, 18:59:28",243.0,2096203.0,0,"But it practice, it never should.",0.0,0.0,1.0,0.0
AppDynamics,16108770,16090702,0,"2013/04/19, 18:59:28",False,"2013/04/19, 18:59:28",243.0,2096203.0,0,"In most Java virtual machines, application threads are not the only threads that are running.",0.0,0.0,1.0,0.0
AppDynamics,16108770,16090702,0,"2013/04/19, 18:59:28",False,"2013/04/19, 18:59:28",243.0,2096203.0,0,"Apart from the application threads, there are compilation threads, finalizer threads, garbage collection threads, and some more.",0.0,0.0,1.0,0.0
AppDynamics,16108770,16090702,0,"2013/04/19, 18:59:28",False,"2013/04/19, 18:59:28",243.0,2096203.0,0,"Scheduling decisions for allocating CPU cores to these threads and other threads from other programs running on the machine are based on many parameters (thread priorities, their last execution time, etc), which try be fair to all threads.",0.0,0.059,0.941,0.3182
AppDynamics,16108770,16090702,0,"2013/04/19, 18:59:28",False,"2013/04/19, 18:59:28",243.0,2096203.0,0,"So, in practice no thread in the system should be waiting for CPU allocation for an unreasonably long time and the operating system should not block any thread for an unlimited amount of time.",0.067,0.065,0.868,-0.0149
AppDynamics,16108770,16090702,0,"2013/04/19, 18:59:28",False,"2013/04/19, 18:59:28",243.0,2096203.0,0,There is minimal activity that the garbage collection threads (and other VM threads) need to do.,0.0,0.0,1.0,0.0
AppDynamics,16108770,16090702,0,"2013/04/19, 18:59:28",False,"2013/04/19, 18:59:28",243.0,2096203.0,0,They need to check periodically to see if a garbage collection is needed.,0.0,0.0,1.0,0.0
AppDynamics,16108770,16090702,0,"2013/04/19, 18:59:28",False,"2013/04/19, 18:59:28",243.0,2096203.0,0,"Even if the application threads are all suspended, there could be other VM threads, such as the JIT compiler thread or the finalizer thread, that do work and ,hence, allocate objects and trigger garbage collection.",0.084,0.0,0.916,-0.4767
AppDynamics,16108770,16090702,0,"2013/04/19, 18:59:28",False,"2013/04/19, 18:59:28",243.0,2096203.0,0,This is particularly true for meta-circular JVM that implement VM threads in Java and not in a C/C++;,0.0,0.162,0.838,0.4754
AppDynamics,16108770,16090702,0,"2013/04/19, 18:59:28",False,"2013/04/19, 18:59:28",243.0,2096203.0,0,"Moreover, most modern JVM use a generational garbage collector (A garbage collector that partitions the heap into separate spaces and puts objects with different ages in different parts of the heap) This means as objects get older and older, they need to be moved to other older spaces.",0.0,0.0,1.0,0.0
AppDynamics,16108770,16090702,0,"2013/04/19, 18:59:28",False,"2013/04/19, 18:59:28",243.0,2096203.0,0,"Hence, even if there is no need to collect objects, a generational garbage collector may move objects from one space to another.",0.099,0.0,0.901,-0.296
AppDynamics,16108770,16090702,0,"2013/04/19, 18:59:28",False,"2013/04/19, 18:59:28",243.0,2096203.0,0,Of course the details of each garbage collector in different from JVM to JVM.,0.0,0.0,1.0,0.0
AppDynamics,16108770,16090702,0,"2013/04/19, 18:59:28",False,"2013/04/19, 18:59:28",243.0,2096203.0,0,"To put more salt on the injury, some JVMs support more than one type of garbage collector.",0.137,0.132,0.732,-0.0258
AppDynamics,16108770,16090702,0,"2013/04/19, 18:59:28",False,"2013/04/19, 18:59:28",243.0,2096203.0,0,But seeing a minimal garbage collection activity in an idle application is no surprise.,0.17,0.161,0.669,-0.0387
AppDynamics,16108782,16090702,3,"2013/04/19, 19:00:01",True,"2013/04/19, 19:32:49",59821.0,12943.0,1,Does your OS have swapping enabled.,0.0,0.0,1.0,0.0
AppDynamics,16108782,16090702,3,"2013/04/19, 19:00:01",True,"2013/04/19, 19:32:49",59821.0,12943.0,1,"I've noticed HUGE problems with Java once it fills up all the ram on an OS with swapping enabled--it will actually devistate windows systems, effictevly locking them up and causing a reboot.",0.078,0.087,0.835,0.0857
AppDynamics,16108782,16090702,3,"2013/04/19, 19:00:01",True,"2013/04/19, 19:32:49",59821.0,12943.0,1,My theory is this:,0.0,0.0,1.0,0.0
AppDynamics,16108782,16090702,3,"2013/04/19, 19:00:01",True,"2013/04/19, 19:32:49",59821.0,12943.0,1,"At first it doesn't effect the system much, but if you try to launch an app that wants a bunch of memory it can take a really long time, and your system just keeps degrading.",0.14,0.0,0.86,-0.7351
AppDynamics,16108782,16090702,3,"2013/04/19, 19:00:01",True,"2013/04/19, 19:32:49",59821.0,12943.0,1,"Multiple large VMs can make this worse, I run 3 or 4 huge ones and my system now starts to sieze when I get over 60-70% RAM usage.",0.13,0.082,0.788,-0.3237
AppDynamics,16108782,16090702,3,"2013/04/19, 19:00:01",True,"2013/04/19, 19:32:49",59821.0,12943.0,1,This is conjecture but it describes the behavior I've seen after days of testing.,0.0,0.0,1.0,0.0
AppDynamics,16108782,16090702,3,"2013/04/19, 19:00:01",True,"2013/04/19, 19:32:49",59821.0,12943.0,1,"The effect is that all the swapping seems to ""Prevent"" gc.",0.0,0.0,1.0,0.0
AppDynamics,16108782,16090702,3,"2013/04/19, 19:00:01",True,"2013/04/19, 19:32:49",59821.0,12943.0,1,More accurately the OS is spending most of the GC time swapping which makes it look like it's hanging doing nothing during GC.,0.0,0.102,0.898,0.3612
AppDynamics,16108782,16090702,3,"2013/04/19, 19:00:01",True,"2013/04/19, 19:32:49",59821.0,12943.0,1,"A fix--set -Xmx to a lower value, drop it until you allow enough room to avoid swapping.",0.313,0.207,0.481,-0.296
AppDynamics,16108782,16090702,3,"2013/04/19, 19:00:01",True,"2013/04/19, 19:32:49",59821.0,12943.0,1,"This has always fixed my problem, if it doesn't fix yours then I'm wrong about the cause of your problem :)",0.298,0.105,0.596,-0.6705
AppDynamics,18276354,18276208,5,"2013/08/16, 18:00:12",True,"2013/08/16, 18:39:33",177788.0,1103872.0,3,One major GC per minute doesn't at all seem troublesome.,0.268,0.0,0.732,-0.5106
AppDynamics,18276354,18276208,5,"2013/08/16, 18:00:12",True,"2013/08/16, 18:39:33",177788.0,1103872.0,3,"Usually it takes about half a second, so that's 1/120th of your overall CPU usage.",0.0,0.0,1.0,0.0
AppDynamics,18276354,18276208,5,"2013/08/16, 18:00:12",True,"2013/08/16, 18:39:33",177788.0,1103872.0,3,It is also quite natural that heavy application load results in more memory allocation.,0.0,0.177,0.823,0.4201
AppDynamics,18276354,18276208,5,"2013/08/16, 18:00:12",True,"2013/08/16, 18:39:33",177788.0,1103872.0,3,Apparently you are allocating some objects that live on for a while (could be caching).,0.0,0.0,1.0,0.0
AppDynamics,18276354,18276208,5,"2013/08/16, 18:00:12",True,"2013/08/16, 18:39:33",177788.0,1103872.0,3,My conclusion: the demonstrated GC behavior is not a proof that there is something wrong with your application's memory allocation.,0.147,0.0,0.853,-0.4767
AppDynamics,18276354,18276208,5,"2013/08/16, 18:00:12",True,"2013/08/16, 18:39:33",177788.0,1103872.0,3,I have looked more carefully at your diagrams (unfortunately they are quite difficult to read).,0.168,0.108,0.724,-0.25
AppDynamics,18276354,18276208,5,"2013/08/16, 18:00:12",True,"2013/08/16, 18:39:33",177788.0,1103872.0,3,"You don't have one GC per min; you have 60  seconds  of major GC per minute, which would mean it's happening all the time.",0.0,0.0,1.0,0.0
AppDynamics,18276354,18276208,5,"2013/08/16, 18:00:12",True,"2013/08/16, 18:39:33",177788.0,1103872.0,3,That  does  look like major trouble; in fact in those conditions you usually get an OOME due to &quot;GC time percentage threshold crossed&quot;.,0.103,0.095,0.802,-0.0516
AppDynamics,18276354,18276208,5,"2013/08/16, 18:00:12",True,"2013/08/16, 18:39:33",177788.0,1103872.0,3,Note that the CMS collector which you are using is actually slower than the default one; its advantage is only that it doesn't &quot;stop the world&quot; as much.,0.0,0.069,0.931,0.25
AppDynamics,18276354,18276208,5,"2013/08/16, 18:00:12",True,"2013/08/16, 18:39:33",177788.0,1103872.0,3,It may not be the best choice for you.,0.296,0.0,0.704,-0.5216
AppDynamics,18276354,18276208,5,"2013/08/16, 18:00:12",True,"2013/08/16, 18:39:33",177788.0,1103872.0,3,"But you do look to either have a memory leak, or a general issue in program design.",0.181,0.0,0.819,-0.4767
AppDynamics,18276716,18276208,0,"2013/08/16, 18:16:46",False,"2013/08/16, 18:16:46",470.0,2623382.0,1,Are you always waiting for GC to take care of removing unused references?,0.0,0.211,0.789,0.4939
AppDynamics,18276716,18276208,0,"2013/08/16, 18:16:46",False,"2013/08/16, 18:16:46",470.0,2623382.0,1,"Is there some places in your application that you know a reference to a heavy weight object won't be used anymore from that point, but it is not manually nulled?",0.0,0.0,1.0,0.0
AppDynamics,18276716,18276208,0,"2013/08/16, 18:16:46",False,"2013/08/16, 18:16:46",470.0,2623382.0,1,Perhaps setting such heavy weight objects to null manually at the right places could prevent them growing till they hit the end of the heap....,0.0,0.109,0.891,0.2023
AppDynamics,18277366,18276208,0,"2013/08/16, 18:49:24",False,"2013/08/16, 18:56:19",4061.0,2031799.0,2,"When JVM reaches heap maximum size, GC is called more frequently to prevent OOM exception.",0.0,0.165,0.835,0.144
AppDynamics,18277366,18276208,0,"2013/08/16, 18:49:24",False,"2013/08/16, 18:56:19",4061.0,2031799.0,2,Normal program execution should not happen at such circumstances.,0.0,0.0,1.0,0.0
AppDynamics,18277366,18276208,0,"2013/08/16, 18:49:24",False,"2013/08/16, 18:56:19",4061.0,2031799.0,2,"When a new object is allocated and JVM cant get enough of free space, GC is called.",0.0,0.18,0.82,0.5106
AppDynamics,18277366,18276208,0,"2013/08/16, 18:49:24",False,"2013/08/16, 18:56:19",4061.0,2031799.0,2,This might postpone object allocation process and thus slowdown overall performance.,0.16,0.0,0.84,-0.2263
AppDynamics,18277366,18276208,0,"2013/08/16, 18:49:24",False,"2013/08/16, 18:56:19",4061.0,2031799.0,2,Garbage collection happens not concurrently in this case and you do not benefit from CMS collector.,0.142,0.0,0.858,-0.357
AppDynamics,18277366,18276208,0,"2013/08/16, 18:49:24",False,"2013/08/16, 18:56:19",4061.0,2031799.0,2,"Try to play with  CMSInitiatingOccupancyFraction , its default value is about 90%.",0.0,0.348,0.652,0.5859
AppDynamics,18277366,18276208,0,"2013/08/16, 18:49:24",False,"2013/08/16, 18:56:19",4061.0,2031799.0,2,"Setting this parameter to lower values, will force garbage collection before application reaches heap maximum.",0.122,0.215,0.663,0.1779
AppDynamics,18277366,18276208,0,"2013/08/16, 18:49:24",False,"2013/08/16, 18:56:19",4061.0,2031799.0,2,Thus GC will work in parallel with application not clashing with it.,0.0,0.0,1.0,0.0
AppDynamics,18277366,18276208,0,"2013/08/16, 18:49:24",False,"2013/08/16, 18:56:19",4061.0,2031799.0,2,Have a look at article  Starting a Concurrent Collection Cycle .,0.0,0.0,1.0,0.0
AppDynamics,53270016,53269696,2,"2018/11/12, 22:59:13",True,"2018/11/12, 23:07:57",64495.0,413337.0,2,"Looking at the  source code  of  UTF8JsonGenerator._flushBuffer() , there is no indication of  LockSupport.parkNanos() .",0.155,0.0,0.845,-0.296
AppDynamics,53270016,53269696,2,"2018/11/12, 22:59:13",True,"2018/11/12, 23:07:57",64495.0,413337.0,2,So it has probably been inlined by the JIT compiler from  OutputStream.write() .,0.0,0.0,1.0,0.0
AppDynamics,53270016,53269696,2,"2018/11/12, 22:59:13",True,"2018/11/12, 23:07:57",64495.0,413337.0,2,My guess is it's the place where – for your application – Tomcat typically waits until the client has accepted all the output (expect for the last piece that fits into the typical connection buffer size) before it can close the connection.,0.0,0.051,0.949,0.2732
AppDynamics,53270016,53269696,2,"2018/11/12, 22:59:13",True,"2018/11/12, 23:07:57",64495.0,413337.0,2,We have had bad experience with slow clients in the past.,0.259,0.0,0.741,-0.5423
AppDynamics,53270016,53269696,2,"2018/11/12, 22:59:13",True,"2018/11/12, 23:07:57",64495.0,413337.0,2,"Until they have retrieved all the output, they block a thread in Tomcat.",0.209,0.0,0.791,-0.4404
AppDynamics,53270016,53269696,2,"2018/11/12, 22:59:13",True,"2018/11/12, 23:07:57",64495.0,413337.0,2,And blocking a few dozens threads in Tomcat seriously reduces the throughput of a busy web app.,0.249,0.0,0.751,-0.5106
AppDynamics,53270016,53269696,2,"2018/11/12, 22:59:13",True,"2018/11/12, 23:07:57",64495.0,413337.0,2,Increasing the number of threads isn't the best option as the blocked threads also occupy a considerable amount of memory.,0.24,0.057,0.703,-0.6331
AppDynamics,53270016,53269696,2,"2018/11/12, 22:59:13",True,"2018/11/12, 23:07:57",64495.0,413337.0,2,So what you want is that Tomcat can handle a request as quickly as possible and then move on to the next request.,0.0,0.069,0.931,0.144
AppDynamics,53270016,53269696,2,"2018/11/12, 22:59:13",True,"2018/11/12, 23:07:57",64495.0,413337.0,2,"We have solved the problem by configuring our reverse proxy, which we always had in front of Tomcat, to immediately consume all output from Tomcat and deliver it to the client at the client's speed.",0.071,0.056,0.873,-0.1531
AppDynamics,53270016,53269696,2,"2018/11/12, 22:59:13",True,"2018/11/12, 23:07:57",64495.0,413337.0,2,The reverse proxy is very efficient at handling slow clients.,0.0,0.256,0.744,0.4754
AppDynamics,53270016,53269696,2,"2018/11/12, 22:59:13",True,"2018/11/12, 23:07:57",64495.0,413337.0,2,"In our case, we have used  nginx .",0.0,0.0,1.0,0.0
AppDynamics,53270016,53269696,2,"2018/11/12, 22:59:13",True,"2018/11/12, 23:07:57",64495.0,413337.0,2,We also looked at  Apache httpd .,0.0,0.0,1.0,0.0
AppDynamics,53270016,53269696,2,"2018/11/12, 22:59:13",True,"2018/11/12, 23:07:57",64495.0,413337.0,2,"But at the time, it wasn't capable of doing it.",0.236,0.0,0.764,-0.4168
AppDynamics,53270016,53269696,2,"2018/11/12, 22:59:13",True,"2018/11/12, 23:07:57",64495.0,413337.0,2,Additional Note,0.0,0.0,1.0,0.0
AppDynamics,53270016,53269696,2,"2018/11/12, 22:59:13",True,"2018/11/12, 23:07:57",64495.0,413337.0,2,Clients that unexpectedly disconnect also look like slow clients to the server as it takes some time until it has been fully established that the connection is broken.,0.098,0.079,0.823,-0.1531
AppDynamics,7413670,7409112,7,"2011/09/14, 11:52:55",False,"2011/09/15, 05:34:03",706.0,795321.0,1,"You could use Javamelody, but you have to:",0.0,0.0,1.0,0.0
AppDynamics,7413670,7409112,7,"2011/09/14, 11:52:55",False,"2011/09/15, 05:34:03",706.0,795321.0,1,Generate war file from your play framework web,0.317,0.195,0.488,-0.3612
AppDynamics,7413670,7409112,7,"2011/09/14, 11:52:55",False,"2011/09/15, 05:34:03",706.0,795321.0,1,Edit web.xml in war file (http://code.google.com/p/javamelody/wiki/UserGuide?tm=6),0.438,0.0,0.562,-0.5994
AppDynamics,60742827,60741008,0,"2020/03/18, 17:37:23",False,"2020/03/18, 17:37:23",6820.0,1709216.0,7,You need to set Multi-Release to true in the jar's MANIFEST.MF.,0.0,0.219,0.781,0.4215
AppDynamics,60742827,60741008,0,"2020/03/18, 17:37:23",False,"2020/03/18, 17:37:23",6820.0,1709216.0,7,In the assembly plugin you should be able to do that by adding,0.0,0.0,1.0,0.0
AppDynamics,60742827,60741008,0,"2020/03/18, 17:37:23",False,"2020/03/18, 17:37:23",6820.0,1709216.0,7,to the configuration section of your assembly configuration.,0.0,0.0,1.0,0.0
AppDynamics,60742827,60741008,0,"2020/03/18, 17:37:23",False,"2020/03/18, 17:37:23",6820.0,1709216.0,7,You could also use the jar plugin to create your jar.,0.0,0.174,0.826,0.2732
AppDynamics,60742827,60741008,0,"2020/03/18, 17:37:23",False,"2020/03/18, 17:37:23",6820.0,1709216.0,7,For that you would do,0.0,0.0,1.0,0.0
AppDynamics,43779475,43771883,0,"2017/05/04, 12:49:36",False,"2017/05/04, 12:49:36",383.0,2305216.0,2,Use Stackify Prefix for this kind of things:,0.0,0.0,1.0,0.0
AppDynamics,43779475,43771883,0,"2017/05/04, 12:49:36",False,"2017/05/04, 12:49:36",383.0,2305216.0,2,https://stackify.com/prefix/,0.0,0.0,1.0,0.0
AppDynamics,43779475,43771883,0,"2017/05/04, 12:49:36",False,"2017/05/04, 12:49:36",383.0,2305216.0,2,"View SQL queries: Including SQL parameters, affected records and how long it took to download the result set.",0.086,0.0,0.914,-0.1531
AppDynamics,22447659,19772523,0,"2014/03/17, 07:40:09",True,"2014/03/17, 07:40:09",391.0,1801817.0,2,"use tracer plugin where u can import the information in csv/html/xml.This is for jvisualvm
 http://visualvm.java.net/plugins.html",0.0,0.0,1.0,0.0
AppDynamics,6588384,6588296,1,"2011/07/05, 23:19:08",True,"2011/07/05, 23:19:08",37594.0,192444.0,3,Start the application and run jconsole on the PID.,0.0,0.0,1.0,0.0
AppDynamics,6588384,6588296,1,"2011/07/05, 23:19:08",True,"2011/07/05, 23:19:08",37594.0,192444.0,3,While its running look at the heap in the console.,0.0,0.0,1.0,0.0
AppDynamics,6588384,6588296,1,"2011/07/05, 23:19:08",True,"2011/07/05, 23:19:08",37594.0,192444.0,3,When it near maxes get a heap dump.,0.302,0.0,0.698,-0.3818
AppDynamics,6588384,6588296,1,"2011/07/05, 23:19:08",True,"2011/07/05, 23:19:08",37594.0,192444.0,3,Download Eclipse MAT and parse the heap dump.,0.271,0.0,0.729,-0.3818
AppDynamics,6588384,6588296,1,"2011/07/05, 23:19:08",True,"2011/07/05, 23:19:08",37594.0,192444.0,3,If you notice the retained heap size is vastly less then the actual binary file parse the heap dump with -keep_unreachable_objects being set.,0.105,0.045,0.85,-0.3612
AppDynamics,6588384,6588296,1,"2011/07/05, 23:19:08",True,"2011/07/05, 23:19:08",37594.0,192444.0,3,If the latter is true and you are doing a full GC often you probably have some kind of leak going on.,0.11,0.114,0.776,0.0276
AppDynamics,6588384,6588296,1,"2011/07/05, 23:19:08",True,"2011/07/05, 23:19:08",37594.0,192444.0,3,"Keep in mind when I say leak I don't mean a leak where the GC cannot retain memory, rather some how you are building large objects and making them unreachable often enough to cause the GC to consume a lot of CPU time.",0.115,0.0,0.885,-0.5859
AppDynamics,6588384,6588296,1,"2011/07/05, 23:19:08",True,"2011/07/05, 23:19:08",37594.0,192444.0,3,If you were seeing true memory leaks you would see GC Over head reached errors,0.129,0.226,0.645,0.2023
AppDynamics,52681154,52680807,5,"2018/10/06, 19:38:02",False,"2018/10/08, 13:21:09",465.0,6645806.0,4,You can get  POD_NAME  and  POD_NAMESPACE  passing them as environment variables via  fieldRef .,0.0,0.0,1.0,0.0
AppDynamics,52681154,52680807,5,"2018/10/06, 19:38:02",False,"2018/10/08, 13:21:09",465.0,6645806.0,4,EDIT :  Added example env  REFERENCE_EXAMPLE  to show how to reference variables.,0.0,0.0,1.0,0.0
AppDynamics,52681154,52680807,5,"2018/10/06, 19:38:02",False,"2018/10/08, 13:21:09",465.0,6645806.0,4,Thanks to  this  answer for pointing out the  $()  interpolation.,0.0,0.244,0.756,0.4404
AppDynamics,52681154,52680807,5,"2018/10/06, 19:38:02",False,"2018/10/08, 13:21:09",465.0,6645806.0,4,"You can reference  supports metadata.name, metadata.namespace, metadata.labels, metadata.annotations, spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP  as mentioned in the documentation  here .",0.0,0.128,0.872,0.3612
AppDynamics,52681154,52680807,5,"2018/10/06, 19:38:02",False,"2018/10/08, 13:21:09",465.0,6645806.0,4,"However,  CLUSTERNAME  is not a standard property available.",0.0,0.0,1.0,0.0
AppDynamics,52681154,52680807,5,"2018/10/06, 19:38:02",False,"2018/10/08, 13:21:09",465.0,6645806.0,4,"According to this  PR #22043 , the  CLUSTERNAME  should be injected to the  .metadata  field if using GCE.",0.0,0.0,1.0,0.0
AppDynamics,52681154,52680807,5,"2018/10/06, 19:38:02",False,"2018/10/08, 13:21:09",465.0,6645806.0,4,"Otherwise, you'll have to specific the  CLUSTERNAME  manually in the  .metadata  field  and then use  fieldRef  to inject it as an environment variable.",0.0,0.0,1.0,0.0
AppDynamics,52696997,52680807,0,"2018/10/08, 10:02:32",False,"2018/10/08, 10:08:15",415.0,6220162.0,1,"Below format helped me, suggested by ewok2030 and Praveen.",0.0,0.0,1.0,0.0
AppDynamics,52696997,52680807,0,"2018/10/08, 10:02:32",False,"2018/10/08, 10:08:15",415.0,6220162.0,1,Only one thing to make sure that the variable should be declared before they are used as JAVA_OPTS.,0.0,0.119,0.881,0.3182
AppDynamics,52696997,52680807,0,"2018/10/08, 10:02:32",False,"2018/10/08, 10:08:15",415.0,6220162.0,1,containers:,0.0,0.0,1.0,0.0
AppDynamics,60428806,50355471,1,"2020/02/27, 10:11:48",False,"2020/02/27, 10:11:48",85.0,2574522.0,0,Are you sure you're not getting the FileNotFound when trying to do getInputStream on the connection later (or in your real code in case this is a simplified example)?,0.0,0.078,0.922,0.3182
AppDynamics,60428806,50355471,1,"2020/02/27, 10:11:48",False,"2020/02/27, 10:11:48",85.0,2574522.0,0,Since HttpUrlConnection implements http protocol and 4xx codes are error codes trying to call getInputStream generates FNF.,0.144,0.0,0.856,-0.4019
AppDynamics,60428806,50355471,1,"2020/02/27, 10:11:48",False,"2020/02/27, 10:11:48",85.0,2574522.0,0,Instead you should call getErrorStream.,0.0,0.0,1.0,0.0
AppDynamics,60428806,50355471,1,"2020/02/27, 10:11:48",False,"2020/02/27, 10:11:48",85.0,2574522.0,0,I ran your code (without the auth part) and I don't get any FilenotFoundException testing on url's that return 404.,0.0,0.0,1.0,0.0
AppDynamics,60428806,50355471,1,"2020/02/27, 10:11:48",False,"2020/02/27, 10:11:48",85.0,2574522.0,0,So in this case HttpUrlConnection correctly implements the http protocol and appdynamics correctly catches the error.,0.153,0.0,0.847,-0.4019
AppDynamics,60428806,50355471,1,"2020/02/27, 10:11:48",False,"2020/02/27, 10:11:48",85.0,2574522.0,0,"I'm facing the same issue with jersey wrapping the FnF in a UniformResourceException but after some analyzing it's actually either jersey that should provide ways of checking the status code before returning output or correctly use httpurlconnection, and in our case - the webservice should not return 404 for requests that yields no found results but rather an empty collection.",0.082,0.0,0.918,-0.6124
AppDynamics,47301009,47300483,0,"2017/11/15, 08:46:45",False,"2017/11/15, 19:23:51",89858.0,1945651.0,-1,"Not sure whether it would be the cause of memory leaks, but your function definitely has a ton of unnecessary cruft that could be cleaned up with Bluebird's  Promise.mapSeries()  and other Bluebird helpers.",0.04,0.169,0.791,0.6926
AppDynamics,47301009,47300483,0,"2017/11/15, 08:46:45",False,"2017/11/15, 19:23:51",89858.0,1945651.0,-1,Doing so may very well help cut down on memory leaks.,0.148,0.352,0.5,0.5059
AppDynamics,47301009,47300483,0,"2017/11/15, 08:46:45",False,"2017/11/15, 19:23:51",89858.0,1945651.0,-1,doSomething  function reduced to 8 lines:,0.0,0.0,1.0,0.0
AppDynamics,38628646,38600280,0,"2016/07/28, 09:30:52",False,"2016/07/28, 09:30:52",9374.0,5493302.0,2,After this operations,0.0,0.0,1.0,0.0
AppDynamics,38628646,38600280,0,"2016/07/28, 09:30:52",False,"2016/07/28, 09:30:52",9374.0,5493302.0,2,will be,0.0,0.0,1.0,0.0
AppDynamics,38628646,38600280,0,"2016/07/28, 09:30:52",False,"2016/07/28, 09:30:52",9374.0,5493302.0,2,and,0.0,0.0,1.0,0.0
AppDynamics,38628646,38600280,0,"2016/07/28, 09:30:52",False,"2016/07/28, 09:30:52",9374.0,5493302.0,2,will be,0.0,0.0,1.0,0.0
AppDynamics,29033676,29023156,4,"2015/03/13, 15:37:34",True,"2015/03/13, 15:37:34",129048.0,1240763.0,3,"Since you are a Spring Framework user, consider using  Spring AMQP .",0.0,0.0,1.0,0.0
AppDynamics,29033676,29023156,4,"2015/03/13, 15:37:34",True,"2015/03/13, 15:37:34",129048.0,1240763.0,3,The  RabbitTemplate  uses cached channels over a single connection with the channel being checked out of the cache (and returned) for each operation.,0.0,0.0,1.0,0.0
AppDynamics,29033676,29023156,4,"2015/03/13, 15:37:34",True,"2015/03/13, 15:37:34",129048.0,1240763.0,3,"The default cache size is 1, so it generally needs to be configured for an environment like yours.",0.0,0.128,0.872,0.3612
AppDynamics,26594982,26555066,0,"2014/10/27, 21:06:16",False,"2014/10/27, 21:06:16",41.0,702498.0,2,I'm leaning towards it being an issue with the number of channels and the channel cache size.,0.0,0.075,0.925,0.0772
AppDynamics,26594982,26555066,0,"2014/10/27, 21:06:16",False,"2014/10/27, 21:06:16",41.0,702498.0,2,Does anyone know if there's a limit on the number of channels on a queue?,0.0,0.098,0.902,0.0772
AppDynamics,26594982,26555066,0,"2014/10/27, 21:06:16",False,"2014/10/27, 21:06:16",41.0,702498.0,2,It seems like specifying connections rather than channels might help here.,0.0,0.366,0.634,0.6369
AppDynamics,26594982,26555066,0,"2014/10/27, 21:06:16",False,"2014/10/27, 21:06:16",41.0,702498.0,2,"If anyone has any information it would really help,  getting stuck for time :)",0.118,0.311,0.571,0.576
AppDynamics,23488652,23480106,2,"2014/05/06, 10:35:58",False,"2014/05/06, 10:35:58",670.0,2080975.0,1,I think you should use  @PersistenceContext  annotation to obtain  EntityManager  from Spring context and  @Transactional  annotation to drive your transactions.,0.0,0.0,1.0,0.0
AppDynamics,23488652,23480106,2,"2014/05/06, 10:35:58",False,"2014/05/06, 10:35:58",670.0,2080975.0,1,"This way you won't need to worry about closing Hibernate sessions manually, and number of connections will not increase until there are  too many connections .",0.071,0.134,0.795,0.1887
AppDynamics,23682727,23480106,1,"2014/05/15, 18:33:56",True,"2014/05/15, 18:33:56",962.0,360903.0,4,"The problem did not lay in the sessions but in the connection pool, sessions where being managed correctly, but the connection pool in the JDBC layer wasn't closing the connections.",0.06,0.0,0.94,-0.2144
AppDynamics,23682727,23480106,1,"2014/05/15, 18:33:56",True,"2014/05/15, 18:33:56",962.0,360903.0,4,These are the things that I did to fix them.,0.0,0.0,1.0,0.0
AppDynamics,23682727,23480106,1,"2014/05/15, 18:33:56",True,"2014/05/15, 18:33:56",962.0,360903.0,4,JDBC context configuration,0.0,0.0,1.0,0.0
AppDynamics,23682727,23480106,1,"2014/05/15, 18:33:56",True,"2014/05/15, 18:33:56",962.0,360903.0,4,1.- Changed the JDBC connection factory from tomcat's old BasicDataSourceFactory to tomcat's new DataSourceFactory,0.0,0.0,1.0,0.0
AppDynamics,23682727,23480106,1,"2014/05/15, 18:33:56",True,"2014/05/15, 18:33:56",962.0,360903.0,4,"2.-  Tuned the JDBC settings based on this article: 
 http://www.tomcatexpert.com/blog/2010/04/01/configuring-jdbc-pool-high-concurrency",0.0,0.0,1.0,0.0
AppDynamics,23682727,23480106,1,"2014/05/15, 18:33:56",True,"2014/05/15, 18:33:56",962.0,360903.0,4,Session factory xml configuration,0.0,0.0,1.0,0.0
AppDynamics,23682727,23480106,1,"2014/05/15, 18:33:56",True,"2014/05/15, 18:33:56",962.0,360903.0,4,3.- Deleted this line from the session factory configuration:,0.0,0.0,1.0,0.0
AppDynamics,23682727,23480106,1,"2014/05/15, 18:33:56",True,"2014/05/15, 18:33:56",962.0,360903.0,4,This is how my configuration ended up:,0.0,0.0,1.0,0.0
AppDynamics,23682727,23480106,1,"2014/05/15, 18:33:56",True,"2014/05/15, 18:33:56",962.0,360903.0,4,JDBC context configuration,0.0,0.0,1.0,0.0
AppDynamics,23682727,23480106,1,"2014/05/15, 18:33:56",True,"2014/05/15, 18:33:56",962.0,360903.0,4,Session factory xml configuration,0.0,0.0,1.0,0.0
AppDynamics,23100111,23099992,2,"2014/04/16, 07:39:32",False,"2014/04/16, 07:39:32",755.0,3535576.0,0,"Hey there, Just crawl in to this URL.",0.0,0.0,1.0,0.0
AppDynamics,23100111,23099992,2,"2014/04/16, 07:39:32",False,"2014/04/16, 07:39:32",755.0,3535576.0,0,https://help.ubuntu.com/community/EnvironmentVariables,0.0,0.0,1.0,0.0
AppDynamics,23100111,23099992,2,"2014/04/16, 07:39:32",False,"2014/04/16, 07:39:32",755.0,3535576.0,0,"It will better help you.The above URL gives all informations about
  Environment Variable Ubuntu.",0.0,0.318,0.682,0.6808
AppDynamics,23100111,23099992,2,"2014/04/16, 07:39:32",False,"2014/04/16, 07:39:32",755.0,3535576.0,0,"ANd the above POST is updated in 3 Jan
  2014.",0.0,0.0,1.0,0.0
AppDynamics,23100276,23099992,0,"2014/04/16, 07:54:30",False,"2014/04/16, 07:54:30",6589.0,3145373.0,4,Put the environment variables into the global /etc/environment file:,0.0,0.0,1.0,0.0
AppDynamics,23100276,23099992,0,"2014/04/16, 07:54:30",False,"2014/04/16, 07:54:30",6589.0,3145373.0,4,...,0.0,0.0,1.0,0.0
AppDynamics,23100276,23099992,0,"2014/04/16, 07:54:30",False,"2014/04/16, 07:54:30",6589.0,3145373.0,4,...,0.0,0.0,1.0,0.0
AppDynamics,23100276,23099992,0,"2014/04/16, 07:54:30",False,"2014/04/16, 07:54:30",6589.0,3145373.0,4,"Execute ""source /etc/environment"" in every shell where you want the variables to be updated:",0.0,0.091,0.909,0.0772
AppDynamics,23100276,23099992,0,"2014/04/16, 07:54:30",False,"2014/04/16, 07:54:30",6589.0,3145373.0,4,Check that it works:,0.0,0.0,1.0,0.0
AppDynamics,23100276,23099992,0,"2014/04/16, 07:54:30",False,"2014/04/16, 07:54:30",6589.0,3145373.0,4,Here is a other link from mkyong,0.0,0.0,1.0,0.0
AppDynamics,23100418,23099992,0,"2014/04/16, 08:06:09",False,"2014/04/16, 08:06:09",526.0,1494927.0,2,"Here is some info on environment variables, setting your path and where to install things that I hope is useful for you to get your environment setup.",0.0,0.195,0.805,0.7003
AppDynamics,23100418,23099992,0,"2014/04/16, 08:06:09",False,"2014/04/16, 08:06:09",526.0,1494927.0,2,.bashrc : is specific for the  bash  shell.,0.0,0.0,1.0,0.0
AppDynamics,23100418,23099992,0,"2014/04/16, 08:06:09",False,"2014/04/16, 08:06:09",526.0,1494927.0,2,".profile : is used by several shells, and was originally used by the bourne shell (from memory).",0.0,0.0,1.0,0.0
AppDynamics,23100418,23099992,0,"2014/04/16, 08:06:09",False,"2014/04/16, 08:06:09",526.0,1494927.0,2,.profile  might not be loaded by bash if there is a  .bashrc  present.,0.0,0.0,1.0,0.0
AppDynamics,23100418,23099992,0,"2014/04/16, 08:06:09",False,"2014/04/16, 08:06:09",526.0,1494927.0,2,Some shells read it only if there is no shell specific configuration present.,0.155,0.0,0.845,-0.296
AppDynamics,23100418,23099992,0,"2014/04/16, 08:06:09",False,"2014/04/16, 08:06:09",526.0,1494927.0,2,"If you happen to be using a different shell, you will need to look at how best to configure environment variables for that shell.",0.0,0.16,0.84,0.6369
AppDynamics,23100418,23099992,0,"2014/04/16, 08:06:09",False,"2014/04/16, 08:06:09",526.0,1494927.0,2,"Note that adding to the above files only effect the user that you set them for, since they live in  /home/username/ .",0.0,0.0,1.0,0.0
AppDynamics,23100418,23099992,0,"2014/04/16, 08:06:09",False,"2014/04/16, 08:06:09",526.0,1494927.0,2,"Also remember to source the file again, or reload the shell so that your settings take effect.",0.0,0.0,1.0,0.0
AppDynamics,23100418,23099992,0,"2014/04/16, 08:06:09",False,"2014/04/16, 08:06:09",526.0,1494927.0,2,You can achieve this with something like  source .bashrc  after you edit it at the command line to avoid having to restart or reopen terminal.,0.079,0.09,0.83,0.0772
AppDynamics,23100418,23099992,0,"2014/04/16, 08:06:09",False,"2014/04/16, 08:06:09",526.0,1494927.0,2,"If you would like to set system wide variables, you can do that in  /etc/environment .",0.0,0.152,0.848,0.3612
AppDynamics,23100418,23099992,0,"2014/04/16, 08:06:09",False,"2014/04/16, 08:06:09",526.0,1494927.0,2,"If you would like to execute java / ant / maven, etc.",0.0,0.217,0.783,0.3612
AppDynamics,23100418,23099992,0,"2014/04/16, 08:06:09",False,"2014/04/16, 08:06:09",526.0,1494927.0,2,"from the command line, or enable applications that require the  PATH  environment variable to be set correctly to work, you will also need to add the  ./bin  directories to your PATH.",0.0,0.0,1.0,0.0
AppDynamics,23100418,23099992,0,"2014/04/16, 08:06:09",False,"2014/04/16, 08:06:09",526.0,1494927.0,2,Depending on your preference regarding system wide or user specific path settings:,0.0,0.0,1.0,0.0
AppDynamics,23100418,23099992,0,"2014/04/16, 08:06:09",False,"2014/04/16, 08:06:09",526.0,1494927.0,2,etc.,0.0,0.0,1.0,0.0
AppDynamics,23100418,23099992,0,"2014/04/16, 08:06:09",False,"2014/04/16, 08:06:09",526.0,1494927.0,2,in the relevant file.,0.0,0.0,1.0,0.0
AppDynamics,23100418,23099992,0,"2014/04/16, 08:06:09",False,"2014/04/16, 08:06:09",526.0,1494927.0,2,"A side point and entirely optional, the correct place to install java, ant, maven, etc if not from .deb's would be in  /opt , according to the  FilesystemHierarchyStandard",0.0,0.0,1.0,0.0
AppDynamics,11917597,11916710,1,"2012/08/11, 23:56:01",False,"2012/08/11, 23:56:01",2045.0,1469663.0,0,You might check out Spring Insight.,0.0,0.0,1.0,0.0
AppDynamics,11917597,11916710,1,"2012/08/11, 23:56:01",False,"2012/08/11, 23:56:01",2045.0,1469663.0,0,"Spring-insight source, design and alternatives",0.0,0.0,1.0,0.0
AppDynamics,11917597,11916710,1,"2012/08/11, 23:56:01",False,"2012/08/11, 23:56:01",2045.0,1469663.0,0,http://www.springsource.org/insight,0.0,0.0,1.0,0.0
AppDynamics,12173401,11916710,0,"2012/08/29, 11:08:08",False,"2012/08/29, 11:08:08",275412.0,40342.0,0,Java Melody  might be relevant for you.,0.0,0.0,1.0,0.0
AppDynamics,61748113,60872992,0,"2020/05/12, 12:22:07",True,"2020/05/12, 12:33:22",56.0,9641196.0,1,This is an issue with the AppDynamics plugin and gradle plugin tools 3.6.x.,0.0,0.0,1.0,0.0
AppDynamics,61748113,60872992,0,"2020/05/12, 12:22:07",True,"2020/05/12, 12:33:22",56.0,9641196.0,1,You can see this link:  https://community.appdynamics.com/t5/End-User-Monitoring-EUM/AppDynamic-EUM-setup-for-Android-Cordova-project/td-p/38864,0.0,0.0,1.0,0.0
AppDynamics,61748113,60872992,0,"2020/05/12, 12:22:07",True,"2020/05/12, 12:33:22",56.0,9641196.0,1,I have downgrade gradle plugin tools to 3.5.x and solve it,0.0,0.167,0.833,0.2023
AppDynamics,59252943,58873513,0,"2019/12/09, 18:27:29",False,"2019/12/09, 18:27:29",392.0,2801554.0,0,"You can create a cron job to run from Monday to Saturday, here for each hour:",0.0,0.13,0.87,0.2732
AppDynamics,59252943,58873513,0,"2019/12/09, 18:27:29",False,"2019/12/09, 18:27:29",392.0,2801554.0,0,"And another to cover the interval you want on Sunday, here one by one hour from 10:00 AM to 03:00 PM:",0.0,0.061,0.939,0.0772
AppDynamics,50644421,50618877,0,"2018/06/01, 16:20:18",False,"2018/06/01, 16:20:18",23.0,9436164.0,0,Looks like httprequest plugin does not support uploading zip file.,0.177,0.196,0.627,0.0624
AppDynamics,50644421,50618877,0,"2018/06/01, 16:20:18",False,"2018/06/01, 16:20:18",23.0,9436164.0,0,This is my observation.,0.0,0.0,1.0,0.0
AppDynamics,50857929,50618877,0,"2018/06/14, 15:47:37",False,"2018/06/14, 15:47:37",61.0,6101424.0,0,I think upload will use  Content-Type: multipart/form-data .,0.0,0.0,1.0,0.0
AppDynamics,50857929,50618877,0,"2018/06/14, 15:47:37",False,"2018/06/14, 15:47:37",61.0,6101424.0,0,But httpRequest plugin is not supporting this type.,0.308,0.0,0.692,-0.4782
AppDynamics,50857929,50618877,0,"2018/06/14, 15:47:37",False,"2018/06/14, 15:47:37",61.0,6101424.0,0,However it supports  APPLICATION_OCTETSTREAM(ContentType.APPLICATION_OCTET_STREAM),0.0,0.455,0.545,0.3612
AppDynamics,50857929,50618877,0,"2018/06/14, 15:47:37",False,"2018/06/14, 15:47:37",61.0,6101424.0,0,Could you post output from your curl?,0.0,0.0,1.0,0.0
AppDynamics,57322375,50618877,3,"2019/08/02, 11:13:00",False,"2019/08/02, 11:13:00",177.0,277547.0,1,Following Code worked for me :,0.0,0.0,1.0,0.0
AppDynamics,47084941,47024920,0,"2017/11/02, 23:29:17",True,"2017/11/02, 23:29:17",190.0,550621.0,1,It turns out the code I was given was completely wrong for angular 2 implementation.,0.22,0.0,0.78,-0.5256
AppDynamics,47084941,47024920,0,"2017/11/02, 23:29:17",True,"2017/11/02, 23:29:17",190.0,550621.0,1,The code they gave me is for running on the web server's side with node js.,0.0,0.0,1.0,0.0
AppDynamics,47084941,47024920,0,"2017/11/02, 23:29:17",True,"2017/11/02, 23:29:17",190.0,550621.0,1,"Since angular 2 is an SPA that runs on the browser, it would never work.",0.0,0.0,1.0,0.0
AppDynamics,47084941,47024920,0,"2017/11/02, 23:29:17",True,"2017/11/02, 23:29:17",190.0,550621.0,1,I did some research and found this example application that I added a few tweaks to:  https://github.com/derrekyoung/appd-sampleapp-angular2,0.0,0.0,1.0,0.0
AppDynamics,47319681,45650349,0,"2017/11/16, 02:37:15",False,"2017/11/16, 02:37:15",4583.0,756095.0,1,"in your appdynamics.cfg, change",0.0,0.0,1.0,0.0
AppDynamics,47319681,45650349,0,"2017/11/16, 02:37:15",False,"2017/11/16, 02:37:15",4583.0,756095.0,1,to,0.0,0.0,1.0,0.0
AppDynamics,43387027,41357203,0,"2017/04/13, 11:20:58",False,"2017/04/13, 11:20:58",324.0,7220665.0,0,"I am not exactly sure what the problem is, but I just tried it with AppDynamics Controller Version 4.2 to share a custom dashboard and open the shared Link in a different browser and I didn't need to put in any credentials.",0.079,0.14,0.781,0.5528
AppDynamics,43387027,41357203,0,"2017/04/13, 11:20:58",False,"2017/04/13, 11:20:58",324.0,7220665.0,0,Maybe AppDynamics Version 3.9 had a bug (assuming you used version 3.9 according to the docs link you posted),0.0,0.0,1.0,0.0
AppDynamics,43387027,41357203,0,"2017/04/13, 11:20:58",False,"2017/04/13, 11:20:58",324.0,7220665.0,0,Here is what I did:,0.0,0.0,1.0,0.0
AppDynamics,38639601,38635466,3,"2016/07/28, 17:40:52",False,"2016/07/28, 17:40:52",211.0,5400918.0,1,"CloudForms is not trying to substitute OpenShift UI, but complement it, giving you more information about the environment and providing additional capabilities on top of OPenShift.",0.0,0.181,0.819,0.6486
AppDynamics,38639601,38635466,3,"2016/07/28, 17:40:52",False,"2016/07/28, 17:40:52",211.0,5400918.0,1,You can see information about what is being done and demos in videos:,0.0,0.0,1.0,0.0
AppDynamics,38639601,38635466,3,"2016/07/28, 17:40:52",False,"2016/07/28, 17:40:52",211.0,5400918.0,1,https://www.youtube.com/watch?v=FVLo9Nc_10E&amp;list=PLQAAGwo9CYO-4tQsnC6oWgzhPNOykOOMd&amp;index=15,0.0,0.0,1.0,0.0
AppDynamics,38639601,38635466,3,"2016/07/28, 17:40:52",False,"2016/07/28, 17:40:52",211.0,5400918.0,1,"And you can find the presentations here
 http://www.slideshare.net/ManageIQ/presentations",0.0,0.0,1.0,0.0
AppDynamics,30225687,30191131,1,"2015/05/14, 00:43:25",True,"2015/05/14, 00:43:25",185615.0,2016562.0,0,Just follow this istructions:,0.0,0.0,1.0,0.0
AppDynamics,30225687,30191131,1,"2015/05/14, 00:43:25",True,"2015/05/14, 00:43:25",185615.0,2016562.0,0,"Clone a local copy of this project with
git clone  https://github.com/Appdynamics/ECommerce-Android",0.0,0.0,1.0,0.0
AppDynamics,30225687,30191131,1,"2015/05/14, 00:43:25",True,"2015/05/14, 00:43:25",185615.0,2016562.0,0,Open Android Studio and Import Project (select app/build.gradle),0.0,0.0,1.0,0.0
AppDynamics,30225687,30191131,1,"2015/05/14, 00:43:25",True,"2015/05/14, 00:43:25",185615.0,2016562.0,0,Android Studio will ask you to build the project with gradle.,0.0,0.0,1.0,0.0
AppDynamics,30225687,30191131,1,"2015/05/14, 00:43:25",True,"2015/05/14, 00:43:25",185615.0,2016562.0,0,Gradle will use the  build.gradle  inside the project.,0.0,0.0,1.0,0.0
AppDynamics,30225687,30191131,1,"2015/05/14, 00:43:25",True,"2015/05/14, 00:43:25",185615.0,2016562.0,0,The version of gradle is inside the  gradle/wrapper/gradle-wrapper.properties  file:,0.0,0.0,1.0,0.0
AppDynamics,30225687,30191131,1,"2015/05/14, 00:43:25",True,"2015/05/14, 00:43:25",185615.0,2016562.0,0,Check for example the file inside the  app :,0.0,0.0,1.0,0.0
AppDynamics,30268298,30191131,0,"2015/05/15, 23:34:56",False,"2015/05/15, 23:34:56",166.0,4226112.0,0,"You need put a directory called ""adeum-maven-repo"" in your project setup .",0.0,0.0,1.0,0.0
AppDynamics,30268298,30191131,0,"2015/05/15, 23:34:56",False,"2015/05/15, 23:34:56",166.0,4226112.0,0,https://docs.appdynamics.com/display/PRO39/Instrument+an+Android+Application#InstrumentanAndroidApplication-SetupforGradle,0.0,0.0,1.0,0.0
AppDynamics,27420289,27415368,0,"2014/12/11, 12:14:49",True,"2014/12/11, 12:20:03",628370.0,139985.0,1,Whether this is an issue or not is really up to you to decide.,0.0,0.0,1.0,0.0
AppDynamics,27420289,27415368,0,"2014/12/11, 12:14:49",True,"2014/12/11, 12:20:03",628370.0,139985.0,1,"On the one hand, a full GC that runs for one second every 10 minutes is not a significant issue for throughput.",0.07,0.14,0.79,0.3834
AppDynamics,27420289,27415368,0,"2014/12/11, 12:14:49",True,"2014/12/11, 12:20:03",628370.0,139985.0,1,"On the other hand, the full GC is probably going to dramatically reduce response times during that those one second windows.",0.0,0.138,0.862,0.4939
AppDynamics,27420289,27415368,0,"2014/12/11, 12:14:49",True,"2014/12/11, 12:20:03",628370.0,139985.0,1,But that may not be important or even relevant to your application.,0.146,0.0,0.854,-0.2235
AppDynamics,27420289,27415368,0,"2014/12/11, 12:14:49",True,"2014/12/11, 12:20:03",628370.0,139985.0,1,The thing I would be worried about is whether your load testing is a realistic test.,0.145,0.0,0.855,-0.296
AppDynamics,27420289,27415368,0,"2014/12/11, 12:14:49",True,"2014/12/11, 12:20:03",628370.0,139985.0,1,"The application appears to need 4Gb of heap space under test, but is it also going to need that in a real-world use?",0.0,0.0,1.0,0.0
AppDynamics,27420289,27415368,0,"2014/12/11, 12:14:49",True,"2014/12/11, 12:20:03",628370.0,139985.0,1,I'd be worried that a memory leak might show up when it is deployed into production.,0.261,0.0,0.739,-0.5574
AppDynamics,27420289,27415368,0,"2014/12/11, 12:14:49",True,"2014/12/11, 12:20:03",628370.0,139985.0,1,Or that the load in your load testing is causing the application's in-memory caching to reach a steady state in that won't be reproduced in production.,0.0,0.044,0.956,0.0258
AppDynamics,27420289,27415368,0,"2014/12/11, 12:14:49",True,"2014/12/11, 12:20:03",628370.0,139985.0,1,"As a general rule, it is a bad thing for the heap to be running close to full, so an increase in the heap is probably advisable.",0.12,0.089,0.791,-0.2315
AppDynamics,27420289,27415368,0,"2014/12/11, 12:14:49",True,"2014/12/11, 12:20:03",628370.0,139985.0,1,"Your application's performance doesn't appear to be suffering, but you could be ""on the cusp"".",0.128,0.0,0.872,-0.2617
AppDynamics,27420289,27415368,0,"2014/12/11, 12:14:49",True,"2014/12/11, 12:20:03",628370.0,139985.0,1,"I thought that once old gen utilization is above certain level, GC will try to free/compact old gen area and if nothing is freed it would either fail with OOM and start crazy full GC cycles just before it.",0.184,0.047,0.768,-0.7234
AppDynamics,27420289,27415368,0,"2014/12/11, 12:14:49",True,"2014/12/11, 12:20:03",628370.0,139985.0,1,I suspect that the monitoring reports might be misleading you.,0.412,0.0,0.588,-0.5994
AppDynamics,27420289,27415368,0,"2014/12/11, 12:14:49",True,"2014/12/11, 12:20:03",628370.0,139985.0,1,"If the full GC cycles were  really  not reclaiming anything, I'd expect the behaviour to be different.",0.0,0.0,1.0,0.0
AppDynamics,27420289,27415368,0,"2014/12/11, 12:14:49",True,"2014/12/11, 12:20:03",628370.0,139985.0,1,"Try turning on JVM GC log message, and see what they tell you about the amount of memory that the full GC cycles are managing to reclaim.",0.0,0.0,1.0,0.0
AppDynamics,25416702,25134646,0,"2014/08/21, 03:50:57",True,"2014/08/21, 03:50:57",525.0,2479518.0,1,My answer with links got deleted by another SO user so I'm listing the steps here.,0.0,0.0,1.0,0.0
AppDynamics,25416702,25134646,0,"2014/08/21, 03:50:57",True,"2014/08/21, 03:50:57",525.0,2479518.0,1,First uninstall using this,0.0,0.0,1.0,0.0
AppDynamics,25416702,25134646,0,"2014/08/21, 03:50:57",True,"2014/08/21, 03:50:57",525.0,2479518.0,1,"$ npm uninstall -g strong-cli
$ npm uninstall -g loopback-sdk-angular-cli",0.0,0.0,1.0,0.0
AppDynamics,25416702,25134646,0,"2014/08/21, 03:50:57",True,"2014/08/21, 03:50:57",525.0,2479518.0,1,and then install,0.0,0.0,1.0,0.0
AppDynamics,25416702,25134646,0,"2014/08/21, 03:50:57",True,"2014/08/21, 03:50:57",525.0,2479518.0,1,npm install -g strongloop,0.0,0.0,1.0,0.0
AppDynamics,25416702,25134646,0,"2014/08/21, 03:50:57",True,"2014/08/21, 03:50:57",525.0,2479518.0,1,You can now run slc strongops,0.0,0.0,1.0,0.0
AppDynamics,25416702,25134646,0,"2014/08/21, 03:50:57",True,"2014/08/21, 03:50:57",525.0,2479518.0,1,and let us know how it goes.,0.0,0.0,1.0,0.0
AppDynamics,27635551,25134646,0,"2014/12/24, 12:38:57",False,"2014/12/24, 12:38:57",924.0,427300.0,1,"I created a Linux init.d Daemon script which you can use to run your app with slc as service:
 https://gist.github.com/gurdotan/23311a236fc65dc212da",0.0,0.105,0.895,0.25
AppDynamics,27635551,25134646,0,"2014/12/24, 12:38:57",False,"2014/12/24, 12:38:57",924.0,427300.0,1,Might be useful to some of you.,0.0,0.326,0.674,0.4404
AppDynamics,19326222,19191781,1,"2013/10/11, 22:56:19",False,"2013/10/11, 22:56:19",11.0,2872398.0,1,You should try Compuware's dynaTrace,0.0,0.0,1.0,0.0
AppDynamics,11687150,11687049,8,"2012/07/27, 14:47:42",True,"2012/07/27, 14:47:42",2990.0,1554255.0,3,"i don't think that you can do full-featured profiling of distributed request across number of JVM's - AppDynamics from what i can remember understands the EE stuff - like calling DB, EJB, RMI, or remote webservice - however it still works in scope of JVM.",0.0,0.091,0.909,0.4215
AppDynamics,11687150,11687049,8,"2012/07/27, 14:47:42",True,"2012/07/27, 14:47:42",2990.0,1554255.0,3,"Isn't it suffient in your case just to use java profiler (like yourkit, jprofiler)?",0.0,0.0,1.0,0.0
AppDynamics,20809292,11687049,0,"2013/12/28, 01:37:03",False,"2013/12/28, 01:37:03",4378.0,341191.0,1,you can try 24x7monitoring,0.0,0.0,1.0,0.0
AppDynamics,20809292,11687049,0,"2013/12/28, 01:37:03",False,"2013/12/28, 01:37:03",4378.0,341191.0,1,https://code.google.com/p/monitor-24x7/,0.0,0.0,1.0,0.0
AppDynamics,20809292,11687049,0,"2013/12/28, 01:37:03",False,"2013/12/28, 01:37:03",4378.0,341191.0,1,"it provides method level monitoring, SQL queries, business transactions...",0.0,0.0,1.0,0.0
AppDynamics,21438317,11687049,0,"2014/01/29, 19:29:28",False,"2014/01/29, 19:29:28",31.0,3249955.0,3,Did you try the free version of AppDynamics.,0.0,0.32,0.68,0.5106
AppDynamics,21438317,11687049,0,"2014/01/29, 19:29:28",False,"2014/01/29, 19:29:28",31.0,3249955.0,3,It's called AppDynamics LITE.,0.0,0.0,1.0,0.0
AppDynamics,21438317,11687049,0,"2014/01/29, 19:29:28",False,"2014/01/29, 19:29:28",31.0,3249955.0,3,You can take a look also to EXTRAHOP free version.,0.0,0.292,0.708,0.5106
AppDynamics,21438317,11687049,0,"2014/01/29, 19:29:28",False,"2014/01/29, 19:29:28",31.0,3249955.0,3,Maybe it is good enough for your needs.,0.0,0.293,0.707,0.4404
AppDynamics,21438317,11687049,0,"2014/01/29, 19:29:28",False,"2014/01/29, 19:29:28",31.0,3249955.0,3,Also you can try using a SaaS solutions such as NewRelic or Boundary.,0.0,0.134,0.866,0.1779
AppDynamics,21438317,11687049,0,"2014/01/29, 19:29:28",False,"2014/01/29, 19:29:28",31.0,3249955.0,3,They have free accounts that could also be good enough for your needs.,0.0,0.36,0.64,0.7351
AppDynamics,21438317,11687049,0,"2014/01/29, 19:29:28",False,"2014/01/29, 19:29:28",31.0,3249955.0,3,"Finally if you want to monitor the performance of any specific JAVA application, you can use  http://www.moskito.org/ .",0.0,0.075,0.925,0.0772
AppDynamics,21438317,11687049,0,"2014/01/29, 19:29:28",False,"2014/01/29, 19:29:28",31.0,3249955.0,3,It's totaly FREE.,0.0,0.668,0.332,0.6166
AppDynamics,7264935,7077885,0,"2011/09/01, 03:19:24",False,"2011/09/01, 03:19:24",251.0,914642.0,0,can you tell me the methodology for deriving those stats?,0.0,0.0,1.0,0.0
AppDynamics,7264935,7077885,0,"2011/09/01, 03:19:24",False,"2011/09/01, 03:19:24",251.0,914642.0,0,"In most cases, this behavior comes down to poor code implementation in a custom portlet or a JVM configuration issue and more likely the latter.",0.124,0.0,0.876,-0.4767
AppDynamics,7264935,7077885,0,"2011/09/01, 03:19:24",False,"2011/09/01, 03:19:24",251.0,914642.0,0,Take a look at the  Ehcache Garbage Collection Tuning Documentation .,0.0,0.0,1.0,0.0
AppDynamics,7264935,7077885,0,"2011/09/01, 03:19:24",False,"2011/09/01, 03:19:24",251.0,914642.0,0,"When you run  jstat , how do the garbage collection times look?",0.0,0.0,1.0,0.0
AppDynamics,64358686,63590062,0,"2020/10/14, 20:36:25",False,"2020/10/14, 20:36:25",1.0,14450632.0,0,"I realize it's been a while, but I'll contribute anyway as it may help other people too.",0.0,0.202,0.798,0.5499
AppDynamics,64358686,63590062,0,"2020/10/14, 20:36:25",False,"2020/10/14, 20:36:25",1.0,14450632.0,0,"In my case, importing Instrumentation in iOS caused this error; it seems to be a problem in the latest version of @appdynamics/react-native-agent (version 20.7.0 as of writing).",0.195,0.0,0.805,-0.7027
AppDynamics,64358686,63590062,0,"2020/10/14, 20:36:25",False,"2020/10/14, 20:36:25",1.0,14450632.0,0,"I instead initialized AppDynamics in native code (in the AppDelegate.m file), as follows:",0.0,0.0,1.0,0.0
AppDynamics,64358686,63590062,0,"2020/10/14, 20:36:25",False,"2020/10/14, 20:36:25",1.0,14450632.0,0,"For more info, check the iOS guide:
 https://docs.appdynamics.com/display/PRO45/Instrument+an+iOS+Application",0.0,0.0,1.0,0.0
AppDynamics,64358686,63590062,0,"2020/10/14, 20:36:25",False,"2020/10/14, 20:36:25",1.0,14450632.0,0,"Additionally, I avoided importing AppDynamics in javascript by requiring it in runtime, in Android only.",0.156,0.0,0.844,-0.34
AppDynamics,63117665,62998835,0,"2020/07/27, 17:38:53",False,"2020/07/27, 17:38:53",213.0,7933630.0,1,"There's a lot of different questions here, and some of them don't have answers without more information about your specific setup, but I'll try to give you a good overview.",0.0,0.125,0.875,0.5927
AppDynamics,63117665,62998835,0,"2020/07/27, 17:38:53",False,"2020/07/27, 17:38:53",213.0,7933630.0,1,Why Tracing?,0.0,0.0,1.0,0.0
AppDynamics,63117665,62998835,0,"2020/07/27, 17:38:53",False,"2020/07/27, 17:38:53",213.0,7933630.0,1,You've already intuited that there are a lot of similarities between &quot;APM&quot; and &quot;tracing&quot; - the differences are fairly minimal.,0.0,0.0,1.0,0.0
AppDynamics,63117665,62998835,0,"2020/07/27, 17:38:53",False,"2020/07/27, 17:38:53",213.0,7933630.0,1,"Distributed Tracing is a superset of capabilities marketed as APM (application performance monitoring) and RUM (real user monitoring), as it allows you to capture performance information about the work being done in your services to handle a single, logical request both at a per-service level, and at the level of an entire request (or transaction) from client to DB and back.",0.0,0.0,1.0,0.0
AppDynamics,63117665,62998835,0,"2020/07/27, 17:38:53",False,"2020/07/27, 17:38:53",213.0,7933630.0,1,"Trace data, like other forms of telemetry, can be aggregated and analyzed in different ways - for example, unsampled trace data can be used to generate RED (rate, error, duration) metrics for a given API endpoint or function call.",0.067,0.062,0.871,-0.0516
AppDynamics,63117665,62998835,0,"2020/07/27, 17:38:53",False,"2020/07/27, 17:38:53",213.0,7933630.0,1,"Conventionally, trace data is annotated (tagged) with properties about a request or the underlying infrastructure handling a request (things like a customer identifier, or the host name of the server handling a request, or the DB partition being accessed for a given query) that allows for powerful exploratory queries in a tool like Jaeger or a commercial tracing tool.",0.0,0.137,0.863,0.7783
AppDynamics,63117665,62998835,0,"2020/07/27, 17:38:53",False,"2020/07/27, 17:38:53",213.0,7933630.0,1,Sampling,0.0,0.0,1.0,0.0
AppDynamics,63117665,62998835,0,"2020/07/27, 17:38:53",False,"2020/07/27, 17:38:53",213.0,7933630.0,1,The overall performance impact of generating traces varies.,0.0,0.0,1.0,0.0
AppDynamics,63117665,62998835,0,"2020/07/27, 17:38:53",False,"2020/07/27, 17:38:53",213.0,7933630.0,1,"In general, tracing libraries are designed to be fairly lightweight - although there are a lot of factors that influence this overhead, such as the amount of attributes on a span, the log events attached to it, and the request rate of a service.",0.0,0.0,1.0,0.0
AppDynamics,63117665,62998835,0,"2020/07/27, 17:38:53",False,"2020/07/27, 17:38:53",213.0,7933630.0,1,"Companies like Google will aggressively sample due to their scale, but to be honest, sampling is more beneficial to consider from a long-term storage perspective rather than an up-front overhead perspective.",0.043,0.275,0.682,0.8702
AppDynamics,63117665,62998835,0,"2020/07/27, 17:38:53",False,"2020/07/27, 17:38:53",213.0,7933630.0,1,"While the additional overhead per-request to create a span and transmit it to your tracing backend might be small, the cost to store trace data over time can quickly become prohibitive.",0.0,0.068,0.932,0.2732
AppDynamics,63117665,62998835,0,"2020/07/27, 17:38:53",False,"2020/07/27, 17:38:53",213.0,7933630.0,1,"In addition, most traces from most systems aren't terribly interesting.",0.168,0.237,0.595,0.2334
AppDynamics,63117665,62998835,0,"2020/07/27, 17:38:53",False,"2020/07/27, 17:38:53",213.0,7933630.0,1,This is why dynamic and tail-based sampling approaches have become more popular.,0.0,0.363,0.637,0.6901
AppDynamics,63117665,62998835,0,"2020/07/27, 17:38:53",False,"2020/07/27, 17:38:53",213.0,7933630.0,1,"These systems move the sampling decision from an individual service layer to some external process, such as the  OpenTelemetry Collector , which can analyze an entire trace and determine if it should be sampled in or out based on user-defined criteria.",0.0,0.0,1.0,0.0
AppDynamics,63117665,62998835,0,"2020/07/27, 17:38:53",False,"2020/07/27, 17:38:53",213.0,7933630.0,1,"You could, for example, ensure that any trace where an error occurred is sampled in, while 'baseline' traces are sampled at a rate of 1%, in order to preserve important error information while giving you an idea of steady-state performance.",0.117,0.147,0.736,0.1027
AppDynamics,63117665,62998835,0,"2020/07/27, 17:38:53",False,"2020/07/27, 17:38:53",213.0,7933630.0,1,Proprietary APM vs. OSS,0.0,0.0,1.0,0.0
AppDynamics,63117665,62998835,0,"2020/07/27, 17:38:53",False,"2020/07/27, 17:38:53",213.0,7933630.0,1,One important distinction between something like AppDynamics or New Relic and tools like Jaeger is that Jaeger does not rely on proprietary instrumentation agents in order to generate trace data.,0.0,0.201,0.799,0.7003
AppDynamics,63117665,62998835,0,"2020/07/27, 17:38:53",False,"2020/07/27, 17:38:53",213.0,7933630.0,1,"Jaeger supports OpenTelemetry, allowing you to use open source tools like the  OpenTelemetry Java Automatic Instrumentation  libraries, which will automatically generate spans for many popular Java frameworks and libraries, such as Spring.",0.0,0.212,0.788,0.7783
AppDynamics,63117665,62998835,0,"2020/07/27, 17:38:53",False,"2020/07/27, 17:38:53",213.0,7933630.0,1,"In addition, since OpenTelemetry is available in multiple languages with a shared data format and trace context format, you can guarantee that your traces will work properly in a polyglot environment (so, if you have Node.JS or Golang services in addition to your Java services, you could use OpenTelemetry for each language, and trace context propagation would work seamlessly between all of them).",0.0,0.069,0.931,0.5267
AppDynamics,63117665,62998835,0,"2020/07/27, 17:38:53",False,"2020/07/27, 17:38:53",213.0,7933630.0,1,"Even more advantageous, though, is that your instrumentation is decoupled from a specific vendor or tool.",0.0,0.166,0.834,0.4201
AppDynamics,63117665,62998835,0,"2020/07/27, 17:38:53",False,"2020/07/27, 17:38:53",213.0,7933630.0,1,"You can instrument your service with OpenTelemetry and then send data to one - or more - analysis tools, both commercial and open source.",0.0,0.0,1.0,0.0
AppDynamics,63117665,62998835,0,"2020/07/27, 17:38:53",False,"2020/07/27, 17:38:53",213.0,7933630.0,1,"This frees you from vendor lock-in, and allows you to select the best tool for the job.",0.0,0.299,0.701,0.7506
AppDynamics,63117665,62998835,0,"2020/07/27, 17:38:53",False,"2020/07/27, 17:38:53",213.0,7933630.0,1,"If you'd like to learn more about OpenTelemetry, observability, and other topics I wrote a longer series that you can find  here  (look for the other 'OpenTelemetry 101' posts).",0.0,0.088,0.912,0.3612
AppDynamics,59198597,59191709,1,"2019/12/05, 17:40:40",False,"2019/12/09, 14:27:54",722.0,8826818.0,0,Windows 10 64-bit.,0.0,0.0,1.0,0.0
AppDynamics,59198597,59191709,1,"2019/12/05, 17:40:40",False,"2019/12/09, 14:27:54",722.0,8826818.0,0,Powershell 5.,0.0,0.0,1.0,0.0
AppDynamics,59198597,59191709,1,"2019/12/05, 17:40:40",False,"2019/12/09, 14:27:54",722.0,8826818.0,0,Does not require admin privileges.,0.353,0.0,0.647,-0.2924
AppDynamics,59198597,59191709,1,"2019/12/05, 17:40:40",False,"2019/12/09, 14:27:54",722.0,8826818.0,0,How to quickly and efficiently extract text from a large logfile from input1 to input2 using powershell 5?,0.0,0.144,0.856,0.4019
AppDynamics,59198597,59191709,1,"2019/12/05, 17:40:40",False,"2019/12/09, 14:27:54",722.0,8826818.0,0,Sample logfile below.,0.0,0.0,1.0,0.0
AppDynamics,59198597,59191709,1,"2019/12/05, 17:40:40",False,"2019/12/09, 14:27:54",722.0,8826818.0,0,For testing purposes copy your logfile to the desktop and name it logfile.txt,0.0,0.0,1.0,0.0
AppDynamics,59198597,59191709,1,"2019/12/05, 17:40:40",False,"2019/12/09, 14:27:54",722.0,8826818.0,0,What is the default text editor in Windows Server 2012 R2 Standard 64-bit?,0.0,0.0,1.0,0.0
AppDynamics,59198597,59191709,1,"2019/12/05, 17:40:40",False,"2019/12/09, 14:27:54",722.0,8826818.0,0,See line 42.,0.0,0.0,1.0,0.0
AppDynamics,59198597,59191709,1,"2019/12/05, 17:40:40",False,"2019/12/09, 14:27:54",722.0,8826818.0,0,What program do you have associated with .txt files?,0.0,0.0,1.0,0.0
AppDynamics,59198597,59191709,1,"2019/12/05, 17:40:40",False,"2019/12/09, 14:27:54",722.0,8826818.0,0,See line 42.,0.0,0.0,1.0,0.0
AppDynamics,59198597,59191709,1,"2019/12/05, 17:40:40",False,"2019/12/09, 14:27:54",722.0,8826818.0,0,logfile.txt:,0.0,0.0,1.0,0.0
AppDynamics,59198597,59191709,1,"2019/12/05, 17:40:40",False,"2019/12/09, 14:27:54",722.0,8826818.0,0,Results of running script on logfile.txt:,0.0,0.0,1.0,0.0
AppDynamics,59198597,59191709,1,"2019/12/05, 17:40:40",False,"2019/12/09, 14:27:54",722.0,8826818.0,0,Powershell in four hours at Youtube,0.0,0.0,1.0,0.0
AppDynamics,59198597,59191709,1,"2019/12/05, 17:40:40",False,"2019/12/09, 14:27:54",722.0,8826818.0,0,Parse and extract text with powershell at Bing,0.0,0.0,1.0,0.0
AppDynamics,59198597,59191709,1,"2019/12/05, 17:40:40",False,"2019/12/09, 14:27:54",722.0,8826818.0,0,How to quickly and efficiently extract text from a large logfile from line x to line y using powershell 5?,0.0,0.144,0.856,0.4019
AppDynamics,59198597,59191709,1,"2019/12/05, 17:40:40",False,"2019/12/09, 14:27:54",722.0,8826818.0,0,How to quickly and efficiently extract text from a large logfile from date1 to date2 using powershell 5?,0.0,0.144,0.856,0.4019
AppDynamics,55505955,55505087,0,"2019/04/04, 02:58:03",True,"2019/04/04, 02:58:03",1220.0,796375.0,2,curl's  --user  command line option sets up HTTP authentication for the request ( username:password ).,0.0,0.0,1.0,0.0
AppDynamics,55505955,55505087,0,"2019/04/04, 02:58:03",True,"2019/04/04, 02:58:03",1220.0,796375.0,2,"In the case of AppDynamics' Configuration API (which is a subset of their Controller API),  user1@customer1:secret  are your account credentials in the format documented  here :",0.0,0.0,1.0,0.0
AppDynamics,55002450,54993328,1,"2019/03/05, 14:04:18",False,"2019/03/05, 14:04:18",116702.0,2897748.0,0,"You should not be retrieving any embedded resources in your load test, you need to limit the scope of the embedded resources scanning to  your application under test domain only",0.0,0.0,1.0,0.0
AppDynamics,55002450,54993328,1,"2019/03/05, 14:04:18",False,"2019/03/05, 14:04:18",116702.0,2897748.0,0,"Any external domains like  googleapis  or   appdynamics  must be excluded via  URLs must match  input (lives at ""Advanced"" tab of the  HTTP Request  sampler, or even better  HTTP Request Defaults )",0.069,0.155,0.776,0.4588
AppDynamics,55002450,54993328,1,"2019/03/05, 14:04:18",False,"2019/03/05, 14:04:18",116702.0,2897748.0,0,More information:  Excluding Domains from the Load Test,0.0,0.0,1.0,0.0
AppDynamics,44071970,44038577,0,"2017/05/19, 16:57:39",False,"2017/05/19, 16:57:39",474.0,4682632.0,0,The problem with this approach is you'll have to manually do that each time.,0.172,0.0,0.828,-0.4019
AppDynamics,44071970,44038577,0,"2017/05/19, 16:57:39",False,"2017/05/19, 16:57:39",474.0,4682632.0,0,I would highly recommend just configuring your app server to automatically load the AppDynamics agent.,0.0,0.177,0.823,0.4201
AppDynamics,44071970,44038577,0,"2017/05/19, 16:57:39",False,"2017/05/19, 16:57:39",474.0,4682632.0,0,"Another option is using the universal agent, which does auto-attach:  https://docs.appdynamics.com/display/PRO43/Install+the+Universal+Agent  Doing this one off attach is never really a good idea, as you'll have to get the PID each time.",0.083,0.0,0.917,-0.3865
AppDynamics,44071970,44038577,0,"2017/05/19, 16:57:39",False,"2017/05/19, 16:57:39",474.0,4682632.0,0,"The error indicates that you are probably not running the attach as the same user the JVM is running under, but it could also be permissions or something else as well, hence I would use the methods that work all the time :)",0.039,0.14,0.821,0.7003
AppDynamics,39731234,39730780,3,"2016/09/27, 20:49:55",False,"2016/09/27, 20:49:55",6408.0,5004822.0,0,Use Dependency Injection instead of creating the actual WCF endpoints and passing them around.,0.0,0.145,0.855,0.296
AppDynamics,39731234,39730780,3,"2016/09/27, 20:49:55",False,"2016/09/27, 20:49:55",6408.0,5004822.0,0,Then mocking them up is trivial.,0.487,0.0,0.513,-0.4215
AppDynamics,39731234,39730780,3,"2016/09/27, 20:49:55",False,"2016/09/27, 20:49:55",6408.0,5004822.0,0,You would then use the interface and let DI take care of the rest!,0.0,0.212,0.788,0.5411
AppDynamics,26850353,26778994,2,"2014/11/10, 20:22:08",False,"2014/11/10, 20:22:08",4817.0,3817025.0,0,The best way to logging the requests to an external logging provider.,0.0,0.276,0.724,0.6369
AppDynamics,26850353,26778994,2,"2014/11/10, 20:22:08",False,"2014/11/10, 20:22:08",4817.0,3817025.0,0,Check out  http://docs.cloudfoundry.org/devguide/services/log-management-thirdparty-svc.html .,0.0,0.0,1.0,0.0
AppDynamics,26850353,26778994,2,"2014/11/10, 20:22:08",False,"2014/11/10, 20:22:08",4817.0,3817025.0,0,You can actually log to any http endpoint that supports POST.,0.0,0.2,0.8,0.3612
AppDynamics,26850353,26778994,2,"2014/11/10, 20:22:08",False,"2014/11/10, 20:22:08",4817.0,3817025.0,0,You can use Splunk to calculate your response time and requests per second.,0.0,0.0,1.0,0.0
AppDynamics,26850353,26778994,2,"2014/11/10, 20:22:08",False,"2014/11/10, 20:22:08",4817.0,3817025.0,0,The logs that come out of that are in real time and are streamed to your logging endpoint.,0.0,0.0,1.0,0.0
AppDynamics,26850353,26778994,2,"2014/11/10, 20:22:08",False,"2014/11/10, 20:22:08",4817.0,3817025.0,0,It contains information about the requests as well as log messages from your app.,0.0,0.139,0.861,0.2732
AppDynamics,26850353,26778994,2,"2014/11/10, 20:22:08",False,"2014/11/10, 20:22:08",4817.0,3817025.0,0,ex.,0.0,0.0,1.0,0.0
AppDynamics,27005164,26778994,2,"2014/11/19, 00:15:20",False,"2014/11/19, 00:15:20",10201.0,1585136.0,0,There are basic stats available when you run  cf app &lt;app-name&gt; .,0.0,0.0,1.0,0.0
AppDynamics,27005164,26778994,2,"2014/11/19, 00:15:20",False,"2014/11/19, 00:15:20",10201.0,1585136.0,0,"These include the memory, cpu and disk utilization of your app.",0.0,0.0,1.0,0.0
AppDynamics,27005164,26778994,2,"2014/11/19, 00:15:20",False,"2014/11/19, 00:15:20",10201.0,1585136.0,0,"You can also access these via the REST api, documented here.",0.0,0.0,1.0,0.0
AppDynamics,27005164,26778994,2,"2014/11/19, 00:15:20",False,"2014/11/19, 00:15:20",10201.0,1585136.0,0,https://s3.amazonaws.com/cc-api-docs/41397913/apps/get_detailed_stats_for_a_started_app.html,0.0,0.0,1.0,0.0
AppDynamics,27005164,26778994,2,"2014/11/19, 00:15:20",False,"2014/11/19, 00:15:20",10201.0,1585136.0,0,That's not going to help with requests per second or response time though.,0.158,0.0,0.842,-0.3089
AppDynamics,27005164,26778994,2,"2014/11/19, 00:15:20",False,"2014/11/19, 00:15:20",10201.0,1585136.0,0,"@jsloyer's solution would work for that, or you could use an APM like NewRelic, which will give you a wealth of data for virtually nothing.",0.0,0.276,0.724,0.7906
AppDynamics,27018574,26778994,1,"2014/11/19, 15:53:11",True,"2014/11/19, 15:53:11",4817.0,3817025.0,1,Yes this is pretty easy to use.,0.0,0.688,0.312,0.8316
AppDynamics,27018574,26778994,1,"2014/11/19, 15:53:11",True,"2014/11/19, 15:53:11",4817.0,3817025.0,1,"You can use Logstash as the ingesting engine, you just need the correct parser.",0.0,0.0,1.0,0.0
AppDynamics,27018574,26778994,1,"2014/11/19, 15:53:11",True,"2014/11/19, 15:53:11",4817.0,3817025.0,1,Check out  http://scottfrederick.cfapps.io/blog/2014/02/20/cloud-foundry-and-logstash  for the parser and config for ingesting cloud foundry logs.,0.0,0.0,1.0,0.0
AppDynamics,27018574,26778994,1,"2014/11/19, 15:53:11",True,"2014/11/19, 15:53:11",4817.0,3817025.0,1,I was playing around with this before and it worked quite well.,0.0,0.318,0.682,0.4927
AppDynamics,27018574,26778994,1,"2014/11/19, 15:53:11",True,"2014/11/19, 15:53:11",4817.0,3817025.0,1,Let me know if you have any issues.,0.0,0.0,1.0,0.0
AppDynamics,67137278,66872969,0,"2021/04/17, 13:46:07",False,"2021/04/17, 13:46:07",3170.0,1237595.0,0,From checking out the code for Istio -  https://github.com/istio  - this appears to be an application with components written in Go and C++ which are deployed to containers using Kubernetes.,0.0,0.0,1.0,0.0
AppDynamics,67137278,66872969,0,"2021/04/17, 13:46:07",False,"2021/04/17, 13:46:07",3170.0,1237595.0,0,This would mean that for monitoring you should be looking at:,0.0,0.0,1.0,0.0
AppDynamics,67137278,66872969,0,"2021/04/17, 13:46:07",False,"2021/04/17, 13:46:07",3170.0,1237595.0,0,You have you work cut out for you here - both SDK's would require changes to the code being ran to get visibility.,0.091,0.0,0.909,-0.2732
AppDynamics,65073399,65072887,0,"2020/11/30, 14:14:21",False,"2020/11/30, 14:14:21",116702.0,2897748.0,0,It's hard to say what's wrong without seeing your Jenkins job and JMeter  Thread Group  configuration.,0.243,0.0,0.757,-0.5423
AppDynamics,65073399,65072887,0,"2020/11/30, 14:14:21",False,"2020/11/30, 14:14:21",116702.0,2897748.0,0,"In order to apply external settings in JMeter you need to define threads, ramp-up and the number of iterations using JMeter Properties via  __P() function  like:",0.0,0.137,0.863,0.4215
AppDynamics,65073399,65072887,0,"2020/11/30, 14:14:21",False,"2020/11/30, 14:14:21",116702.0,2897748.0,0,"Once done you will be able to  override the values of these properties using  -J  command line argument , for example in Jenkins:",0.099,0.107,0.794,0.0516
AppDynamics,65073399,65072887,0,"2020/11/30, 14:14:21",False,"2020/11/30, 14:14:21",116702.0,2897748.0,0,This way you will be able to pass whatever number of virtual users/iterations without having to change your script.,0.0,0.067,0.933,0.0772
AppDynamics,65073399,65072887,0,"2020/11/30, 14:14:21",False,"2020/11/30, 14:14:21",116702.0,2897748.0,0,More information:  Apache JMeter Properties Customization Guide,0.0,0.0,1.0,0.0
AppDynamics,64909534,64881926,0,"2020/11/19, 12:13:48",False,"2020/11/19, 12:13:48",615.0,2319439.0,0,"just to make it clear, a  live connection  is (in the world of Power BI 😉) a connection to either a Power BI dataset or a SSAS tabular model.",0.0,0.098,0.902,0.3818
AppDynamics,64909534,64881926,0,"2020/11/19, 12:13:48",False,"2020/11/19, 12:13:48",615.0,2319439.0,0,"I think what you are looking for is  DirectQuery , but it is currently not supported for URL GET commands.",0.126,0.0,0.874,-0.3491
AppDynamics,64909534,64881926,0,"2020/11/19, 12:13:48",False,"2020/11/19, 12:13:48",615.0,2319439.0,0,MS docs - Power BI data sources,0.0,0.0,1.0,0.0
AppDynamics,64909534,64881926,0,"2020/11/19, 12:13:48",False,"2020/11/19, 12:13:48",615.0,2319439.0,0,To get realtime data you need to use one of the supported sources.,0.0,0.161,0.839,0.3182
AppDynamics,64909534,64881926,0,"2020/11/19, 12:13:48",False,"2020/11/19, 12:13:48",615.0,2319439.0,0,maybe AppDynamics supports direct DB access.,0.0,0.333,0.667,0.3612
AppDynamics,64909534,64881926,0,"2020/11/19, 12:13:48",False,"2020/11/19, 12:13:48",615.0,2319439.0,0,DirectQuery is supported by SQL databases.,0.0,0.315,0.685,0.3182
AppDynamics,64909534,64881926,0,"2020/11/19, 12:13:48",False,"2020/11/19, 12:13:48",615.0,2319439.0,0,Another way is to offload the data to a supported source eg.,0.0,0.187,0.813,0.3182
AppDynamics,64909534,64881926,0,"2020/11/19, 12:13:48",False,"2020/11/19, 12:13:48",615.0,2319439.0,0,SQL-db or CDS-service and then connect your pbi to that source.,0.0,0.0,1.0,0.0
AppDynamics,64443455,64442861,0,"2020/10/20, 13:40:45",True,"2020/10/20, 13:49:24",2274.0,6266192.0,1,The correct and hard way is to amend the SELinux boolean on Tomcat directories to allow Tomcat amend files created by other users.,0.055,0.154,0.791,0.3612
AppDynamics,64443455,64442861,0,"2020/10/20, 13:40:45",True,"2020/10/20, 13:49:24",2274.0,6266192.0,1,Read  here .,0.0,0.0,1.0,0.0
AppDynamics,64443455,64442861,0,"2020/10/20, 13:40:45",True,"2020/10/20, 13:49:24",2274.0,6266192.0,1,The easy and dirty solution is to  start-up process includes a step for renaming yesterdays AppDynamics logs  as user  sysXYZ  .,0.12,0.216,0.664,0.3182
AppDynamics,64443455,64442861,0,"2020/10/20, 13:40:45",True,"2020/10/20, 13:49:24",2274.0,6266192.0,1,And that way you avoid the problem.,0.495,0.0,0.505,-0.5994
AppDynamics,64443455,64442861,0,"2020/10/20, 13:40:45",True,"2020/10/20, 13:49:24",2274.0,6266192.0,1,Use  su - sysXYZ &lt;script&gt;  command or  sudo -iu sysXYZ &lt;script&gt;  command or  sudo -t &lt;Tomcat role&gt; &lt;script&gt;,0.0,0.0,1.0,0.0
AppDynamics,64443455,64442861,0,"2020/10/20, 13:40:45",True,"2020/10/20, 13:49:24",2274.0,6266192.0,1,Good luck.,0.0,1.0,0.0,0.7096
AppDynamics,55963734,55948183,0,"2019/05/03, 08:23:29",False,"2019/05/03, 08:23:29",4861.0,192923.0,0,Add  CheckedParameter=false  in the connection string to fix the issue.,0.0,0.0,1.0,0.0
AppDynamics,53293120,53271297,0,"2018/11/14, 06:16:21",False,"2018/11/14, 06:16:21",342.0,1394897.0,0,I found my answer on the Flow Force online help ( https://manual.altova.com/flowforceserver/flowforceserver/ ),0.0,0.231,0.769,0.4019
AppDynamics,53293120,53271297,0,"2018/11/14, 06:16:21",False,"2018/11/14, 06:16:21",342.0,1394897.0,0,"The Flow Force is deployed as two servers, which in a window env, can be started and stopped as windows services (can be found via ""Control Panel"" ""Administrative Tools"" Services).",0.064,0.0,0.936,-0.2263
AppDynamics,53293120,53271297,0,"2018/11/14, 06:16:21",False,"2018/11/14, 06:16:21",342.0,1394897.0,0,"With this information, I can monitor them via NAGIOS.",0.0,0.0,1.0,0.0
AppDynamics,51469112,51468443,0,"2018/07/22, 23:45:36",False,"2018/07/22, 23:45:36",45031.0,213136.0,0,"One possibility is that you've got a cached execution plan which works fine for most parameter values, or combination of parameter values, but which fails badly for certain values/combinations.",0.207,0.212,0.581,-0.4256
AppDynamics,51469112,51468443,0,"2018/07/22, 23:45:36",False,"2018/07/22, 23:45:36",45031.0,213136.0,0,You can try adding a non-filtering predicate such as  1 = 1  to your WHERE clause.,0.0,0.0,1.0,0.0
AppDynamics,51469112,51468443,0,"2018/07/22, 23:45:36",False,"2018/07/22, 23:45:36",45031.0,213136.0,0,"I've read  but haven't tested that this can be used to force a hard parse, but it may be that you need to change the value (e.g.",0.056,0.108,0.836,0.3612
AppDynamics,51469112,51468443,0,"2018/07/22, 23:45:36",False,"2018/07/22, 23:45:36",45031.0,213136.0,0,"1 = 1 ,  2 = 2 ,  3 = 3 , etc) for each execution of your query.",0.0,0.0,1.0,0.0
AppDynamics,48974838,48974684,0,"2018/02/25, 16:57:04",False,"2018/02/25, 17:12:50",49358.0,6779307.0,1,"Here's a recursive solution that yields key ""paths"".",0.0,0.277,0.723,0.3182
AppDynamics,48974838,48974684,0,"2018/02/25, 16:57:04",False,"2018/02/25, 17:12:50",49358.0,6779307.0,1,"The  (*keys, k)  syntax is available in Python versions  = 3.5, you can also use  keys + (k,)",0.0,0.0,1.0,0.0
AppDynamics,48983005,48974684,0,"2018/02/26, 08:59:10",False,"2018/02/26, 08:59:10",2251.0,5550284.0,0,"As a little modifcation to @Jon Clements code, this is what gives me what I need.",0.0,0.0,1.0,0.0
AppDynamics,48983245,48974684,0,"2018/02/26, 09:16:56",False,"2018/02/26, 09:16:56",1513.0,7352806.0,0,Try this:-,0.0,0.0,1.0,0.0
AppDynamics,47754925,47754856,0,"2017/12/11, 16:20:02",True,"2017/12/11, 16:20:02",5644.0,9908141.0,1,Application Performance Management(APM),0.0,0.0,1.0,0.0
AppDynamics,47754925,47754856,0,"2017/12/11, 16:20:02",True,"2017/12/11, 16:20:02",5644.0,9908141.0,1,In simpler terms:,0.0,0.0,1.0,0.0
AppDynamics,47754925,47754856,0,"2017/12/11, 16:20:02",True,"2017/12/11, 16:20:02",5644.0,9908141.0,1,"APM monitors the speed at which transactions are performed both by
  end-users and by the systems and network infrastructure that support a
  software application, providing an end-to-end overview of potential
  bottlenecks and service interruptions.",0.074,0.074,0.852,0.0
AppDynamics,47754925,47754856,0,"2017/12/11, 16:20:02",True,"2017/12/11, 16:20:02",5644.0,9908141.0,1,"In pragmatic terms, this typically involves the use of a suite of software tools—or a single integrated SaaS or on-premises tool—to view and diagnose an application’s speed, reliability, and other performance metrics in order to maintain an optimal level of service.",0.0,0.062,0.938,0.3612
AppDynamics,47754925,47754856,0,"2017/12/11, 16:20:02",True,"2017/12/11, 16:20:02",5644.0,9908141.0,1,"Here is
 Wiki description.",0.0,0.0,1.0,0.0
AppDynamics,43064247,42076632,0,"2017/03/28, 11:21:22",False,"2017/03/28, 11:21:22",324.0,7220665.0,0,You don't need a machine agent extension (and additional custom metrics) to capture if a JVM goes down.,0.0,0.0,1.0,0.0
AppDynamics,43064247,42076632,0,"2017/03/28, 11:21:22",False,"2017/03/28, 11:21:22",324.0,7220665.0,0,The machine agent delivers these kind of information out of the box.,0.0,0.0,1.0,0.0
AppDynamics,43064247,42076632,0,"2017/03/28, 11:21:22",False,"2017/03/28, 11:21:22",324.0,7220665.0,0,"You have to start the java process with the app server agent and associate the application, that is instrumented with the machine agent.",0.0,0.0,1.0,0.0
AppDynamics,43064247,42076632,0,"2017/03/28, 11:21:22",False,"2017/03/28, 11:21:22",324.0,7220665.0,0,Now you can go to the  application dashboard -&gt; Events tab  and check for the  JVM Crash Event .,0.137,0.0,0.863,-0.4019
AppDynamics,43064247,42076632,0,"2017/03/28, 11:21:22",False,"2017/03/28, 11:21:22",324.0,7220665.0,0,See also  here  and  here .,0.0,0.0,1.0,0.0
AppDynamics,36874297,36871506,0,"2016/04/26, 22:41:42",False,"2016/04/26, 22:41:42",14666.0,2067527.0,1,I'm not familiar with the AppDynamics output.,0.0,0.0,1.0,0.0
AppDynamics,36874297,36871506,0,"2016/04/26, 22:41:42",False,"2016/04/26, 22:41:42",14666.0,2067527.0,1,I assume that's a cumulative view of Threads and their sleep times.,0.0,0.0,1.0,0.0
AppDynamics,36874297,36871506,0,"2016/04/26, 22:41:42",False,"2016/04/26, 22:41:42",14666.0,2067527.0,1,So Threads get reused and so the sleep times add up.,0.0,0.0,1.0,0.0
AppDynamics,36874297,36871506,0,"2016/04/26, 22:41:42",False,"2016/04/26, 22:41:42",14666.0,2067527.0,1,"In some cases, a Thread gets a connection directly, without any waiting and in another call the Thread has to wait until the connection can be provided.",0.0,0.0,1.0,0.0
AppDynamics,36874297,36871506,0,"2016/04/26, 22:41:42",False,"2016/04/26, 22:41:42",14666.0,2067527.0,1,"The wait duration depends on when a connection becomes available, or the wait limit is hit.",0.0,0.0,1.0,0.0
AppDynamics,36874297,36871506,0,"2016/04/26, 22:41:42",False,"2016/04/26, 22:41:42",14666.0,2067527.0,1,"Your screenshot shows a Thread, which waited  172ms .",0.0,0.0,1.0,0.0
AppDynamics,36874297,36871506,0,"2016/04/26, 22:41:42",False,"2016/04/26, 22:41:42",14666.0,2067527.0,1,"Assuming the  sleep  is only called within the Jedis/Redis invocation path, the Thread waited  172ms  in total to get a connection.",0.0,0.0,1.0,0.0
AppDynamics,36874297,36871506,0,"2016/04/26, 22:41:42",False,"2016/04/26, 22:41:42",14666.0,2067527.0,1,"Another Thread, which waited  530ms  looks to me as if the first attempt to get a connection wasn't successful (which explains the first  500ms ) and on a second attempt, it had to wait for  30ms .",0.088,0.0,0.912,-0.4717
AppDynamics,36874297,36871506,0,"2016/04/26, 22:41:42",False,"2016/04/26, 22:41:42",14666.0,2067527.0,1,It could also be that it waited 2x for  265ms .,0.0,0.0,1.0,0.0
AppDynamics,36874297,36871506,0,"2016/04/26, 22:41:42",False,"2016/04/26, 22:41:42",14666.0,2067527.0,1,Sidenote:,0.0,0.0,1.0,0.0
AppDynamics,36874297,36871506,0,"2016/04/26, 22:41:42",False,"2016/04/26, 22:41:42",14666.0,2067527.0,1,1000+ connections could severely limit scalability.,0.375,0.0,0.625,-0.4588
AppDynamics,36874297,36871506,0,"2016/04/26, 22:41:42",False,"2016/04/26, 22:41:42",14666.0,2067527.0,1,Spring Data Redis also supports other drivers which don't require pooling but work with fewer connections (see  Spring Data Redis Connectors  and  here ).,0.0,0.071,0.929,0.1901
AppDynamics,36460104,36459842,7,"2016/04/06, 22:01:39",False,"2016/04/06, 22:01:39",6806.0,807193.0,0,You are getting this error as you have not provided the  values required by app to work.,0.264,0.0,0.736,-0.6579
AppDynamics,36460104,36459842,7,"2016/04/06, 22:01:39",False,"2016/04/06, 22:01:39",6806.0,807193.0,0,You need to add the values in  PreferenceConstants  .,0.0,0.278,0.722,0.4019
AppDynamics,38212864,36459842,0,"2016/07/06, 00:16:07",False,"2016/07/06, 01:16:50",4169.0,6146338.0,0,Did you get it working?,0.0,0.0,1.0,0.0
AppDynamics,38212864,36459842,0,"2016/07/06, 00:16:07",False,"2016/07/06, 01:16:50",4169.0,6146338.0,0,"Actually, you know.",0.0,0.0,1.0,0.0
AppDynamics,38212864,36459842,0,"2016/07/06, 00:16:07",False,"2016/07/06, 01:16:50",4169.0,6146338.0,0,You are trying to connect to server and app key is the one generated at server and has to be implemented at end application.,0.0,0.0,1.0,0.0
AppDynamics,38212864,36459842,0,"2016/07/06, 00:16:07",False,"2016/07/06, 01:16:50",4169.0,6146338.0,0,"Since this is a template, there is no server allocated by the code writer to test and so you don't have app key.",0.095,0.0,0.905,-0.296
AppDynamics,38212864,36459842,0,"2016/07/06, 00:16:07",False,"2016/07/06, 01:16:50",4169.0,6146338.0,0,"Try exploring IoT frameworks by IBM any other or even you can try open source, Kaa.",0.0,0.0,1.0,0.0
AppDynamics,38212864,36459842,0,"2016/07/06, 00:16:07",False,"2016/07/06, 01:16:50",4169.0,6146338.0,0,It will be worth if want explore this kind of cloud dependent apps.,0.0,0.225,0.775,0.296
AppDynamics,34131925,34103927,2,"2015/12/07, 12:58:09",False,"2015/12/07, 12:58:09",11383.0,18591.0,0,This sounds like you open a transaction and get a database connection always and only after check cache content.,0.0,0.135,0.865,0.3612
AppDynamics,34131925,34103927,2,"2015/12/07, 12:58:09",False,"2015/12/07, 12:58:09",11383.0,18591.0,0,"But since you have opened the transactional context, it will be closed, issuing the commit, as no exceptions happened.",0.124,0.124,0.752,0.0
AppDynamics,34131925,34103927,2,"2015/12/07, 12:58:09",False,"2015/12/07, 12:58:09",11383.0,18591.0,0,You most likely need to move the checking of the cache outside of the transactional context.,0.0,0.0,1.0,0.0
AppDynamics,34131925,34103927,2,"2015/12/07, 12:58:09",False,"2015/12/07, 12:58:09",11383.0,18591.0,0,"Of course, confirming this depends on your application code not included in the question.",0.0,0.0,1.0,0.0
AppDynamics,34137555,34103927,0,"2015/12/07, 17:44:30",True,"2015/12/07, 17:44:30",1220.0,526781.0,1,The transaction demarcation related invokes on the  Connection  will happen nevertheless.,0.0,0.0,1.0,0.0
AppDynamics,34137555,34103927,0,"2015/12/07, 17:44:30",True,"2015/12/07, 17:44:30",1220.0,526781.0,1,You could use something like Spring's  LazyConnectionDataSourceProxy  (doc'ed  here ) to avoid having these sent when they are not required.,0.101,0.115,0.783,0.0772
AppDynamics,28040996,28027105,4,"2015/01/20, 10:48:34",False,"2015/01/20, 10:48:34",13083.0,1880810.0,0,The  asadmin  manual page says the following:,0.0,0.0,1.0,0.0
AppDynamics,28040996,28027105,4,"2015/01/20, 10:48:34",False,"2015/01/20, 10:48:34",13083.0,1880810.0,0,"For the Windows operating system in single mode, a backslash is
  required to escape the colon and the backslash characters.",0.0,0.086,0.914,0.1779
AppDynamics,28040996,28027105,4,"2015/01/20, 10:48:34",False,"2015/01/20, 10:48:34",13083.0,1880810.0,0,So try the following:,0.0,0.0,1.0,0.0
AppDynamics,28040996,28027105,4,"2015/01/20, 10:48:34",False,"2015/01/20, 10:48:34",13083.0,1880810.0,0,See also:,0.0,0.0,1.0,0.0
AppDynamics,61510109,61509776,0,"2020/04/29, 22:29:43",False,"2020/04/29, 22:29:43",6328.0,10726850.0,1,"In theory, yes: if Resource Manager has been enabled it could be the case that different Resource Manager plans have such an impact but experience shows that this feature is seldom used.",0.0,0.056,0.944,0.2144
AppDynamics,61510109,61509776,0,"2020/04/29, 22:29:43",False,"2020/04/29, 22:29:43",6328.0,10726850.0,1,In practive this kind of difference can have many cause:-,0.0,0.0,1.0,0.0
AppDynamics,61510109,61509776,0,"2020/04/29, 22:29:43",False,"2020/04/29, 22:29:43",6328.0,10726850.0,1,The first thing to look at database level is something similar to Statspack report (or AWR if licensing allows) to compare database configuration and activity.,0.0,0.0,1.0,0.0
AppDynamics,61510109,61509776,0,"2020/04/29, 22:29:43",False,"2020/04/29, 22:29:43",6328.0,10726850.0,1,"And don't forget that application performance is not only database performance it depends also on application server, network and front-end.",0.0,0.081,0.919,0.1695
AppDynamics,37659288,37658175,0,"2016/06/06, 16:52:28",False,"2016/06/06, 17:01:30",5857.0,3641043.0,0,I think this should do what you want.,0.0,0.178,0.822,0.0772
AppDynamics,37659288,37658175,0,"2016/06/06, 16:52:28",False,"2016/06/06, 17:01:30",5857.0,3641043.0,0,"I'm afraid it's not tested, but the principle is this:",0.0,0.0,1.0,0.0
AppDynamics,37659288,37658175,0,"2016/06/06, 16:52:28",False,"2016/06/06, 17:01:30",5857.0,3641043.0,0,"1) if the Set-Cookie header starts with ADRUM, then set the env variable ADRUM_cookie.",0.0,0.0,1.0,0.0
AppDynamics,37659288,37658175,0,"2016/06/06, 16:52:28",False,"2016/06/06, 17:01:30",5857.0,3641043.0,0,"2) if the ADRUM_cookie env variable is /not/ set, edit the Set-Cookie header.",0.0,0.0,1.0,0.0
AppDynamics,32768918,32768813,10,"2015/09/24, 22:00:00",True,"2016/02/03, 19:13:19",3381.0,868947.0,1,"If we are talking about ""run tool-get result"" the best option -  Java Mission Control .",0.0,0.244,0.756,0.6369
AppDynamics,32768918,32768813,10,"2015/09/24, 22:00:00",True,"2016/02/03, 19:13:19",3381.0,868947.0,1,It's free in test environment.,0.0,0.452,0.548,0.5106
AppDynamics,32768918,32768813,10,"2015/09/24, 22:00:00",True,"2016/02/03, 19:13:19",3381.0,868947.0,1,You need to pay only for some features in production.,0.135,0.0,0.865,-0.1027
AppDynamics,32768918,32768813,10,"2015/09/24, 22:00:00",True,"2016/02/03, 19:13:19",3381.0,868947.0,1,It's much better than old VisualVM.,0.0,0.367,0.633,0.4404
AppDynamics,32768918,32768813,10,"2015/09/24, 22:00:00",True,"2016/02/03, 19:13:19",3381.0,868947.0,1,You can write a data to file using  Flight Recorder .,0.0,0.0,1.0,0.0
AppDynamics,32768918,32768813,10,"2015/09/24, 22:00:00",True,"2016/02/03, 19:13:19",3381.0,868947.0,1,You can setup start point and duration.,0.0,0.0,1.0,0.0
AppDynamics,32768918,32768813,10,"2015/09/24, 22:00:00",True,"2016/02/03, 19:13:19",3381.0,868947.0,1,You just need to start your application like this:,0.0,0.238,0.762,0.3612
AppDynamics,32768918,32768813,10,"2015/09/24, 22:00:00",True,"2016/02/03, 19:13:19",3381.0,868947.0,1,"-XX:+UnlockCommercialFeatures -XX:+FlightRecorder -XX:StartFlightRecording=duration=60s,filename=myrecording.jfr",0.0,0.0,1.0,0.0
AppDynamics,67107284,66992681,0,"2021/04/15, 14:03:49",False,"2021/04/15, 14:03:49",3170.0,1237595.0,0,I think what you are wanting is info around instrumentation of the front end of an Angular SPA.,0.0,0.0,1.0,0.0
AppDynamics,67107284,66992681,0,"2021/04/15, 14:03:49",False,"2021/04/15, 14:03:49",3170.0,1237595.0,0,Please see documentation here:  https://docs.appdynamics.com/display/PRO21/Monitor+Single-Page+Applications  - Angular is mentioned as being available using SPA2.,0.0,0.161,0.839,0.3182
AppDynamics,67107284,66992681,0,"2021/04/15, 14:03:49",False,"2021/04/15, 14:03:49",3170.0,1237595.0,0,"(Note for the front end we use Javascript BRUM for monitoring, general docs are here:  https://docs.appdynamics.com/display/PRO21/Browser+Monitoring  - this is a seperate part of the product than the APM used to monitor backends)",0.0,0.0,1.0,0.0
AppDynamics,50694003,50682773,2,"2018/06/05, 10:03:15",False,"2018/06/05, 10:03:15",30936.0,6587650.0,0,"Since some packages are not working with different OS configurations, you need to setup a new build agent.",0.0,0.0,1.0,0.0
AppDynamics,50694003,50682773,2,"2018/06/05, 10:03:15",False,"2018/06/05, 10:03:15",30936.0,6587650.0,0,Deploy an agent on Windows,0.0,0.0,1.0,0.0
Datadog,52609589,49671175,1,"2018/10/02, 16:40:17",False,"2018/11/16, 12:34:17",107.0,4772912.0,2,It's possible to solve this by to changing the query by using zero interpolation.,0.0,0.122,0.878,0.2023
Datadog,52609589,49671175,1,"2018/10/02, 16:40:17",False,"2018/11/16, 12:34:17",107.0,4772912.0,2,"You can put "".fill(zero)"" behind your query in the json or choose the option from the UI.",0.0,0.0,1.0,0.0
Datadog,52609589,49671175,1,"2018/10/02, 16:40:17",False,"2018/11/16, 12:34:17",107.0,4772912.0,2,EDIT:,0.0,0.0,1.0,0.0
Datadog,52609589,49671175,1,"2018/10/02, 16:40:17",False,"2018/11/16, 12:34:17",107.0,4772912.0,2,"You're right, the interpolation is not working when no data is available.",0.0,0.146,0.854,0.2235
Datadog,52609589,49671175,1,"2018/10/02, 16:40:17",False,"2018/11/16, 12:34:17",107.0,4772912.0,2,I had the same problem in the end.,0.31,0.0,0.69,-0.4019
Datadog,52609589,49671175,1,"2018/10/02, 16:40:17",False,"2018/11/16, 12:34:17",107.0,4772912.0,2,Support of Datadog said it isn't possible to show zero when there is no data for a metric.,0.111,0.136,0.754,0.128
Datadog,52609589,49671175,1,"2018/10/02, 16:40:17",False,"2018/11/16, 12:34:17",107.0,4772912.0,2,Now there is a feature request made for it.,0.0,0.0,1.0,0.0
Datadog,52609589,49671175,1,"2018/10/02, 16:40:17",False,"2018/11/16, 12:34:17",107.0,4772912.0,2,"It would be nice if more people will request for this feature, so it will be prioritized.",0.0,0.149,0.851,0.4215
Datadog,52609589,49671175,1,"2018/10/02, 16:40:17",False,"2018/11/16, 12:34:17",107.0,4772912.0,2,"Nonetheless I have tried to create a workaround by adding a second metric that always has data as a second query and added a formula ((b - b) + a) that negates the second query, but when there is data in the intended query it's show in the graph.",0.0,0.036,0.964,0.1406
Datadog,52609589,49671175,1,"2018/10/02, 16:40:17",False,"2018/11/16, 12:34:17",107.0,4772912.0,2,This will result in a zero line when there is no data available.,0.167,0.0,0.833,-0.296
Datadog,52609589,49671175,1,"2018/10/02, 16:40:17",False,"2018/11/16, 12:34:17",107.0,4772912.0,2,Scenario without data:,0.0,0.0,1.0,0.0
Datadog,52609589,49671175,1,"2018/10/02, 16:40:17",False,"2018/11/16, 12:34:17",107.0,4772912.0,2,"The only problem is that when you have a data in the intended query, it looks ugly and the zero line is gone.",0.231,0.0,0.769,-0.7184
Datadog,52609589,49671175,1,"2018/10/02, 16:40:17",False,"2018/11/16, 12:34:17",107.0,4772912.0,2,As you see in the following screenshot.,0.0,0.0,1.0,0.0
Datadog,52609589,49671175,1,"2018/10/02, 16:40:17",False,"2018/11/16, 12:34:17",107.0,4772912.0,2,Scenario with data:,0.0,0.0,1.0,0.0
Datadog,52609589,49671175,1,"2018/10/02, 16:40:17",False,"2018/11/16, 12:34:17",107.0,4772912.0,2,"Conclusion: 
The workaround is not perfect, but it will work for some situation.",0.143,0.0,0.857,-0.2498
Datadog,52609589,49671175,1,"2018/10/02, 16:40:17",False,"2018/11/16, 12:34:17",107.0,4772912.0,2,"For example, filling up query values with zero instead of (no data).",0.0,0.197,0.803,0.4019
Datadog,52609589,49671175,1,"2018/10/02, 16:40:17",False,"2018/11/16, 12:34:17",107.0,4772912.0,2,I hope this is a bit better answer to the problem.,0.186,0.4,0.414,0.4767
Datadog,53345774,49671175,0,"2018/11/16, 23:38:15",False,"2018/11/16, 23:38:15",1371.0,5540166.0,21,"how about  the ""default"" function ?",0.0,0.0,1.0,0.0
Datadog,53345774,49671175,0,"2018/11/16, 23:38:15",False,"2018/11/16, 23:38:15",1371.0,5540166.0,21,"so  default(sum:foo.bar{hello:world} by {baz}, 0)  or some such?",0.0,0.0,1.0,0.0
Datadog,58276947,49671175,0,"2019/10/07, 23:49:14",False,"2020/08/28, 05:49:42",452.0,1655072.0,6,There is now a  default_zero()  function that can be used in Datadog by modifying through JSON directly.,0.0,0.0,1.0,0.0
Datadog,61641189,49671175,0,"2020/05/06, 20:15:31",False,"2020/05/06, 20:15:31",47308.0,3141234.0,7,The  default_zero()  function does what you're looking for.,0.0,0.0,1.0,0.0
Datadog,61641189,49671175,0,"2020/05/06, 20:15:31",False,"2020/05/06, 20:15:31",47308.0,3141234.0,7,"You can type it in manually, as  stephenlechner suggests .",0.0,0.0,1.0,0.0
Datadog,61641189,49671175,0,"2020/05/06, 20:15:31",False,"2020/05/06, 20:15:31",47308.0,3141234.0,7,There's another way I found:,0.0,0.0,1.0,0.0
Datadog,61641189,49671175,0,"2020/05/06, 20:15:31",False,"2020/05/06, 20:15:31",47308.0,3141234.0,7,"When you save the graph and reopen, you'll see that things have been reshuffled a little, and you'll see a ""default 0"" section tagged onto the end of the metric's definition.",0.0,0.103,0.897,0.4939
Datadog,66855333,55848522,0,"2021/03/29, 16:43:11",False,"2021/03/29, 16:43:11",336.0,7243426.0,0,Metrics queries now support wildcards.,0.0,0.403,0.597,0.4019
Datadog,66855333,55848522,0,"2021/03/29, 16:43:11",False,"2021/03/29, 16:43:11",336.0,7243426.0,0,"Example 1: Getting all the requests with a status tag starting with  2 :
 http.server.requests.count{status:2*}",0.0,0.0,1.0,0.0
Datadog,66855333,55848522,0,"2021/03/29, 16:43:11",False,"2021/03/29, 16:43:11",336.0,7243426.0,0,"Example 1: Getting all the requests with a service tag ending with  mongo :
 http.server.requests.count{service:*mongo}",0.0,0.0,1.0,0.0
Datadog,66855333,55848522,0,"2021/03/29, 16:43:11",False,"2021/03/29, 16:43:11",336.0,7243426.0,0,"Example 3 (advanced): Getting all the requests with a service tag starting with  blob  and ending with  postgres :
 http.server.requests.count{service:blob*,service:*postgres} 
 (this will match  service:blob-foo-postgres  and  service:blob_bar_postgres  but not  service:my_name_postgres )",0.0,0.0,1.0,0.0
Datadog,34400454,34398692,2,"2015/12/21, 18:41:39",True,"2015/12/21, 18:41:39",7984.0,750117.0,11,I've finally found a dropwizzard module that integrates this library with datadog:  metrics-datadog,0.0,0.0,1.0,0.0
Datadog,34400454,34398692,2,"2015/12/21, 18:41:39",True,"2015/12/21, 18:41:39",7984.0,750117.0,11,I've created a Spring configuration class that creates and initializes this Reporter using properties of my YAML.,0.0,0.227,0.773,0.4767
Datadog,34400454,34398692,2,"2015/12/21, 18:41:39",True,"2015/12/21, 18:41:39",7984.0,750117.0,11,Just insert this dependency in your pom:,0.0,0.0,1.0,0.0
Datadog,34400454,34398692,2,"2015/12/21, 18:41:39",True,"2015/12/21, 18:41:39",7984.0,750117.0,11,Add this configuration to your YAML:,0.0,0.0,1.0,0.0
Datadog,34400454,34398692,2,"2015/12/21, 18:41:39",True,"2015/12/21, 18:41:39",7984.0,750117.0,11,and add this configuration class to your project:,0.0,0.0,1.0,0.0
Datadog,34400755,34398692,3,"2015/12/21, 18:57:48",False,"2015/12/21, 18:57:48",2073.0,3239981.0,2,"If JMX is an option for you, you may use the  JMX dropwizrd reporter  combined with  java datalog integration",0.0,0.0,1.0,0.0
Datadog,49444331,34398692,3,"2018/03/23, 09:18:59",False,"2018/03/23, 12:50:05",3355.0,607038.0,6,It seems that Spring Boot 2.x added several monitoring system into its metrics.,0.0,0.0,1.0,0.0
Datadog,49444331,34398692,3,"2018/03/23, 09:18:59",False,"2018/03/23, 12:50:05",3355.0,607038.0,6,DataDog is one of them supported by  micrometer.io .,0.0,0.247,0.753,0.3182
Datadog,49444331,34398692,3,"2018/03/23, 09:18:59",False,"2018/03/23, 12:50:05",3355.0,607038.0,6,See reference documentation:  https://docs.spring.io/spring-boot/docs/current/reference/html/production-ready-metrics.html#production-ready-metrics-export-newrelic,0.0,0.0,1.0,0.0
Datadog,49444331,34398692,3,"2018/03/23, 09:18:59",False,"2018/03/23, 12:50:05",3355.0,607038.0,6,"For Spring Boot 1.x, you can use back-ported package:",0.0,0.0,1.0,0.0
Datadog,49444331,34398692,3,"2018/03/23, 09:18:59",False,"2018/03/23, 12:50:05",3355.0,607038.0,6,compile 'io.micrometer:micrometer-spring-legacy:latest.release',0.0,0.0,1.0,0.0
Datadog,37013010,37010163,1,"2016/05/03, 22:41:22",True,"2016/05/03, 22:41:22",12342.0,496289.0,10,Confirmed on IRC (#datadog on freenode) that:,0.0,0.0,1.0,0.0
Datadog,37013010,37010163,1,"2016/05/03, 22:41:22",True,"2016/05/03, 22:41:22",12342.0,496289.0,10,Datadog doesn't support multiple Y-axis at this time.,0.244,0.0,0.756,-0.3089
Datadog,41817123,37010163,0,"2017/01/24, 00:52:07",False,"2017/01/24, 00:52:07",15978.0,770425.0,0,"If the two axis have the same units but different degrees (10 vs 10 million), then using a non-linear scale such as  log  might provide what you need:",0.0,0.0,1.0,0.0
Datadog,41817123,37010163,0,"2017/01/24, 00:52:07",False,"2017/01/24, 00:52:07",15978.0,770425.0,0,https://help.datadoghq.com/hc/en-us/articles/203038729-Is-it-possible-to-adjust-the-y-axis-for-my-graphs-,0.0,0.0,1.0,0.0
Datadog,64150659,37010163,2,"2020/10/01, 10:13:05",False,"2020/10/01, 10:13:05",399.0,1619555.0,3,They do allow dual y-axis now,0.0,0.275,0.725,0.2263
Datadog,64150659,37010163,2,"2020/10/01, 10:13:05",False,"2020/10/01, 10:13:05",399.0,1619555.0,3,https://docs.datadoghq.com/dashboards/widgets/timeseries/#y-axis-controls,0.0,0.0,1.0,0.0
Datadog,64150659,37010163,2,"2020/10/01, 10:13:05",False,"2020/10/01, 10:13:05",399.0,1619555.0,3,Introducing dual Y-axis for time series widgets.,0.0,0.0,1.0,0.0
Datadog,64150659,37010163,2,"2020/10/01, 10:13:05",False,"2020/10/01, 10:13:05",399.0,1619555.0,3,"The time series widget on dashboards now support dual y-axis, making it easier than ever to compare two sets of data on a single graph.",0.0,0.2,0.8,0.6705
Datadog,64150659,37010163,2,"2020/10/01, 10:13:05",False,"2020/10/01, 10:13:05",399.0,1619555.0,3,"By removing the need to create separate graphs, your dashboards can show even more valuable information viewable at a glance.",0.0,0.244,0.756,0.6697
Datadog,35113402,35111147,2,"2016/01/31, 13:39:25",False,"2016/01/31, 13:39:25",4968.0,19892.0,3,If  dd-agent  listens on  localhost  it can receive data only from localhost (127.0.0.1).,0.0,0.0,1.0,0.0
Datadog,35113402,35111147,2,"2016/01/31, 13:39:25",False,"2016/01/31, 13:39:25",4968.0,19892.0,3,Try to change the  dd-agent  host to  0.0.0.0  instead of  localhost .,0.0,0.0,1.0,0.0
Datadog,35113402,35111147,2,"2016/01/31, 13:39:25",False,"2016/01/31, 13:39:25",4968.0,19892.0,3,We are using  docker-dd-agent  and it works OOTB.,0.0,0.0,1.0,0.0
Datadog,35133077,35111147,3,"2016/02/01, 16:31:05",False,"2016/02/01, 16:31:05",111.0,5868112.0,11,You will need to set  non_local_traffic: yes  in your  /etc/dd-agent/datadog.conf  file.,0.0,0.213,0.787,0.4019
Datadog,35133077,35111147,3,"2016/02/01, 16:31:05",False,"2016/02/01, 16:31:05",111.0,5868112.0,11,Otherwise the agent will reject metrics from containers.,0.278,0.0,0.722,-0.4019
Datadog,35133077,35111147,3,"2016/02/01, 16:31:05",False,"2016/02/01, 16:31:05",111.0,5868112.0,11,"After setting, you will need to restart the agent for the change to take effect:  sudo /etc/init.d/datadog-agent restart  or  sudo service datadog-agent restart",0.0,0.0,1.0,0.0
Datadog,35133077,35111147,3,"2016/02/01, 16:31:05",False,"2016/02/01, 16:31:05",111.0,5868112.0,11,The  docker-dd-agent  image enables  non_local_traffic: yes  by default.,0.0,0.278,0.722,0.4019
Datadog,39639402,35111147,0,"2016/09/22, 15:43:05",False,"2016/09/22, 15:43:05",296.0,166816.0,0,You don't actually want to use the IP of the host in this case.,0.086,0.0,0.914,-0.0572
Datadog,39639402,35111147,0,"2016/09/22, 15:43:05",False,"2016/09/22, 15:43:05",296.0,166816.0,0,"If you're running the docker dd-agent, there are two environment variables you can tap into:",0.0,0.0,1.0,0.0
Datadog,39639402,35111147,0,"2016/09/22, 15:43:05",False,"2016/09/22, 15:43:05",296.0,166816.0,0,"statsd.connect(DOGSTATSD_PORT_8125_UDP_ADDR, DOGSTATSD_PORT_8125_UDP_PORT)",0.0,0.0,1.0,0.0
Datadog,39639402,35111147,0,"2016/09/22, 15:43:05",False,"2016/09/22, 15:43:05",296.0,166816.0,0,That should do the trick.,0.231,0.0,0.769,-0.0516
Datadog,39639402,35111147,0,"2016/09/22, 15:43:05",False,"2016/09/22, 15:43:05",296.0,166816.0,0,"If not, you should be able to find the relevant info to your problem in  this section of the Datadog docs .",0.119,0.0,0.881,-0.4019
Datadog,39639402,35111147,0,"2016/09/22, 15:43:05",False,"2016/09/22, 15:43:05",296.0,166816.0,0,"Also, I should point out that the only Python library that Datadog shows in their docs is  datadogpy .",0.0,0.0,1.0,0.0
Datadog,59791963,57918254,0,"2020/01/17, 19:12:16",True,"2020/01/17, 19:12:16",10672.0,75801.0,10,"I asked DataDog support, and apparently as of January 2020 this is not possible, but is a feature request in their backlog.",0.0,0.089,0.911,0.2144
Datadog,59791963,57918254,0,"2020/01/17, 19:12:16",True,"2020/01/17, 19:12:16",10672.0,75801.0,10,"I know this is not a great answer to the question but if I hear that this changes, I will update my answer.",0.107,0.0,0.893,-0.284
Datadog,64519899,35608127,0,"2020/10/25, 04:57:48",False,"2020/10/25, 04:57:48",493.0,244037.0,0,There doesn't appear to be a Crashalytics direct integration yet.,0.0,0.0,1.0,0.0
Datadog,64519899,35608127,0,"2020/10/25, 04:57:48",False,"2020/10/25, 04:57:48",493.0,244037.0,0,Datadog maintains mobile SDKs that can be included in mobile clients to produce metrics and logs to the platform.,0.0,0.0,1.0,0.0
Datadog,64519899,35608127,0,"2020/10/25, 04:57:48",False,"2020/10/25, 04:57:48",493.0,244037.0,0,"For iOS Logs, see here:  https://docs.datadoghq.com/logs/log_collection/ios/",0.0,0.0,1.0,0.0
Datadog,64519899,35608127,0,"2020/10/25, 04:57:48",False,"2020/10/25, 04:57:48",493.0,244037.0,0,For Android Logs:  https://docs.datadoghq.com/logs/log_collection/android/,0.0,0.0,1.0,0.0
Datadog,64519899,35608127,0,"2020/10/25, 04:57:48",False,"2020/10/25, 04:57:48",493.0,244037.0,0,"There's also a public Android SDK for Real User Monitoring, which can be read here:  https://docs.datadoghq.com/real_user_monitoring/android/",0.0,0.0,1.0,0.0
Datadog,64519899,35608127,0,"2020/10/25, 04:57:48",False,"2020/10/25, 04:57:48",493.0,244037.0,0,"And the announcement, with a link for the private beta for iOS signup here:  https://www.datadoghq.com/blog/datadog-mobile-rum/",0.0,0.0,1.0,0.0
Datadog,42023919,40741803,0,"2017/02/03, 14:05:43",True,"2017/02/03, 14:05:43",1199.0,2525626.0,4,"After speaking to the support team at DataDog, I managed to find out the following information relating to what the no_pod pods were.",0.0,0.114,0.886,0.4019
Datadog,42023919,40741803,0,"2017/02/03, 14:05:43",True,"2017/02/03, 14:05:43",1199.0,2525626.0,4,"Our Kubernetes check is getting the list of containers from the Kubernetes API, which exposes aggregated data.",0.086,0.0,0.914,-0.128
Datadog,42023919,40741803,0,"2017/02/03, 14:05:43",True,"2017/02/03, 14:05:43",1199.0,2525626.0,4,"In the metric explorer configuration here, you can see a couple of containers named /docker and / that are getting picked up along with the other containers.",0.0,0.0,1.0,0.0
Datadog,42023919,40741803,0,"2017/02/03, 14:05:43",True,"2017/02/03, 14:05:43",1199.0,2525626.0,4,Metrics with pod_name:no_pod that come from container_name:/ and container_name:/docker  are just metrics aggregated across multiple containers.,0.0,0.0,1.0,0.0
Datadog,42023919,40741803,0,"2017/02/03, 14:05:43",True,"2017/02/03, 14:05:43",1199.0,2525626.0,4,(So it makes sense that these are the highest values in your graphs.),0.0,0.184,0.816,0.4019
Datadog,42023919,40741803,0,"2017/02/03, 14:05:43",True,"2017/02/03, 14:05:43",1199.0,2525626.0,4,"If you don't want your graphs to show these aggregated container metrics though, you can clone the dashboard and then exclude these pods from the query.",0.115,0.0,0.885,-0.2783
Datadog,42023919,40741803,0,"2017/02/03, 14:05:43",True,"2017/02/03, 14:05:43",1199.0,2525626.0,4,"To do so, on the cloned dashboard, just edit the query in the JSON tab, and in the tag scope, add !pod_name:no_pod.",0.0,0.0,1.0,0.0
Datadog,42023919,40741803,0,"2017/02/03, 14:05:43",True,"2017/02/03, 14:05:43",1199.0,2525626.0,4,So it appears that these pods are the docker and root level containers running outside of the cluster and will always display unless you want to filter them out specifically which I now do.,0.0,0.039,0.961,0.0772
Datadog,42023919,40741803,0,"2017/02/03, 14:05:43",True,"2017/02/03, 14:05:43",1199.0,2525626.0,4,Many thanks to the support guys at DataDog for looking into the issue for me and giving me a great explanation as to what the pods were and essentially confirming that I can just safely filter these out and not worry about them.,0.0,0.336,0.664,0.9494
Datadog,42935965,42927552,0,"2017/03/21, 20:59:47",False,"2017/03/21, 20:59:47",2676.0,1230594.0,0,The free text editor you have in the screenshot is for metric queries.,0.0,0.216,0.784,0.5106
Datadog,42935965,42927552,0,"2017/03/21, 20:59:47",False,"2017/03/21, 20:59:47",2676.0,1230594.0,0,Events in graphs are added as overlay to show when events happened over time.,0.0,0.0,1.0,0.0
Datadog,42935965,42927552,0,"2017/03/21, 20:59:47",False,"2017/03/21, 20:59:47",2676.0,1230594.0,0,"There is no widget, as of now, that shows a single value for the number of times an event occurred.",0.1,0.169,0.731,0.128
Datadog,42935965,42927552,0,"2017/03/21, 20:59:47",False,"2017/03/21, 20:59:47",2676.0,1230594.0,0,"But you can use the event timeline widget, which will show a timeline of events grouped by status and bucketed over a defined period of time.",0.0,0.0,1.0,0.0
Datadog,42935965,42927552,0,"2017/03/21, 20:59:47",False,"2017/03/21, 20:59:47",2676.0,1230594.0,0,See below:,0.0,0.0,1.0,0.0
Datadog,42999464,42927552,0,"2017/03/24, 14:26:09",False,"2017/03/24, 14:26:09",1929.0,2599875.0,3,"Well I realized that the  query value  only works with metrics, so to create a counter we can emit metrics with  value: 1  and then count them with the  rollup(sum, 60)  function.",0.0,0.271,0.729,0.8062
Datadog,42999464,42927552,0,"2017/03/24, 14:26:09",False,"2017/03/24, 14:26:09",1929.0,2599875.0,3,"dog.emit_point('some.event.name', 1)",0.0,0.0,1.0,0.0
Datadog,42999464,42927552,0,"2017/03/24, 14:26:09",False,"2017/03/24, 14:26:09",1929.0,2599875.0,3,"sum:some.event.name{*}.rollup(sum, 60)",0.0,0.0,1.0,0.0
Datadog,42999464,42927552,0,"2017/03/24, 14:26:09",False,"2017/03/24, 14:26:09",1929.0,2599875.0,3,The main thing to understand here is that DataDog does not retrieve all the points for a given timeframe.,0.0,0.0,1.0,0.0
Datadog,42999464,42927552,0,"2017/03/24, 14:26:09",False,"2017/03/24, 14:26:09",1929.0,2599875.0,3,"Actually as  McCloud  says,  for a given time range we do not return more than 350 points , which is very important to have in mind when you create a counter.",0.0,0.139,0.861,0.4927
Datadog,42999464,42927552,0,"2017/03/24, 14:26:09",False,"2017/03/24, 14:26:09",1929.0,2599875.0,3,"When you query value from a metric in a timeframe, DataDog return a group of points that represents the real stored points, not all the points; the level of how those points are represented (as I understand) is called granurallity, and what you do with this  rollup  function is to define how those points are going to represent the real points, which in this case is going to be using the  sum  function.",0.0,0.034,0.966,0.34
Datadog,42999464,42927552,0,"2017/03/24, 14:26:09",False,"2017/03/24, 14:26:09",1929.0,2599875.0,3,"I hope this helps somebody, I'm still learning about it.",0.0,0.44,0.56,0.6705
Datadog,42999464,42927552,0,"2017/03/24, 14:26:09",False,"2017/03/24, 14:26:09",1929.0,2599875.0,3,Regards,0.0,0.0,1.0,0.0
Datadog,65565807,42927552,0,"2021/01/04, 18:05:02",False,"2021/01/04, 18:05:02",66.0,2625807.0,0,"In Ruby on the client, I use:",0.0,0.0,1.0,0.0
Datadog,65565807,42927552,0,"2021/01/04, 18:05:02",False,"2021/01/04, 18:05:02",66.0,2625807.0,0,"I then have a dashboard with a
 Query Value 
widget, set to &quot;take the [Sum] value from the displayed timeframe&quot;.",0.0,0.242,0.758,0.5859
Datadog,65565807,42927552,0,"2021/01/04, 18:05:02",False,"2021/01/04, 18:05:02",66.0,2625807.0,0,I tested this and it seems to give the right numbers.,0.0,0.0,1.0,0.0
Datadog,65565807,42927552,0,"2021/01/04, 18:05:02",False,"2021/01/04, 18:05:02",66.0,2625807.0,0,"The
 .as_count() 
seems key.",0.0,0.0,1.0,0.0
Datadog,58493964,45506314,0,"2019/10/21, 23:51:33",True,"2019/10/21, 23:51:33",20407.0,1017941.0,1,Answering just in case someone will spot this question via Google.,0.0,0.0,1.0,0.0
Datadog,58493964,45506314,0,"2019/10/21, 23:51:33",True,"2019/10/21, 23:51:33",20407.0,1017941.0,1,You cannot.,0.0,0.0,1.0,0.0
Datadog,58493964,45506314,0,"2019/10/21, 23:51:33",True,"2019/10/21, 23:51:33",20407.0,1017941.0,1,"StatsD protocol do not define tags or comments at all, so there is no possibility for that.",0.133,0.0,0.867,-0.3535
Datadog,58493964,45506314,0,"2019/10/21, 23:51:33",True,"2019/10/21, 23:51:33",20407.0,1017941.0,1,You need to use different library like  Statix  for that.,0.0,0.217,0.783,0.3612
Datadog,44207761,42588899,1,"2017/05/26, 21:08:43",False,"2017/05/26, 21:08:43",1.0,1976300.0,-1,Yes it is possible to emit metrics to DataDog from a AWS Lambda function.,0.0,0.184,0.816,0.4019
Datadog,44207761,42588899,1,"2017/05/26, 21:08:43",False,"2017/05/26, 21:08:43",1.0,1976300.0,-1,If you were using node.js you could use  https://www.npmjs.com/package/datadog-metrics  to emit metrics to the API.,0.0,0.0,1.0,0.0
Datadog,44207761,42588899,1,"2017/05/26, 21:08:43",False,"2017/05/26, 21:08:43",1.0,1976300.0,-1,"It supports counters, gauges and histograms.",0.0,0.333,0.667,0.3612
Datadog,44207761,42588899,1,"2017/05/26, 21:08:43",False,"2017/05/26, 21:08:43",1.0,1976300.0,-1,You just need to pass in your app/api key as environment variables.,0.0,0.0,1.0,0.0
Datadog,44207761,42588899,1,"2017/05/26, 21:08:43",False,"2017/05/26, 21:08:43",1.0,1976300.0,-1,Matt,0.0,0.0,1.0,0.0
Datadog,54256464,42588899,0,"2019/01/18, 16:57:13",False,"2019/01/18, 16:57:13",809.0,1647226.0,0,The easier way is using this library:  https://github.com/marceloboeira/aws-lambda-datadog,0.0,0.286,0.714,0.4215
Datadog,54256464,42588899,0,"2019/01/18, 16:57:13",False,"2019/01/18, 16:57:13",809.0,1647226.0,0,"It has runtime no dependencies, doesn't require authentication and reports everything to cloud-watch too.",0.145,0.0,0.855,-0.296
Datadog,54256464,42588899,0,"2019/01/18, 16:57:13",False,"2019/01/18, 16:57:13",809.0,1647226.0,0,You can read more about it here:  https://www.datadoghq.com/blog/how-to-monitor-lambda-functions/,0.0,0.0,1.0,0.0
Datadog,51941286,42722353,0,"2018/08/21, 06:48:18",False,"2018/08/21, 06:57:14",1474.0,10109833.0,0,"In most cases, the datadog agent retrieves metrics from an integration by connecting to a URL endpoint.",0.0,0.0,1.0,0.0
Datadog,51941286,42722353,0,"2018/08/21, 06:48:18",False,"2018/08/21, 06:57:14",1474.0,10109833.0,0,"This would be the case for services such as nginx, mysql etc.",0.0,0.0,1.0,0.0
Datadog,51941286,42722353,0,"2018/08/21, 06:48:18",False,"2018/08/21, 06:57:14",1474.0,10109833.0,0,"This means that you can run just one datadog agent on the host, and configure it to listen to URL endpoints of services exposed from each container.",0.048,0.0,0.952,-0.0772
Datadog,51941286,42722353,0,"2018/08/21, 06:48:18",False,"2018/08/21, 06:57:14",1474.0,10109833.0,0,"For example, assuming a mysql docker container is run with the following command:",0.0,0.0,1.0,0.0
Datadog,51941286,42722353,0,"2018/08/21, 06:48:18",False,"2018/08/21, 06:57:14",1474.0,10109833.0,0,You can instruct the agent running on the host to connect to the container IP in the  mysql.yaml  agent configuration:,0.0,0.0,1.0,0.0
Datadog,51941286,42722353,0,"2018/08/21, 06:48:18",False,"2018/08/21, 06:57:14",1474.0,10109833.0,0,Varnish is slightly different as the agent retrieves metrics using the  varnishstat  binary.,0.0,0.0,1.0,0.0
Datadog,51941286,42722353,0,"2018/08/21, 06:48:18",False,"2018/08/21, 06:57:14",1474.0,10109833.0,0,According to the example template:,0.0,0.0,1.0,0.0
Datadog,51941286,42722353,0,"2018/08/21, 06:48:18",False,"2018/08/21, 06:57:14",1474.0,10109833.0,0,In order to support monitoring a Varnish instance which is running as a Docker container we need to wrap commands (varnishstat) with scripts which perform a docker exec on the running container.,0.0,0.088,0.912,0.4019
Datadog,51941286,42722353,0,"2018/08/21, 06:48:18",False,"2018/08/21, 06:57:14",1474.0,10109833.0,0,"To do this, on the host, create a wrapper script for the container:",0.0,0.16,0.84,0.2732
Datadog,51941286,42722353,0,"2018/08/21, 06:48:18",False,"2018/08/21, 06:57:14",1474.0,10109833.0,0,Then specify the script location in the  varnish.yaml  agent configuration:,0.0,0.0,1.0,0.0
Datadog,47167529,45188547,0,"2017/11/07, 23:12:04",False,"2017/11/07, 23:12:04",6992.0,902415.0,2,There are 2 relevant options in  /etc/dd-agent/conf.d/docker_daemon.yaml :,0.0,0.0,1.0,0.0
Datadog,47167529,45188547,0,"2017/11/07, 23:12:04",False,"2017/11/07, 23:12:04",6992.0,902415.0,2,"collect_disk_stats 
If you use devicemapper-backed storage (which is default in ECS but not in vanilla Docker or Kubernetes), docker.data.",0.0,0.0,1.0,0.0
Datadog,47167529,45188547,0,"2017/11/07, 23:12:04",False,"2017/11/07, 23:12:04",6992.0,902415.0,2,* and docker.metadata.,0.0,0.0,1.0,0.0
Datadog,47167529,45188547,0,"2017/11/07, 23:12:04",False,"2017/11/07, 23:12:04",6992.0,902415.0,2,* statistics should do what you are looking for.,0.0,0.0,1.0,0.0
Datadog,47167529,45188547,0,"2017/11/07, 23:12:04",False,"2017/11/07, 23:12:04",6992.0,902415.0,2,"collect_container_size 
A generic way, using the docker API but virtually running df in every container.",0.0,0.0,1.0,0.0
Datadog,47167529,45188547,0,"2017/11/07, 23:12:04",False,"2017/11/07, 23:12:04",6992.0,902415.0,2,This enables the docker.container.,0.0,0.0,1.0,0.0
Datadog,47167529,45188547,0,"2017/11/07, 23:12:04",False,"2017/11/07, 23:12:04",6992.0,902415.0,2,* metrics.,0.0,0.0,1.0,0.0
Datadog,47167529,45188547,0,"2017/11/07, 23:12:04",False,"2017/11/07, 23:12:04",6992.0,902415.0,2,"See more here:
 https://help.datadoghq.com/hc/en-us/articles/115001786703-How-to-report-host-disk-metrics-when-dd-agent-runs-in-a-docker-container-",0.0,0.0,1.0,0.0
Datadog,47167529,45188547,0,"2017/11/07, 23:12:04",False,"2017/11/07, 23:12:04",6992.0,902415.0,2,"and here:
 https://github.com/DataDog/docker-dd-agent/blob/master/conf.d/docker_daemon.yaml#L46",0.0,0.0,1.0,0.0
Datadog,55916769,45954769,0,"2019/04/30, 11:27:17",False,"2019/04/30, 11:27:17",11.0,11431260.0,1,"In your question, I looked for your purpose in terms of using the port 8125 or 8126 ports.",0.0,0.0,1.0,0.0
Datadog,55916769,45954769,0,"2019/04/30, 11:27:17",False,"2019/04/30, 11:27:17",11.0,11431260.0,1,"8125 port is used for stasd metrics, and 8126 is used for APM (trace) data.",0.0,0.0,1.0,0.0
Datadog,55916769,45954769,0,"2019/04/30, 11:27:17",False,"2019/04/30, 11:27:17",11.0,11431260.0,1,So if you want to use 8125 the important thing to do is having  non_local_traffic : yes .,0.0,0.318,0.682,0.6204
Datadog,55916769,45954769,0,"2019/04/30, 11:27:17",False,"2019/04/30, 11:27:17",11.0,11431260.0,1,So there must be another problem which I don't know yet.,0.231,0.0,0.769,-0.4019
Datadog,55916769,45954769,0,"2019/04/30, 11:27:17",False,"2019/04/30, 11:27:17",11.0,11431260.0,1,But if your purpose is using APM/trace port: 8126 is only bound to localhost by default.,0.0,0.0,1.0,0.0
Datadog,55916769,45954769,0,"2019/04/30, 11:27:17",False,"2019/04/30, 11:27:17",11.0,11431260.0,1,You should make it listen to any network interface by the  bind_host: 0.0.0.0  configuration.,0.0,0.0,1.0,0.0
Datadog,55916769,45954769,0,"2019/04/30, 11:27:17",False,"2019/04/30, 11:27:17",11.0,11431260.0,1,"Currently, it will refuse the requests from your containers since they are not coming from localhost.",0.128,0.0,0.872,-0.296
Datadog,55916769,45954769,0,"2019/04/30, 11:27:17",False,"2019/04/30, 11:27:17",11.0,11431260.0,1,I had a similar problem and this page helped me:  https://github.com/DataDog/ansible-datadog/issues/149,0.252,0.0,0.748,-0.4019
Datadog,53449880,53435248,2,"2018/11/23, 18:09:47",True,"2019/07/29, 16:36:02",1371.0,5540166.0,5,UPDATED ANSWER:,0.0,0.0,1.0,0.0
Datadog,53449880,53435248,2,"2018/11/23, 18:09:47",True,"2019/07/29, 16:36:02",1371.0,5540166.0,5,Still yes.,0.0,0.73,0.27,0.4019
Datadog,53449880,53435248,2,"2018/11/23, 18:09:47",True,"2019/07/29, 16:36:02",1371.0,5540166.0,5,Docs for new Dashboard endpoint  here .,0.0,0.0,1.0,0.0
Datadog,53449880,53435248,2,"2018/11/23, 18:09:47",True,"2019/07/29, 16:36:02",1371.0,5540166.0,5,ORIGINAL ANSWER:,0.0,0.697,0.303,0.3182
Datadog,53449880,53435248,2,"2018/11/23, 18:09:47",True,"2019/07/29, 16:36:02",1371.0,5540166.0,5,Yes.,0.0,1.0,0.0,0.4019
Datadog,53449880,53435248,2,"2018/11/23, 18:09:47",True,"2019/07/29, 16:36:02",1371.0,5540166.0,5,Docs for screenboards  here .,0.0,0.0,1.0,0.0
Datadog,53449880,53435248,2,"2018/11/23, 18:09:47",True,"2019/07/29, 16:36:02",1371.0,5540166.0,5,Docs for timeboards  here .,0.0,0.0,1.0,0.0
Datadog,54194264,53880368,2,"2019/01/15, 09:19:00",True,"2019/01/15, 09:19:00",747.0,2461574.0,1,"I doubt the Datadog agent will ever work on App Services web app as you do not have access to the running host, it was designed for VMs.",0.088,0.0,0.912,-0.3612
Datadog,54194264,53880368,2,"2019/01/15, 09:19:00",True,"2019/01/15, 09:19:00",747.0,2461574.0,1,Have you tried this  https://www.datadoghq.com/blog/azure-monitoring-enhancements/  ?,0.0,0.0,1.0,0.0
Datadog,54194264,53880368,2,"2019/01/15, 09:19:00",True,"2019/01/15, 09:19:00",747.0,2461574.0,1,They say they support AppServices,0.0,0.403,0.597,0.4019
Datadog,55897902,53880368,2,"2019/04/29, 09:16:35",False,"2019/04/29, 09:16:35",27.0,5404159.0,0,"To respond to your comment on wanting custom metrics, this is still possible without the agent at the same location.",0.0,0.0,1.0,0.0
Datadog,55897902,53880368,2,"2019/04/29, 09:16:35",False,"2019/04/29, 09:16:35",27.0,5404159.0,0,After installing the nuget package of datadog called statsdclient you can then configure it to send the custom metrics to an agent located elsewhere.,0.0,0.0,1.0,0.0
Datadog,55897902,53880368,2,"2019/04/29, 09:16:35",False,"2019/04/29, 09:16:35",27.0,5404159.0,0,Example below:,0.0,0.0,1.0,0.0
Datadog,56799311,53880368,0,"2019/06/28, 01:59:38",False,"2019/06/28, 01:59:38",55.0,11661354.0,0,I have written a app service extension for sending Datadog APM metrics with .NET core and provided instructions for how to set it up here:  https://github.com/payscale/datadog-app-service-extension,0.0,0.0,1.0,0.0
Datadog,56799311,53880368,0,"2019/06/28, 01:59:38",False,"2019/06/28, 01:59:38",55.0,11661354.0,0,Let me know if you have any questions or if this doesn't apply to your situation.,0.0,0.0,1.0,0.0
Datadog,56808294,53880368,0,"2019/06/28, 16:56:58",False,"2019/06/28, 16:56:58",11.0,11713986.0,1,Logs from App Services can also be sent to Blob storage and forwarded from there via an Azure Function.,0.0,0.0,1.0,0.0
Datadog,56808294,53880368,0,"2019/06/28, 16:56:58",False,"2019/06/28, 16:56:58",11.0,11713986.0,1,"Unlike traces and custom metrics from App Services, this does not require a VM running the agent.",0.0,0.0,1.0,0.0
Datadog,56808294,53880368,0,"2019/06/28, 16:56:58",False,"2019/06/28, 16:56:58",11.0,11713986.0,1,Docs and code for the Function are available here:,0.0,0.0,1.0,0.0
Datadog,56808294,53880368,0,"2019/06/28, 16:56:58",False,"2019/06/28, 16:56:58",11.0,11713986.0,1,https://github.com/DataDog/datadog-serverless-functions/tree/master/azure/blobs_logs_monitoring,0.0,0.0,1.0,0.0
Datadog,58550213,53880368,0,"2019/10/25, 02:03:41",False,"2019/10/25, 02:03:41",1630.0,1246590.0,1,If you want to use DataDog for logging from Azure Function of App Service you can use Serilog and DataDog Sink to the log files:,0.0,0.051,0.949,0.0772
Datadog,58550213,53880368,0,"2019/10/25, 02:03:41",False,"2019/10/25, 02:03:41",1630.0,1246590.0,1,Full source code and required NuGet packages  are here:,0.0,0.0,1.0,0.0
Datadog,61118441,59500017,0,"2020/04/09, 12:53:48",False,"2020/04/09, 12:53:48",385.0,1068531.0,0,You can deploy the Datadog agent in a container / instance that you manage and the configure it according to  these instructions  to gather metrics from the remote ElasticSearch cluster that is hosted on Elastic Cloud.,0.0,0.0,1.0,0.0
Datadog,61118441,59500017,0,"2020/04/09, 12:53:48",False,"2020/04/09, 12:53:48",385.0,1068531.0,0,"You need to create a  conf.yaml  file in the  elastic.d/  directory and provide the required information (Elasticsearch endpoint/URL, username, password, port, etc) for the agent to be able to connect to the cluster.",0.0,0.063,0.937,0.2732
Datadog,61118441,59500017,0,"2020/04/09, 12:53:48",False,"2020/04/09, 12:53:48",385.0,1068531.0,0,You may find a sample configuration file  here .,0.0,0.0,1.0,0.0
Datadog,61276030,59500017,0,"2020/04/17, 19:13:14",True,"2020/04/17, 19:13:14",2287.0,466390.0,0,"As George Tseres mentioned above, the way I had to get this working was to set up collection on a separate instance (through docker) and then to configure it to read the specific Elastic Cloud instances.",0.0,0.0,1.0,0.0
Datadog,61276030,59500017,0,"2020/04/17, 19:13:14",True,"2020/04/17, 19:13:14",2287.0,466390.0,0,"I ended up making this:  https://github.com/crwang/datadog-elasticsearch , building that docker image, and then pushing it up to AWS ECR.",0.0,0.0,1.0,0.0
Datadog,61276030,59500017,0,"2020/04/17, 19:13:14",True,"2020/04/17, 19:13:14",2287.0,466390.0,0,"Then, I spun up a Fargate service / task to run the container.",0.0,0.0,1.0,0.0
Datadog,61276030,59500017,0,"2020/04/17, 19:13:14",True,"2020/04/17, 19:13:14",2287.0,466390.0,0,I also set it to run locally with  docker-compose  as a test.,0.0,0.0,1.0,0.0
Datadog,35509278,35224366,1,"2016/02/19, 17:47:27",True,"2016/02/19, 17:47:27",455.0,4676932.0,10,These system.io metrics are reported from a  system agent check  that uses  iostat  under the hood.,0.0,0.0,1.0,0.0
Datadog,35509278,35224366,1,"2016/02/19, 17:47:27",True,"2016/02/19, 17:47:27",455.0,4676932.0,10,According to the  iostat manpage  one of the metrics  %util  (reported as  system.io.util  within Datadog) seems to do the job:,0.0,0.0,1.0,0.0
Datadog,35509278,35224366,1,"2016/02/19, 17:47:27",True,"2016/02/19, 17:47:27",455.0,4676932.0,10,%util: Percentage of CPU time during which I/O requests were issued to the device (bandwidth utilization for the device).,0.0,0.0,1.0,0.0
Datadog,35509278,35224366,1,"2016/02/19, 17:47:27",True,"2016/02/19, 17:47:27",455.0,4676932.0,10,Device saturation occurs when this value is close to 100%.,0.0,0.234,0.766,0.4118
Datadog,35509278,35224366,1,"2016/02/19, 17:47:27",True,"2016/02/19, 17:47:27",455.0,4676932.0,10,"You can create a monitor, as a multi alert on host/device, when this metric is over 90 on the last 30 minutes on average, here is a current screenshot of such an example:",0.0,0.133,0.867,0.5106
Datadog,35509278,35224366,1,"2016/02/19, 17:47:27",True,"2016/02/19, 17:47:27",455.0,4676932.0,10,Of course one can also monitor other iostat metrics to identify other I/O performance failure modes.,0.18,0.0,0.82,-0.5106
Datadog,35560762,35554988,2,"2016/02/22, 20:06:38",True,"2016/02/22, 20:06:38",261.0,4172512.0,10,This can be achieved in two ways.,0.0,0.0,1.0,0.0
Datadog,35560762,35554988,2,"2016/02/22, 20:06:38",True,"2016/02/22, 20:06:38",261.0,4172512.0,10,"If you only want this logic to applied on this one graph, you can divide metric either using the UI editor and clicking advanced or using the JSON editor:",0.0,0.109,0.891,0.3182
Datadog,35560762,35554988,2,"2016/02/22, 20:06:38",True,"2016/02/22, 20:06:38",261.0,4172512.0,10,UI Editor:  http://cl.ly/1c0K2O3P1E2K,0.0,0.0,1.0,0.0
Datadog,35560762,35554988,2,"2016/02/22, 20:06:38",True,"2016/02/22, 20:06:38",261.0,4172512.0,10,Or JSON editor:  https://help.datadoghq.com/hc/en-us/articles/203764925-How-do-I-use-arithmetic-for-my-time-series-data-,0.0,0.0,1.0,0.0
Datadog,35560762,35554988,2,"2016/02/22, 20:06:38",True,"2016/02/22, 20:06:38",261.0,4172512.0,10,"Alternatively, you can use the Metric Summary page to edit this metric's metadata and alter this metric's unit throughout the application as seen here:  http://cl.ly/2x0Z290w2I3V",0.0,0.0,1.0,0.0
Datadog,35560762,35554988,2,"2016/02/22, 20:06:38",True,"2016/02/22, 20:06:38",261.0,4172512.0,10,https://app.datadoghq.com/metric/summary,0.0,0.0,1.0,0.0
Datadog,35560762,35554988,2,"2016/02/22, 20:06:38",True,"2016/02/22, 20:06:38",261.0,4172512.0,10,Hope this helps.,0.0,0.846,0.154,0.6705
Datadog,35560762,35554988,2,"2016/02/22, 20:06:38",True,"2016/02/22, 20:06:38",261.0,4172512.0,10,"Also, you can reach out to support@datadoghq.com if you run into any other issues in the future.",0.0,0.064,0.936,0.0258
Datadog,42481675,42441120,3,"2017/02/27, 11:09:46",True,"2017/02/27, 11:09:46",6657.0,86.0,1,"So, while trying to debug this I deleted the deployment + dameonset and service and recreated it.",0.0,0.0,1.0,0.0
Datadog,42481675,42441120,3,"2017/02/27, 11:09:46",True,"2017/02/27, 11:09:46",6657.0,86.0,1,Afterwards it worked....,0.0,0.0,1.0,0.0
Datadog,44038527,42441120,0,"2017/05/18, 07:21:01",False,"2017/05/18, 07:21:01",9.0,7901388.0,0,Have you seen the  Discovering Services  docs?,0.0,0.0,1.0,0.0
Datadog,44038527,42441120,0,"2017/05/18, 07:21:01",False,"2017/05/18, 07:21:01",9.0,7901388.0,0,I'd recommend using DNS for service discovery rather than environment variables since environment variables require services to come up in a particular order.,0.0,0.106,0.894,0.3612
Datadog,43669352,42538664,1,"2017/04/28, 02:11:48",False,"2017/04/28, 02:11:48",162.0,2254902.0,2,There are two error in your code:,0.31,0.0,0.69,-0.4019
Datadog,43669352,42538664,1,"2017/04/28, 02:11:48",False,"2017/04/28, 02:11:48",162.0,2254902.0,2,"Once done, your code will run flawlessly.",0.0,0.231,0.769,0.2023
Datadog,44193701,42958937,4,"2017/05/26, 07:35:57",True,"2017/05/26, 07:35:57",1371.0,5540166.0,5,"So from that log line, it appears as though  this  try  is excepting  in the library's  hostname.py .",0.0,0.0,1.0,0.0
Datadog,44193701,42958937,4,"2017/05/26, 07:35:57",True,"2017/05/26, 07:35:57",1371.0,5540166.0,5,So either...,0.0,0.0,1.0,0.0
Datadog,44193701,42958937,4,"2017/05/26, 07:35:57",True,"2017/05/26, 07:35:57",1371.0,5540166.0,5,"(A) The  hostname line  is where it's excepting, and (weirdly) the
library requires that a  hostname  option be set in your
 datadog.conf  file.",0.0,0.0,1.0,0.0
Datadog,44193701,42958937,4,"2017/05/26, 07:35:57",True,"2017/05/26, 07:35:57",1371.0,5540166.0,5,"Maybe worth setting a hostname there if you
haven't already.",0.0,0.192,0.808,0.2263
Datadog,44193701,42958937,4,"2017/05/26, 07:35:57",True,"2017/05/26, 07:35:57",1371.0,5540166.0,5,"Or,",0.0,0.0,1.0,0.0
Datadog,44193701,42958937,4,"2017/05/26, 07:35:57",True,"2017/05/26, 07:35:57",1371.0,5540166.0,5,"(B) The  get_config() line  is where it's excepting, and so the
library isn't able to correctly identify the configuration file
location (or access it, possibly related to permissions).",0.0,0.0,1.0,0.0
Datadog,44193701,42958937,4,"2017/05/26, 07:35:57",True,"2017/05/26, 07:35:57",1371.0,5540166.0,5,"Based on
the directory structure in your question, I think you're working on
an OSX / mac environment, which means the library will be using the
function  _mac_config_path()  in  config.py  to try to identify the
configuration path, which from  this line in the function  would
make it  seem  as though the library were looking for the
configuration file in  ~/.datadog-agent/agent/datadog.conf  instead
of the appropriate  ~/.datadog-agent/datadog.conf .",0.0,0.0,1.0,0.0
Datadog,44193701,42958937,4,"2017/05/26, 07:35:57",True,"2017/05/26, 07:35:57",1371.0,5540166.0,5,"Which might be a
legitimate bug...",0.0,0.0,1.0,0.0
Datadog,44193701,42958937,4,"2017/05/26, 07:35:57",True,"2017/05/26, 07:35:57",1371.0,5540166.0,5,"So if I were you, and if all this seemed right, I'd try adding a  hostname in the  datadog.conf  to see if that helped, and if it didn't, then I'd try making a  ~/.datadog-agent/agent/  directory and copying your  datadog.conf  file there as well, just to see if that got things working.",0.0,0.043,0.957,0.2732
Datadog,44193701,42958937,4,"2017/05/26, 07:35:57",True,"2017/05/26, 07:35:57",1371.0,5540166.0,5,"This answer assumes you are working in an OSX / mac environment, and will likely not be correct otherwise.",0.0,0.0,1.0,0.0
Datadog,44193701,42958937,4,"2017/05/26, 07:35:57",True,"2017/05/26, 07:35:57",1371.0,5540166.0,5,"If either (A) or (B) are the case, then that's a problem with the library and should be updated--it would be kind of you to open an issue on  the library itself  to bring this up so the Datadog team that supports that library can be made aware.",0.054,0.05,0.896,-0.0516
Datadog,44193701,42958937,4,"2017/05/26, 07:35:57",True,"2017/05/26, 07:35:57",1371.0,5540166.0,5,"I suspect not many people end up this library in OSX / mac environments to begin with, so that could explain all this.",0.099,0.0,0.901,-0.296
Datadog,56620105,52390804,0,"2019/06/16, 18:14:27",False,"2019/06/16, 18:14:27",301.0,2708541.0,0,"You installed the Datadog agent &amp; trace agent on a Mac, listening on localhost.",0.0,0.0,1.0,0.0
Datadog,56620105,52390804,0,"2019/06/16, 18:14:27",False,"2019/06/16, 18:14:27",301.0,2708541.0,0,You installed the flask application and ddtrace library in a docker container on a linux vm sending traffic to localhost.,0.0,0.0,1.0,0.0
Datadog,56620105,52390804,0,"2019/06/16, 18:14:27",False,"2019/06/16, 18:14:27",301.0,2708541.0,0,Those two localhosts are describing two different machines.,0.0,0.0,1.0,0.0
Datadog,56620105,52390804,0,"2019/06/16, 18:14:27",False,"2019/06/16, 18:14:27",301.0,2708541.0,0,"The easiest option is going to be running both the Agent &amp; flask app on the Mac, or running both in docker.",0.0,0.118,0.882,0.4215
Datadog,56620105,52390804,0,"2019/06/16, 18:14:27",False,"2019/06/16, 18:14:27",301.0,2708541.0,0,The latter is most similar to an eventual production deployment.,0.0,0.0,1.0,0.0
Datadog,56620105,52390804,0,"2019/06/16, 18:14:27",False,"2019/06/16, 18:14:27",301.0,2708541.0,0,Do that.,0.0,0.0,1.0,0.0
Datadog,58681226,58476988,0,"2019/11/03, 16:41:58",True,"2019/11/03, 16:41:58",2206.0,712543.0,3,"According to the documentation, this can be achieved using following properties in telegraf.conf:",0.0,0.0,1.0,0.0
Datadog,58681226,58476988,0,"2019/11/03, 16:41:58",True,"2019/11/03, 16:41:58",2206.0,712543.0,3,https://docs.influxdata.com/telegraf/v1.12/administration/configuration/#measurement-filtering,0.0,0.0,1.0,0.0
Datadog,58681226,58476988,0,"2019/11/03, 16:41:58",True,"2019/11/03, 16:41:58",2206.0,712543.0,3,where  namepass  defines pattern list of points which will be emitted.,0.0,0.0,1.0,0.0
Datadog,60213083,60064427,1,"2020/02/13, 19:02:26",True,"2020/02/14, 09:17:50",27576.0,1839482.0,3,Datadog has two agents.,0.0,0.0,1.0,0.0
Datadog,60213083,60064427,1,"2020/02/13, 19:02:26",True,"2020/02/14, 09:17:50",27576.0,1839482.0,3,And yes for DogStatsD the node agents need to be deployed as Daemonset.,0.0,0.184,0.816,0.4019
Datadog,60213083,60064427,1,"2020/02/13, 19:02:26",True,"2020/02/14, 09:17:50",27576.0,1839482.0,3,Here is the the deployment manifest for  cluster agent  and  node agent .,0.0,0.0,1.0,0.0
Datadog,64168376,60290897,0,"2020/10/02, 11:24:58",True,"2020/10/02, 11:24:58",347.0,1485995.0,0,"Yes, the following should work:",0.0,0.403,0.597,0.4019
Datadog,64168376,60290897,0,"2020/10/02, 11:24:58",True,"2020/10/02, 11:24:58",347.0,1485995.0,0,tag_one:(A OR B),0.0,0.0,1.0,0.0
Datadog,64168376,60290897,0,"2020/10/02, 11:24:58",True,"2020/10/02, 11:24:58",347.0,1485995.0,0,"Unfortunately the query syntax is slightly different in different contexts, I find, so I don't know if that will solve your particular problem.",0.205,0.072,0.723,-0.5106
Datadog,60667516,60662893,0,"2020/03/13, 11:01:40",False,"2020/03/13, 11:01:40",336.0,7243426.0,1,"If you want to remove the warning, you can try adding  none  and  shm  to the  excluded_filesystems  in disk.yaml.",0.116,0.063,0.821,-0.2732
Datadog,60667516,60662893,0,"2020/03/13, 11:01:40",False,"2020/03/13, 11:01:40",336.0,7243426.0,1,This file should exist or be created in the Agent's conf.d directory.,0.0,0.154,0.846,0.25
Datadog,60667516,60662893,0,"2020/03/13, 11:01:40",False,"2020/03/13, 11:01:40",336.0,7243426.0,1,"Otherwise, you'll find more options  here .",0.0,0.0,1.0,0.0
Datadog,60667516,60662893,0,"2020/03/13, 11:01:40",False,"2020/03/13, 11:01:40",336.0,7243426.0,1,If you are looking to exclude the logs from the agent within the platform you can look at excluding the agent ( doc ),0.083,0.0,0.917,-0.2263
Datadog,60677166,60662893,0,"2020/03/13, 22:56:50",True,"2020/03/13, 22:56:50",787.0,1363715.0,2,Found the answer here:  https://github.com/DataDog/datadog-agent/issues/3329,0.0,0.0,1.0,0.0
Datadog,60677166,60662893,0,"2020/03/13, 22:56:50",True,"2020/03/13, 22:56:50",787.0,1363715.0,2,The field is  mount_point_blacklist,0.0,0.0,1.0,0.0
Datadog,61972253,61900949,0,"2020/05/23, 15:46:33",False,"2020/05/23, 15:46:33",493.0,244037.0,4,"In Datadog Logs, there's a difference between the Tags associated with the execution environment, and Attributes set on Log entry content.",0.0,0.0,1.0,0.0
Datadog,61972253,61900949,0,"2020/05/23, 15:46:33",False,"2020/05/23, 15:46:33",493.0,244037.0,4,From  this section in the docs :,0.0,0.0,1.0,0.0
Datadog,61972253,61900949,0,"2020/05/23, 15:46:33",False,"2020/05/23, 15:46:33",493.0,244037.0,4,Context  refers to the infrastructure and application context in which the log has been generated.,0.0,0.0,1.0,0.0
Datadog,61972253,61900949,0,"2020/05/23, 15:46:33",False,"2020/05/23, 15:46:33",493.0,244037.0,4,"Information is gathered from tags—whether automatically attached (host name, container name, log file name, serverless function name, etc.",0.0,0.0,1.0,0.0
Datadog,61972253,61900949,0,"2020/05/23, 15:46:33",False,"2020/05/23, 15:46:33",493.0,244037.0,4,")—or added through custom tags (team in charge, environment, application version, etc.)",0.0,0.0,1.0,0.0
Datadog,61972253,61900949,0,"2020/05/23, 15:46:33",False,"2020/05/23, 15:46:33",493.0,244037.0,4,on the log by the Datadog Agent or Log Forwarder.,0.0,0.0,1.0,0.0
Datadog,61972253,61900949,0,"2020/05/23, 15:46:33",False,"2020/05/23, 15:46:33",493.0,244037.0,4,"And looking into the  source for the browser SDK , we can see:",0.0,0.0,1.0,0.0
Datadog,61972253,61900949,0,"2020/05/23, 15:46:33",False,"2020/05/23, 15:46:33",493.0,244037.0,4,"This shows us that the  tags  query string parameter being submitted is being calculated based on configuration, and only provides a small amount of user-configurable entries, like  env ,  service  - these were released very recently in version 1.11.5 -  here's the change  introducing them.",0.0,0.059,0.941,0.3612
Datadog,61972253,61900949,0,"2020/05/23, 15:46:33",False,"2020/05/23, 15:46:33",493.0,244037.0,4,"So you may not be able to set  tags  for a specific log entry - rather you can set  attributes  per log entry, like in the example you shared, which is setting  Attributes  for the logger instance as a whole.",0.0,0.123,0.877,0.5994
Datadog,61972253,61900949,0,"2020/05/23, 15:46:33",False,"2020/05/23, 15:46:33",493.0,244037.0,4,Attributes are part of the log  Content  - which will be viewable in the body of the log entry.,0.0,0.0,1.0,0.0
Datadog,61972253,61900949,0,"2020/05/23, 15:46:33",False,"2020/05/23, 15:46:33",493.0,244037.0,4,"Yes, this is confusing since the function used is named  addContext / setContext  - and these don't set the same thing as the documentation's ""Context"" - rather they modify the Attributes that are associated with the log entry.",0.051,0.072,0.878,0.2023
Datadog,61972253,61900949,0,"2020/05/23, 15:46:33",False,"2020/05/23, 15:46:33",493.0,244037.0,4,"In that event, you may want to have either custom logger instances that provide specific attributes for that logger, or add context inline to the log entry, like this:",0.0,0.123,0.877,0.4215
Datadog,61972253,61900949,0,"2020/05/23, 15:46:33",False,"2020/05/23, 15:46:33",493.0,244037.0,4,Here's the docs  on this approach which show what other default attributes are being set per log entry.,0.0,0.0,1.0,0.0
Datadog,31682034,29697464,0,"2015/07/28, 19:24:53",False,"2015/07/28, 19:24:53",455.0,4676932.0,1,"The Monitor section of Datadog now includes a ""Monitor status"" page for each of the monitor you define (for example the URL monitoring you already have).",0.0,0.0,1.0,0.0
Datadog,31682034,29697464,0,"2015/07/28, 19:24:53",False,"2015/07/28, 19:24:53",455.0,4676932.0,1,"On this page, you can see by group/scope the monitor history and it shows you the uptime for that monitor.",0.0,0.0,1.0,0.0
Datadog,31682034,29697464,0,"2015/07/28, 19:24:53",False,"2015/07/28, 19:24:53",455.0,4676932.0,1,More to read about this new feature  here,0.0,0.0,1.0,0.0
Datadog,31682034,29697464,0,"2015/07/28, 19:24:53",False,"2015/07/28, 19:24:53",455.0,4676932.0,1,"It's not yet available as a ""report"" but that's a good idea!",0.0,0.315,0.685,0.63
Datadog,29994613,29949571,0,"2015/05/01, 23:26:19",False,"2015/05/01, 23:26:19",455.0,4676932.0,2,The exception you are seeing is related to the IO system metrics collection and has nothing to do with your custom dogstream parser.,0.0,0.0,1.0,0.0
Datadog,29994613,29949571,0,"2015/05/01, 23:26:19",False,"2015/05/01, 23:26:19",455.0,4676932.0,2,If you look at the stack trace it says that it wasn't able to apply the  _parse_linux2  function.,0.0,0.0,1.0,0.0
Datadog,29994613,29949571,0,"2015/05/01, 23:26:19",False,"2015/05/01, 23:26:19",455.0,4676932.0,2,To troubleshoot that further you should take a look at the output of,0.0,0.141,0.859,0.2023
Datadog,29994613,29949571,0,"2015/05/01, 23:26:19",False,"2015/05/01, 23:26:19",455.0,4676932.0,2,which is the command launched by the agent.,0.0,0.176,0.824,0.128
Datadog,29994613,29949571,0,"2015/05/01, 23:26:19",False,"2015/05/01, 23:26:19",455.0,4676932.0,2,Feel free to open a bug on the agent GitHub repository.,0.0,0.268,0.732,0.5106
Datadog,29994613,29949571,0,"2015/05/01, 23:26:19",False,"2015/05/01, 23:26:19",455.0,4676932.0,2,References:,0.0,0.0,1.0,0.0
Datadog,34909299,34893203,2,"2016/01/20, 21:54:55",False,"2016/01/20, 21:54:55",261.0,4172512.0,4,Just a few items to note to get this working:,0.0,0.0,1.0,0.0
Datadog,34909299,34893203,2,"2016/01/20, 21:54:55",False,"2016/01/20, 21:54:55",261.0,4172512.0,4,"
dogstatsd = Statsd.new('MY_API_KEY')",0.0,0.0,1.0,0.0
Datadog,34909299,34893203,2,"2016/01/20, 21:54:55",False,"2016/01/20, 21:54:55",261.0,4172512.0,4,"This line of code is trying to use your API key to establish a statsD connection, but this should actually be trying to establish the statsD connection via the statsD port currently configured on your Agent as seen here:",0.0,0.0,1.0,0.0
Datadog,34909299,34893203,2,"2016/01/20, 21:54:55",False,"2016/01/20, 21:54:55",261.0,4172512.0,4,"
Create a stats instance.",0.0,0.512,0.488,0.2732
Datadog,34909299,34893203,2,"2016/01/20, 21:54:55",False,"2016/01/20, 21:54:55",261.0,4172512.0,4,"statsd = Statsd.new('localhost', 8125)",0.0,0.0,1.0,0.0
Datadog,34909299,34893203,2,"2016/01/20, 21:54:55",False,"2016/01/20, 21:54:55",261.0,4172512.0,4,"The easiest way to get your custom metrics into Datadog is to send them to DogStatsD, a metrics aggregation server bundled with the Datadog Agent (in versions 3.0 and above).",0.0,0.091,0.909,0.4215
Datadog,34909299,34893203,2,"2016/01/20, 21:54:55",False,"2016/01/20, 21:54:55",261.0,4172512.0,4,"DogStatsD implements the StatsD protocol, along with a few extensions for special Datadog features.",0.0,0.184,0.816,0.4019
Datadog,34909299,34893203,2,"2016/01/20, 21:54:55",False,"2016/01/20, 21:54:55",261.0,4172512.0,4,http://docs.datadoghq.com/guides/dogstatsd/,0.0,0.0,1.0,0.0
Datadog,34909299,34893203,2,"2016/01/20, 21:54:55",False,"2016/01/20, 21:54:55",261.0,4172512.0,4,"If you would not like to deploy an Agent on the host running the RoR application, you can utilize DogAPI gem:",0.095,0.0,0.905,-0.2755
Datadog,34909299,34893203,2,"2016/01/20, 21:54:55",False,"2016/01/20, 21:54:55",261.0,4172512.0,4,https://github.com/DataDog/dogapi-rb,0.0,0.0,1.0,0.0
Datadog,34909299,34893203,2,"2016/01/20, 21:54:55",False,"2016/01/20, 21:54:55",261.0,4172512.0,4,Which has additional documentation to get this custom metrics submitted:,0.0,0.0,1.0,0.0
Datadog,34909299,34893203,2,"2016/01/20, 21:54:55",False,"2016/01/20, 21:54:55",261.0,4172512.0,4,"If you have additional questions, please reach out to support@datadoghq.com",0.0,0.298,0.702,0.34
Datadog,41144060,41142456,0,"2016/12/14, 15:38:50",True,"2016/12/14, 15:38:50",101.0,6197827.0,2,This Datadog blog should guide you on how to build a monitor.,0.0,0.0,1.0,0.0
Datadog,41144060,41142456,0,"2016/12/14, 15:38:50",True,"2016/12/14, 15:38:50",101.0,6197827.0,2,https://www.datadoghq.com/blog/monitoring-cassandra-with-datadog/,0.0,0.0,1.0,0.0
Datadog,41149100,41142456,0,"2016/12/14, 19:56:34",False,"2016/12/14, 19:56:34",41.0,7297928.0,4,There is indeed a  template Cassandra dashboard  in Datadog (where I work) that should appear as soon as you enable the integration.,0.0,0.0,1.0,0.0
Datadog,41149100,41142456,0,"2016/12/14, 19:56:34",False,"2016/12/14, 19:56:34",41.0,7297928.0,4,This dash has a mix of Cassandra-specific metrics (e.g.,0.0,0.0,1.0,0.0
Datadog,41149100,41142456,0,"2016/12/14, 19:56:34",False,"2016/12/14, 19:56:34",41.0,7297928.0,4,"cache hit rates), plus metrics from the host (e.g.",0.0,0.0,1.0,0.0
Datadog,41149100,41142456,0,"2016/12/14, 19:56:34",False,"2016/12/14, 19:56:34",41.0,7297928.0,4,CPU).,0.0,0.0,1.0,0.0
Datadog,41149100,41142456,0,"2016/12/14, 19:56:34",False,"2016/12/14, 19:56:34",41.0,7297928.0,4,"You can select a particular host or subset of hosts to make those host-level metrics more meaningful by  changing the scope of the dashboard , and the graphs will re-render on the fly.",0.0,0.08,0.92,0.3804
Datadog,41149100,41142456,0,"2016/12/14, 19:56:34",False,"2016/12/14, 19:56:34",41.0,7297928.0,4,You can also clone and modify the dashboard as you wish by clicking the gear icon in the upper right.,0.0,0.124,0.876,0.4019
Datadog,41149100,41142456,0,"2016/12/14, 19:56:34",False,"2016/12/14, 19:56:34",41.0,7297928.0,4,"This dash should provide a good starting point for monitoring Cassandra, but we have an even better template dashboard in the works.",0.0,0.234,0.766,0.7003
Datadog,41149100,41142456,0,"2016/12/14, 19:56:34",False,"2016/12/14, 19:56:34",41.0,7297928.0,4,I'll update this answer as soon as it's released.,0.0,0.0,1.0,0.0
Datadog,41149100,41142456,0,"2016/12/14, 19:56:34",False,"2016/12/14, 19:56:34",41.0,7297928.0,4,"In the meantime, the blog post shared by John KVS should help you to identify key metrics that you might want to monitor.",0.0,0.242,0.758,0.6597
Datadog,41995839,41992681,1,"2017/02/02, 08:48:29",True,"2017/02/02, 08:48:29",1981.0,2473382.0,4,Duplicate the definition of  event.sent  in  event.failed .,0.0,0.0,1.0,0.0
Datadog,41995839,41992681,1,"2017/02/02, 08:48:29",True,"2017/02/02, 08:48:29",1981.0,2473382.0,4,"As soon as you restart the agent, any  sent  event will be seen as  sent   and   failed .",0.171,0.0,0.829,-0.5106
Datadog,41995839,41992681,1,"2017/02/02, 08:48:29",True,"2017/02/02, 08:48:29",1981.0,2473382.0,4,"After a minute or so you revert the definition of  failed  to the proper definition, but you now have a few (admittedly bogus) failed event in datadog, allowing you to use the metric.",0.185,0.0,0.815,-0.765
Datadog,41995839,41992681,1,"2017/02/02, 08:48:29",True,"2017/02/02, 08:48:29",1981.0,2473382.0,4,"On your datadog dashboard, edit the relevant metric, and instead of using the graphical interface (tab  edit ) got to the  JSON  tab, where you can manually enter your metric name (probably a slightly updated cut &amp; paste of your  sent  metric), no matter if an event exists or not.",0.08,0.022,0.898,-0.4445
Datadog,43699066,43146756,0,"2017/04/29, 21:21:56",False,"2017/04/29, 21:21:56",1371.0,5540166.0,0,"Datadog monitors evaluate every minute, I think.",0.0,0.0,1.0,0.0
Datadog,43699066,43146756,0,"2017/04/29, 21:21:56",False,"2017/04/29, 21:21:56",1371.0,5540166.0,0,"So in your  sum(last_30m){X}  example, every minute, the monitor would sum the values of  {X}  over the last 30 minutes, and if that value was above whatever threshold you set, then it would trigger an alert.",0.0,0.181,0.819,0.743
Datadog,43699066,43146756,0,"2017/04/29, 21:21:56",False,"2017/04/29, 21:21:56",1371.0,5540166.0,0,"Same thing for  sum(last_1h){X} , but every minute it would evaluate the sum over the last 1 hour.",0.0,0.0,1.0,0.0
Datadog,43698915,43224591,0,"2017/04/29, 21:05:40",False,"2017/04/29, 21:05:40",1371.0,5540166.0,1,"The first step will be to make sure you have the  datadog agent running , and that the APM component of it is running and ready to receive trace data from your applications ( this option in your datadog.conf , which must be set to ""true"").",0.0,0.105,0.895,0.5859
Datadog,43698915,43224591,0,"2017/04/29, 21:05:40",False,"2017/04/29, 21:05:40",1371.0,5540166.0,1,"Second, you'll want to install the appropriate library(ies) for the languages your applications are written in.",0.0,0.08,0.92,0.0772
Datadog,43698915,43224591,0,"2017/04/29, 21:05:40",False,"2017/04/29, 21:05:40",1371.0,5540166.0,1,You can find them all listed in your datadog account on this page:  https://app.datadoghq.com/apm/docs,0.0,0.0,1.0,0.0
Datadog,43698915,43224591,0,"2017/04/29, 21:05:40",False,"2017/04/29, 21:05:40",1371.0,5540166.0,1,"Third, once the trace libraries are installed, you'll want to add trace integrations for the tools you're interested in collecting APM data on, and the instructions for those will be found in each library's docs.",0.0,0.108,0.892,0.4588
Datadog,43698915,43224591,0,"2017/04/29, 21:05:40",False,"2017/04/29, 21:05:40",1371.0,5540166.0,1,"(E.g,  Python ,  Ruby , and  Go )",0.0,0.0,1.0,0.0
Datadog,43698915,43224591,0,"2017/04/29, 21:05:40",False,"2017/04/29, 21:05:40",1371.0,5540166.0,1,"The integraitons will be a fairly quick way to get pretty granular spans on where your applications have higher latency, errors, etc.",0.098,0.13,0.772,0.2023
Datadog,43698915,43224591,0,"2017/04/29, 21:05:40",False,"2017/04/29, 21:05:40",1371.0,5540166.0,1,"If from there you'd like to go even further, each library's docs also have instructions on how you can write your own custom tracing functions to expose more info on your custom applications--that's a little more work, but is fairly straight-forward.",0.032,0.043,0.926,0.1154
Datadog,43698915,43224591,0,"2017/04/29, 21:05:40",False,"2017/04/29, 21:05:40",1371.0,5540166.0,1,You'll probably want to add those bit-by-bit as you go.,0.0,0.126,0.874,0.0772
Datadog,43698915,43224591,0,"2017/04/29, 21:05:40",False,"2017/04/29, 21:05:40",1371.0,5540166.0,1,"Then you'd be all set, I think.",0.0,0.0,1.0,0.0
Datadog,43698915,43224591,0,"2017/04/29, 21:05:40",False,"2017/04/29, 21:05:40",1371.0,5540166.0,1,"You'll be tracing services, resources to get the latency, request-count, and error-count of your application requests, and you can drill down to the flame-graphs to further understand what requests spend the most time where in your applications.",0.0,0.0,1.0,0.0
Datadog,43698915,43224591,0,"2017/04/29, 21:05:40",False,"2017/04/29, 21:05:40",1371.0,5540166.0,1,"Looking back now, seems like they made some recent changes to the setup process that makes it even easier to get the web framework and database integrations added if you're using Python.",0.0,0.15,0.85,0.6486
Datadog,43698915,43224591,0,"2017/04/29, 21:05:40",False,"2017/04/29, 21:05:40",1371.0,5540166.0,1,They've even got a command line tool in their  get-started section now .,0.0,0.0,1.0,0.0
Datadog,43698915,43224591,0,"2017/04/29, 21:05:40",False,"2017/04/29, 21:05:40",1371.0,5540166.0,1,Hope this helps!,0.0,0.853,0.147,0.6996
Datadog,43698915,43224591,0,"2017/04/29, 21:05:40",False,"2017/04/29, 21:05:40",1371.0,5540166.0,1,And reach out to their support team (support@datadoghq.com) if you run into issues along the way--they're always happy to lend a hand.,0.0,0.386,0.614,0.8658
Datadog,51896300,47577288,2,"2018/08/17, 16:18:03",False,"2018/08/18, 20:06:12",11.0,7120456.0,0,For now I see only two possibilities:,0.0,0.0,1.0,0.0
Datadog,51896300,47577288,2,"2018/08/17, 16:18:03",False,"2018/08/18, 20:06:12",11.0,7120456.0,0,"Use GCP custom metrics
 https://cloud.google.com/monitoring/custom-metrics/creating-metrics 
and datadog integration with GCP
 https://www.datadoghq.com/product/integrations/#cat-google-cloud",0.0,0.0,1.0,0.0
Datadog,51896300,47577288,2,"2018/08/17, 16:18:03",False,"2018/08/18, 20:06:12",11.0,7120456.0,0,"Use datadog statsd client, java one -
 https://github.com/DataDog/java-dogstatsd-client  so you can deploy
datadog agent on GCP and connect through it.",0.0,0.0,1.0,0.0
Datadog,51896300,47577288,2,"2018/08/17, 16:18:03",False,"2018/08/18, 20:06:12",11.0,7120456.0,0,Sample with kubernetes.,0.0,0.0,1.0,0.0
Datadog,51896300,47577288,2,"2018/08/17, 16:18:03",False,"2018/08/18, 20:06:12",11.0,7120456.0,0,https://docs.datadoghq.com/tracing/setup/kubernetes/#deploy-agent-daemonset,0.0,0.0,1.0,0.0
Datadog,51896300,47577288,2,"2018/08/17, 16:18:03",False,"2018/08/18, 20:06:12",11.0,7120456.0,0,datadog deployment.yaml for kubernetes,0.0,0.0,1.0,0.0
Datadog,51896300,47577288,2,"2018/08/17, 16:18:03",False,"2018/08/18, 20:06:12",11.0,7120456.0,0,"Currently I'm investigating this so I'm not sure how to do this, yet.",0.156,0.0,0.844,-0.3017
Datadog,49867829,49690040,0,"2018/04/17, 02:53:52",True,"2018/04/17, 02:53:52",356.0,2750290.0,1,It has support to CURL means you can make REST API calls.,0.0,0.197,0.803,0.4019
Datadog,49867829,49690040,0,"2018/04/17, 02:53:52",True,"2018/04/17, 02:53:52",356.0,2750290.0,1,Try using some Http libraries like  HttpURLConnection  in java to make those POST requests.,0.0,0.161,0.839,0.3612
Datadog,49867829,49690040,0,"2018/04/17, 02:53:52",True,"2018/04/17, 02:53:52",356.0,2750290.0,1,I don't think we need a java based SDK for that as you can frame your own SDK on top of REST api's.,0.0,0.083,0.917,0.2023
Datadog,49706349,49699969,1,"2018/04/07, 13:26:10",False,"2018/04/07, 13:26:10",17842.0,7862821.0,0,There are two ways to access  dd-trace agent  on host from a container:,0.0,0.0,1.0,0.0
Datadog,49706349,49699969,1,"2018/04/07, 13:26:10",False,"2018/04/07, 13:26:10",17842.0,7862821.0,0,1.,0.0,0.0,1.0,0.0
Datadog,49706349,49699969,1,"2018/04/07, 13:26:10",False,"2018/04/07, 13:26:10",17842.0,7862821.0,0,"Only on  &lt;HOST_IP&gt;:8126 , if docker container is started in a bridge network:",0.0,0.0,1.0,0.0
Datadog,49706349,49699969,1,"2018/04/07, 13:26:10",False,"2018/04/07, 13:26:10",17842.0,7862821.0,0,dd-trace agent  should be bound to  &lt;HOST_IP&gt;  or  0.0.0.0  (which includes  &lt;HOST_IP&gt; ).,0.0,0.0,1.0,0.0
Datadog,49706349,49699969,1,"2018/04/07, 13:26:10",False,"2018/04/07, 13:26:10",17842.0,7862821.0,0,2.,0.0,0.0,1.0,0.0
Datadog,49706349,49699969,1,"2018/04/07, 13:26:10",False,"2018/04/07, 13:26:10",17842.0,7862821.0,0,"On  &lt;HOST_IP&gt;:8126  (if  dd-trace agent  is bound to  &lt;HOST_IP&gt;  or  0.0.0.0 ) and  localhost:8126 , if docker container is started in the host network:",0.0,0.0,1.0,0.0
Datadog,49706349,49699969,1,"2018/04/07, 13:26:10",False,"2018/04/07, 13:26:10",17842.0,7862821.0,0,"As you already try to reach  dd-trace agent  on  localhost:8126 , so the second way is the best solution.",0.0,0.336,0.664,0.765
Datadog,51125051,51124961,2,"2018/07/01, 19:25:56",False,"2018/07/02, 02:14:02",10159.0,4907630.0,2,First step is to install the DataDog agent on the server in which you are running your application:,0.0,0.0,1.0,0.0
Datadog,51125051,51124961,2,"2018/07/01, 19:25:56",False,"2018/07/02, 02:14:02",10159.0,4907630.0,2,https://docs.datadoghq.com/agent/,0.0,0.0,1.0,0.0
Datadog,51125051,51124961,2,"2018/07/01, 19:25:56",False,"2018/07/02, 02:14:02",10159.0,4907630.0,2,You then need to enable the  DogStatsD  service in the DataDog agent:,0.0,0.0,1.0,0.0
Datadog,51125051,51124961,2,"2018/07/01, 19:25:56",False,"2018/07/02, 02:14:02",10159.0,4907630.0,2,https://docs.datadoghq.com/developers/dogstatsd/,0.0,0.0,1.0,0.0
Datadog,51125051,51124961,2,"2018/07/01, 19:25:56",False,"2018/07/02, 02:14:02",10159.0,4907630.0,2,"After that, you can send metrics to the  statsd  agent using any Go library that connects to  statsd .",0.0,0.0,1.0,0.0
Datadog,51125051,51124961,2,"2018/07/01, 19:25:56",False,"2018/07/02, 02:14:02",10159.0,4907630.0,2,For example:,0.0,0.0,1.0,0.0
Datadog,51125051,51124961,2,"2018/07/01, 19:25:56",False,"2018/07/02, 02:14:02",10159.0,4907630.0,2,https://github.com/DataDog/datadog-go,0.0,0.0,1.0,0.0
Datadog,51125051,51124961,2,"2018/07/01, 19:25:56",False,"2018/07/02, 02:14:02",10159.0,4907630.0,2,https://github.com/go-kit/kit/tree/master/metrics/statsd,0.0,0.0,1.0,0.0
Datadog,51125051,51124961,2,"2018/07/01, 19:25:56",False,"2018/07/02, 02:14:02",10159.0,4907630.0,2,Here's an example program sending some counts using the first library:,0.0,0.0,1.0,0.0
Datadog,51145348,51124961,0,"2018/07/03, 04:00:56",False,"2018/07/03, 04:00:56",3174.0,4639336.0,0,Here is a convenience wrapper for DD.,0.0,0.0,1.0,0.0
Datadog,51145348,51124961,0,"2018/07/03, 04:00:56",False,"2018/07/03, 04:00:56",3174.0,4639336.0,0,"It uses ENV vars to configure it, but it's a nice utility to package out common DD calls once you have the agent running in the background.",0.0,0.129,0.871,0.5719
Datadog,52615071,51975736,0,"2018/10/02, 22:23:00",True,"2018/10/04, 04:20:38",899.0,4386440.0,1,I figured out how to do this using the datadog api  https://docs.datadoghq.com/api/?lang=python#post-timeseries-points .,0.0,0.0,1.0,0.0
Datadog,52615071,51975736,0,"2018/10/02, 22:23:00",True,"2018/10/04, 04:20:38",899.0,4386440.0,1,"The following python script takes in the jtl file (jmeter results) and posts the transaction name, response time, and status (pass/fail) to datadog.",0.0,0.0,1.0,0.0
Datadog,52828393,52828258,1,"2018/10/16, 08:20:06",False,"2018/10/16, 08:44:48",21.0,10510929.0,2,You can actually just use:,0.0,0.0,1.0,0.0
Datadog,52828393,52828258,1,"2018/10/16, 08:20:06",False,"2018/10/16, 08:44:48",21.0,10510929.0,2,Datadog's monitor templating feature will fill in the blank and forward to the relevant channel as long as you whitelisted it in the integrations tile for Slack.,0.0,0.0,1.0,0.0
Datadog,61282340,52828258,0,"2020/04/18, 02:32:29",False,"2020/04/18, 02:32:29",5547.0,296829.0,1,"Starts to get messy, but you could nest two ""does not"" conditional variables, like this:",0.254,0.0,0.746,-0.5291
Datadog,60719420,54508636,0,"2020/03/17, 11:20:42",False,"2020/03/17, 11:20:42",23.0,6286219.0,0,AFAIK it is not possible at the Moment to use micrometer to send events to datadog.,0.0,0.0,1.0,0.0
Datadog,60719420,54508636,0,"2020/03/17, 11:20:42",False,"2020/03/17, 11:20:42",23.0,6286219.0,0,"Micrometer states that  ""Micrometer is not a distributed tracing system or an event logger. """,0.0,0.0,1.0,0.0
Datadog,60719420,54508636,0,"2020/03/17, 11:20:42",False,"2020/03/17, 11:20:42",23.0,6286219.0,0,"on its section ""1.",0.0,0.0,1.0,0.0
Datadog,60719420,54508636,0,"2020/03/17, 11:20:42",False,"2020/03/17, 11:20:42",23.0,6286219.0,0,"Purpose"" on its  concepts page .",0.0,0.0,1.0,0.0
Datadog,55661412,55588976,0,"2019/04/13, 05:13:40",True,"2019/04/13, 05:13:40",1371.0,5540166.0,1,This doc will show you a comprehensive list  of all integrations that involve log collection.,0.0,0.133,0.867,0.25
Datadog,55661412,55588976,0,"2019/04/13, 05:13:40",True,"2019/04/13, 05:13:40",1371.0,5540166.0,1,"Some of these include other common log shippers, which can also be used to forward logs to Datadog.",0.0,0.0,1.0,0.0
Datadog,55661412,55588976,0,"2019/04/13, 05:13:40",True,"2019/04/13, 05:13:40",1371.0,5540166.0,1,Among these you'd find...,0.0,0.0,1.0,0.0
Datadog,55661412,55588976,0,"2019/04/13, 05:13:40",True,"2019/04/13, 05:13:40",1371.0,5540166.0,1,"That said, you  can still just use the Datadog agent to collect logs only  (they want you to collect everything with their agent, that's why they warn you against collecting just their logs).",0.042,0.039,0.92,-0.0258
Datadog,55661412,55588976,0,"2019/04/13, 05:13:40",True,"2019/04/13, 05:13:40",1371.0,5540166.0,1,"If you want to collect logs from docker containers, the Datadog agent is an easy way to do that, and it has the benefit of adding lots of relevant docker-metadata as tags to your logs.",0.0,0.184,0.816,0.7351
Datadog,55661412,55588976,0,"2019/04/13, 05:13:40",True,"2019/04/13, 05:13:40",1371.0,5540166.0,1,( Docker log collection instructions here .),0.0,0.0,1.0,0.0
Datadog,55661412,55588976,0,"2019/04/13, 05:13:40",True,"2019/04/13, 05:13:40",1371.0,5540166.0,1,"If you don't want to do that, I'd look at Fluentd first on the list above -- it has a good reputation for containerized log collection, promotes JSON log formatting (for easier processing), and scales reasonably well.",0.029,0.24,0.731,0.8393
Datadog,58421772,58414654,1,"2019/10/17, 00:11:59",True,"2019/10/17, 00:11:59",6375.0,1803990.0,2,You need to tell Datadog to pull custom metric.,0.0,0.0,1.0,0.0
Datadog,58421772,58414654,1,"2019/10/17, 00:11:59",True,"2019/10/17, 00:11:59",6375.0,1803990.0,2,Go to AWS integrations configuration page (Integrations side menu -  Integrations -  Amazon Web Services).,0.0,0.124,0.876,0.1779
Datadog,58421772,58414654,1,"2019/10/17, 00:11:59",True,"2019/10/17, 00:11:59",6375.0,1803990.0,2,"You will see a list of services to integrate with, custom metrics is the last option on list.",0.0,0.0,1.0,0.0
Datadog,58421772,58414654,1,"2019/10/17, 00:11:59",True,"2019/10/17, 00:11:59",6375.0,1803990.0,2,Make sure it's ticked.,0.394,0.324,0.282,-0.128
Datadog,58421772,58414654,1,"2019/10/17, 00:11:59",True,"2019/10/17, 00:11:59",6375.0,1803990.0,2,Takes a while for Datadog to actually start pulling the metric.,0.0,0.0,1.0,0.0
Datadog,58548229,58505214,1,"2019/10/24, 22:47:27",False,"2019/10/24, 22:47:27",336.0,7243426.0,1,Do you have the ability to add some parameters in the logs sent.,0.0,0.161,0.839,0.3182
Datadog,58548229,58505214,1,"2019/10/24, 22:47:27",False,"2019/10/24, 22:47:27",336.0,7243426.0,1,From  the documentation  you should be able to inject the trace id into your logs in a way that Datadog will interpret them.,0.0,0.0,1.0,0.0
Datadog,58548229,58505214,1,"2019/10/24, 22:47:27",False,"2019/10/24, 22:47:27",336.0,7243426.0,1,You can also look at a parser to extract the trace id and span id from the raw log.,0.0,0.0,1.0,0.0
Datadog,58548229,58505214,1,"2019/10/24, 22:47:27",False,"2019/10/24, 22:47:27",336.0,7243426.0,1,This documentation  should help you out on that.,0.0,0.278,0.722,0.4019
Datadog,61929834,58505214,1,"2020/05/21, 10:55:37",False,"2020/05/21, 10:55:37",8464.0,85348.0,1,"From the documentation, if you don't have JSON logs, you need to include  dd.trace_id  and  dd.span_id  in your formatter:",0.0,0.0,1.0,0.0
Datadog,61929834,58505214,1,"2020/05/21, 10:55:37",False,"2020/05/21, 10:55:37",8464.0,85348.0,1,"If your logs are raw formatted, update your formatter to include
   dd.trace_id  and  dd.span_id  in your logger configuration:",0.0,0.0,1.0,0.0
Datadog,61929834,58505214,1,"2020/05/21, 10:55:37",False,"2020/05/21, 10:55:37",8464.0,85348.0,1,"So if you add  %X{dd.trace_id:-0} %X{ dd.span_id:-0} , it should work.",0.0,0.0,1.0,0.0
Datadog,61783745,61769846,0,"2020/05/13, 22:59:10",True,"2020/05/13, 23:10:16",2172.0,3514300.0,1,"A better way would be to use a sidecar container with a logging agent, it won't increase the load on the API server.",0.086,0.127,0.787,0.2354
Datadog,61783745,61769846,0,"2020/05/13, 22:59:10",True,"2020/05/13, 23:10:16",2172.0,3514300.0,1,Reference:  https://kubernetes.io/docs/concepts/cluster-administration/logging/#sidecar-container-with-a-logging-agent,0.0,0.0,1.0,0.0
Datadog,61783745,61769846,0,"2020/05/13, 22:59:10",True,"2020/05/13, 23:10:16",2172.0,3514300.0,1,Datadog agent looks like doesn't support /suggest running as a sidecar ( https://github.com/DataDog/datadog-agent/issues/2203#issuecomment-416180642 ),0.164,0.182,0.654,0.0624
Datadog,61783745,61769846,0,"2020/05/13, 22:59:10",True,"2020/05/13, 23:10:16",2172.0,3514300.0,1,I suggest looking at using other logging agent and pointing the backend to datadog.,0.0,0.0,1.0,0.0
Datadog,61783745,61769846,0,"2020/05/13, 22:59:10",True,"2020/05/13, 23:10:16",2172.0,3514300.0,1,Some options are:,0.0,0.0,1.0,0.0
Datadog,61783745,61769846,0,"2020/05/13, 22:59:10",True,"2020/05/13, 23:10:16",2172.0,3514300.0,1,Datadog supports them,0.0,0.556,0.444,0.3612
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,Is that sample text formatted properly?,0.0,0.0,1.0,0.0
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,The final entity object is missing a  ]  from the end.,0.216,0.0,0.784,-0.296
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,"entity=[HttpEntity.Strict application/json {""type"":""text"",""extract"": ""text"", ""field2"":""text2"",""duration"": 451 }",0.0,0.0,1.0,0.0
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,should be,0.0,0.0,1.0,0.0
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,"entity=[HttpEntity.Strict application/json {""type"":""text"",""extract"": ""text"", ""field2"":""text2"",""duration"": 451 }]",0.0,0.0,1.0,0.0
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,I'm going to continue these instructions assuming that was a typo and the entity field actually ends with  ] .,0.0,0.0,1.0,0.0
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,"If it doesn't, I think you need to fix the underlying log to be formatted properly and close out the bracket.",0.0,0.0,1.0,0.0
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,"Instead of just skipping the entire log and only parsing out that json bit, I decided to parse the entire thing and show what would look good as a final result.",0.0,0.094,0.906,0.4404
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,So the first thing we need to do is pull out that set of key/value pairs after the request object:,0.0,0.0,1.0,0.0
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,"Example Input:  thread-191555 app.main - [cid: 2cacd6f9-546d-41ew-a7ce-d5d41b39eb8f, uid: e6ffc3b0-2f39-44f7-85b6-1abf5f9ad970] Request: protocol=[HTTP/1.0] method=[POST] path=[/metrics] headers=[Timeout-Access: &lt;function1&gt;, Remote-Address: 192.168.0.1:37936, Host: app:5000, Connection: close, X-Real-Ip: 192.168.1.1, X-Forwarded-For: 192.168.1.1, Authorization: ***, Accept: application/json, text/plain, */*, Referer: https://google.com, Accept-Language: cs-CZ, Accept-Encoding: gzip, deflate, User-Agent: Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko, Cache-Control: no-cache] entity=[HttpEntity.Strict application/json {""type"":""text"",""extract"": ""text"", ""field2"":""text2"",""duration"": 451 }]",0.0,0.086,0.914,0.6249
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,"Grok parser rule:  app_log thread-%{integer:thread} %{notSpace:file} - \[%{data::keyvalue("": "")}\] Request: %{data:request:keyvalue(""="","""",""[]"")}",0.0,0.0,1.0,0.0
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,Result:,0.0,0.0,1.0,0.0
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,"Notice how we use the keyvalue parser with a quoting string of  [] , that allows us to easily pull out everything from the request object.",0.0,0.094,0.906,0.34
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,Now the goal is to pull out the details from that entity field inside of the request object.,0.0,0.0,1.0,0.0
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,With Grok parsers you can specify a specific attribute to parse further.,0.0,0.0,1.0,0.0
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,"So in that same pipeline we'll add another grok parser processor, right after our first",0.0,0.0,1.0,0.0
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,"And then configure the advanced options section to run on  request.entity , since that is what we called the attribute",0.0,0.1,0.9,0.25
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,"Example Input:  HttpEntity.Strict application/json {""type"":""text"",""extract"": ""text"", ""field2"":""text2"",""duration"": 451 }",0.0,0.0,1.0,0.0
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,Grok Parser Rule:  entity_rule %{notSpace:request.entity.class} %{notSpace:request.entity.media_type} %{data:request.entity.json:json},0.0,0.0,1.0,0.0
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,Result:,0.0,0.0,1.0,0.0
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,Now when we look at the final parsed log it has everything we need broken out:,0.171,0.0,0.829,-0.4767
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,"Also just because it was really simple, I threw in a third grok processor for the headers chunk as well (the advanced settings are set to parse from  request.headers ):",0.088,0.129,0.784,0.0772
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,"Example Input:  Timeout-Access: &lt;function1&gt;, Remote-Address: 192.168.0.1:37936, Host: app:5000, Connection: close, X-Real-Ip: 192.168.1.1, X-Forwarded-For: 192.168.1.1, Authorization: ***, Accept: application/json, text/plain, */*, Referer: https://google.com, Accept-Language: cs-CZ, Accept-Encoding: gzip, deflate, User-Agent: Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko, Cache-Control: no-cache",0.0,0.121,0.879,0.6249
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,"Grok Parser Rule:  headers_rule %{data:request.headers:keyvalue("": "", ""/)(; :"")}",0.0,0.0,1.0,0.0
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,Result:,0.0,0.0,1.0,0.0
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,The only tricky bit here is that I had to define a characterWhiteList of  /)(; : .,0.118,0.0,0.882,-0.1531
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,Mostly to handle all those special characters are in the  User-Agent  field.,0.0,0.197,0.803,0.4019
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,References :,0.0,0.0,1.0,0.0
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,Just the documentation and some guess &amp; checking in my personal Datadog account.,0.0,0.0,1.0,0.0
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156.0,2676108.0,3,https://docs.datadoghq.com/logs/processing/parsing/?tab=matcher#key-value-or-logfmt,0.0,0.0,1.0,0.0
Datadog,62551284,62549173,1,"2020/06/24, 11:56:43",False,"2020/06/24, 11:56:43",8138.0,2385808.0,2,"When installing Datadog in your K8s Cluster, you install a  Node Logging Agent  as a Daemonset with various volume mounts on the hosting nodes.",0.0,0.0,1.0,0.0
Datadog,62551284,62549173,1,"2020/06/24, 11:56:43",False,"2020/06/24, 11:56:43",8138.0,2385808.0,2,"Among other things, this gives Datadog access to the Pod logs at /var/log/pods and the container logs at /var/lib/docker/containers.",0.0,0.0,1.0,0.0
Datadog,62551284,62549173,1,"2020/06/24, 11:56:43",False,"2020/06/24, 11:56:43",8138.0,2385808.0,2,Kubernetes and the underlying Docker engine will only include output from stdout and stderror in those two locations (see  here  for more information).,0.0,0.0,1.0,0.0
Datadog,62551284,62549173,1,"2020/06/24, 11:56:43",False,"2020/06/24, 11:56:43",8138.0,2385808.0,2,"Everything that is written by containers to log files residing inside the containers, will be invisible to K8s, unless more configuration is applied to extract that data, e.g.",0.0,0.0,1.0,0.0
Datadog,62551284,62549173,1,"2020/06/24, 11:56:43",False,"2020/06/24, 11:56:43",8138.0,2385808.0,2,by applying the  side care container pattern .,0.0,0.348,0.652,0.4939
Datadog,62551284,62549173,1,"2020/06/24, 11:56:43",False,"2020/06/24, 11:56:43",8138.0,2385808.0,2,"So, to get things working in your setup,  configure logback to log to stdout rather than /var/app/logs/myapp.log",0.0,0.0,1.0,0.0
Datadog,62551284,62549173,1,"2020/06/24, 11:56:43",False,"2020/06/24, 11:56:43",8138.0,2385808.0,2,"Also, if you don't use APM there is no need to instrument your code with the datadog.jar and do all that tracing setup (setting up ports etc).",0.078,0.0,0.922,-0.296
Datadog,63974474,63650568,0,"2020/09/20, 03:18:54",False,"2020/09/20, 03:18:54",3852.0,1601506.0,2,You need to tell Datadog that you're interested in that content by creating a facet from the field.,0.0,0.246,0.754,0.5994
Datadog,63974474,63650568,0,"2020/09/20, 03:18:54",False,"2020/09/20, 03:18:54",3852.0,1601506.0,2,"Click a log message, mouse over the attribute name, click the gear on the left, then Create facet for @...",0.0,0.104,0.896,0.2732
Datadog,63974474,63650568,0,"2020/09/20, 03:18:54",False,"2020/09/20, 03:18:54",3852.0,1601506.0,2,"For logs indexed after you create the facet, you can search with  @fieldName:text* , where  fieldName  is the name of your field.",0.0,0.095,0.905,0.2732
Datadog,63974474,63650568,0,"2020/09/20, 03:18:54",False,"2020/09/20, 03:18:54",3852.0,1601506.0,2,You'll need to re-hydrate (reprocess) earlier logs to make them searchable.,0.0,0.0,1.0,0.0
Datadog,63974474,63650568,0,"2020/09/20, 03:18:54",False,"2020/09/20, 03:18:54",3852.0,1601506.0,2,You won't need to create a facet if you use fields from the  standard attributes list .,0.115,0.0,0.885,-0.2057
Datadog,66879609,63650568,0,"2021/03/31, 02:09:46",False,"2021/04/01, 21:13:00",11.0,6062224.0,1,The error message itself is not a good fit to be defined as a facet.,0.419,0.0,0.581,-0.7364
Datadog,66879609,63650568,0,"2021/03/31, 02:09:46",False,"2021/04/01, 21:13:00",11.0,6062224.0,1,If you are using JSON and want the main message (say from a  msg  json field) to be searchable in the Datadog  content  field.,0.0,0.056,0.944,0.0772
Datadog,66879609,63650568,0,"2021/03/31, 02:09:46",False,"2021/04/01, 21:13:00",11.0,6062224.0,1,"Instead of making
facet for  msg , you can define a &quot;Message Remapper&quot; in the log configuration to map it to the  Content .",0.0,0.0,1.0,0.0
Datadog,66879609,63650568,0,"2021/03/31, 02:09:46",False,"2021/04/01, 21:13:00",11.0,6062224.0,1,And then you can do wildcard searches.,0.0,0.0,1.0,0.0
Datadog,66879609,63650568,0,"2021/03/31, 02:09:46",False,"2021/04/01, 21:13:00",11.0,6062224.0,1,log config screenshot,0.0,0.0,1.0,0.0
Datadog,66307900,64126426,0,"2021/02/22, 00:39:04",False,"2021/02/22, 00:44:46",1272.0,1563480.0,0,"After reading this documentation,  https://docs.datadoghq.com/tracing/setup_overview/setup/ruby/#resque , did you try making the options a hash with curly braces surrounding?",0.0,0.0,1.0,0.0
Datadog,66307900,64126426,0,"2021/02/22, 00:39:04",False,"2021/02/22, 00:44:46",1272.0,1563480.0,0,Options is specified as being a hash.,0.0,0.0,1.0,0.0
Datadog,66307900,64126426,0,"2021/02/22, 00:39:04",False,"2021/02/22, 00:44:46",1272.0,1563480.0,0,"So everything after  c.use :resque,   should be a  hash .",0.0,0.0,1.0,0.0
Datadog,37795216,37188932,0,"2016/06/13, 19:53:12",False,"2016/06/13, 19:53:12",261.0,4172512.0,2,You can use the Agent's info command to see if the check is reporting correctly:,0.0,0.0,1.0,0.0
Datadog,37795216,37188932,0,"2016/06/13, 19:53:12",False,"2016/06/13, 19:53:12",261.0,4172512.0,2,https://help.datadoghq.com/hc/en-us/articles/203764635,0.0,0.0,1.0,0.0
Datadog,37795216,37188932,0,"2016/06/13, 19:53:12",False,"2016/06/13, 19:53:12",261.0,4172512.0,2,If the Agent Status shows the check is reporting correctly but your metrics are still not reporting you can contact support@datadoghq.com and they can troubleshoot this issue further.,0.0,0.075,0.925,0.296
Datadog,37795216,37188932,0,"2016/06/13, 19:53:12",False,"2016/06/13, 19:53:12",261.0,4172512.0,2,Hope this helps!,0.0,0.853,0.147,0.6996
Datadog,38752963,38746266,1,"2016/08/03, 23:23:06",False,"2016/08/03, 23:23:06",15334.0,3348604.0,1,This is not correct graph to detect correct resource limit.,0.0,0.0,1.0,0.0
Datadog,38752963,38746266,1,"2016/08/03, 23:23:06",False,"2016/08/03, 23:23:06",15334.0,3348604.0,1,"You graph shows CPU usage of your app in the cluster, but resource limit is per pod (container).",0.0,0.0,1.0,0.0
Datadog,38752963,38746266,1,"2016/08/03, 23:23:06",False,"2016/08/03, 23:23:06",15334.0,3348604.0,1,We (and you as well) don't know from the graph how many containers were up and running.,0.0,0.0,1.0,0.0
Datadog,38752963,38746266,1,"2016/08/03, 23:23:06",False,"2016/08/03, 23:23:06",15334.0,3348604.0,1,You can determinate right CPU limit from the container CPU usage graph(s).,0.0,0.141,0.859,0.2023
Datadog,38752963,38746266,1,"2016/08/03, 23:23:06",False,"2016/08/03, 23:23:06",15334.0,3348604.0,1,You will need Datadog-Docker integration:,0.0,0.0,1.0,0.0
Datadog,38752963,38746266,1,"2016/08/03, 23:23:06",False,"2016/08/03, 23:23:06",15334.0,3348604.0,1,"Please be aware that Kubernetes relies on Heapster to report metrics,
  rather than the cgroup file directly.",0.0,0.126,0.874,0.3182
Datadog,38752963,38746266,1,"2016/08/03, 23:23:06",False,"2016/08/03, 23:23:06",15334.0,3348604.0,1,"The collection interval for
  Heapster is unknown which can lead to innacurate time-related data,
  such as CPU usage.",0.0,0.0,1.0,0.0
Datadog,38752963,38746266,1,"2016/08/03, 23:23:06",False,"2016/08/03, 23:23:06",15334.0,3348604.0,1,"If you require more precise metrics, we recommend
  using the Datadog-Docker Integration.",0.0,0.185,0.815,0.3612
Datadog,38752963,38746266,1,"2016/08/03, 23:23:06",False,"2016/08/03, 23:23:06",15334.0,3348604.0,1,Then it will depends how Datadog measure CPU utilization per container.,0.0,0.0,1.0,0.0
Datadog,38752963,38746266,1,"2016/08/03, 23:23:06",False,"2016/08/03, 23:23:06",15334.0,3348604.0,1,"If container CPU utilization has max 100%, then 100% CPU container utilization ~ 1000m ~ 1.",0.0,0.0,1.0,0.0
Datadog,38752963,38746266,1,"2016/08/03, 23:23:06",False,"2016/08/03, 23:23:06",15334.0,3348604.0,1,I recommend you to read how and when cgroup limits CPU -  https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Resource_Management_Guide/sec-cpu.html,0.0,0.2,0.8,0.3612
Datadog,38752963,38746266,1,"2016/08/03, 23:23:06",False,"2016/08/03, 23:23:06",15334.0,3348604.0,1,You will need a deep knowledge to set proper CPU limits.,0.0,0.0,1.0,0.0
Datadog,38752963,38746266,1,"2016/08/03, 23:23:06",False,"2016/08/03, 23:23:06",15334.0,3348604.0,1,"If you don't need to prioritize any container, then IMHO the best practice is to set 1 ( resources.requests.cpu ) for all your containers - they will have always equal CPU times.",0.0,0.135,0.865,0.6369
Datadog,39751062,39750699,0,"2016/09/28, 17:55:57",False,"2016/09/28, 18:05:43",4213.0,5802417.0,-1,"I am not familiar with the products and libraries that you are using, but there is an open source library  MgntUtils  that can extract full or filtered stacktrace from exception as a String.",0.0,0.0,1.0,0.0
Datadog,39751062,39750699,0,"2016/09/28, 17:55:57",False,"2016/09/28, 18:05:43",4213.0,5802417.0,-1,Since you mentioned that you can pass the text (i.e.,0.0,0.0,1.0,0.0
Datadog,39751062,39750699,0,"2016/09/28, 17:55:57",False,"2016/09/28, 18:05:43",4213.0,5802417.0,-1,String) this library may help you.,0.0,0.351,0.649,0.4019
Datadog,39751062,39750699,0,"2016/09/28, 17:55:57",False,"2016/09/28, 18:05:43",4213.0,5802417.0,-1,Here are the links to  MgntUtils  library:,0.0,0.0,1.0,0.0
Datadog,39751062,39750699,0,"2016/09/28, 17:55:57",False,"2016/09/28, 18:05:43",4213.0,5802417.0,-1,I hope this helps,0.0,0.846,0.154,0.6705
Datadog,41968804,41964906,0,"2017/02/01, 00:39:07",True,"2017/02/01, 00:39:07",26.0,4313641.0,1,"You need to pass  Content-Type  as a header with the request, as shown  in the docs",0.0,0.0,1.0,0.0
Datadog,41968804,41964906,0,"2017/02/01, 00:39:07",True,"2017/02/01, 00:39:07",26.0,4313641.0,1,Response:,0.0,0.0,1.0,0.0
Datadog,41968804,41964906,0,"2017/02/01, 00:39:07",True,"2017/02/01, 00:39:07",26.0,4313641.0,1,"Your data is also not formatted according to the docs (there should be no  dash  field at the top level, for starters).",0.092,0.075,0.833,-0.1027
Datadog,42839492,42815252,1,"2017/03/16, 18:18:52",True,"2017/03/16, 18:35:39",455.0,4676932.0,3,The format you're using to send the data does not comply with the  documentation  and your call fails to complete.,0.128,0.0,0.872,-0.4215
Datadog,42839492,42815252,1,"2017/03/16, 18:18:52",True,"2017/03/16, 18:35:39",455.0,4676932.0,3,The call would work if you change your  $data  to:,0.0,0.0,1.0,0.0
Datadog,42839492,42815252,1,"2017/03/16, 18:18:52",True,"2017/03/16, 18:35:39",455.0,4676932.0,3,"Agreed that the error returned by the API is not really helpful, filed an issue in Datadog to correct that  behavior and return the proper error code and not a 500 (it's actually a 500 and you can see it by printing  curl_getinfo($ch)  after you executed your curl session).",0.15,0.04,0.811,-0.7049
Datadog,44193846,44155361,6,"2017/05/26, 07:50:45",True,"2017/05/26, 07:50:45",1371.0,5540166.0,2,Why not use dogstatsd instead of threadstats?,0.0,0.0,1.0,0.0
Datadog,44193846,44155361,6,"2017/05/26, 07:50:45",True,"2017/05/26, 07:50:45",1371.0,5540166.0,2,"If you're already running the dd-agent on your node in a way that's reachable by your containers, you can use the  datadog.statsd.increment()  method instead to send the metric over statsd to the agent, and from there it would get forwarded to your datadog account.",0.0,0.0,1.0,0.0
Datadog,44193846,44155361,6,"2017/05/26, 07:50:45",True,"2017/05/26, 07:50:45",1371.0,5540166.0,2,"Dogstatsd has the benefits of being more straight-forward and of being somewhat easier to troublehsoot, at least with debug-level logging.",0.0,0.221,0.779,0.6258
Datadog,44193846,44155361,6,"2017/05/26, 07:50:45",True,"2017/05/26, 07:50:45",1371.0,5540166.0,2,"Threadstats sometimes has the benefit of not requiring a dd-agent, but it does very little (if any) error logging, so makes it difficult to troubleshoot cases like these.",0.188,0.206,0.607,-0.09
Datadog,44193846,44155361,6,"2017/05/26, 07:50:45",True,"2017/05/26, 07:50:45",1371.0,5540166.0,2,"If you went the dogstatsd route, you'd use the following code:",0.0,0.0,1.0,0.0
Datadog,44193846,44155361,6,"2017/05/26, 07:50:45",True,"2017/05/26, 07:50:45",1371.0,5540166.0,2,"And from there you'd find your metric metadata with the ""rate"" type and with an interval of ""10"", and you could use the ""as_count"" function to translate the values to counts.",0.0,0.083,0.917,0.4019
Datadog,44406163,44155361,1,"2017/06/07, 10:26:03",False,"2017/06/07, 10:26:03",2644.0,4279006.0,0,In the python script I was initializing with an api key:,0.0,0.0,1.0,0.0
Datadog,44406163,44155361,1,"2017/06/07, 10:26:03",False,"2017/06/07, 10:26:03",2644.0,4279006.0,0,And sending some events,0.0,0.0,1.0,0.0
Datadog,44406163,44155361,1,"2017/06/07, 10:26:03",False,"2017/06/07, 10:26:03",2644.0,4279006.0,0,"When I changed it to initialize like this, it started working with the dd-agent:",0.0,0.172,0.828,0.3612
Datadog,44406163,44155361,1,"2017/06/07, 10:26:03",False,"2017/06/07, 10:26:03",2644.0,4279006.0,0,I didn't need the link command (--link dogstatsd:dogstastd).,0.0,0.0,1.0,0.0
Datadog,44406163,44155361,1,"2017/06/07, 10:26:03",False,"2017/06/07, 10:26:03",2644.0,4279006.0,0,"With that setup it now works in the staging environment, but not in production.",0.0,0.0,1.0,0.0
Datadog,44406163,44155361,1,"2017/06/07, 10:26:03",False,"2017/06/07, 10:26:03",2644.0,4279006.0,0,:/,1.0,0.0,0.0,-0.34
Datadog,45014280,44996259,0,"2017/07/10, 17:09:28",False,"2017/07/10, 17:09:28",1371.0,5540166.0,0,Maybe the  Datadog events-post api endpoint ?,0.0,0.0,1.0,0.0
Datadog,45014280,44996259,0,"2017/07/10, 17:09:28",False,"2017/07/10, 17:09:28",1371.0,5540166.0,0,Or their  metrics-post endpoint ?,0.0,0.0,1.0,0.0
Datadog,45014280,44996259,0,"2017/07/10, 17:09:28",False,"2017/07/10, 17:09:28",1371.0,5540166.0,0,"Although, it's not altogether clear to me what is meant by ""send this record only to Datadog"".",0.12,0.0,0.88,-0.2924
Datadog,45014280,44996259,0,"2017/07/10, 17:09:28",False,"2017/07/10, 17:09:28",1371.0,5540166.0,0,"If you use the log setup for Lambda log parsing that is described in  Datadog's AWS Lambda integration tile , that should be another perfectly good way to get the data collected into Datadog, but I think it may only collect those logs as metrics instead of events.",0.0,0.094,0.906,0.5499
Datadog,46122641,46122135,0,"2017/09/08, 21:34:13",True,"2017/09/08, 22:02:50",1269.0,3294286.0,1,I found the answer thanks to  @tqr_aupa_atleti  and the support team from Datadog.,0.0,0.359,0.641,0.6808
Datadog,46122641,46122135,0,"2017/09/08, 21:34:13",True,"2017/09/08, 22:02:50",1269.0,3294286.0,1,"On the Datadog dashboard panel, I had to click Metrics -  Summary and look for my metric.",0.0,0.0,1.0,0.0
Datadog,46122641,46122135,0,"2017/09/08, 21:34:13",True,"2017/09/08, 22:02:50",1269.0,3294286.0,1,I looked at the tags and I could figure out it was a custom metric form my company that uses data from Amplitude.,0.0,0.0,1.0,0.0
Datadog,49818303,46300932,0,"2018/04/13, 16:34:59",True,"2018/04/13, 16:56:36",939.0,2506172.0,2,"However if you look at this metric carefully it appears to be
  calculating these percentiles on a short range (not sure what) and for
  each tuple of the tags that exist.",0.0,0.119,0.881,0.4215
Datadog,49818303,46300932,0,"2018/04/13, 16:34:59",True,"2018/04/13, 16:56:36",939.0,2506172.0,2,The short range that you have noticed is actually the flush interval which defaults to 10 seconds.,0.0,0.0,1.0,0.0
Datadog,49818303,46300932,0,"2018/04/13, 16:34:59",True,"2018/04/13, 16:56:36",939.0,2506172.0,2,"As per  this  article on histogram metric by datadog,",0.0,0.0,1.0,0.0
Datadog,49818303,46300932,0,"2018/04/13, 16:34:59",True,"2018/04/13, 16:56:36",939.0,2506172.0,2,"It aggregates the values that are sent during the flush interval
  (usually defaults to 10 seconds).",0.0,0.153,0.847,0.4019
Datadog,49818303,46300932,0,"2018/04/13, 16:34:59",True,"2018/04/13, 16:56:36",939.0,2506172.0,2,"So if you send 20 values for a
  metric during the flush interval, it'll give you the aggregation of
  those values for the flush interval",0.0,0.197,0.803,0.6597
Datadog,49818303,46300932,0,"2018/04/13, 16:34:59",True,"2018/04/13, 16:56:36",939.0,2506172.0,2,For your query -,0.0,0.0,1.0,0.0
Datadog,49818303,46300932,0,"2018/04/13, 16:34:59",True,"2018/04/13, 16:56:36",939.0,2506172.0,2,"Ideally what I'd like to get is a 95th percentile of the ResponseTime
  metric over all the tags (maybe I filter it down by 1 or 2 and have a
  couple of different graphs) but over the last week or so.",0.0,0.097,0.903,0.3919
Datadog,49818303,46300932,0,"2018/04/13, 16:34:59",True,"2018/04/13, 16:56:36",939.0,2506172.0,2,"Is there an
  easy way to do this?",0.0,0.293,0.707,0.4404
Datadog,49818303,46300932,0,"2018/04/13, 16:34:59",True,"2018/04/13, 16:56:36",939.0,2506172.0,2,"as per my reading of the datadog docs, there isn't a way to get this done at the moment.",0.0,0.0,1.0,0.0
Datadog,49818303,46300932,0,"2018/04/13, 16:34:59",True,"2018/04/13, 16:56:36",939.0,2506172.0,2,It might be a good idea to check with datadog support regarding this.,0.0,0.359,0.641,0.6808
Datadog,49818303,46300932,0,"2018/04/13, 16:34:59",True,"2018/04/13, 16:56:36",939.0,2506172.0,2,More details  here .,0.0,0.0,1.0,0.0
Datadog,46827281,46826873,0,"2017/10/19, 12:56:44",True,"2017/10/19, 14:14:51",1371.0,5540166.0,3,"I think what you want is to add the  &quot;exact_match: false&quot;  option, like so:",0.0,0.257,0.743,0.4215
Datadog,46827281,46826873,0,"2017/10/19, 12:56:44",True,"2017/10/19, 14:14:51",1371.0,5540166.0,3,This should match on any process whose path+name  include  the search string you provide.,0.0,0.0,1.0,0.0
Datadog,46827281,46826873,0,"2017/10/19, 12:56:44",True,"2017/10/19, 14:14:51",1371.0,5540166.0,3,"Alternatively, if you only want it to match on the name of the process, you'll want to set the search_string to be the exact name of the process that's running (so whatever is given as the name when you run a  ps | grep &quot;ecommerce-order&quot; , which in your case seems to be  ecommerce-order-0.0.1-SNAPSHOT.jar )",0.0,0.05,0.95,0.1531
Datadog,47394149,47142664,0,"2017/11/20, 16:17:28",True,"2017/11/20, 16:17:28",1629.0,1970882.0,1,I am on the same boat.,0.0,0.0,1.0,0.0
Datadog,47394149,47142664,0,"2017/11/20, 16:17:28",True,"2017/11/20, 16:17:28",1629.0,1970882.0,1,I found this link:  datadog instrumentation .,0.0,0.0,1.0,0.0
Datadog,47394149,47142664,0,"2017/11/20, 16:17:28",True,"2017/11/20, 16:17:28",1629.0,1970882.0,1,"So, currently (20.11.2017) there are not agents for C#.",0.0,0.0,1.0,0.0
Datadog,47394149,47142664,0,"2017/11/20, 16:17:28",True,"2017/11/20, 16:17:28",1629.0,1970882.0,1,"Only Go, python and ruby are available.",0.0,0.0,1.0,0.0
Datadog,54639737,47142664,0,"2019/02/11, 23:52:44",False,"2019/03/01, 18:33:14",16566.0,24231.0,0,(Disclosure: I'm a software developer at Datadog.),0.0,0.0,1.0,0.0
Datadog,54639737,47142664,0,"2019/02/11, 23:52:44",False,"2019/03/01, 18:33:14",16566.0,24231.0,0,Datadog's open-source .NET Tracer is currently (2019-02-11) in open beta.,0.0,0.0,1.0,0.0
Datadog,54639737,47142664,0,"2019/02/11, 23:52:44",False,"2019/03/01, 18:33:14",16566.0,24231.0,0,It supports manual and automatic instrumentation and  OpenTracing .,0.0,0.263,0.737,0.3612
Datadog,54639737,47142664,0,"2019/02/11, 23:52:44",False,"2019/03/01, 18:33:14",16566.0,24231.0,0,"For a list of currently supported languages/frameworks, see the  updated documentation .",0.0,0.204,0.796,0.3182
Datadog,54639737,47142664,0,"2019/02/11, 23:52:44",False,"2019/03/01, 18:33:14",16566.0,24231.0,0,Happy tracing!,0.0,0.8,0.2,0.6114
Datadog,47769427,47739117,1,"2017/12/12, 11:41:56",True,"2017/12/12, 11:41:56",2084.0,5156990.0,2,Looks like I found the problem -  https://github.com/DataDog/jenkins-datadog-plugin/issues/101,0.293,0.272,0.435,-0.0516
Datadog,47769427,47739117,1,"2017/12/12, 11:41:56",True,"2017/12/12, 11:41:56",2084.0,5156990.0,2,"current Datadog version 0.6.1. has a bug , after change the Jenkins main config ( any change , not related to Datadog configuration) it stop works.",0.095,0.0,0.905,-0.296
Datadog,47769427,47739117,1,"2017/12/12, 11:41:56",True,"2017/12/12, 11:41:56",2084.0,5156990.0,2,I downgrade it to 0.5.7 and it works OK,0.0,0.295,0.705,0.4466
Datadog,50768198,50768038,0,"2018/06/08, 23:48:11",False,"2018/06/08, 23:48:11",1371.0,5540166.0,0,"Not possible today, but that is in Datadog's plans for development.",0.0,0.0,1.0,0.0
Datadog,50768198,50768038,0,"2018/06/08, 23:48:11",False,"2018/06/08, 23:48:11",1371.0,5540166.0,0,"What you can do as a workaround, though, is add a link to your logs explorer with the query that triggered the monitor alert, so you can get a quick reference to what were the logs that triggered it.",0.0,0.059,0.941,0.296
Datadog,50768198,50768038,0,"2018/06/08, 23:48:11",False,"2018/06/08, 23:48:11",1371.0,5540166.0,0,"This link, for example, would quickly scope you to the error logs over the last 15 minutes:  https://app.datadoghq.com/logs?live=true&amp;query=status%3Aerror&amp;stream_sort=desc",0.137,0.0,0.863,-0.4019
Datadog,50768198,50768038,0,"2018/06/08, 23:48:11",False,"2018/06/08, 23:48:11",1371.0,5540166.0,0,"And markdown is supported, so you can keep your monitor messages prettier without long links in the messages.",0.0,0.252,0.748,0.6597
Datadog,50768198,50768038,0,"2018/06/08, 23:48:11",False,"2018/06/08, 23:48:11",1371.0,5540166.0,0,"Like so:
 [Check the error logs here](https://app.datadoghq.com/logs?live=true&amp;query=status%3Aerror&amp;stream_sort=desc)",0.283,0.239,0.478,-0.1189
Datadog,51045835,51033962,0,"2018/06/26, 17:52:41",False,"2018/06/26, 17:52:41",23418.0,4379151.0,0,It looks to me that these are Datadog specific configuration parameters.,0.0,0.0,1.0,0.0
Datadog,51045835,51033962,0,"2018/06/26, 17:52:41",False,"2018/06/26, 17:52:41",23418.0,4379151.0,0,"So you first need to install the Datadog app to your Slack workspace, which you find on the Slack App Directory.",0.0,0.0,1.0,0.0
Datadog,51045835,51033962,0,"2018/06/26, 17:52:41",False,"2018/06/26, 17:52:41",23418.0,4379151.0,0,Here is how the process is described in the official documentation:,0.0,0.0,1.0,0.0
Datadog,51045835,51033962,0,"2018/06/26, 17:52:41",False,"2018/06/26, 17:52:41",23418.0,4379151.0,0,Installation,0.0,0.0,1.0,0.0
Datadog,51045835,51033962,0,"2018/06/26, 17:52:41",False,"2018/06/26, 17:52:41",23418.0,4379151.0,0,"The Slack integration is installed via its integration tile in the
  Datadog application.",0.0,0.0,1.0,0.0
Datadog,51045835,51033962,0,"2018/06/26, 17:52:41",False,"2018/06/26, 17:52:41",23418.0,4379151.0,0,Configuration,0.0,0.0,1.0,0.0
Datadog,51045835,51033962,0,"2018/06/26, 17:52:41",False,"2018/06/26, 17:52:41",23418.0,4379151.0,0,Source:  official documentation  from Datadog,0.0,0.0,1.0,0.0
Datadog,53290808,52891518,0,"2018/11/14, 01:03:15",False,"2018/11/14, 01:03:15",51.0,6111276.0,1,This might be a problem that  other people are running into too .,0.213,0.0,0.787,-0.4019
Datadog,53290808,52891518,0,"2018/11/14, 01:03:15",False,"2018/11/14, 01:03:15",51.0,6111276.0,1,"kubelet is no longer listening on the ReadOnlyPort in newer Kubernetes versions, and the port is being deprecated.",0.115,0.0,0.885,-0.296
Datadog,53290808,52891518,0,"2018/11/14, 01:03:15",False,"2018/11/14, 01:03:15",51.0,6111276.0,1,Samuel Cormier-Iijima reports that the issue can be solved by adding adding  KUBELET_EXTRA_ARGS=--read-only-port=10255  in  /etc/default/kubelet  on the node host.,0.0,0.104,0.896,0.2732
Datadog,53029359,52989371,0,"2018/10/28, 09:39:29",False,"2018/10/28, 09:39:29",113.0,4742614.0,1,It is in fact possible to send an alert if a metric shows the same value for a fix period of time.,0.0,0.204,0.796,0.5574
Datadog,53029359,52989371,0,"2018/10/28, 09:39:29",False,"2018/10/28, 09:39:29",113.0,4742614.0,1,You can do this by using the diff() function to your query to produce delta values from consecutive delta points and then apply the abs() function to take absolute values of these deltas.,0.0,0.148,0.852,0.6597
Datadog,53029359,52989371,0,"2018/10/28, 09:39:29",False,"2018/10/28, 09:39:29",113.0,4742614.0,1,"To do this we use the Arithmetic functions available, which can be applied using the '+' button to your query in UI.",0.0,0.0,1.0,0.0
Datadog,53029359,52989371,0,"2018/10/28, 09:39:29",False,"2018/10/28, 09:39:29",113.0,4742614.0,1,"For alert conditions in the metric monitor itself, configure as follows:
Select threshold alert
Set the “Trigger when the metric is…” dropdown selector to below or equal to
Set the “Alert Threshold” field to 0 (zero)",0.0,0.118,0.882,0.5267
Datadog,53029359,52989371,0,"2018/10/28, 09:39:29",False,"2018/10/28, 09:39:29",113.0,4742614.0,1,This configuration will trigger an alert event when no change in value has been registered over the selected timeframe.,0.096,0.202,0.702,0.34
Datadog,53029359,52989371,0,"2018/10/28, 09:39:29",False,"2018/10/28, 09:39:29",113.0,4742614.0,1,Here is a link to datadog article:  https://docs.datadoghq.com/monitors/faq/how-can-i-configure-a-metric-monitor-to-alert-on-no-change-in-value/,0.0,0.0,1.0,0.0
Datadog,54813734,54811591,0,"2019/02/21, 20:18:42",False,"2019/02/21, 20:18:42",790.0,1958107.0,1,By introducing space in datadog.json.j2 template definition .i.e.,0.0,0.0,1.0,0.0
Datadog,54813734,54811591,0,"2019/02/21, 20:18:42",False,"2019/02/21, 20:18:42",790.0,1958107.0,1,and running deployment again I got the working config as below,0.0,0.0,1.0,0.0
Datadog,54813734,54811591,0,"2019/02/21, 20:18:42",False,"2019/02/21, 20:18:42",790.0,1958107.0,1,However I am not able to understand the behaviour if anyone could help me understand it,0.0,0.162,0.838,0.4019
Datadog,55152101,55087322,0,"2019/03/14, 00:27:39",False,"2019/03/14, 00:27:39",1371.0,5540166.0,0,it appears to me like you're missing a couple environment vars in your docker-compose  datadog  service configuration.,0.118,0.134,0.749,0.0772
Datadog,55152101,55087322,0,"2019/03/14, 00:27:39",False,"2019/03/14, 00:27:39",1371.0,5540166.0,0,And also the volume that adds the registry for tailing the logs from the docker socket.,0.0,0.0,1.0,0.0
Datadog,55152101,55087322,0,"2019/03/14, 00:27:39",False,"2019/03/14, 00:27:39",1371.0,5540166.0,0,Maybe try something like this if you haven't?,0.0,0.263,0.737,0.3612
Datadog,55152101,55087322,0,"2019/03/14, 00:27:39",False,"2019/03/14, 00:27:39",1371.0,5540166.0,0,"from there, if you still end up with trouble, you may want to reach out to support@datadoghq.com to ask for help.",0.109,0.206,0.685,0.1027
Datadog,55152101,55087322,0,"2019/03/14, 00:27:39",False,"2019/03/14, 00:27:39",1371.0,5540166.0,0,They're pretty quick to reply.,0.0,0.444,0.556,0.4939
Datadog,55806500,55632833,0,"2019/04/23, 10:46:35",True,"2019/04/23, 10:46:35",410.0,5254815.0,0,"Installed it as ""Run with Admin rights"" and it fixed the issue.",0.0,0.0,1.0,0.0
Datadog,57805506,55632833,0,"2019/09/05, 15:28:28",False,"2019/09/05, 15:28:28",971.0,3271599.0,0,"For me, I had to manually give the  ddagentuser  account read access to the file",0.0,0.0,1.0,0.0
Datadog,57805506,55632833,0,"2019/09/05, 15:28:28",False,"2019/09/05, 15:28:28",971.0,3271599.0,0,C:\ProgramData\Datadog\auth_token,0.0,0.0,1.0,0.0
Datadog,59403759,58599000,0,"2019/12/19, 07:30:11",False,"2019/12/19, 09:15:25",1.0,12559838.0,0,Have you tried specifying dependencies with  go mod  yet?,0.0,0.0,1.0,0.0
Datadog,59403759,58599000,0,"2019/12/19, 07:30:11",False,"2019/12/19, 09:15:25",1.0,12559838.0,0,"I faced the same issue and finally can solved by generating  go.mod  file with these command,",0.0,0.13,0.87,0.2732
Datadog,59403759,58599000,0,"2019/12/19, 07:30:11",False,"2019/12/19, 09:15:25",1.0,12559838.0,0,Here  is the details explanation.,0.0,0.0,1.0,0.0
Datadog,61108577,61091978,0,"2020/04/08, 22:31:05",False,"2020/04/08, 22:31:05",61.0,6722990.0,2,"I think the disconnect here is that the pattern needs to be in the Jboss log manager, and then they're encoded to JSON.",0.0,0.0,1.0,0.0
Datadog,61108577,61091978,0,"2020/04/08, 22:31:05",False,"2020/04/08, 22:31:05",61.0,6722990.0,2,Have you tried puttinng  %X{dd.trace_id:-0} %X{dd.span_id:-0}  into your Jboss logging pattern?,0.0,0.0,1.0,0.0
Datadog,61108577,61091978,0,"2020/04/08, 22:31:05",False,"2020/04/08, 22:31:05",61.0,6722990.0,2,"If not, I'd also recommend opening a ticket at support@datadoghq.com and we can work this through with you.",0.117,0.0,0.883,-0.2755
Datadog,66962237,61092487,0,"2021/04/06, 06:12:46",False,"2021/04/06, 06:12:46",733.0,1219379.0,0,"I’m not sure if it is a recent addition, but the Datadog public API supports configuring Log Archives:  https://docs.datadoghq.com/api/latest/logs-archives/",0.071,0.157,0.772,0.4155
Datadog,66962237,61092487,0,"2021/04/06, 06:12:46",False,"2021/04/06, 06:12:46",733.0,1219379.0,0,You can also use tools like Terraform to configure them:  https://registry.terraform.io/providers/DataDog/datadog/latest/docs/resources/logs_archive  (it uses the Datadog API internally),0.0,0.135,0.865,0.3612
Datadog,61247522,61148607,1,"2020/04/16, 13:00:50",True,"2020/04/16, 13:00:50",408.0,3754710.0,1,"If you are trying to connect to an HTTPS URL for Datadog ( https://app.datadoghq.com  in your example), then you will need to set the  https.proxyHost  system property for it to have effect -  http.proxyHost  is for HTTP URLs[1].",0.0,0.0,1.0,0.0
Datadog,61247522,61148607,1,"2020/04/16, 13:00:50",True,"2020/04/16, 13:00:50",408.0,3754710.0,1,These are system-wide settings that will be used by the default  HttpSender  ( HttpUrlConnectionSender ) if a  Proxy  is not passed to its constructor.,0.0,0.0,1.0,0.0
Datadog,61247522,61148607,1,"2020/04/16, 13:00:50",True,"2020/04/16, 13:00:50",408.0,3754710.0,1,The micrometer doc says,0.0,0.0,1.0,0.0
Datadog,61247522,61148607,1,"2020/04/16, 13:00:50",True,"2020/04/16, 13:00:50",408.0,3754710.0,1,But I dont understand what it means?,0.0,0.0,1.0,0.0
Datadog,61247522,61148607,1,"2020/04/16, 13:00:50",True,"2020/04/16, 13:00:50",408.0,3754710.0,1,"should I replace this url with my
  proxy url or is there any specific uri pattern with the proxy?",0.0,0.0,1.0,0.0
Datadog,61247522,61148607,1,"2020/04/16, 13:00:50",True,"2020/04/16, 13:00:50",408.0,3754710.0,1,"This is referring to a different kind of proxy that you would configure to receive your Datadog traffic on your internal network, and it would forward it to Datadog outside of your network.",0.0,0.0,1.0,0.0
Datadog,61247522,61148607,1,"2020/04/16, 13:00:50",True,"2020/04/16, 13:00:50",408.0,3754710.0,1,If you are using an HTTP proxy then you should use the system properties or an  HttpSender  with your HTTP proxy configured (e.g.,0.0,0.0,1.0,0.0
Datadog,61247522,61148607,1,"2020/04/16, 13:00:50",True,"2020/04/16, 13:00:50",408.0,3754710.0,1,an  HttpUrlConnectionSender  and passing a  Proxy  to its constructor).,0.0,0.0,1.0,0.0
Datadog,61247522,61148607,1,"2020/04/16, 13:00:50",True,"2020/04/16, 13:00:50",408.0,3754710.0,1,You can configure a custom  HttpSender  with a  DatadogMeterRegistry  using its  Builder .,0.0,0.0,1.0,0.0
Datadog,61247522,61148607,1,"2020/04/16, 13:00:50",True,"2020/04/16, 13:00:50",408.0,3754710.0,1,"If you expose this as a  Bean  in a  @Configuration  class, Spring Boot will use it in its auto-configuration.",0.091,0.0,0.909,-0.1531
Datadog,61247522,61148607,1,"2020/04/16, 13:00:50",True,"2020/04/16, 13:00:50",408.0,3754710.0,1,For example:,0.0,0.0,1.0,0.0
Datadog,61247522,61148607,1,"2020/04/16, 13:00:50",True,"2020/04/16, 13:00:50",408.0,3754710.0,1,[1]  https://docs.oracle.com/javase/8/docs/technotes/guides/net/proxies.html,0.0,0.0,1.0,0.0
Datadog,62798229,62787931,3,"2020/07/08, 18:23:57",True,"2020/07/08, 18:23:57",1371.0,5540166.0,2,That's all handled in the  message  attribute with  conditional logic variables .,0.0,0.0,1.0,0.0
Datadog,62798229,62787931,3,"2020/07/08, 18:23:57",True,"2020/07/08, 18:23:57",1371.0,5540166.0,2,"If, for example, you define your  message  value to be this...",0.0,0.194,0.806,0.34
Datadog,62798229,62787931,3,"2020/07/08, 18:23:57",True,"2020/07/08, 18:23:57",1371.0,5540166.0,2,... then ...,0.0,0.0,1.0,0.0
Datadog,63907036,63859370,0,"2020/09/15, 20:27:53",False,"2020/09/16, 01:09:03",31.0,12517930.0,0,got this resolved with below,0.0,0.298,0.702,0.1779
Datadog,63907036,63859370,0,"2020/09/15, 20:27:53",False,"2020/09/16, 01:09:03",31.0,12517930.0,0,"rule1  %{ipv4:network.client.ip}\s+-\s+%{word:user}\s+\[%{date(&quot;dd/MMM/yyyy:HH:mm:ss Z&quot;):date}\]\s+\&quot;%{word:http.module}\s+\/v1\/resources\/+%{word:onemds.module}\?+%{data:onemds.params:keyvalue(&quot;=&quot;,&quot;/:&quot;,&quot;&quot;,&quot;:&amp;&quot;)}",0.0,0.0,1.0,0.0
Datadog,63907036,63859370,0,"2020/09/15, 20:27:53",False,"2020/09/16, 01:09:03",31.0,12517930.0,0,was never expecting the delimiter that can take 2 characters above  :&amp;,0.0,0.0,1.0,0.0
Datadog,63907036,63859370,0,"2020/09/15, 20:27:53",False,"2020/09/16, 01:09:03",31.0,12517930.0,0,this helped to remove  rs:  for all except the first one.,0.0,0.0,1.0,0.0
Datadog,63907036,63859370,0,"2020/09/15, 20:27:53",False,"2020/09/16, 01:09:03",31.0,12517930.0,0,not an elegant approach but it worked for my use case.,0.151,0.0,0.849,-0.1967
Datadog,64447994,64395922,0,"2020/10/20, 18:05:44",False,"2020/10/20, 18:05:44",23.0,9053059.0,1,Found the answer:,0.0,0.0,1.0,0.0
Datadog,64447994,64395922,0,"2020/10/20, 18:05:44",False,"2020/10/20, 18:05:44",23.0,9053059.0,1,The metric  kubernetes.kubelet.volume.stats.used_bytes  will allow you to get the disk usage on PersistentVolumes.,0.0,0.137,0.863,0.2263
Datadog,64447994,64395922,0,"2020/10/20, 18:05:44",False,"2020/10/20, 18:05:44",23.0,9053059.0,1,This can be achieved using the tag  persistentvolumeclaim  on this metric.,0.0,0.0,1.0,0.0
Datadog,64447994,64395922,0,"2020/10/20, 18:05:44",False,"2020/10/20, 18:05:44",23.0,9053059.0,1,You can view this metric and tag combination in your account here:  https://app.datadoghq.com/metric/summary?filter=kubernetes.kubelet.volume.stats.used_bytes&amp;metric=kubernetes.kubelet.volume.stats.used_bytes,0.0,0.0,1.0,0.0
Datadog,64447994,64395922,0,"2020/10/20, 18:05:44",False,"2020/10/20, 18:05:44",23.0,9053059.0,1,Documentation on this metric can be found here:  https://docs.datadoghq.com/agent/kubernetes/data_collected/#kubelet,0.0,0.0,1.0,0.0
Datadog,64484497,64482767,4,"2020/10/22, 17:32:13",False,"2020/10/22, 19:17:02",156.0,2676108.0,1,"This answer doesn't solve the problem, because postgres is not running in the cluster, it's running in Azure.",0.08,0.114,0.806,0.1695
Datadog,64484497,64482767,4,"2020/10/22, 17:32:13",False,"2020/10/22, 19:17:02",156.0,2676108.0,1,"I'll leave it up since it might be interesting, but I posted another answer for the actually environment setup.",0.058,0.098,0.844,0.1901
Datadog,64484497,64482767,4,"2020/10/22, 17:32:13",False,"2020/10/22, 19:17:02",156.0,2676108.0,1,"For containerized setups, it's not usually recommended to set up a configmap or try giving the agent a yaml file.",0.08,0.12,0.8,0.2042
Datadog,64484497,64482767,4,"2020/10/22, 17:32:13",False,"2020/10/22, 19:17:02",156.0,2676108.0,1,Instead the recommended configuration is to put annotations on the postgres pod:  https://docs.datadoghq.com/integrations/postgres/?tab=containerized#containerized .,0.0,0.13,0.87,0.2023
Datadog,64484497,64482767,4,"2020/10/22, 17:32:13",False,"2020/10/22, 19:17:02",156.0,2676108.0,1,"This concept of placing the config on the application pod, not with the datadog agent, is called autodiscovery.",0.0,0.0,1.0,0.0
Datadog,64484497,64482767,4,"2020/10/22, 17:32:13",False,"2020/10/22, 19:17:02",156.0,2676108.0,1,This blog post does a good job explaining the benefits of this solution:  https://www.datadoghq.com/blog/monitoring-kubernetes-with-datadog/#autodiscovery,0.0,0.438,0.562,0.7783
Datadog,64484497,64482767,4,"2020/10/22, 17:32:13",False,"2020/10/22, 19:17:02",156.0,2676108.0,1,Here is a picture diagram showing how the agent goes out to the pods on the same node and would pull the configuration from them:,0.0,0.0,1.0,0.0
Datadog,64484497,64482767,4,"2020/10/22, 17:32:13",False,"2020/10/22, 19:17:02",156.0,2676108.0,1,"To configure this, you'd take each of the sections of the yaml config, convert them to json, and set them as annotations on the postgres manifest.",0.0,0.0,1.0,0.0
Datadog,64484497,64482767,4,"2020/10/22, 17:32:13",False,"2020/10/22, 19:17:02",156.0,2676108.0,1,"An example of how to set up pod annotations is provided for redis, apache, and http here:  https://docs.datadoghq.com/agent/kubernetes/integrations/?tab=kubernetes#examples",0.0,0.0,1.0,0.0
Datadog,64484497,64482767,4,"2020/10/22, 17:32:13",False,"2020/10/22, 19:17:02",156.0,2676108.0,1,For your scenario I would do something like:,0.0,0.294,0.706,0.3612
Datadog,64484497,64482767,4,"2020/10/22, 17:32:13",False,"2020/10/22, 19:17:02",156.0,2676108.0,1,"notice how the folder name  postgres.d/conf.yaml  maps to the  check_names  annotation, the  init_configs  section maps to  init_configs  annotation, etc.",0.0,0.0,1.0,0.0
Datadog,64484497,64482767,4,"2020/10/22, 17:32:13",False,"2020/10/22, 19:17:02",156.0,2676108.0,1,"For the section on custom metrics, since I personally am more familiar with the yaml config, and it's easier to just fill out, I'll usually go to a yaml to json converter, and copy the json from there",0.0,0.074,0.926,0.4215
Datadog,64484497,64482767,4,"2020/10/22, 17:32:13",False,"2020/10/22, 19:17:02",156.0,2676108.0,1,A key thing to notice for all those configs is that I never set the hostname.,0.0,0.0,1.0,0.0
Datadog,64484497,64482767,4,"2020/10/22, 17:32:13",False,"2020/10/22, 19:17:02",156.0,2676108.0,1,That is automatically discovered by the agent as it scans through containers.,0.0,0.0,1.0,0.0
Datadog,64484497,64482767,4,"2020/10/22, 17:32:13",False,"2020/10/22, 19:17:02",156.0,2676108.0,1,"However you may have set  my-postgres-host.com  because this postgres instance is not actually running in your kubernetes cluster, and is instead living on its own, and not in a container.",0.0,0.0,1.0,0.0
Datadog,64484497,64482767,4,"2020/10/22, 17:32:13",False,"2020/10/22, 19:17:02",156.0,2676108.0,1,"If that is the case, I would recommend trying to just put the agent on the postgres node directly, all the yaml stuff you've done would work just fine, if that db and the agent are both directly on the vm.",0.0,0.102,0.898,0.5106
Datadog,64486131,64482767,0,"2020/10/22, 19:02:54",False,"2020/10/22, 19:11:25",156.0,2676108.0,1,It looks like the question has been updated to say that this postgres db you are trying to monitor is not actually running the the cluster.,0.0,0.091,0.909,0.3612
Datadog,64486131,64482767,0,"2020/10/22, 19:02:54",False,"2020/10/22, 19:11:25",156.0,2676108.0,1,"And you are not able to put an agent directly on the postgres server since it's a managed service in Azure, so you don't have access to the underlying host.",0.0,0.0,1.0,0.0
Datadog,64486131,64482767,0,"2020/10/22, 19:02:54",False,"2020/10/22, 19:11:25",156.0,2676108.0,1,"In those situations it is common to have a random datadog agent on some other host set up the postgres integration anyway, but instead of having  host: localhost  in the yaml config, put the hostname you would put to access the db externally.",0.0,0.0,1.0,0.0
Datadog,64486131,64482767,0,"2020/10/22, 19:02:54",False,"2020/10/22, 19:11:25",156.0,2676108.0,1,In your example it was  host: my-postgres-host.com .,0.0,0.0,1.0,0.0
Datadog,64486131,64482767,0,"2020/10/22, 19:02:54",False,"2020/10/22, 19:11:25",156.0,2676108.0,1,This provides all the same benefits of the normal integration (except you won't have cpu/disk/resource metrics available obviously),0.0,0.133,0.867,0.3818
Datadog,64486131,64482767,0,"2020/10/22, 19:02:54",False,"2020/10/22, 19:11:25",156.0,2676108.0,1,"This is all fine and makes sense, but what if all of the agents you have installed are the agents in the kubernetes daemonset you created?",0.0,0.14,0.86,0.4404
Datadog,64486131,64482767,0,"2020/10/22, 19:02:54",False,"2020/10/22, 19:11:25",156.0,2676108.0,1,You don't have any hosts directly on VMs to run this check.,0.0,0.0,1.0,0.0
Datadog,64486131,64482767,0,"2020/10/22, 19:02:54",False,"2020/10/22, 19:11:25",156.0,2676108.0,1,But we definitely don't recommend configuring the daemonset to run this check directly.,0.155,0.206,0.639,0.2228
Datadog,64486131,64482767,0,"2020/10/22, 19:02:54",False,"2020/10/22, 19:11:25",156.0,2676108.0,1,"If you did, that would mean you are collecting duplicate metrics from that one postgres db in every single node in your cluster.",0.0,0.0,1.0,0.0
Datadog,64486131,64482767,0,"2020/10/22, 19:02:54",False,"2020/10/22, 19:11:25",156.0,2676108.0,1,"Since every agent is a copy, they'd each be running the same check on the same db you define.",0.0,0.0,1.0,0.0
Datadog,64486131,64482767,0,"2020/10/22, 19:02:54",False,"2020/10/22, 19:11:25",156.0,2676108.0,1,Luckily I notice that you are running the Datadog Cluster Agent.,0.0,0.268,0.732,0.5106
Datadog,64486131,64482767,0,"2020/10/22, 19:02:54",False,"2020/10/22, 19:11:25",156.0,2676108.0,1,"This is a separate Datadog tool that is deployed as a single service once per cluster, instead of a daemonset running once per node.",0.0,0.0,1.0,0.0
Datadog,64486131,64482767,0,"2020/10/22, 19:02:54",False,"2020/10/22, 19:11:25",156.0,2676108.0,1,It is possible to have the cluster agent configured to run 'cluster level' checks.,0.0,0.0,1.0,0.0
Datadog,64486131,64482767,0,"2020/10/22, 19:02:54",False,"2020/10/22, 19:11:25",156.0,2676108.0,1,"Perfect for things like databases, message queues, or http checks.",0.0,0.437,0.563,0.7351
Datadog,64486131,64482767,0,"2020/10/22, 19:02:54",False,"2020/10/22, 19:11:25",156.0,2676108.0,1,The basic idea is that (in addition to it's other jobs) the cluster agent will also schedule checks.,0.0,0.0,1.0,0.0
Datadog,64486131,64482767,0,"2020/10/22, 19:02:54",False,"2020/10/22, 19:11:25",156.0,2676108.0,1,"the DCA (datadog cluster agent) will choose one agent from the daemonset to run the check, and if that node agent pod dies, the DCA will find a new one to run the cluster check.",0.0,0.0,1.0,0.0
Datadog,64486131,64482767,0,"2020/10/22, 19:02:54",False,"2020/10/22, 19:11:25",156.0,2676108.0,1,Here are the docs on how to set up the DCA to run cluster checks:  https://docs.datadoghq.com/agent/cluster_agent/clusterchecks/#how-it-works,0.0,0.0,1.0,0.0
Datadog,64486131,64482767,0,"2020/10/22, 19:02:54",False,"2020/10/22, 19:11:25",156.0,2676108.0,1,"To configure it you would enable some flags, and give the DCA the yaml file you created with a config map, or just mounting the file directly.",0.0,0.074,0.926,0.25
Datadog,64486131,64482767,0,"2020/10/22, 19:02:54",False,"2020/10/22, 19:11:25",156.0,2676108.0,1,The DCA will pass along that config to whichever node agent it chooses to run the check.,0.0,0.0,1.0,0.0
Datadog,65583978,65580606,0,"2021/01/05, 19:47:25",False,"2021/01/05, 19:47:25",45878.0,180368.0,1,Answering my own question.,0.0,0.0,1.0,0.0
Datadog,65583978,65580606,0,"2021/01/05, 19:47:25",False,"2021/01/05, 19:47:25",45878.0,180368.0,1,The DataDog logging page has a Configuration section.,0.0,0.0,1.0,0.0
Datadog,65583978,65580606,0,"2021/01/05, 19:47:25",False,"2021/01/05, 19:47:25",45878.0,180368.0,1,On that page the &quot;Pre processing for JSON logs&quot; section allows you to specify alternate property names for a few of the major log message properties.,0.0,0.0,1.0,0.0
Datadog,65583978,65580606,0,"2021/01/05, 19:47:25",False,"2021/01/05, 19:47:25",45878.0,180368.0,1,If you add @m to the Message attributes section and @l to the Status attributes section you will correctly ingest JSON messages from the  RenderedCompactJsonFormatter  formatter.,0.0,0.0,1.0,0.0
Datadog,65583978,65580606,0,"2021/01/05, 19:47:25",False,"2021/01/05, 19:47:25",45878.0,180368.0,1,If you add RenderedMessage and Level respectively you will correctly ingest  JsonFormatter(renderMessage: true)  formatter.,0.0,0.156,0.844,0.34
Datadog,65583978,65580606,0,"2021/01/05, 19:47:25",False,"2021/01/05, 19:47:25",45878.0,180368.0,1,"You can specify multiple attributes in each section, so you can simultaneously support both formats.",0.0,0.162,0.838,0.4019
Datadog,66284080,66113635,0,"2021/02/19, 21:45:06",False,"2021/02/19, 21:45:06",383.0,2032722.0,1,"After few days of research and follow up with datadog support team, I am able to get the APM logs on datadog portal.",0.0,0.114,0.886,0.4019
Datadog,66284080,66113635,0,"2021/02/19, 21:45:06",False,"2021/02/19, 21:45:06",383.0,2032722.0,1,"Below is my  docker-compose.yml  file configuration,  I believe it helps someone in future",0.0,0.191,0.809,0.3818
Datadog,66284080,66113635,0,"2021/02/19, 21:45:06",False,"2021/02/19, 21:45:06",383.0,2032722.0,1,The  Dockerfile  for my python long running application,0.0,0.0,1.0,0.0
Datadog,66284080,66113635,0,"2021/02/19, 21:45:06",False,"2021/02/19, 21:45:06",383.0,2032722.0,1,"Please note, on the requirements.txt file I have   ddtrace  package listed",0.0,0.204,0.796,0.3182
Datadog,66329196,66172172,1,"2021/02/23, 10:00:35",False,"2021/02/23, 10:00:35",871.0,7779815.0,0,"In the  instructions  it refers to http://ccloudexporter_ccloud_exporter_1:2112/metrics in the open metrics file, but in my setup docker-compose gave the ccloud exporter container the name ccloud_ccloud_exporter_1.",0.0,0.0,1.0,0.0
Datadog,66329196,66172172,1,"2021/02/23, 10:00:35",False,"2021/02/23, 10:00:35",871.0,7779815.0,0,To prevent this I added &quot;container_name: ccloud_ccloud_exporter_1&quot; in the docker compose file and used &quot;http://ccloud_ccloud_exporter_1:2112/metrics&quot; as prometheus url in the openmetrics file.,0.0,0.052,0.948,0.0258
Datadog,64973799,64929616,0,"2020/11/23, 20:02:47",True,"2020/11/23, 20:08:08",251.0,5477963.0,0,Tracing for requests coming from the browser are handled by RUM and not APM.,0.0,0.0,1.0,0.0
Datadog,64973799,64929616,0,"2020/11/23, 20:02:47",True,"2020/11/23, 20:08:08",251.0,5477963.0,0,I can't find the documentation for it but there is a configuration option on the  browser SDK  to allow tracing to specific endpoints.,0.0,0.105,0.895,0.3291
Datadog,64973799,64929616,0,"2020/11/23, 20:02:47",True,"2020/11/23, 20:08:08",251.0,5477963.0,0,"The tracing context will automatically be propagated to APM if enabled on the server side, and both the browser and server spans will appear in the same trace.",0.0,0.0,1.0,0.0
Datadog,63815085,63814864,11,"2020/09/09, 18:41:28",False,"2020/09/09, 18:41:28",30563.0,605153.0,2,Usually metrics exposed to  /actuator/metrics  are sent to the metrics system like datadog.,0.088,0.169,0.743,0.296
Datadog,63815085,63814864,11,"2020/09/09, 18:41:28",False,"2020/09/09, 18:41:28",30563.0,605153.0,2,You can try to check what exactly gets sent to datadog by examining the source code of  DatadogMeterRegistry,0.0,0.0,1.0,0.0
Datadog,63815085,63814864,11,"2020/09/09, 18:41:28",False,"2020/09/09, 18:41:28",30563.0,605153.0,2,"Put a breakpoint in the publish method and see what gets sent, or, alternatively set the logger of the class to &quot;trace&quot; so that it will print the information that gets sent to the datadog (line 131 in the linked source code).",0.0,0.0,1.0,0.0
Datadog,63815085,63814864,11,"2020/09/09, 18:41:28",False,"2020/09/09, 18:41:28",30563.0,605153.0,2,Another possible direction to check is usage of filters (see  MeterFilter ) that can filter out some metrics.,0.0,0.0,1.0,0.0
Datadog,63835825,63814864,0,"2020/09/10, 21:39:35",True,"2020/09/10, 22:25:47",205.0,8534030.0,2,This did the trick : Thanks to @MarkBramnik,0.132,0.319,0.549,0.4019
Datadog,65920537,63314162,2,"2021/01/27, 15:50:52",True,"2021/01/27, 15:50:52",48.0,8161041.0,1,Try this,0.0,0.0,1.0,0.0
Datadog,62545412,62545185,0,"2020/06/24, 02:43:20",False,"2020/06/24, 02:43:20",1371.0,5540166.0,1,You might be able to get this if you...,0.0,0.0,1.0,0.0
Datadog,62545412,62545185,0,"2020/06/24, 02:43:20",False,"2020/06/24, 02:43:20",1371.0,5540166.0,1,"If you start having trouble as you start setting it up, you may want to reach out to support@datadoghq.com to get their assistance.",0.108,0.096,0.797,-0.3182
Datadog,61524549,61521576,0,"2020/04/30, 16:28:56",False,"2020/04/30, 16:28:56",1824.0,944768.0,3,"this seem to be working:
 -@userId:*?",0.0,0.0,1.0,0.0
Datadog,61524549,61521576,0,"2020/04/30, 16:28:56",False,"2020/04/30, 16:28:56",1824.0,944768.0,3,*  do not forget the minus at the start.,0.0,0.192,0.808,0.1695
Datadog,61128911,61108009,2,"2020/04/09, 22:36:58",False,"2020/04/15, 21:52:15",1371.0,5540166.0,1,Either of the  count_not_null()  or  count_nonzero()  functions should get you where you want.,0.0,0.098,0.902,0.0772
Datadog,61128911,61108009,2,"2020/04/09, 22:36:58",False,"2020/04/15, 21:52:15",1371.0,5540166.0,1,"If graph your metric, grouped by your tag, and then apply one of those functions, it should return the count of unique tag values under that tag key.",0.0,0.091,0.909,0.4019
Datadog,61128911,61108009,2,"2020/04/09, 22:36:58",False,"2020/04/15, 21:52:15",1371.0,5540166.0,1,So in your case:,0.0,0.0,1.0,0.0
Datadog,61128911,61108009,2,"2020/04/09, 22:36:58",False,"2020/04/15, 21:52:15",1371.0,5540166.0,1,count_not_null(sum:your.metric.name{*} by {file_name}),0.0,0.0,1.0,0.0
Datadog,61128911,61108009,2,"2020/04/09, 22:36:58",False,"2020/04/15, 21:52:15",1371.0,5540166.0,1,"And it works with multiple group-by tags too, so if you had separate tags for  file_name  and  directory  then you could use this same approach to graph the count of unique  combinations  of these tag values, or the count of unique combinations of  directory + file_name :",0.0,0.058,0.942,0.4019
Datadog,61128911,61108009,2,"2020/04/09, 22:36:58",False,"2020/04/15, 21:52:15",1371.0,5540166.0,1,"count_not_null(your.metric.name{*} by {file_name,directory})",0.0,0.0,1.0,0.0
Datadog,60171576,60170609,3,"2020/02/11, 16:50:50",False,"2020/02/11, 16:50:50",1371.0,5540166.0,0,"Yes, it is possible.",0.0,0.474,0.526,0.4019
Datadog,60171576,60170609,3,"2020/02/11, 16:50:50",False,"2020/02/11, 16:50:50",1371.0,5540166.0,0,"You can do that in a  processing pipeline  with a grok parser, but you'll want to configure which attribute the grok parser applies to in the advanced settings ( docs here ).",0.0,0.128,0.872,0.4497
Datadog,60171576,60170609,3,"2020/02/11, 16:50:50",False,"2020/02/11, 16:50:50",1371.0,5540166.0,0,"(By default grok parsers apply to the ""message"" attribute, but you can configure them to parse any attribute.)",0.0,0.0,1.0,0.0
Datadog,60171576,60170609,3,"2020/02/11, 16:50:50",False,"2020/02/11, 16:50:50",1371.0,5540166.0,0,"In this case, you'd set the  Extract From  field to  requestUri .",0.0,0.0,1.0,0.0
Datadog,60171576,60170609,3,"2020/02/11, 16:50:50",False,"2020/02/11, 16:50:50",1371.0,5540166.0,0,The  Helper Rules  section is not necessary for this.,0.0,0.231,0.769,0.34
Datadog,60171576,60170609,3,"2020/02/11, 16:50:50",False,"2020/02/11, 16:50:50",1371.0,5540166.0,0,"And then in the main  Define Parsing Rules  section, you'll plug in a rule similar to this:",0.0,0.0,1.0,0.0
Datadog,60171576,60170609,3,"2020/02/11, 16:50:50",False,"2020/02/11, 16:50:50",1371.0,5540166.0,0,or even further,0.0,0.0,1.0,0.0
Datadog,59625224,59622491,2,"2020/01/07, 11:08:08",True,"2020/01/08, 11:23:00",336.0,7243426.0,1,What about using the template variables  doc ?,0.0,0.0,1.0,0.0
Datadog,59625224,59622491,2,"2020/01/07, 11:08:08",True,"2020/01/08, 11:23:00",336.0,7243426.0,1,You could select:,0.0,0.0,1.0,0.0
Datadog,59625224,59622491,2,"2020/01/07, 11:08:08",True,"2020/01/08, 11:23:00",336.0,7243426.0,1,Then you'll be able to replace your  {name:$flavor-db-master}  with  {$Name},0.0,0.0,1.0,0.0
Datadog,59625224,59622491,2,"2020/01/07, 11:08:08",True,"2020/01/08, 11:23:00",336.0,7243426.0,1,"Otherwise, if you actually wants the value of the template variable you have to use  $flavor.value .",0.0,0.138,0.862,0.34
Datadog,59625224,59622491,2,"2020/01/07, 11:08:08",True,"2020/01/08, 11:23:00",336.0,7243426.0,1,I advise to use a not widget to check the actual behavior.,0.0,0.0,1.0,0.0
Datadog,59625224,59622491,2,"2020/01/07, 11:08:08",True,"2020/01/08, 11:23:00",336.0,7243426.0,1,EDIT:,0.0,0.0,1.0,0.0
Datadog,59625224,59622491,2,"2020/01/07, 11:08:08",True,"2020/01/08, 11:23:00",336.0,7243426.0,1,This kind of setup is not the recommended.,0.185,0.0,0.815,-0.1511
Datadog,59625224,59622491,2,"2020/01/07, 11:08:08",True,"2020/01/08, 11:23:00",336.0,7243426.0,1,It would be better to set two tags on your database:,0.0,0.225,0.775,0.4404
Datadog,59625224,59622491,2,"2020/01/07, 11:08:08",True,"2020/01/08, 11:23:00",336.0,7243426.0,1,"You would then have a unique selection of tags,  env:dev,dbname:db1-master .",0.0,0.0,1.0,0.0
Datadog,59625224,59622491,2,"2020/01/07, 11:08:08",True,"2020/01/08, 11:23:00",336.0,7243426.0,1,It would then be easy to have a query such as:,0.0,0.244,0.756,0.4404
Datadog,59359866,59357130,0,"2019/12/16, 17:42:43",True,"2019/12/16, 17:42:43",1371.0,5540166.0,2,You can do this in a processing pipeline with 2 steps:,0.0,0.0,1.0,0.0
Datadog,59359866,59357130,0,"2019/12/16, 17:42:43",True,"2019/12/16, 17:42:43",1371.0,5540166.0,2,"If there are other queries/patterns you want to use to determine the log level/status, you can add multiple rules to the  Category Processor  in (1), and you can map the  level  value to  info/warn/error  and any other relevant status value.",0.0,0.142,0.858,0.6249
Datadog,58760072,58725596,0,"2019/11/08, 05:46:30",False,"2019/11/08, 05:46:30",156.0,2676108.0,1,You mostly have to wait for it all to fill in over time.,0.0,0.0,1.0,0.0
Datadog,58760072,58725596,0,"2019/11/08, 05:46:30",False,"2019/11/08, 05:46:30",156.0,2676108.0,1,Metric timestamps cannot be more than 10 minutes in the future or more than 1 hour in the past.,0.0,0.0,1.0,0.0
Datadog,58760072,58725596,0,"2019/11/08, 05:46:30",False,"2019/11/08, 05:46:30",156.0,2676108.0,1,https://docs.datadoghq.com/developers/metrics/,0.0,0.0,1.0,0.0
Datadog,58963300,58564144,0,"2019/11/20, 23:00:45",False,"2019/11/20, 23:00:45",107.0,10913713.0,1,I got the answer to the second question.,0.0,0.0,1.0,0.0
Datadog,58963300,58564144,0,"2019/11/20, 23:00:45",False,"2019/11/20, 23:00:45",107.0,10913713.0,1,"Now, I can get all tables from one database that I specified.",0.0,0.0,1.0,0.0
Datadog,58963300,58564144,0,"2019/11/20, 23:00:45",False,"2019/11/20, 23:00:45",107.0,10913713.0,1,All I needed to do; relation_regex: '.,0.0,0.0,1.0,0.0
Datadog,58963300,58564144,0,"2019/11/20, 23:00:45",False,"2019/11/20, 23:00:45",107.0,10913713.0,1,*' and disabled relation_name.,0.0,0.0,1.0,0.0
Datadog,58963300,58564144,0,"2019/11/20, 23:00:45",False,"2019/11/20, 23:00:45",107.0,10913713.0,1,Answer to the first question I got from datadog is that there is no way to monitor all the DBs without listing them individually.,0.091,0.0,0.909,-0.296
Datadog,58963300,58564144,0,"2019/11/20, 23:00:45",False,"2019/11/20, 23:00:45",107.0,10913713.0,1,"They may change this in future, but for now we have to add blocks for each and every database that we want to monitor",0.091,0.056,0.853,-0.2263
Datadog,61927265,56382266,0,"2020/05/21, 07:25:17",False,"2020/05/21, 07:25:17",728.0,2218580.0,0,This works for me:,0.0,0.0,1.0,0.0
Datadog,55850182,55794663,0,"2019/04/25, 16:15:12",False,"2019/04/25, 16:15:12",4059.0,8763847.0,4,"What you are doing is correct only, however, the common mistake is not following the below.",0.138,0.0,0.862,-0.34
Datadog,55850182,55794663,0,"2019/04/25, 16:15:12",False,"2019/04/25, 16:15:12",4059.0,8763847.0,4,"This library MUST be imported and initialized before any instrumented
  module.",0.0,0.0,1.0,0.0
Datadog,55850182,55794663,0,"2019/04/25, 16:15:12",False,"2019/04/25, 16:15:12",4059.0,8763847.0,4,"When using a transpiler, you MUST import and initialize the
  tracer library in an external file and then import that file as a
  whole when building your application.",0.0,0.0,1.0,0.0
Datadog,55850182,55794663,0,"2019/04/25, 16:15:12",False,"2019/04/25, 16:15:12",4059.0,8763847.0,4,"This prevents hoisting and
  ensures that the tracer library gets imported and initialized before
  importing any other instrumented module.",0.0,0.067,0.933,0.0772
Datadog,55850182,55794663,0,"2019/04/25, 16:15:12",False,"2019/04/25, 16:15:12",4059.0,8763847.0,4,"Basically, you cannot have  require(any instrumented lib)  (e.g.",0.0,0.0,1.0,0.0
Datadog,55850182,55794663,0,"2019/04/25, 16:15:12",False,"2019/04/25, 16:15:12",4059.0,8763847.0,4,"http, express, etc) before calling init() tracing function.",0.0,0.0,1.0,0.0
Datadog,55850182,55794663,0,"2019/04/25, 16:15:12",False,"2019/04/25, 16:15:12",4059.0,8763847.0,4,https://docs.datadoghq.com/tracing/setup/nodejs/,0.0,0.0,1.0,0.0
Datadog,53585633,52176297,0,"2018/12/03, 01:28:11",False,"2018/12/03, 01:28:11",41.0,1970447.0,4,"It's probably too late, but it may be useful to others.",0.0,0.278,0.722,0.5927
Datadog,53585633,52176297,0,"2018/12/03, 01:28:11",False,"2018/12/03, 01:28:11",41.0,1970447.0,4,You can set tags by using the environment variables of the application container (not the agent container) by using DD_TAGS.,0.0,0.0,1.0,0.0
Datadog,51933718,51866333,0,"2018/08/20, 18:11:48",True,"2018/08/20, 18:11:48",3212.0,282172.0,2,Apparently there is a datadog/agent:latest-jmx that should be used that contains the java image...,0.0,0.0,1.0,0.0
Datadog,51933718,51866333,0,"2018/08/20, 18:11:48",True,"2018/08/20, 18:11:48",3212.0,282172.0,2,I just missed it in the docs.,0.306,0.0,0.694,-0.296
Datadog,51451079,51443070,0,"2018/07/21, 01:20:42",True,"2018/07/21, 01:20:42",46.0,9872725.0,3,"I discussed this with Datadog support, and they confirmed that the  awslogs  logging driver prevents the Datadog agent container from accessing a container's logs.",0.0,0.167,0.833,0.4588
Datadog,51451079,51443070,0,"2018/07/21, 01:20:42",True,"2018/07/21, 01:20:42",46.0,9872725.0,3,"Since  awslogs  is currently the only logging driver available to tasks using the Fargate launch type, getting logs into Datadog will require another method.",0.0,0.0,1.0,0.0
Datadog,51451079,51443070,0,"2018/07/21, 01:20:42",True,"2018/07/21, 01:20:42",46.0,9872725.0,3,"Since the  awslogs  logging driver emits logs to CloudWatch, one method that I have used is to create a subscription to stream those log groups to Datadog's Lambda function as configured  here .",0.0,0.068,0.932,0.2732
Datadog,51451079,51443070,0,"2018/07/21, 01:20:42",True,"2018/07/21, 01:20:42",46.0,9872725.0,3,"You can do that from the  Lambda side  using CloudWatch logs as the trigger, or from the CloudWatch Logs side, by clicking  Actions    Stream to AWS Lambda .",0.0,0.0,1.0,0.0
Datadog,51451079,51443070,0,"2018/07/21, 01:20:42",True,"2018/07/21, 01:20:42",46.0,9872725.0,3,I chose the Lambda option because it was quick and easy and required no code changes to our applications (since we are still in the evaluation stage).,0.076,0.1,0.825,0.1779
Datadog,51451079,51443070,0,"2018/07/21, 01:20:42",True,"2018/07/21, 01:20:42",46.0,9872725.0,3,Datadog support advised me that it was necessary to modify the Lambda function in order to attribute logs to the corresponding service:,0.0,0.114,0.886,0.4019
Datadog,51451079,51443070,0,"2018/07/21, 01:20:42",True,"2018/07/21, 01:20:42",46.0,9872725.0,3,"In  this block , modify it to something like:",0.254,0.219,0.526,-0.1027
Datadog,51451079,51443070,0,"2018/07/21, 01:20:42",True,"2018/07/21, 01:20:42",46.0,9872725.0,3,According to Datadog support:,0.0,0.474,0.526,0.4019
Datadog,51451079,51443070,0,"2018/07/21, 01:20:42",True,"2018/07/21, 01:20:42",46.0,9872725.0,3,I had to make further modifications to set the value of the  syslog.,0.0,0.179,0.821,0.34
Datadog,51451079,51443070,0,"2018/07/21, 01:20:42",True,"2018/07/21, 01:20:42",46.0,9872725.0,3,"*  keys in a way that made sense for our applications, but it works great.",0.0,0.32,0.68,0.7684
Datadog,51324294,51323878,0,"2018/07/13, 14:39:40",False,"2018/07/13, 14:39:40",61.0,9763778.0,1,I found the solution: the user  datadog  didnt have permission to read connections that wasnt form him.,0.0,0.133,0.867,0.3182
Datadog,51324294,51323878,0,"2018/07/13, 14:39:40",False,"2018/07/13, 14:39:40",61.0,9763778.0,1,So it was just getting a single row.,0.0,0.0,1.0,0.0
Datadog,51324294,51323878,0,"2018/07/13, 14:39:40",False,"2018/07/13, 14:39:40",61.0,9763778.0,1,I gave permissions for that user to read  pg_stat_activity,0.0,0.0,1.0,0.0
Datadog,50793070,50792558,0,"2018/06/11, 11:09:13",True,"2018/06/11, 11:09:13",7647.0,1958151.0,2,"It looks like you created  some  policy, but not the policy of required type.",0.0,0.213,0.787,0.3071
Datadog,50793070,50792558,0,"2018/06/11, 11:09:13",True,"2018/06/11, 11:09:13",7647.0,1958151.0,2,"When you create the role for Datadog, you have to choose a very specific role type:",0.0,0.13,0.87,0.2732
Datadog,50793070,50792558,0,"2018/06/11, 11:09:13",True,"2018/06/11, 11:09:13",7647.0,1958151.0,2,Select Another AWS account for the Role Type.,0.0,0.0,1.0,0.0
Datadog,50793070,50792558,0,"2018/06/11, 11:09:13",True,"2018/06/11, 11:09:13",7647.0,1958151.0,2,and then create a policy for that role.,0.0,0.259,0.741,0.2732
Datadog,50793070,50792558,0,"2018/06/11, 11:09:13",True,"2018/06/11, 11:09:13",7647.0,1958151.0,2,"Also, don't forget to",0.0,0.357,0.643,0.1695
Datadog,50793070,50792558,0,"2018/06/11, 11:09:13",True,"2018/06/11, 11:09:13",7647.0,1958151.0,2,Check off Require external ID,0.0,0.0,1.0,0.0
Datadog,50793070,50792558,0,"2018/06/11, 11:09:13",True,"2018/06/11, 11:09:13",7647.0,1958151.0,2,You shouldn't have any problems as long as you follow the guideline step by step:  https://docs.datadoghq.com/integrations/amazon_web_services/,0.0,0.131,0.869,0.3089
Datadog,61044468,50792558,1,"2020/04/05, 17:51:15",False,"2020/04/05, 17:51:15",1.0,12211192.0,0,"I had this problem, when I tried to both use the role-assumption role as an assumption role on the  assume_role_policy , as well as trying to attach it.",0.097,0.076,0.827,-0.1531
Datadog,61044468,50792558,1,"2020/04/05, 17:51:15",False,"2020/04/05, 17:51:15",1.0,12211192.0,0,"Once I got rid of the aws_iam_policy that I created with the role-assumption policy doc as well as the role-policy attachment, it worked.",0.0,0.259,0.741,0.6486
Datadog,61044468,50792558,1,"2020/04/05, 17:51:15",False,"2020/04/05, 17:51:15",1.0,12211192.0,0,Hope this helps.,0.0,0.846,0.154,0.6705
Datadog,55032271,49022133,0,"2019/03/06, 23:16:09",False,"2019/03/06, 23:16:09",141.0,8721010.0,1,One of Dockers main features is portability and it makes sense to bind datadog into that environment.,0.0,0.0,1.0,0.0
Datadog,55032271,49022133,0,"2019/03/06, 23:16:09",False,"2019/03/06, 23:16:09",141.0,8721010.0,1,That way they are packaged and deployed together and you don't have the overhead of installing datadog manually everywhere you choose to deploy.,0.0,0.0,1.0,0.0
Datadog,55032271,49022133,0,"2019/03/06, 23:16:09",False,"2019/03/06, 23:16:09",141.0,8721010.0,1,"What they are also implying is that you should use  docker-compose  and turn your application / docker container into an multi-container Docker application, running your image(s) alongside the docker agent.",0.0,0.0,1.0,0.0
Datadog,55032271,49022133,0,"2019/03/06, 23:16:09",False,"2019/03/06, 23:16:09",141.0,8721010.0,1,"Thus you will not need to write/build/run/manage a container via Dockerfile, but rather add the agent image to your  docker-compose.yml  along with its configuration.",0.0,0.0,1.0,0.0
Datadog,55032271,49022133,0,"2019/03/06, 23:16:09",False,"2019/03/06, 23:16:09",141.0,8721010.0,1,Starting your multi-container application will still be easy via:,0.0,0.266,0.734,0.4404
Datadog,55032271,49022133,0,"2019/03/06, 23:16:09",False,"2019/03/06, 23:16:09",141.0,8721010.0,1,Its really convenient and gives you additional features like their  autodiscovery  service.,0.0,0.185,0.815,0.3612
Datadog,47085123,47069283,1,"2017/11/02, 23:45:28",False,"2017/11/02, 23:45:28",5158.0,7147666.0,2,Just use a find me Twimlet.,0.0,0.0,1.0,0.0
Datadog,47085123,47069283,1,"2017/11/02, 23:45:28",False,"2017/11/02, 23:45:28",5158.0,7147666.0,2,Enter up to 10 numbers and a timeout between moving on to the next number.,0.0,0.091,0.909,0.0772
Datadog,47085123,47069283,1,"2017/11/02, 23:45:28",False,"2017/11/02, 23:45:28",5158.0,7147666.0,2,Twilio will do the rest.,0.0,0.0,1.0,0.0
Datadog,47085123,47069283,1,"2017/11/02, 23:45:28",False,"2017/11/02, 23:45:28",5158.0,7147666.0,2,https://www.twilio.com/labs/twimlets/findme,0.0,0.0,1.0,0.0
Datadog,47730118,47069283,0,"2017/12/09, 17:29:38",False,"2017/12/09, 17:29:38",1150.0,1394755.0,1,If you are looking for a more full featured paid solution I'd recommend PagerDuty.,0.0,0.304,0.696,0.5859
Datadog,47730118,47069283,0,"2017/12/09, 17:29:38",False,"2017/12/09, 17:29:38",1150.0,1394755.0,1,DataDog has an integration for PagerDuty.,0.0,0.0,1.0,0.0
Datadog,47730118,47069283,0,"2017/12/09, 17:29:38",False,"2017/12/09, 17:29:38",1150.0,1394755.0,1,Any monitor that gets triggered that mentions  @pagerduty-myteamname (as example) in the monitor message will cause PagerDuty to page the on call person.,0.0,0.0,1.0,0.0
Datadog,47730118,47069283,0,"2017/12/09, 17:29:38",False,"2017/12/09, 17:29:38",1150.0,1394755.0,1,If that person does not acknowledge the page you can configure it to go through list of people to contact next until it is acknowledged by someone.,0.0,0.0,1.0,0.0
Datadog,46202190,45695083,0,"2017/09/13, 19:05:55",True,"2017/09/13, 19:05:55",421.0,2016045.0,4,The role arn:aws:iam::xxxxxxxxxx:role/DatadogAWSIntegrationRole also has to have permission to assume the role on the other account.,0.0,0.0,1.0,0.0
Datadog,46202190,45695083,0,"2017/09/13, 19:05:55",True,"2017/09/13, 19:05:55",421.0,2016045.0,4,You'll have to update the DatadogAWSIntegrationRole on the primary account to include:,0.0,0.0,1.0,0.0
Datadog,45438610,45438114,9,"2017/08/01, 16:00:52",True,"2017/08/01, 16:00:52",36.0,3185251.0,2,I suspect you're probably looking to query the event stream which is where all alerts from monitors can be found.,0.109,0.0,0.891,-0.296
Datadog,45438610,45438114,9,"2017/08/01, 16:00:52",True,"2017/08/01, 16:00:52",36.0,3185251.0,2,The docs at  https://docs.datadoghq.com/api/#events-get-all  are a pretty good starting place.,0.0,0.466,0.534,0.7269
Datadog,45438610,45438114,9,"2017/08/01, 16:00:52",True,"2017/08/01, 16:00:52",36.0,3185251.0,2,"You'll want to query this endpoint with the proper source and tags, but this should be a starting point.",0.0,0.063,0.937,0.0387
Datadog,45438610,45438114,9,"2017/08/01, 16:00:52",True,"2017/08/01, 16:00:52",36.0,3185251.0,2,"If this doesn't quite work, I'd recommend looking at pulling the details from the monitor as shown here:   https://docs.datadoghq.com/api/#monitor-get-details .",0.0,0.133,0.867,0.4144
Datadog,45438610,45438114,9,"2017/08/01, 16:00:52",True,"2017/08/01, 16:00:52",36.0,3185251.0,2,This may be a second option if you're unable to get the information you're looking for from the event stream.,0.0,0.0,1.0,0.0
Datadog,45124573,45104434,1,"2017/07/16, 06:03:15",True,"2017/07/16, 06:03:15",1171.0,6826691.0,2,"The ""ReadTimeout: HTTPConnectionPool"" error can be corrected by adding a timeout parameter under instances in the elasticsearch.yaml",0.153,0.0,0.847,-0.4019
Datadog,41008234,41007369,0,"2016/12/07, 04:05:10",False,"2016/12/07, 04:05:10",261.0,4172512.0,1,"Frank,",0.0,0.0,1.0,0.0
Datadog,41008234,41007369,0,"2016/12/07, 04:05:10",False,"2016/12/07, 04:05:10",261.0,4172512.0,1,"Your use case follows the standard ""custom metric"" submission that is common within Datadog.",0.0,0.0,1.0,0.0
Datadog,41008234,41007369,0,"2016/12/07, 04:05:10",False,"2016/12/07, 04:05:10",261.0,4172512.0,1,Using one of the supported libraries:,0.0,0.315,0.685,0.3182
Datadog,41008234,41007369,0,"2016/12/07, 04:05:10",False,"2016/12/07, 04:05:10",261.0,4172512.0,1,http://docs.datadoghq.com/libraries/#java,0.0,0.0,1.0,0.0
Datadog,41008234,41007369,0,"2016/12/07, 04:05:10",False,"2016/12/07, 04:05:10",261.0,4172512.0,1,You can leverage the statsD port of an Agent running on your host to submit these custom metrics:,0.0,0.0,1.0,0.0
Datadog,41008234,41007369,0,"2016/12/07, 04:05:10",False,"2016/12/07, 04:05:10",261.0,4172512.0,1,http://docs.datadoghq.com/guides/dogstatsd/,0.0,0.0,1.0,0.0
Datadog,41008234,41007369,0,"2016/12/07, 04:05:10",False,"2016/12/07, 04:05:10",261.0,4172512.0,1,You will want to install the Agent on either the host running this function or point your statsD connection towards an accepting host:,0.0,0.157,0.843,0.4404
Datadog,41008234,41007369,0,"2016/12/07, 04:05:10",False,"2016/12/07, 04:05:10",261.0,4172512.0,1,http://docs.datadoghq.com/guides/basic_agent_usage/,0.0,0.0,1.0,0.0
Datadog,41008234,41007369,0,"2016/12/07, 04:05:10",False,"2016/12/07, 04:05:10",261.0,4172512.0,1,There are additional docs found here that should help you understand how custom metrics work in Datadog:,0.0,0.144,0.856,0.4019
Datadog,41008234,41007369,0,"2016/12/07, 04:05:10",False,"2016/12/07, 04:05:10",261.0,4172512.0,1,https://help.datadoghq.com/hc/en-us/articles/204271775-What-is-a-custom-metric-and-what-is-the-limit-on-the-number-of-custom-metrics-I-can-have-,0.0,0.0,1.0,0.0
Datadog,41008234,41007369,0,"2016/12/07, 04:05:10",False,"2016/12/07, 04:05:10",261.0,4172512.0,1,https://help.datadoghq.com/hc/en-us/articles/203765485-How-do-I-submit-custom-metrics-What-s-their-overhead-,0.0,0.0,1.0,0.0
Datadog,41008234,41007369,0,"2016/12/07, 04:05:10",False,"2016/12/07, 04:05:10",261.0,4172512.0,1,"Usually when troubleshooting custom metric submissions, we try to implement some form of local printing/logging to ensure the statsD connection is being made and that the custom function is being called and submitted.",0.0,0.122,0.878,0.5106
Datadog,41008234,41007369,0,"2016/12/07, 04:05:10",False,"2016/12/07, 04:05:10",261.0,4172512.0,1,"Once you can confirm the metric is being sent to the Agent, use the Metric Summary page to check for the custom metric:",0.0,0.0,1.0,0.0
Datadog,41008234,41007369,0,"2016/12/07, 04:05:10",False,"2016/12/07, 04:05:10",261.0,4172512.0,1,https://app.datadoghq.com/metric/summary,0.0,0.0,1.0,0.0
Datadog,41008234,41007369,0,"2016/12/07, 04:05:10",False,"2016/12/07, 04:05:10",261.0,4172512.0,1,"If all else fails, reach out to Datadog at support@datadoghq.com",0.235,0.092,0.672,-0.4019
Datadog,41220782,40933155,0,"2016/12/19, 12:42:07",True,"2016/12/19, 12:42:07",1981.0,2473382.0,0,"It is very much possible, just use the alias property of the  attribute  filter:",0.0,0.0,1.0,0.0
Datadog,39235805,39235163,0,"2016/08/30, 22:38:33",True,"2016/08/30, 22:40:17",496.0,2050873.0,6,Verify if the datadog package is installed in your environment.,0.0,0.0,1.0,0.0
Datadog,39235805,39235163,0,"2016/08/30, 22:38:33",True,"2016/08/30, 22:40:17",496.0,2050873.0,6,You can do this with this command:,0.0,0.0,1.0,0.0
Datadog,39235805,39235163,0,"2016/08/30, 22:38:33",True,"2016/08/30, 22:40:17",496.0,2050873.0,6,"If it's not installed, you can install it with this command:",0.0,0.0,1.0,0.0
Datadog,66955358,66943560,0,"2021/04/05, 18:20:48",False,"2021/04/05, 18:20:48",156.0,2676108.0,1,You could tag your metrics with the name or ID of the agent it is collecting metrics from (if you aren't already).,0.0,0.0,1.0,0.0
Datadog,66955358,66943560,0,"2021/04/05, 18:20:48",False,"2021/04/05, 18:20:48",156.0,2676108.0,1,Then in Datadog you could write a query that groups by the agent ID and applies a count_not_null function:  https://docs.datadoghq.com/dashboards/functions/count/,0.0,0.0,1.0,0.0
Datadog,66955358,66943560,0,"2021/04/05, 18:20:48",False,"2021/04/05, 18:20:48",156.0,2676108.0,1,This basically hijacks a random metric to extract the unique count of agents reporting that metric to assume the total count of agents.,0.0,0.0,1.0,0.0
Datadog,66955358,66943560,0,"2021/04/05, 18:20:48",False,"2021/04/05, 18:20:48",156.0,2676108.0,1,"You wouldn't be able to easily group by queue though, so idk if it would be a good solution to your use case.",0.068,0.277,0.655,0.694
Datadog,66955358,66943560,0,"2021/04/05, 18:20:48",False,"2021/04/05, 18:20:48",156.0,2676108.0,1,Your idea around using gauges sounds good to me.,0.0,0.266,0.734,0.4404
Datadog,66955358,66943560,0,"2021/04/05, 18:20:48",False,"2021/04/05, 18:20:48",156.0,2676108.0,1,You can send a new metric called something like  myagent.running  which sends a value of 1 for each of your agents and does a sum of all gauges in order to get a count.,0.0,0.154,0.846,0.5994
Datadog,66955358,66943560,0,"2021/04/05, 18:20:48",False,"2021/04/05, 18:20:48",156.0,2676108.0,1,That is actually how the metric  datadog.agent.running  is implemented:  https://docs.datadoghq.com/integrations/agent_metrics/#metrics,0.0,0.0,1.0,0.0
Datadog,66632382,66606637,0,"2021/03/15, 06:26:59",True,"2021/03/15, 06:26:59",902.0,10709519.0,0,After testing different queries I found that running this query groups the results by query statements and will return the count of each.,0.0,0.0,1.0,0.0
Datadog,66632382,66606637,0,"2021/03/15, 06:26:59",True,"2021/03/15, 06:26:59",902.0,10709519.0,0,In Datadogs mysql configuration I added tags for the query statement and database name.,0.0,0.0,1.0,0.0
Datadog,66632382,66606637,0,"2021/03/15, 06:26:59",True,"2021/03/15, 06:26:59",902.0,10709519.0,0,"Now since they are grouped, I can see information per different statement in datadog.",0.0,0.0,1.0,0.0
Datadog,66453250,66443203,2,"2021/03/03, 10:27:46",False,"2021/03/04, 10:26:16",1249.0,13336642.0,0,Datadog’s IIS integration queries the Web Service performance counters automatically and sends the results to Datadog.,0.0,0.0,1.0,0.0
Datadog,66453250,66443203,2,"2021/03/03, 10:27:46",False,"2021/03/04, 10:26:16",1249.0,13336642.0,0,The Web Service performance counter class collects information from the World Wide Web Publishing Service.,0.0,0.0,1.0,0.0
Datadog,66453250,66443203,2,"2021/03/03, 10:27:46",False,"2021/03/04, 10:26:16",1249.0,13336642.0,0,You can enable the IIS integration by creating a configuration file either manually or through the Datadog Agent GUI.,0.0,0.115,0.885,0.296
Datadog,66453250,66443203,2,"2021/03/03, 10:27:46",False,"2021/03/04, 10:26:16",1249.0,13336642.0,0,"To create a configuration file through the GUI, navigate to the “Checks” tab, choose “Manage Checks,” and select the iis check from the “Add a Check” menu.",0.0,0.08,0.92,0.2732
Datadog,66453250,66443203,2,"2021/03/03, 10:27:46",False,"2021/03/04, 10:26:16",1249.0,13336642.0,0,You can also manually create a conf.yaml file in C:\ProgramData\Datadog\conf.d\iis.d.,0.0,0.208,0.792,0.2732
Datadog,66453250,66443203,2,"2021/03/03, 10:27:46",False,"2021/03/04, 10:26:16",1249.0,13336642.0,0,There is a sites attribute in the conf.yaml file.,0.0,0.0,1.0,0.0
Datadog,66453250,66443203,2,"2021/03/03, 10:27:46",False,"2021/03/04, 10:26:16",1249.0,13336642.0,0,This attribute represents the IIS site you want to monitor.,0.0,0.126,0.874,0.0772
Datadog,66453250,66443203,2,"2021/03/03, 10:27:46",False,"2021/03/04, 10:26:16",1249.0,13336642.0,0,You only need to delete the sites you want to exclude.,0.156,0.107,0.738,-0.1531
Datadog,66453250,66443203,2,"2021/03/03, 10:27:46",False,"2021/03/04, 10:26:16",1249.0,13336642.0,0,More information you can refer to this link:  IIS monitoring with Datadog .,0.0,0.0,1.0,0.0
Datadog,66642805,66443203,0,"2021/03/15, 19:33:06",True,"2021/03/15, 19:33:06",16566.0,24231.0,0,"If you are using Datadog's .NET Tracer, you can set  DD_TRACE_ENABLED=false  in the  appSettings  section of the  web.config  file ( docs ).",0.0,0.0,1.0,0.0
Datadog,66642805,66443203,0,"2021/03/15, 19:33:06",True,"2021/03/15, 19:33:06",16566.0,24231.0,0,For example:,0.0,0.0,1.0,0.0
Datadog,66642805,66443203,0,"2021/03/15, 19:33:06",True,"2021/03/15, 19:33:06",16566.0,24231.0,0,Another option is to deploy a  datadog.json  file ( docs ) in the root of your app that contains:,0.0,0.0,1.0,0.0
Datadog,66642805,66443203,0,"2021/03/15, 19:33:06",True,"2021/03/15, 19:33:06",16566.0,24231.0,0,(Disclaimer: I work at Datadog),0.0,0.0,1.0,0.0
Datadog,66357392,66316616,2,"2021/02/24, 21:12:06",False,"2021/02/25, 00:49:28",156.0,2676108.0,0,Update: I found this bit of code in the tracing client repo:,0.0,0.0,1.0,0.0
Datadog,66357392,66316616,2,"2021/02/24, 21:12:06",False,"2021/02/25, 00:49:28",156.0,2676108.0,0,https://github.com/DataDog/dd-trace-js/issues/1249,0.0,0.0,1.0,0.0
Datadog,66357392,66316616,2,"2021/02/24, 21:12:06",False,"2021/02/25, 00:49:28",156.0,2676108.0,0,maybe it would help,0.0,0.474,0.526,0.4019
Datadog,66357392,66316616,2,"2021/02/24, 21:12:06",False,"2021/02/25, 00:49:28",156.0,2676108.0,0,Old message:,0.0,0.0,1.0,0.0
Datadog,66357392,66316616,2,"2021/02/24, 21:12:06",False,"2021/02/25, 00:49:28",156.0,2676108.0,0,Never mind.,0.0,0.0,1.0,0.0
Datadog,66357392,66316616,2,"2021/02/24, 21:12:06",False,"2021/02/25, 00:49:28",156.0,2676108.0,0,"seems like my solution is only for express, graphql doesn't support that property",0.132,0.281,0.586,0.3699
Datadog,66357392,66316616,2,"2021/02/24, 21:12:06",False,"2021/02/25, 00:49:28",156.0,2676108.0,0,You probably want to just modify the validateStatus property in the http module:,0.0,0.098,0.902,0.0772
Datadog,66357392,66316616,2,"2021/02/24, 21:12:06",False,"2021/02/25, 00:49:28",156.0,2676108.0,0,Callback function to determine if there was an error.,0.252,0.0,0.748,-0.4019
Datadog,66357392,66316616,2,"2021/02/24, 21:12:06",False,"2021/02/25, 00:49:28",156.0,2676108.0,0,It should take a status code as its only parameter and return true for success or false for errors,0.1,0.272,0.628,0.6249
Datadog,66357392,66316616,2,"2021/02/24, 21:12:06",False,"2021/02/25, 00:49:28",156.0,2676108.0,0,https://datadoghq.dev/dd-trace-js/interfaces/plugins.http.html#validatestatus,0.0,0.0,1.0,0.0
Datadog,66357392,66316616,2,"2021/02/24, 21:12:06",False,"2021/02/25, 00:49:28",156.0,2676108.0,0,As an example you should be able to mark 403s as not be errors with something like this:,0.0,0.221,0.779,0.5478
Datadog,66299116,66282520,0,"2021/02/21, 06:18:23",False,"2021/02/21, 06:18:23",493.0,244037.0,0,"Unfortunately, that doesn't seem to be a setting you can directly control at this time.",0.156,0.0,0.844,-0.34
Datadog,66299116,66282520,0,"2021/02/21, 06:18:23",False,"2021/02/21, 06:18:23",493.0,244037.0,0,"The reasoning is that a given timeseries could be split by tag, so setting a single color for a timeseries split by tag would amount to multiple entires of the same color, and that wouldn't make sense.",0.0,0.0,1.0,0.0
Datadog,66299116,66282520,0,"2021/02/21, 06:18:23",False,"2021/02/21, 06:18:23",493.0,244037.0,0,"To support the semantic meaning, I've often used the following settings:",0.0,0.213,0.787,0.4019
Datadog,66299116,66282520,0,"2021/02/21, 06:18:23",False,"2021/02/21, 06:18:23",493.0,244037.0,0,"I'm not sure I know what distinction there is of Error vs Critical in your definition, but using these palettes has proven useful for my team.",0.167,0.129,0.704,0.2189
Datadog,66299116,66282520,0,"2021/02/21, 06:18:23",False,"2021/02/21, 06:18:23",493.0,244037.0,0,"If you're looking for a specific widget to change color based on value - so if the number exceeds a threshold - take a look at the  Query Value Widget , as that can be customized to change color based on the current value.",0.0,0.205,0.795,0.7759
Datadog,66299116,66282520,0,"2021/02/21, 06:18:23",False,"2021/02/21, 06:18:23",493.0,244037.0,0,"Alternately, if you have a Monitor already set for the timeseries, use the  Alert Value Widget  to show the current status, with less configuration, since the thresholds are managed in the Monitor's definition.",0.0,0.133,0.867,0.5574
Datadog,66356118,66282520,0,"2021/02/24, 19:43:52",False,"2021/02/24, 19:43:52",156.0,2676108.0,0,"You can't set a color per line, but you can set the color per query.",0.0,0.0,1.0,0.0
Datadog,66356118,66282520,0,"2021/02/24, 19:43:52",False,"2021/02/24, 19:43:52",156.0,2676108.0,0,"If you edit the graph using json, that field  requests.style.palette  is exposed and you can just try typing in whatever color you want there.",0.053,0.053,0.894,0.0
Datadog,66356118,66282520,0,"2021/02/24, 19:43:52",False,"2021/02/24, 19:43:52",156.0,2676108.0,0,https://imgur.com/VrZZl72,0.0,0.0,1.0,0.0
Datadog,66356118,66282520,0,"2021/02/24, 19:43:52",False,"2021/02/24, 19:43:52",156.0,2676108.0,0,"If you want to have one time series that is green for hits, and one that is red for errors, you just make two metric queries, and then color one green and one red.",0.067,0.036,0.896,-0.2732
Datadog,66356118,66282520,0,"2021/02/24, 19:43:52",False,"2021/02/24, 19:43:52",156.0,2676108.0,0,https://imgur.com/AHGi1Hk,0.0,0.0,1.0,0.0
Datadog,66355760,66271979,0,"2021/02/24, 19:20:38",False,"2021/02/24, 19:20:38",156.0,2676108.0,0,"That endpoint is used to send events to Datadog, if your one account is sending them to the logs product, it is most likely because that account has a special flag that converts all your incoming events into logs.",0.0,0.068,0.932,0.4019
Datadog,66355760,66271979,0,"2021/02/24, 19:20:38",False,"2021/02/24, 19:20:38",156.0,2676108.0,0,This is common for users that use the Security Monitoring product.,0.0,0.194,0.806,0.34
Datadog,66355760,66271979,0,"2021/02/24, 19:20:38",False,"2021/02/24, 19:20:38",156.0,2676108.0,0,I would recommend reaching out to support@datadoghq.com to see if this flag is enabled in that one account.,0.0,0.223,0.777,0.5106
Datadog,66355760,66271979,0,"2021/02/24, 19:20:38",False,"2021/02/24, 19:20:38",156.0,2676108.0,0,"And if you want to replicate that behavior in another account, you can ask them to enable that feature.",0.0,0.067,0.933,0.0772
Datadog,64847039,64847038,3,"2020/11/15, 18:43:14",False,"2020/11/15, 18:43:14",1072.0,252340.0,2,"In Datadog, create an API key in Integrations, APIs.",0.0,0.208,0.792,0.2732
Datadog,64847039,64847038,3,"2020/11/15, 18:43:14",False,"2020/11/15, 18:43:14",1072.0,252340.0,2,Give the API key a name.,0.0,0.0,1.0,0.0
Datadog,64847039,64847038,3,"2020/11/15, 18:43:14",False,"2020/11/15, 18:43:14",1072.0,252340.0,2,In NLog.config create a target.,0.0,0.412,0.588,0.2732
Datadog,64847039,64847038,3,"2020/11/15, 18:43:14",False,"2020/11/15, 18:43:14",1072.0,252340.0,2,The url is either datadoghq.com or datadoghq.eu (for europe).,0.0,0.0,1.0,0.0
Datadog,64847039,64847038,3,"2020/11/15, 18:43:14",False,"2020/11/15, 18:43:14",1072.0,252340.0,2,Now create a rule to write to the target and you are done!,0.0,0.179,0.821,0.3382
Datadog,64847039,64847038,3,"2020/11/15, 18:43:14",False,"2020/11/15, 18:43:14",1072.0,252340.0,2,"All of the parameters can be configured to become columns in Datadog, and/or facets to select on.",0.0,0.0,1.0,0.0
Datadog,64847039,64847038,3,"2020/11/15, 18:43:14",False,"2020/11/15, 18:43:14",1072.0,252340.0,2,"I am using a date parameter so that the date matches other logs, rather than displaying the built in date.",0.0,0.0,1.0,0.0
Datadog,64484665,64476688,0,"2020/10/22, 17:40:57",True,"2020/10/22, 17:40:57",156.0,2676108.0,0,The datadog agent you deployed has no power to run scripts or take action.,0.145,0.0,0.855,-0.296
Datadog,64484665,64476688,0,"2020/10/22, 17:40:57",True,"2020/10/22, 17:40:57",156.0,2676108.0,0,It is purely a monitoring/data collection tool.,0.0,0.0,1.0,0.0
Datadog,64484665,64476688,0,"2020/10/22, 17:40:57",True,"2020/10/22, 17:40:57",156.0,2676108.0,0,However one of the things your monitors in the Datadog application can do is trigger events when they go into an alert state.,0.0,0.091,0.909,0.296
Datadog,64484665,64476688,0,"2020/10/22, 17:40:57",True,"2020/10/22, 17:40:57",156.0,2676108.0,0,"There are  lots of integrations : creating a ticket in Jira, posting a message to Slack, triggering an SNS topic.",0.0,0.121,0.879,0.296
Datadog,64484665,64476688,0,"2020/10/22, 17:40:57",True,"2020/10/22, 17:40:57",156.0,2676108.0,0,"What I recommend you try to do, is to create some kind of job or script that can be triggered externally, like a lambda function, or a jenkins job, or anything really.",0.0,0.215,0.785,0.7269
Datadog,64484665,64476688,0,"2020/10/22, 17:40:57",True,"2020/10/22, 17:40:57",156.0,2676108.0,0,When the monitor goes off you can use a  webhook  to trigger that script to do whatever you define.,0.0,0.0,1.0,0.0
Datadog,64484665,64476688,0,"2020/10/22, 17:40:57",True,"2020/10/22, 17:40:57",156.0,2676108.0,0,Here is a blog post showing  how twilio sent out a text message by connecting their api to a webhook .,0.0,0.0,1.0,0.0
Datadog,64407940,64108531,1,"2020/10/18, 01:24:48",True,"2020/10/18, 01:24:48",493.0,244037.0,0,"In these kinds of checks, the response order matters, since the columns returned from the DB are going to be mapped back to the names specified in the YAML.",0.0,0.038,0.962,0.0258
Datadog,64407940,64108531,1,"2020/10/18, 01:24:48",True,"2020/10/18, 01:24:48",493.0,244037.0,0,Reading the error message:,0.474,0.0,0.526,-0.4019
Datadog,64407940,64108531,1,"2020/10/18, 01:24:48",True,"2020/10/18, 01:24:48",493.0,244037.0,0,Error: postgres:953578488181a512 | (postgres.py:398) | non-numeric value  cldtx  for metric column  active_connections  of metric_prefix  postgresql,0.168,0.149,0.683,-0.0772
Datadog,64407940,64108531,1,"2020/10/18, 01:24:48",True,"2020/10/18, 01:24:48",493.0,244037.0,0,"We can see that the value of  cldtx  is being returned for the  active_connections  column, which in the YAML is declared as a gauge, and this is a string.",0.0,0.085,0.915,0.34
Datadog,64407940,64108531,1,"2020/10/18, 01:24:48",True,"2020/10/18, 01:24:48",493.0,244037.0,0,"The fix should be straightforward, by reordering the YAML, like so:",0.0,0.2,0.8,0.3612
Datadog,64407940,64108531,1,"2020/10/18, 01:24:48",True,"2020/10/18, 01:24:48",493.0,244037.0,0,"Alternately, if you want to keep the YAML ordered, change the query to:",0.0,0.098,0.902,0.0772
Datadog,63912117,63888511,0,"2020/09/16, 05:05:14",False,"2020/09/16, 05:05:14",2509.0,1190203.0,1,So I ended up just looking at the tests in the datadog terraform provider and noticing the query format they are testing.,0.0,0.0,1.0,0.0
Datadog,63912117,63888511,0,"2020/09/16, 05:05:14",False,"2020/09/16, 05:05:14",2509.0,1190203.0,1,It seems you need to specify a time range and also add in a comparison threshold that matches your critical alert threshold.,0.102,0.098,0.8,-0.0258
Datadog,63912117,63888511,0,"2020/09/16, 05:05:14",False,"2020/09/16, 05:05:14",2509.0,1190203.0,1,That was what was missing.,0.355,0.0,0.645,-0.296
Datadog,63779195,63779194,0,"2020/09/07, 17:06:52",True,"2020/09/07, 17:06:52",95.0,285601.0,0,It is not possible no.,0.0,0.321,0.679,0.2235
Datadog,63779195,63779194,0,"2020/09/07, 17:06:52",True,"2020/09/07, 17:06:52",95.0,285601.0,0,Confirmed with DD support.,0.0,0.474,0.526,0.4019
Datadog,64995876,63779194,0,"2020/11/25, 00:36:55",False,"2020/12/02, 18:50:18",906.0,3758005.0,1,"Actually it is possible, but you need to put every json log into quotes (some prefix before each log will also work), so that Datadog agent will consider this as a 'text'.",0.0,0.0,1.0,0.0
Datadog,64995876,63779194,0,"2020/11/25, 00:36:55",False,"2020/12/02, 18:50:18",906.0,3758005.0,1,I.e.,0.0,0.0,1.0,0.0
Datadog,64995876,63779194,0,"2020/11/25, 00:36:55",False,"2020/12/02, 18:50:18",906.0,3758005.0,1,log.json  file should contain quoted logs:,0.0,0.0,1.0,0.0
Datadog,64995876,63779194,0,"2020/11/25, 00:36:55",False,"2020/12/02, 18:50:18",906.0,3758005.0,1,"After that, in Datadog Logs Configuration you need to add a pipeline with Grok parser filter  json  (see filter tab in  Matcher and Filter ):",0.109,0.0,0.891,-0.4215
Datadog,64995876,63779194,0,"2020/11/25, 00:36:55",False,"2020/12/02, 18:50:18",906.0,3758005.0,1,This allowed me to perform full text search thru all fields in my json logs and automatically parse all json fields as attributes.,0.0,0.0,1.0,0.0
Datadog,64995876,63779194,0,"2020/11/25, 00:36:55",False,"2020/12/02, 18:50:18",906.0,3758005.0,1,P.S.,0.0,0.0,1.0,0.0
Datadog,64995876,63779194,0,"2020/11/25, 00:36:55",False,"2020/12/02, 18:50:18",906.0,3758005.0,1,This solution was provided by Datadog support 2 years ago.,0.0,0.417,0.583,0.6124
Datadog,64995876,63779194,0,"2020/11/25, 00:36:55",False,"2020/12/02, 18:50:18",906.0,3758005.0,1,And seems they are working on solution to allow full text search for JSON logs.,0.0,0.244,0.756,0.4939
Datadog,62747357,62739543,0,"2020/07/06, 02:24:28",True,"2020/07/06, 02:24:28",251.0,5477963.0,1,"It looks from the above snippets that the  combined  Morgan format is sent directly sent to Winston, and then parsed within a log pipeline in Datadog.",0.0,0.0,1.0,0.0
Datadog,62747357,62739543,0,"2020/07/06, 02:24:28",True,"2020/07/06, 02:24:28",251.0,5477963.0,1,"Since the  combined  format doesn't include the body and there is no built-in token for it, you would have to use a custom format with your own tokens and then update your pipeline accordingly.",0.064,0.0,0.936,-0.296
Datadog,62747357,62739543,0,"2020/07/06, 02:24:28",True,"2020/07/06, 02:24:28",251.0,5477963.0,1,"For example, to create a custom format in Morgan that includes the status code and the body:",0.0,0.123,0.877,0.2732
Datadog,62747357,62739543,0,"2020/07/06, 02:24:28",True,"2020/07/06, 02:24:28",251.0,5477963.0,1,You can also create a token to achieve the same result with a simpler format definition:,0.0,0.139,0.861,0.2732
Datadog,62747357,62739543,0,"2020/07/06, 02:24:28",True,"2020/07/06, 02:24:28",251.0,5477963.0,1,"You can find the documentation for custom Morgan formats  here , creating tokens  here , and Datadog log pipeline parsing  here .",0.0,0.109,0.891,0.296
Datadog,62747357,62739543,0,"2020/07/06, 02:24:28",True,"2020/07/06, 02:24:28",251.0,5477963.0,1,Hope this helps!,0.0,0.853,0.147,0.6996
Datadog,62228894,62094698,0,"2020/06/06, 10:54:35",False,"2020/06/06, 10:54:35",21.0,7314273.0,0,"Maybe you can ask them to add it by opening a feature request:
 https://github.com/DataDog/documentation/issues/new/choose",0.0,0.0,1.0,0.0
Datadog,62008387,61971889,0,"2020/05/25, 21:21:06",True,"2020/05/25, 21:21:06",2771.0,842302.0,0,but it looks like I'm missing an important part here.,0.184,0.357,0.459,0.3919
Datadog,62008387,61971889,0,"2020/05/25, 21:21:06",True,"2020/05/25, 21:21:06",2771.0,842302.0,0,That was the point.,0.0,0.0,1.0,0.0
Datadog,62008387,61971889,0,"2020/05/25, 21:21:06",True,"2020/05/25, 21:21:06",2771.0,842302.0,0,The lambda itself has not much todo with particular  statusCodes .,0.0,0.0,1.0,0.0
Datadog,62008387,61971889,0,"2020/05/25, 21:21:06",True,"2020/05/25, 21:21:06",2771.0,842302.0,0,So I either may log each status code and let datadog parse it accordingly.,0.0,0.0,1.0,0.0
Datadog,62008387,61971889,0,"2020/05/25, 21:21:06",True,"2020/05/25, 21:21:06",2771.0,842302.0,0,"Or, that's the solution I went for, I can leverage API-Gateway for monitoring status codes per lambda.",0.0,0.141,0.859,0.3182
Datadog,61822822,61817555,1,"2020/05/15, 18:27:58",False,"2020/05/15, 18:27:58",1371.0,5540166.0,2,"You can use most any of the common open source log shippers to send server logs to Datadog without using the Datadog agent, for example  fluentd .",0.0,0.0,1.0,0.0
Datadog,61822822,61817555,1,"2020/05/15, 18:27:58",False,"2020/05/15, 18:27:58",1371.0,5540166.0,2,"But there can be several benefits to using the Datadog agent to collect server logs, such as:",0.0,0.175,0.825,0.5267
Datadog,61822822,61817555,1,"2020/05/15, 18:27:58",False,"2020/05/15, 18:27:58",1371.0,5540166.0,2,"There are other ways to collect logs in Datadog, among those is the  HTTP API .",0.0,0.0,1.0,0.0
Datadog,61822822,61817555,1,"2020/05/15, 18:27:58",False,"2020/05/15, 18:27:58",1371.0,5540166.0,2,"Since this API uses a POST method, I bet you could configure Datadog's  webhook integration  to generate log events from Datadog events and alerts.",0.0,0.0,1.0,0.0
Datadog,61822822,61817555,1,"2020/05/15, 18:27:58",False,"2020/05/15, 18:27:58",1371.0,5540166.0,2,"That said, before you go through the trouble of doing this, if you have a use-case or reason you're interested in doing this, you may want to reach out to Datadog support to see if they have some features coming / in beta that would get you what you want without the extra work on your end.",0.044,0.15,0.806,0.5267
Datadog,61822822,61817555,1,"2020/05/15, 18:27:58",False,"2020/05/15, 18:27:58",1371.0,5540166.0,2,(What  is  your use-case?,0.0,0.0,1.0,0.0
Datadog,61822822,61817555,1,"2020/05/15, 18:27:58",False,"2020/05/15, 18:27:58",1371.0,5540166.0,2,I'm curious),0.0,0.0,1.0,0.0
Datadog,61513379,61511271,1,"2020/04/30, 02:25:23",True,"2020/04/30, 02:25:23",1371.0,5540166.0,1,"Sounds like some of your  http.server.requests.count  metric values do not have any  status  tag, so when you group by the  status  tag, those are being aggregaed with a value of  n/a .",0.0,0.22,0.78,0.765
Datadog,61513379,61511271,1,"2020/04/30, 02:25:23",True,"2020/04/30, 02:25:23",1371.0,5540166.0,1,"If it is intentional/expected that this metric would have values without the  status  tag and you just want to ignored those metric values, then you can use the  exclude_null()  function to remove that tag grouping from your graph (docs  here ).",0.05,0.146,0.804,0.5267
Datadog,61513379,61511271,1,"2020/04/30, 02:25:23",True,"2020/04/30, 02:25:23",1371.0,5540166.0,1,"If it is not intentional/expected that this metric would have values without the  status  tag, then you probably want to reach out to support@datadoghq.com to get that looked into.",0.0,0.164,0.836,0.4767
Datadog,62812732,61405563,0,"2020/07/09, 13:23:09",False,"2020/07/09, 13:23:09",877.0,1570636.0,0,Just add this in the startup.cs.,0.0,0.0,1.0,0.0
Datadog,65504306,61405563,0,"2020/12/30, 10:52:27",False,"2020/12/30, 10:52:27",365.0,5666309.0,0,I'm very sorry for this late answer.,0.21,0.0,0.79,-0.1513
Datadog,65504306,61405563,0,"2020/12/30, 10:52:27",False,"2020/12/30, 10:52:27",365.0,5666309.0,0,"After exchanging a bunch of emails with the Datadog support guys, it turned out that the solution is straightforward:",0.0,0.238,0.762,0.6124
Datadog,65504306,61405563,0,"2020/12/30, 10:52:27",False,"2020/12/30, 10:52:27",365.0,5666309.0,0,The  Dispose()  method force the sinks to gracefully close up and send the logs stored in cache.,0.0,0.175,0.825,0.5267
Datadog,65504306,61405563,0,"2020/12/30, 10:52:27",False,"2020/12/30, 10:52:27",365.0,5666309.0,0,Please note that the logs do not appear instantly in the Datadog console: allow a few seconds (to some minutes) for their systems to process the logs you send.,0.0,0.139,0.861,0.4939
Datadog,61225485,61191225,0,"2020/04/15, 12:32:12",False,"2020/04/15, 12:32:12",21.0,7314273.0,1,"You might want to use datadog service-check which is also can be sent by the StatsDClient, and then add it to your monitor/dashboard page.",0.0,0.053,0.947,0.0772
Datadog,61225485,61191225,0,"2020/04/15, 12:32:12",False,"2020/04/15, 12:32:12",21.0,7314273.0,1,https://docs.datadoghq.com/developers/service_checks/,0.0,0.0,1.0,0.0
Datadog,61232317,61191225,0,"2020/04/15, 18:18:06",False,"2020/04/15, 18:18:06",1371.0,5540166.0,0,"Another option is to use  Datadog's Java / JMX integration  to capture the health data that's exposed over JMX -- this can probably give you up/down health data, and can certainly give you a lot more granular health metrics too.",0.033,0.06,0.907,0.2732
Datadog,60829558,60828990,1,"2020/03/24, 12:51:21",True,"2020/03/24, 18:07:20",3151.0,4409319.0,3,"Well, apparently you can  -@facet:*",0.0,0.344,0.656,0.2732
Datadog,60829558,60828990,1,"2020/03/24, 12:51:21",True,"2020/03/24, 18:07:20",3151.0,4409319.0,3,"Didn't specify it in my question because it was not important, but what I really needed was a way to either  filter by a specific facet value, or get logs without said facet",0.04,0.096,0.864,0.4222
Datadog,60829558,60828990,1,"2020/03/24, 12:51:21",True,"2020/03/24, 18:07:20",3151.0,4409319.0,3,The following works for me:,0.0,0.0,1.0,0.0
Datadog,60754931,60715172,4,"2020/03/19, 12:10:42",False,"2020/03/19, 12:10:42",37971.0,1237575.0,0,Spring is scanning your classpath that seems incomplete.,0.0,0.0,1.0,0.0
Datadog,60754931,60715172,4,"2020/03/19, 12:10:42",False,"2020/03/19, 12:10:42",37971.0,1237575.0,0,Maybe it is related to Spring's class loading mechanisms.,0.0,0.0,1.0,0.0
Datadog,60754931,60715172,4,"2020/03/19, 12:10:42",False,"2020/03/19, 12:10:42",37971.0,1237575.0,0,The  class in question  exists and seems to be part of the agent.,0.0,0.0,1.0,0.0
Datadog,60754931,60715172,4,"2020/03/19, 12:10:42",False,"2020/03/19, 12:10:42",37971.0,1237575.0,0,"Possibly, you are using an outdated version of the agent.",0.0,0.0,1.0,0.0
Datadog,60660835,60615996,1,"2020/03/12, 21:36:18",False,"2020/03/12, 21:36:18",28832.0,281848.0,1,I'm not particularly familiar with this Datadog JSON format but the general pattern I would propose here has multiple steps:,0.0,0.0,1.0,0.0
Datadog,60660835,60615996,1,"2020/03/12, 21:36:18",False,"2020/03/12, 21:36:18",28832.0,281848.0,1,Here's a basic example of that to show the general principle.,0.0,0.0,1.0,0.0
Datadog,60660835,60615996,1,"2020/03/12, 21:36:18",False,"2020/03/12, 21:36:18",28832.0,281848.0,1,It will need more work to capture all of the details you included in your initial example.,0.0,0.0,1.0,0.0
Datadog,60660835,60615996,1,"2020/03/12, 21:36:18",False,"2020/03/12, 21:36:18",28832.0,281848.0,1,"The separate normalization step to build  local.screenboard  here isn't strictly necessary: you could instead put the same sort of normalization expressions (using  try  to set defaults for things that aren't set) directly inside the  resource ""datadog_screenboard""  block arguments if you wanted.",0.126,0.0,0.874,-0.6808
Datadog,60660835,60615996,1,"2020/03/12, 21:36:18",False,"2020/03/12, 21:36:18",28832.0,281848.0,1,"I prefer to treat normalization as a separate step because then this leaves a clear definition in the configuration for what we're expecting to find in the JSON and what default values we'll use for optional items, separate from defining how that result is then mapped onto the physical  datadog_screenboard  resource.",0.0,0.151,0.849,0.7906
Datadog,60660835,60615996,1,"2020/03/12, 21:36:18",False,"2020/03/12, 21:36:18",28832.0,281848.0,1,I wasn't able to test the example above because I don't have a Datadog account.,0.0,0.0,1.0,0.0
Datadog,60660835,60615996,1,"2020/03/12, 21:36:18",False,"2020/03/12, 21:36:18",28832.0,281848.0,1,I'm sorry if there are minor typos/mistakes in it that lead to errors.,0.252,0.0,0.748,-0.4019
Datadog,60660835,60615996,1,"2020/03/12, 21:36:18",False,"2020/03/12, 21:36:18",28832.0,281848.0,1,"My hope was to show the general principle of mapping from a serialized data file to a resource rather than to give a ready-to-use solution, so I hope the above includes enough examples of different situations that you can see how to extend it for the remaining Datadog JSON features you want to support in this module.",0.0,0.227,0.773,0.8957
Datadog,60660835,60615996,1,"2020/03/12, 21:36:18",False,"2020/03/12, 21:36:18",28832.0,281848.0,1,"If this JSON format is a interchange format formally documented by Datadog, it could make sense for Terraform's Datadog provider to have the option of accepting a single JSON string in this format as configuration, for easier exporting.",0.0,0.137,0.863,0.6597
Datadog,60660835,60615996,1,"2020/03/12, 21:36:18",False,"2020/03/12, 21:36:18",28832.0,281848.0,1,"That may require changes to the Datadog provider itself, which is beyond what I can answer here but might be worth raising in the GitHub issues for that provider to streamline this use-case.",0.0,0.07,0.93,0.3291
Datadog,64437131,60326963,0,"2020/10/20, 04:12:43",False,"2020/10/20, 04:12:43",11.0,7000092.0,0,I solved this by adding 'squashfs' to the list of filesystem types to be ignored by the datadog agent.,0.113,0.103,0.784,-0.0516
Datadog,64437131,60326963,0,"2020/10/20, 04:12:43",False,"2020/10/20, 04:12:43",11.0,7000092.0,0,Create a file  /etc/datadog-agent/conf.d/disk.d/conf.yaml :,0.0,0.512,0.488,0.2732
Datadog,64437131,60326963,0,"2020/10/20, 04:12:43",False,"2020/10/20, 04:12:43",11.0,7000092.0,0,Restart datadog agent ( systemctl restart datadog-agent ).,0.0,0.0,1.0,0.0
Datadog,59757720,59757258,2,"2020/01/15, 20:38:32",False,"2020/01/15, 20:38:32",336.0,7243426.0,1,"If you have an ever increasing counter, you can use the a function called  rate .",0.0,0.0,1.0,0.0
Datadog,59757720,59757258,2,"2020/01/15, 20:38:32",False,"2020/01/15, 20:38:32",336.0,7243426.0,1,You'll be able to select it with the  +  on the query line.,0.0,0.0,1.0,0.0
Datadog,59757720,59757258,2,"2020/01/15, 20:38:32",False,"2020/01/15, 20:38:32",336.0,7243426.0,1,"With that you'll be able to have a rate of increase per seconds, minutes or hours.",0.0,0.141,0.859,0.3182
Datadog,59757720,59757258,2,"2020/01/15, 20:38:32",False,"2020/01/15, 20:38:32",336.0,7243426.0,1,"If you are looking to get a difference between the same metric but at another point in the past, you have a function called  timeshift  that could also help.",0.0,0.12,0.88,0.5499
Datadog,59757720,59757258,2,"2020/01/15, 20:38:32",False,"2020/01/15, 20:38:32",336.0,7243426.0,1,This is also accessible with the small  +  on the right of the query line.,0.0,0.0,1.0,0.0
Datadog,59757720,59757258,2,"2020/01/15, 20:38:32",False,"2020/01/15, 20:38:32",336.0,7243426.0,1,"Finally, if you are looking at comparing two different metrics, you  have a button called  Advanced  that will enable you to write more complex queries such as a difference between two metrics.",0.0,0.065,0.935,0.25
Datadog,59177979,59177043,0,"2019/12/04, 16:10:33",False,"2019/12/04, 16:10:33",807.0,1506396.0,2,"I believe you are looking for  clusterName :
 https://github.com/helm/charts/blob/master/stable/datadog/values.yaml#L75",0.0,0.0,1.0,0.0
Datadog,59177979,59177043,0,"2019/12/04, 16:10:33",False,"2019/12/04, 16:10:33",807.0,1506396.0,2,You can add it in your  values.yaml  under the  datadog  section like this:,0.0,0.172,0.828,0.3612
Datadog,59178230,59177043,0,"2019/12/04, 16:24:18",False,"2019/12/04, 16:24:18",9281.0,3270785.0,2,refer below command,0.0,0.0,1.0,0.0
Datadog,59747217,59088171,0,"2020/01/15, 10:05:27",False,"2020/01/15, 10:05:27",111.0,5054074.0,0,"I am not a fan of helm, but you can accomplish this in 2 ways:",0.098,0.244,0.659,0.4971
Datadog,59747217,59088171,0,"2020/01/15, 10:05:27",False,"2020/01/15, 10:05:27",111.0,5054074.0,0,via env vars: make use of  DD_AC_EXCLUDE  variable to exclude the Redis containers: eg  DD_AC_EXCLUDE=name:prefix-redis,0.119,0.0,0.881,-0.2263
Datadog,59747217,59088171,0,"2020/01/15, 10:05:27",False,"2020/01/15, 10:05:27",111.0,5054074.0,0,"via a config map: mount an empty config map in  /etc/datadog-agent/conf.d/redisdb.d/ , below is an example where I renamed the  auto_conf.yaml  to  auto_conf.yaml.example .",0.087,0.0,0.913,-0.2023
Datadog,59747217,59088171,0,"2020/01/15, 10:05:27",False,"2020/01/15, 10:05:27",111.0,5054074.0,0,alter the daemonset/deployment object:,0.0,0.0,1.0,0.0
Datadog,58695216,58694140,1,"2019/11/04, 16:13:25",False,"2019/11/04, 16:13:25",336.0,7243426.0,1,"On the bottom of the infrastructure list, you should see a link called ""JSON API permalink"".",0.0,0.0,1.0,0.0
Datadog,58695216,58694140,1,"2019/11/04, 16:13:25",False,"2019/11/04, 16:13:25",336.0,7243426.0,1,"If you  query it , this should give you a JSON of all your hosts with their agent version.",0.0,0.0,1.0,0.0
Datadog,58695216,58694140,1,"2019/11/04, 16:13:25",False,"2019/11/04, 16:13:25",336.0,7243426.0,1,You can then query it with a quick Python script.,0.0,0.0,1.0,0.0
Datadog,58647078,58641893,0,"2019/10/31, 17:55:11",False,"2019/10/31, 17:55:11",14987.0,90580.0,0,"Your metrics should have a common prefix like  myapp.metric1 ,  myapp.metric2 , etc.",0.0,0.217,0.783,0.3612
Datadog,58647078,58641893,0,"2019/10/31, 17:55:11",False,"2019/10/31, 17:55:11",14987.0,90580.0,0,Then you can disable all metrics and enable explicitly all  myapp.,0.0,0.0,1.0,0.0
Datadog,58647078,58641893,0,"2019/10/31, 17:55:11",False,"2019/10/31, 17:55:11",14987.0,90580.0,0,*  metrics like so:,0.0,0.556,0.444,0.3612
Datadog,58647078,58641893,0,"2019/10/31, 17:55:11",False,"2019/10/31, 17:55:11",14987.0,90580.0,0,application.properties:,0.0,0.0,1.0,0.0
Datadog,58647078,58641893,0,"2019/10/31, 17:55:11",False,"2019/10/31, 17:55:11",14987.0,90580.0,0,the  management.metrics.enable.&lt;your_custom_prefix&gt;  will enable all  &lt;your_custome_prefix&gt;.,0.0,0.0,1.0,0.0
Datadog,58647078,58641893,0,"2019/10/31, 17:55:11",False,"2019/10/31, 17:55:11",14987.0,90580.0,0,*  metrics.,0.0,0.0,1.0,0.0
Datadog,58647078,58641893,0,"2019/10/31, 17:55:11",False,"2019/10/31, 17:55:11",14987.0,90580.0,0,"If you want to enable some of the built-in core metrics again, for example reenabling  jvm.",0.0,0.08,0.92,0.0772
Datadog,58647078,58641893,0,"2019/10/31, 17:55:11",False,"2019/10/31, 17:55:11",14987.0,90580.0,0,"* , you can do:",0.0,0.0,1.0,0.0
Datadog,58647078,58641893,0,"2019/10/31, 17:55:11",False,"2019/10/31, 17:55:11",14987.0,90580.0,0,"I've created a sample project  in github  that disables core metrics, enables custom metrics, and  jvm.",0.0,0.125,0.875,0.25
Datadog,58647078,58641893,0,"2019/10/31, 17:55:11",False,"2019/10/31, 17:55:11",14987.0,90580.0,0,*  metrics and sends to Datadog.,0.0,0.0,1.0,0.0
Datadog,58689932,58607248,0,"2019/11/04, 10:54:36",False,"2019/11/04, 10:54:36",336.0,7243426.0,0,Not sure I fully grasp the issue.,0.282,0.0,0.718,-0.2411
Datadog,58689932,58607248,0,"2019/11/04, 10:54:36",False,"2019/11/04, 10:54:36",336.0,7243426.0,0,Here are some steps to collect your traces:,0.0,0.0,1.0,0.0
Datadog,58689932,58607248,0,"2019/11/04, 10:54:36",False,"2019/11/04, 10:54:36",336.0,7243426.0,0,"Just in case, more info on Open Tracing  here",0.0,0.0,1.0,0.0
Datadog,58708338,58579323,0,"2019/11/05, 11:33:57",False,"2021/02/05, 00:01:35",4059.0,8763847.0,1,Seems like there is a typo in your command.,0.0,0.263,0.737,0.3612
Datadog,58708338,58579323,0,"2019/11/05, 11:33:57",False,"2021/02/05, 00:01:35",4059.0,8763847.0,1,DD_DOGSTATD_NON_LOCAL_TRAFFIC   is used instead of  DD_DOGSTATSD_NON_LOCAL_TRAFFIC,0.0,0.0,1.0,0.0
Datadog,58708338,58579323,0,"2019/11/05, 11:33:57",False,"2021/02/05, 00:01:35",4059.0,8763847.0,1,I usually used the below command for testing with Datadog:,0.0,0.0,1.0,0.0
Datadog,58547485,58502004,6,"2019/10/24, 21:47:16",True,"2019/10/24, 21:47:16",336.0,7243426.0,1,Have you tried  composite monitors ?,0.0,0.0,1.0,0.0
Datadog,58547485,58502004,6,"2019/10/24, 21:47:16",True,"2019/10/24, 21:47:16",336.0,7243426.0,1,You should be able to combine your low CPU Credit monitor with another monitor that looks at events from RDS.,0.093,0.115,0.793,0.128
Datadog,58547485,58502004,6,"2019/10/24, 21:47:16",True,"2019/10/24, 21:47:16",336.0,7243426.0,1,Two monitors such as:,0.0,0.0,1.0,0.0
Datadog,58547485,58502004,6,"2019/10/24, 21:47:16",True,"2019/10/24, 21:47:16",336.0,7243426.0,1,A composite monitor: A &amp;&amp; !B,0.0,0.0,1.0,0.0
Datadog,58547485,58502004,6,"2019/10/24, 21:47:16",True,"2019/10/24, 21:47:16",336.0,7243426.0,1,(I hope my example makes sense),0.0,0.367,0.633,0.4404
Datadog,53935650,53222715,0,"2018/12/26, 20:21:57",True,"2018/12/26, 20:21:57",347.0,2650254.0,0,"so, I had to install a Datadog agent on ec2 instance and configure it to be able to access all mysql dbs and collect the mysql performance schema metrics.",0.0,0.0,1.0,0.0
Datadog,53935650,53222715,0,"2018/12/26, 20:21:57",True,"2018/12/26, 20:21:57",347.0,2650254.0,0,And surely with correct security groups for the ec2.,0.0,0.431,0.569,0.6486
Datadog,53935650,53222715,0,"2018/12/26, 20:21:57",True,"2018/12/26, 20:21:57",347.0,2650254.0,0,I followed this  docs  and  this  and contacted the support.,0.0,0.252,0.748,0.4019
Datadog,51871692,51707255,1,"2018/08/16, 10:24:11",True,"2018/08/16, 10:24:11",76.0,4481158.0,0,"If the Windows OS is D drive, the setting is installed in  D:\ProgramData\Datadog .",0.0,0.0,1.0,0.0
Datadog,51871692,51707255,1,"2018/08/16, 10:24:11",True,"2018/08/16, 10:24:11",76.0,4481158.0,0,"Copying it to  C:\ProgramData\Datadog  will work, but I submitted an improvement request to Datadog Support.",0.0,0.386,0.614,0.8201
Datadog,50785165,47436429,0,"2018/06/10, 18:22:21",True,"2018/06/10, 18:22:21",518.0,3605831.0,0,Yes it does have this functionality in the Audit Association entity.,0.0,0.213,0.787,0.4019
Datadog,50785165,47436429,0,"2018/06/10, 18:22:21",True,"2018/06/10, 18:22:21",518.0,3605831.0,0,The entity stored with the blame_id in the Audit Log entity contains information regarding the user.,0.0,0.0,1.0,0.0
Datadog,50785165,47436429,0,"2018/06/10, 18:22:21",True,"2018/06/10, 18:22:21",518.0,3605831.0,0,The one with source_id contains information regarding the entity itself and thus the ID of the entity in the  fk  field.,0.0,0.0,1.0,0.0
Datadog,47294550,41578245,0,"2017/11/14, 22:22:41",False,"2017/11/14, 22:22:41",51.0,2454643.0,0,"Once the datadog is properly installed on your server, you can use the custom metric feature to let datadog read your query result into a custom metric and then use that metric to create a dashboard.",0.0,0.06,0.94,0.2732
Datadog,47294550,41578245,0,"2017/11/14, 22:22:41",False,"2017/11/14, 22:22:41",51.0,2454643.0,0,You can find more on custom metric on datadog  here,0.0,0.0,1.0,0.0
Datadog,47294550,41578245,0,"2017/11/14, 22:22:41",False,"2017/11/14, 22:22:41",51.0,2454643.0,0,They work with yaml file so be cautious with the formatting of the yaml file that will hold your custom metric.,0.077,0.0,0.923,-0.1725
Datadog,66880008,66875990,0,"2021/03/31, 03:15:15",False,"2021/03/31, 03:15:15",3.0,1467883.0,0,It looks like Datadog uses zstd compression in order to compress its data before sending it:  https://github.com/DataDog/datadog-agent/blob/972c4caf3e6bc7fa877c4a761122aef88e748b48/pkg/util/compression/zlib.go,0.0,0.135,0.865,0.3612
Datadog,66345892,66326300,0,"2021/02/24, 08:42:37",False,"2021/02/24, 08:55:54",3000.0,1335245.0,0,This is what I've got so far.,0.0,0.0,1.0,0.0
Datadog,66345892,66326300,0,"2021/02/24, 08:42:37",False,"2021/02/24, 08:55:54",3000.0,1335245.0,0,Everything but the  source .,0.0,0.0,1.0,0.0
Datadog,65834472,65829133,0,"2021/01/21, 21:29:00",False,"2021/01/21, 21:29:00",17806.0,119790.0,0,Need to use category-processor  https://docs.datadoghq.com/logs/processing/processors/?tab=ui#category-processor,0.0,0.0,1.0,0.0
Datadog,65834472,65829133,0,"2021/01/21, 21:29:00",False,"2021/01/21, 21:29:00",17806.0,119790.0,0,Example:,0.0,0.0,1.0,0.0
Datadog,65834472,65829133,0,"2021/01/21, 21:29:00",False,"2021/01/21, 21:29:00",17806.0,119790.0,0,"I didn't do enough research before posting this question, but the answer for anyone else looking.",0.0,0.0,1.0,0.0
Datadog,65770731,65766741,0,"2021/01/18, 10:05:03",True,"2021/01/18, 10:05:03",756.0,9629802.0,0,"You are using the DataDog configuration for the commercial  k6 Cloud service  ( k6 cloud ), not locally run k6 tests ( k6 run ).",0.0,0.0,1.0,0.0
Datadog,65770731,65766741,0,"2021/01/18, 10:05:03",True,"2021/01/18, 10:05:03",756.0,9629802.0,0,"test_run_id  is a concept in the cloud service, though it's also easy to emulate locally as a way to distinguish between test runs.",0.0,0.127,0.873,0.4404
Datadog,65770731,65766741,0,"2021/01/18, 10:05:03",True,"2021/01/18, 10:05:03",756.0,9629802.0,0,"For local tests, you should enable the DataDog output by running k6 with  k6 run --out datadog script.js .",0.0,0.0,1.0,0.0
Datadog,65770731,65766741,0,"2021/01/18, 10:05:03",True,"2021/01/18, 10:05:03",756.0,9629802.0,0,"I assume you did that, otherwise you wouldn't see any metrics in DataDog.",0.0,0.0,1.0,0.0
Datadog,65770731,65766741,0,"2021/01/18, 10:05:03",True,"2021/01/18, 10:05:03",756.0,9629802.0,0,"Then, you can use the  tags  option  to inject a unique extra tag for all metrics generated by a particular k6 run, so you can differentiate them in DataDog.",0.0,0.0,1.0,0.0
Datadog,65770731,65766741,0,"2021/01/18, 10:05:03",True,"2021/01/18, 10:05:03",756.0,9629802.0,0,For example:,0.0,0.0,1.0,0.0
Datadog,65770731,65766741,0,"2021/01/18, 10:05:03",True,"2021/01/18, 10:05:03",756.0,9629802.0,0,"Of course, you can choose any  key=value  combination, you are not restricted to  test_run_id .",0.0,0.144,0.856,0.2924
Datadog,66783806,65717686,0,"2021/03/24, 17:02:55",True,"2021/03/24, 17:02:55",56.0,10960667.0,1,Nothing is wrong there.,0.0,0.46,0.54,0.3724
Datadog,66783806,65717686,0,"2021/03/24, 17:02:55",True,"2021/03/24, 17:02:55",56.0,10960667.0,1,DataDog conceals that the Kafka integration uses Dogstatsd under the hood.,0.0,0.0,1.0,0.0
Datadog,66783806,65717686,0,"2021/03/24, 17:02:55",True,"2021/03/24, 17:02:55",56.0,10960667.0,1,"When  use_dogstatsd: 'true  within /etc/datadog-agent/datadog.yaml is set, metrics do appear in DataDog webUI.",0.0,0.189,0.811,0.4215
Datadog,66783806,65717686,0,"2021/03/24, 17:02:55",True,"2021/03/24, 17:02:55",56.0,10960667.0,1,If that option is not set the default Broker data is available via JMXFetch using  sudo -u dd-agent datadog-agent status  as also via  sudo -u dd-agent datadog-agent check kafka  but not in the webUI.,0.0,0.0,1.0,0.0
Datadog,65631462,65631278,1,"2021/01/08, 17:04:29",False,"2021/01/08, 17:04:29",1530.0,11866104.0,0,Based on the  doc  you can decide which one is good for your use case.,0.0,0.172,0.828,0.4404
Datadog,65631462,65631278,1,"2021/01/08, 17:04:29",False,"2021/01/08, 17:04:29",1530.0,11866104.0,0,"Stackdriver is detailed as &quot;Monitoring, logging, and diagnostics for applications on Google Cloud Platform and AWS&quot;.",0.0,0.0,1.0,0.0
Datadog,65631462,65631278,1,"2021/01/08, 17:04:29",False,"2021/01/08, 17:04:29",1530.0,11866104.0,0,"Google Stackdriver provides powerful monitoring, logging, and diagnostics.",0.0,0.286,0.714,0.4215
Datadog,65631462,65631278,1,"2021/01/08, 17:04:29",False,"2021/01/08, 17:04:29",1530.0,11866104.0,0,"It equips you with insight into the health, performance, and availability of cloud-powered applications, enabling you to find and fix issues faster.",0.0,0.0,1.0,0.0
Datadog,65631462,65631278,1,"2021/01/08, 17:04:29",False,"2021/01/08, 17:04:29",1530.0,11866104.0,0,"Grafana can be classified as a tool in the &quot;Monitoring Tools&quot; category, while Stackdriver is grouped under &quot;Cloud Monitoring&quot;.",0.0,0.0,1.0,0.0
Datadog,65631654,65629898,0,"2021/01/08, 17:15:24",False,"2021/01/08, 17:15:24",678.0,11584728.0,0,Answering my own question this might be helpful for others,0.0,0.237,0.763,0.4215
Datadog,65631654,65629898,0,"2021/01/08, 17:15:24",False,"2021/01/08, 17:15:24",678.0,11584728.0,0,I had to set,0.0,0.0,1.0,0.0
Datadog,65631654,65629898,0,"2021/01/08, 17:15:24",False,"2021/01/08, 17:15:24",678.0,11584728.0,0,inside  /etc/datadog-agent/datadog.yaml  then I've created  python.d/conf.yaml  with the following configs,0.0,0.182,0.818,0.25
Datadog,65631654,65629898,0,"2021/01/08, 17:15:24",False,"2021/01/08, 17:15:24",678.0,11584728.0,0,Restart the agent with,0.0,0.0,1.0,0.0
Datadog,65631654,65629898,0,"2021/01/08, 17:15:24",False,"2021/01/08, 17:15:24",678.0,11584728.0,0,You can see your logs in the dashboard logs panel,0.0,0.0,1.0,0.0
Datadog,65403420,65320127,0,"2020/12/22, 06:52:25",False,"2020/12/22, 06:52:25",1.0,14869489.0,0,Window option available... Go to integration and  click on agents..,0.0,0.0,1.0,0.0
Datadog,65403420,65320127,0,"2020/12/22, 06:52:25",False,"2020/12/22, 06:52:25",1.0,14869489.0,0,There is an option available windows ( left side ).. Click on windows then u get link..... Just copy the link and paste it on ur windows Server.... Then datadog starts monitoring...,0.0,0.0,1.0,0.0
Datadog,65403420,65320127,0,"2020/12/22, 06:52:25",False,"2020/12/22, 06:52:25",1.0,14869489.0,0,It takes 5 minutes to monitor,0.0,0.0,1.0,0.0
Datadog,64776832,64772196,0,"2020/11/10, 23:11:22",False,"2020/11/10, 23:11:22",11561.0,970308.0,0,The MeterRegistry already has implemented how to send custom metrics (posting to DataDog) See the code  https://github.com/micrometer-metrics/micrometer/blob/master/implementations/micrometer-registry-datadog/src/main/java/io/micrometer/datadog/DatadogMeterRegistry.java#L133,0.0,0.0,1.0,0.0
Datadog,64776832,64772196,0,"2020/11/10, 23:11:22",False,"2020/11/10, 23:11:22",11561.0,970308.0,0,It appears like you are trying to add common tags to each metric.,0.0,0.172,0.828,0.3612
Datadog,64776832,64772196,0,"2020/11/10, 23:11:22",False,"2020/11/10, 23:11:22",11561.0,970308.0,0,"Perhaps you shouldn't implement your own DataDog registry, and instead use the provided one to send the metrics and set the common tags via config:",0.0,0.0,1.0,0.0
Datadog,64542859,64539546,0,"2020/10/26, 20:21:25",True,"2020/10/26, 20:21:25",156.0,2676108.0,1,I don't believe there is any way to graph the historical behavior of the SLI from an SLO.,0.0,0.0,1.0,0.0
Datadog,64542859,64539546,0,"2020/10/26, 20:21:25",True,"2020/10/26, 20:21:25",156.0,2676108.0,1,"The closest you could get would be to measure the underlying metric, so if you had  good events / bad events  you could display that percentage.",0.119,0.099,0.782,-0.1531
Datadog,64542859,64539546,0,"2020/10/26, 20:21:25",True,"2020/10/26, 20:21:25",156.0,2676108.0,1,But the calculation of how often that percentage is above or below a certain threshold would not be possible.,0.0,0.135,0.865,0.3919
Datadog,64542859,64539546,0,"2020/10/26, 20:21:25",True,"2020/10/26, 20:21:25",156.0,2676108.0,1,I recommend reaching out to support@datadoghq.com to let them know it's a feature you're interested in.,0.0,0.389,0.611,0.7184
Datadog,64542859,64539546,0,"2020/10/26, 20:21:25",True,"2020/10/26, 20:21:25",156.0,2676108.0,1,They might be able to provide some updates.,0.0,0.0,1.0,0.0
Datadog,64907275,64453730,0,"2020/11/19, 09:38:41",False,"2020/11/19, 09:38:41",21.0,7314273.0,0,Not sure if this will work but you can give it a try..:,0.119,0.0,0.881,-0.1232
Datadog,64907275,64453730,0,"2020/11/19, 09:38:41",False,"2020/11/19, 09:38:41",21.0,7314273.0,0,{{^is_exact_match a.value b.value }},0.0,0.0,1.0,0.0
Datadog,64907275,64453730,0,"2020/11/19, 09:38:41",False,"2020/11/19, 09:38:41",21.0,7314273.0,0,"@my@mail.com
Alert 2 hosts has passed the threshold",0.0,0.268,0.732,0.296
Datadog,64907275,64453730,0,"2020/11/19, 09:38:41",False,"2020/11/19, 09:38:41",21.0,7314273.0,0,{{/is_exact_match}},0.0,0.0,1.0,0.0
Datadog,64907275,64453730,0,"2020/11/19, 09:38:41",False,"2020/11/19, 09:38:41",21.0,7314273.0,0,same value - ignore - do nothing,0.316,0.304,0.38,-0.0258
Datadog,64907275,64453730,0,"2020/11/19, 09:38:41",False,"2020/11/19, 09:38:41",21.0,7314273.0,0,The problem is that you probably might get 2 alerts at the same time...,0.184,0.0,0.816,-0.4019
Datadog,64396016,64345103,0,"2020/10/16, 23:28:57",True,"2020/10/16, 23:28:57",318.0,1253272.0,0,It turns out that trace id can be set via HTTP endpoint  https://docs.datadoghq.com/api/v1/tracing/#send-traces .,0.0,0.0,1.0,0.0
Datadog,64396016,64345103,0,"2020/10/16, 23:28:57",True,"2020/10/16, 23:28:57",318.0,1253272.0,0,There doesn't seem to be an option for sending traces to the agent directly.,0.0,0.0,1.0,0.0
Datadog,64396016,64345103,0,"2020/10/16, 23:28:57",True,"2020/10/16, 23:28:57",318.0,1253272.0,0,"This can still be useful if the performance penalty of making HTTP calls is not a concern, i.e., if you are not working on a real-time system.",0.104,0.1,0.796,-0.0258
Datadog,63971375,63909996,0,"2020/09/19, 20:06:40",True,"2020/09/19, 20:06:40",3852.0,1601506.0,1,"Datadog keeps the logs for a period of time according to the billing plan you've selected:  https://www.datadoghq.com/pricing/#section-log  If you choose the 7 day plan, logs will be dropped from Datadog after 7 days.",0.0,0.0,1.0,0.0
Datadog,63971375,63909996,0,"2020/09/19, 20:06:40",True,"2020/09/19, 20:06:40",3852.0,1601506.0,1,"The default plan seems to be 15 days, but there are other options between 3-60 days.",0.0,0.0,1.0,0.0
Datadog,63668170,63603260,0,"2020/08/31, 12:25:38",True,"2020/08/31, 12:25:38",2771.0,842302.0,0,I've solved it now by verifying the status via code and by adding tags to the metrics:,0.0,0.116,0.884,0.2732
Datadog,63668170,63603260,0,"2020/08/31, 12:25:38",True,"2020/08/31, 12:25:38",2771.0,842302.0,0,This way I can filter in my dashboard for  occurrence:first  only.,0.0,0.0,1.0,0.0
Datadog,63682403,63603260,0,"2020/09/01, 09:21:17",False,"2020/09/01, 09:21:17",336.0,7243426.0,0,"To make sure things are clear, you have a metric called  myService.errorType  with a tag  entity .",0.0,0.29,0.71,0.5994
Datadog,63682403,63603260,0,"2020/09/01, 09:21:17",False,"2020/09/01, 09:21:17",336.0,7243426.0,0,This metric is a counter that will increase every time an entity is in error.,0.159,0.135,0.706,-0.1027
Datadog,63682403,63603260,0,"2020/09/01, 09:21:17",False,"2020/09/01, 09:21:17",336.0,7243426.0,0,You will then use this metric query:,0.0,0.0,1.0,0.0
Datadog,63682403,63603260,0,"2020/09/01, 09:21:17",False,"2020/09/01, 09:21:17",336.0,7243426.0,0,"When you speak about UUID, it seems that the cardinality is small (here you show 3).",0.0,0.0,1.0,0.0
Datadog,63682403,63603260,0,"2020/09/01, 09:21:17",False,"2020/09/01, 09:21:17",336.0,7243426.0,0,Which means that every hour you will have small amount of UUID available.,0.0,0.0,1.0,0.0
Datadog,63682403,63603260,0,"2020/09/01, 09:21:17",False,"2020/09/01, 09:21:17",336.0,7243426.0,0,"In that case, adding UUID to the metric tags is not as critical as user ID, timestamp, etc.",0.0,0.103,0.897,0.2411
Datadog,63682403,63603260,0,"2020/09/01, 09:21:17",False,"2020/09/01, 09:21:17",336.0,7243426.0,0,which have a limitless number of options.,0.0,0.206,0.794,0.0772
Datadog,63682403,63603260,0,"2020/09/01, 09:21:17",False,"2020/09/01, 09:21:17",336.0,7243426.0,0,"I would invite you to add this uuid tag, and check the cardinality in the  metric summary page  to ensure it works.",0.0,0.181,0.819,0.4939
Datadog,63682403,63603260,0,"2020/09/01, 09:21:17",False,"2020/09/01, 09:21:17",336.0,7243426.0,0,"Then to get the number of UUID concerned by errors, you can use something like:",0.132,0.209,0.659,0.1027
Datadog,63682403,63603260,0,"2020/09/01, 09:21:17",False,"2020/09/01, 09:21:17",336.0,7243426.0,0,"Finally, as an alternative, if the cardinality of UUID can go through the roof, I would invite you to work with logs or work with Christopher's solution which seems to limit the cardinality increase as well.",0.0,0.211,0.789,0.743
Datadog,64256562,63599025,0,"2020/10/08, 08:55:02",False,"2020/10/08, 08:55:02",537.0,3134333.0,0,You may need to set the environment variable  DD_APM_NON_LOCAL_TRAFFIC=true  in your datadog agent container.,0.0,0.0,1.0,0.0
Datadog,64256562,63599025,0,"2020/10/08, 08:55:02",False,"2020/10/08, 08:55:02",537.0,3134333.0,0,Ref:  https://docs.datadoghq.com/agent/docker/apm/?tab=linux#docker-apm-agent-environment-variables,0.0,0.0,1.0,0.0
Datadog,63606465,63326724,0,"2020/08/27, 01:15:40",True,"2020/08/27, 01:15:40",797.0,2135.0,0,Allowing 'triggered' (but not 'recovery') monitor notifications is not a configurable option for the integration on either Opsgenie or Datadog.,0.0,0.0,1.0,0.0
Datadog,63606465,63326724,0,"2020/08/27, 01:15:40",True,"2020/08/27, 01:15:40",797.0,2135.0,0,"You  can , however, control this within the Datadog Monitor message body where you reference opsgenie",0.0,0.0,1.0,0.0
Datadog,63606465,63326724,0,"2020/08/27, 01:15:40",True,"2020/08/27, 01:15:40",797.0,2135.0,0,Lorem ipsum dolor sit amet @opsgenie-oncall @slack-somechannel,0.0,0.0,1.0,0.0
Datadog,63606465,63326724,0,"2020/08/27, 01:15:40",True,"2020/08/27, 01:15:40",797.0,2135.0,0,"You can wrap the opsgenie reference within the message body with conditional tags (datadog actually calls them variables) documented here:
 https://docs.datadoghq.com/monitors/notifications/?tab=is_alert#conditional-variables 
Then the message body might look something like this:",0.0,0.079,0.921,0.3612
Datadog,63606465,63326724,0,"2020/08/27, 01:15:40",True,"2020/08/27, 01:15:40",797.0,2135.0,0,Lorem ipsum dolor sit amet {{#is_alert}}@opsgenie-oncall{{/is_alert}} @slack-somechannel,0.0,0.0,1.0,0.0
Datadog,63606465,63326724,0,"2020/08/27, 01:15:40",True,"2020/08/27, 01:15:40",797.0,2135.0,0,Now the alert to opsgenie will only occur on the 'trigger' of the monitor but not on 'recovery',0.0,0.086,0.914,0.1531
Datadog,62698566,62667694,0,"2020/07/02, 17:24:10",False,"2020/07/02, 17:24:10",251.0,5477963.0,1,"If you are sending metrics to an actual StatsD server, then tags are not supported by the protocol.",0.103,0.0,0.897,-0.2411
Datadog,62698566,62667694,0,"2020/07/02, 17:24:10",False,"2020/07/02, 17:24:10",251.0,5477963.0,1,You would need to instead send the metrics to the Datadog agent's DogStatsD endpoint which extends StatsD with additional features such as tags.,0.0,0.064,0.936,0.128
Datadog,62698566,62667694,0,"2020/07/02, 17:24:10",False,"2020/07/02, 17:24:10",251.0,5477963.0,1,You can find more information about DogStatsD  here .,0.0,0.0,1.0,0.0
Datadog,62698566,62667694,0,"2020/07/02, 17:24:10",False,"2020/07/02, 17:24:10",251.0,5477963.0,1,"If you are already using the DogStatsD endpoint, then I would suspect an incompatibility with the  node-statsd  library.",0.121,0.0,0.879,-0.296
Datadog,62698566,62667694,0,"2020/07/02, 17:24:10",False,"2020/07/02, 17:24:10",251.0,5477963.0,1,"The library has not been updated for 6 years and it's possible that something changed since then, causing it to no longer work.",0.095,0.0,0.905,-0.296
Datadog,62698566,62667694,0,"2020/07/02, 17:24:10",False,"2020/07/02, 17:24:10",251.0,5477963.0,1,In that case I would recommend switching to a more recent DogStatsD client that is still maintained such as  hot-shots .,0.0,0.128,0.872,0.3612
Datadog,62698566,62667694,0,"2020/07/02, 17:24:10",False,"2020/07/02, 17:24:10",251.0,5477963.0,1,Hope this helps!,0.0,0.853,0.147,0.6996
Datadog,62476112,62474317,0,"2020/06/19, 21:03:23",False,"2020/06/19, 21:03:23",1371.0,5540166.0,0,There's a 2-click path from Slack that should already do this for you out-of-the-box.,0.0,0.0,1.0,0.0
Datadog,62476112,62474317,0,"2020/06/19, 21:03:23",False,"2020/06/19, 21:03:23",1371.0,5540166.0,0,"The slack notification gives you a link to the alert event in your Datadog account (click-1), and from the alert event, towards the bottom you'll find a series of links to other relevant places, one of those is ""Related Logs"" (click-2).",0.0,0.106,0.894,0.5267
Datadog,62476112,62474317,0,"2020/06/19, 21:03:23",False,"2020/06/19, 21:03:23",1371.0,5540166.0,0,"That brings you to the Log Explorer scoped to the relevant time period of the alert, and scoped to the tags of whatever it was that was alerted on (so presumably the logs you're looking for).",0.0,0.059,0.941,0.296
Datadog,62476112,62474317,0,"2020/06/19, 21:03:23",False,"2020/06/19, 21:03:23",1371.0,5540166.0,0,"If you want to add a link of this sort as something you can configure in the alert message, that sounds like something you should reach out to support@datadoghq.com for to ask Datadog to implement it.",0.0,0.186,0.814,0.6249
Datadog,62672749,62474317,0,"2020/07/01, 11:29:39",False,"2020/07/01, 11:29:39",1.0,3327131.0,0,In the end we solved the problem dynamically building the url for the logs:,0.148,0.251,0.601,0.2263
Datadog,62301304,62300720,2,"2020/06/10, 13:27:51",False,"2020/06/10, 13:27:51",12179.0,1032890.0,0,There's a Prometheus endpoint for Tibco EMS:,0.0,0.0,1.0,0.0
Datadog,62301304,62300720,2,"2020/06/10, 13:27:51",False,"2020/06/10, 13:27:51",12179.0,1032890.0,0,https://community.tibco.com/wiki/statistics-logger-tibco-enterprise-message-service#toc-15,0.0,0.0,1.0,0.0
Datadog,62301304,62300720,2,"2020/06/10, 13:27:51",False,"2020/06/10, 13:27:51",12179.0,1032890.0,0,"I think you can then add the prometheus integration to your datadog agent to send the data do datadog, and build your own dashboard for that:",0.0,0.0,1.0,0.0
Datadog,62301304,62300720,2,"2020/06/10, 13:27:51",False,"2020/06/10, 13:27:51",12179.0,1032890.0,0,https://docs.datadoghq.com/integrations/prometheus/#data-collected,0.0,0.0,1.0,0.0
Datadog,62228982,62221212,3,"2020/06/06, 11:04:38",False,"2020/06/06, 11:04:38",21.0,7314273.0,1,"I had something similar issue and chose the first option, but i won't say it is from terraform perspective (since i also had lack in experience in terraform).",0.109,0.0,0.891,-0.4497
Datadog,62228982,62221212,3,"2020/06/06, 11:04:38",False,"2020/06/06, 11:04:38",21.0,7314273.0,1,"The first hierarchy was more reasonable in segregation aspects, plus would be easier to add/remove/update organizations by demand.",0.074,0.138,0.788,0.3182
Datadog,62016448,61888648,0,"2020/05/26, 10:07:33",False,"2020/05/26, 10:07:33",744.0,10300113.0,0,Answer from Datadog Support to this:,0.0,0.351,0.649,0.4019
Datadog,62016448,61888648,0,"2020/05/26, 10:07:33",False,"2020/05/26, 10:07:33",744.0,10300113.0,0,Thanks again for reaching out to Datadog!,0.0,0.5,0.5,0.6114
Datadog,62016448,61888648,0,"2020/05/26, 10:07:33",False,"2020/05/26, 10:07:33",744.0,10300113.0,0,"From looking further into this, there does not seem to be a way we can package the JDBC  driver with the Datadog Agent.",0.0,0.0,1.0,0.0
Datadog,62016448,61888648,0,"2020/05/26, 10:07:33",False,"2020/05/26, 10:07:33",744.0,10300113.0,0,I understand that this is not desirable as you would prefer to use a standard image but I believe the best way to have these bundled together would be to have a custom image for your deployment.,0.039,0.152,0.81,0.7445
Datadog,62016448,61888648,0,"2020/05/26, 10:07:33",False,"2020/05/26, 10:07:33",744.0,10300113.0,0,Apologies for any inconveniences that this may cause.,0.0,0.0,1.0,0.0
Datadog,61015284,61010490,2,"2020/04/03, 18:30:55",False,"2020/04/03, 18:30:55",1371.0,5540166.0,3,"Yes, you can configure widgets to exclude results by tags.",0.151,0.214,0.635,0.2023
Datadog,61015284,61010490,2,"2020/04/03, 18:30:55",False,"2020/04/03, 18:30:55",1371.0,5540166.0,3,You can do this by applying a tag prepended with a  !,0.0,0.0,1.0,0.0
Datadog,61015284,61010490,2,"2020/04/03, 18:30:55",False,"2020/04/03, 18:30:55",1371.0,5540166.0,3,"to signify ""not"".",0.0,0.0,1.0,0.0
Datadog,61015284,61010490,2,"2020/04/03, 18:30:55",False,"2020/04/03, 18:30:55",1371.0,5540166.0,3,"So in your case, you can set up your widget scoped over  importance:ignore  and then hit the little  &lt;/&gt;  button on the right to expose the underlying query, and sneak a  !",0.061,0.0,0.939,-0.2244
Datadog,61015284,61010490,2,"2020/04/03, 18:30:55",False,"2020/04/03, 18:30:55",1371.0,5540166.0,3,in front to make it  !importance:ignore .,0.0,0.0,1.0,0.0
Datadog,61015284,61010490,2,"2020/04/03, 18:30:55",False,"2020/04/03, 18:30:55",1371.0,5540166.0,3,"This doc has a nice example  (although it's for notebooks, it works the same in dashboards as well).",0.0,0.149,0.851,0.4215
Datadog,61019569,60940393,0,"2020/04/03, 23:00:56",False,"2020/04/03, 23:00:56",1447.0,554481.0,0,"After talking with Datadog support, it seems like this is a known issue.",0.0,0.342,0.658,0.6369
Datadog,61019569,60940393,0,"2020/04/03, 23:00:56",False,"2020/04/03, 23:00:56",1447.0,554481.0,0,Thanks for your patience while we looked into this issue.,0.0,0.244,0.756,0.4404
Datadog,61019569,60940393,0,"2020/04/03, 23:00:56",False,"2020/04/03, 23:00:56",1447.0,554481.0,0,We we're currently investigating this along with PoolExecutors and will reach out with updates.,0.0,0.078,0.922,0.0258
Datadog,61019569,60940393,0,"2020/04/03, 23:00:56",False,"2020/04/03, 23:00:56",1447.0,554481.0,0,"Right now it looks like those child spans within the async call lose context, so they appear disconnected.",0.127,0.118,0.755,-0.0516
Datadog,61019569,60940393,0,"2020/04/03, 23:00:56",False,"2020/04/03, 23:00:56",1447.0,554481.0,0,The workaround for now is to pass in the parent's context.,0.0,0.0,1.0,0.0
Datadog,61019569,60940393,0,"2020/04/03, 23:00:56",False,"2020/04/03, 23:00:56",1447.0,554481.0,0,Add this line just before calling the thread pool executor.,0.0,0.0,1.0,0.0
Datadog,61019569,60940393,0,"2020/04/03, 23:00:56",False,"2020/04/03, 23:00:56",1447.0,554481.0,0,Then pass that context to the function that gets run in the threadpool:,0.0,0.0,1.0,0.0
Datadog,61019569,60940393,0,"2020/04/03, 23:00:56",False,"2020/04/03, 23:00:56",1447.0,554481.0,0,And use it to create a span inside the function like this:,0.0,0.338,0.662,0.5574
Datadog,61019569,60940393,0,"2020/04/03, 23:00:56",False,"2020/04/03, 23:00:56",1447.0,554481.0,0,The complete example looks like this:,0.0,0.333,0.667,0.3612
Datadog,61019569,60940393,0,"2020/04/03, 23:00:56",False,"2020/04/03, 23:00:56",1447.0,554481.0,0,This will produce a result that looks like this:,0.0,0.263,0.737,0.3612
Datadog,60381277,60369412,3,"2020/02/24, 19:41:28",False,"2020/02/24, 19:41:28",1371.0,5540166.0,0,First  you'll want to make sure your logs are well structured (which you can control in  Datadog's processing pipelines ).,0.0,0.251,0.749,0.5719
Datadog,60381277,60369412,3,"2020/02/24, 19:41:28",False,"2020/02/24, 19:41:28",1371.0,5540166.0,0,"Effectively you'll want to parse out the ""code"" values into some ""error code"" attribute.",0.138,0.352,0.51,0.4939
Datadog,60381277,60369412,3,"2020/02/24, 19:41:28",False,"2020/02/24, 19:41:28",1371.0,5540166.0,0,If your log events are in this format...,0.0,0.0,1.0,0.0
Datadog,60381277,60369412,3,"2020/02/24, 19:41:28",False,"2020/02/24, 19:41:28",1371.0,5540166.0,0,"...Then all you need is a fairly simple grok parser rule, thanks to the ""json"" filter function.",0.0,0.162,0.838,0.4404
Datadog,60381277,60369412,3,"2020/02/24, 19:41:28",False,"2020/02/24, 19:41:28",1371.0,5540166.0,0,"Something like this would get you where you want (note the  %{data::json}  part, that's what parses the in-log JSON).",0.0,0.183,0.817,0.4215
Datadog,60381277,60369412,3,"2020/02/24, 19:41:28",False,"2020/02/24, 19:41:28",1371.0,5540166.0,0,"Once you've configured this, your logs will also have an attribute called ""error.code"" with a value of  2001  or  1001  or whatever.",0.0,0.107,0.893,0.34
Datadog,60381277,60369412,3,"2020/02/24, 19:41:28",False,"2020/02/24, 19:41:28",1371.0,5540166.0,0,Second  you'll want to  create a facet  for that new  error.code  attribute so that you can  make toplist / timeseries / etc.,0.0,0.167,0.833,0.34
Datadog,60381277,60369412,3,"2020/02/24, 19:41:28",False,"2020/02/24, 19:41:28",1371.0,5540166.0,0,"graphs grouped out by your ""error code"" facet .",0.278,0.0,0.722,-0.4019
Datadog,60307539,60216953,0,"2020/02/19, 21:07:06",False,"2020/02/19, 21:07:06",571.0,2019978.0,0,"No, you cannot install the Datadog agent on a Snowflake host.",0.196,0.0,0.804,-0.296
Datadog,60307539,60216953,0,"2020/02/19, 21:07:06",False,"2020/02/19, 21:07:06",571.0,2019978.0,0,We use our separate job scheduling system to monitor Snowflake by running queries (e.g.,0.0,0.0,1.0,0.0
Datadog,60307539,60216953,0,"2020/02/19, 21:07:06",False,"2020/02/19, 21:07:06",571.0,2019978.0,0,"checks on SYSTEM$CLUSTERING_DEPTH, aggregate queries against the QUERY_HISTORY for timing, etc) via the JDBC connector then relaying the results to our monitoring stack (similar to how Datadog agent would work.)",0.0,0.0,1.0,0.0
Datadog,59096255,59092413,0,"2019/11/28, 23:19:23",True,"2019/11/28, 23:19:23",91617.0,354577.0,1,Point both of those at a service like  httpbin  to see how they differ.,0.0,0.172,0.828,0.3612
Datadog,59096255,59092413,0,"2019/11/28, 23:19:23",True,"2019/11/28, 23:19:23",91617.0,354577.0,1,"Requests'  data  option for POST requests  generates form-encoded data  by default, while  curl  passes the JSON string through directly.",0.0,0.0,1.0,0.0
Datadog,59096255,59092413,0,"2019/11/28, 23:19:23",True,"2019/11/28, 23:19:23",91617.0,354577.0,1,You can manually encode your payload as a JSON string:,0.0,0.0,1.0,0.0
Datadog,59096255,59092413,0,"2019/11/28, 23:19:23",True,"2019/11/28, 23:19:23",91617.0,354577.0,1,or if you have Requests version 2.4.2 or later you can use the  json  parameter to have your  dict  converted to JSON automatically:,0.0,0.0,1.0,0.0
Datadog,58877543,58873873,2,"2019/11/15, 14:56:44",False,"2019/11/19, 14:49:29",336.0,7243426.0,0,One solution would be to setup in  logs &gt; configuration &gt; pipelines  a  category processor  to add a new attribute that could be made searchable.,0.0,0.095,0.905,0.3182
Datadog,58877543,58873873,2,"2019/11/15, 14:56:44",False,"2019/11/19, 14:49:29",336.0,7243426.0,0,edit: 19th Nov 2019,0.0,0.0,1.0,0.0
Datadog,58877543,58873873,2,"2019/11/15, 14:56:44",False,"2019/11/19, 14:49:29",336.0,7243426.0,0,Step 1:,0.0,0.0,1.0,0.0
Datadog,58877543,58873873,2,"2019/11/15, 14:56:44",False,"2019/11/19, 14:49:29",336.0,7243426.0,0,Add grok parser to extract sign:,0.0,0.0,1.0,0.0
Datadog,58877543,58873873,2,"2019/11/15, 14:56:44",False,"2019/11/19, 14:49:29",336.0,7243426.0,0,"rule:  detect_dollar .*%{regex(""[$]+""):dollarSign}.",0.0,0.0,1.0,0.0
Datadog,58877543,58873873,2,"2019/11/15, 14:56:44",False,"2019/11/19, 14:49:29",336.0,7243426.0,0,*,0.0,0.0,0.0,0.0
Datadog,58877543,58873873,2,"2019/11/15, 14:56:44",False,"2019/11/19, 14:49:29",336.0,7243426.0,0,Step 2:,0.0,0.0,1.0,0.0
Datadog,58877543,58873873,2,"2019/11/15, 14:56:44",False,"2019/11/19, 14:49:29",336.0,7243426.0,0,Then you can setup a category processor as indicated above.,0.0,0.0,1.0,0.0
Datadog,58877543,58873873,2,"2019/11/15, 14:56:44",False,"2019/11/19, 14:49:29",336.0,7243426.0,0,This could look for the attribute  @dollarSign:$  and set the attribute  hasDollarSign  to True and set it to false otherwise.,0.0,0.128,0.872,0.4215
Datadog,58877543,58873873,2,"2019/11/15, 14:56:44",False,"2019/11/19, 14:49:29",336.0,7243426.0,0,Step 3:,0.0,0.0,1.0,0.0
Datadog,58877543,58873873,2,"2019/11/15, 14:56:44",False,"2019/11/19, 14:49:29",336.0,7243426.0,0,Create a facet on  dollarSign  attribute.,0.0,0.344,0.656,0.2732
Datadog,58877543,58873873,2,"2019/11/15, 14:56:44",False,"2019/11/19, 14:49:29",336.0,7243426.0,0,Following logs can then be searched for.,0.0,0.0,1.0,0.0
Datadog,58877543,58873873,2,"2019/11/15, 14:56:44",False,"2019/11/19, 14:49:29",336.0,7243426.0,0,For logs with no  $,0.423,0.0,0.577,-0.296
Datadog,58877543,58873873,2,"2019/11/15, 14:56:44",False,"2019/11/19, 14:49:29",336.0,7243426.0,0,For logs with  $,0.0,0.0,1.0,0.0
Datadog,58877543,58873873,2,"2019/11/15, 14:56:44",False,"2019/11/19, 14:49:29",336.0,7243426.0,0,You can do the same with the  hasDollarSign  attribute and set it as a facet.,0.0,0.0,1.0,0.0
Datadog,58771357,58765810,1,"2019/11/08, 19:36:03",True,"2019/11/08, 19:36:03",336.0,7243426.0,1,I've tried with this query which is similar to yours:,0.0,0.0,1.0,0.0
Datadog,58771357,58765810,1,"2019/11/08, 19:36:03",True,"2019/11/08, 19:36:03",336.0,7243426.0,1,And this seems to give me a count by  dbinstanceidentifier  results.,0.0,0.0,1.0,0.0
Datadog,58771357,58765810,1,"2019/11/08, 19:36:03",True,"2019/11/08, 19:36:03",336.0,7243426.0,1,Do you have more information to provide?,0.0,0.0,1.0,0.0
Datadog,58771357,58765810,1,"2019/11/08, 19:36:03",True,"2019/11/08, 19:36:03",336.0,7243426.0,1,Maybe an event list and a monitor result screenshot?,0.0,0.0,1.0,0.0
Datadog,58689757,58630932,1,"2019/11/04, 10:41:42",True,"2019/11/04, 11:05:42",336.0,7243426.0,0,Side note : I also use this for Kafka (just as a reference) but it should not be required in your case:,0.0,0.0,1.0,0.0
Datadog,58268784,58267953,0,"2019/10/07, 14:34:45",False,"2019/10/07, 14:34:45",251.0,5477963.0,2,The main issue is that you would have to mount the volume in both the app container and the agent container in order to make it available.,0.0,0.0,1.0,0.0
Datadog,58268784,58267953,0,"2019/10/07, 14:34:45",False,"2019/10/07, 14:34:45",251.0,5477963.0,2,It also means you have to find a place to store the log file before it gets picked up by the agent.,0.0,0.0,1.0,0.0
Datadog,58268784,58267953,0,"2019/10/07, 14:34:45",False,"2019/10/07, 14:34:45",251.0,5477963.0,2,Doing this for every container could become difficult to maintain and time consuming.,0.172,0.0,0.828,-0.3612
Datadog,58268784,58267953,0,"2019/10/07, 14:34:45",False,"2019/10/07, 14:34:45",251.0,5477963.0,2,An alternative approach would be to instead send the logs to  stdout  and let the agent collect them with the Docker integration.,0.0,0.0,1.0,0.0
Datadog,58268784,58267953,0,"2019/10/07, 14:34:45",False,"2019/10/07, 14:34:45",251.0,5477963.0,2,"Since you configured  logsConfigContainerCollectAll  to  true , the agent is already configured to collect the logs from every container output, so configuring Winston to output to  stdout  should just work.",0.0,0.091,0.909,0.4215
Datadog,58268784,58267953,0,"2019/10/07, 14:34:45",False,"2019/10/07, 14:34:45",251.0,5477963.0,2,See:  https://docs.datadoghq.com/agent/docker/log/,0.0,0.0,1.0,0.0
Datadog,58270801,58267953,1,"2019/10/07, 16:41:01",False,"2019/10/07, 16:41:01",336.0,7243426.0,2,"To support rochdev comment, here are a few code snippets to help out (if you do not opt in for the STDOUT method which should be simpler).",0.0,0.184,0.816,0.6597
Datadog,58270801,58267953,1,"2019/10/07, 16:41:01",False,"2019/10/07, 16:41:01",336.0,7243426.0,2,This is only to mount the right volume inside the container agent.,0.0,0.0,1.0,0.0
Datadog,58270801,58267953,1,"2019/10/07, 16:41:01",False,"2019/10/07, 16:41:01",336.0,7243426.0,2,"On your app deployment, add:",0.0,0.0,1.0,0.0
Datadog,58270801,58267953,1,"2019/10/07, 16:41:01",False,"2019/10/07, 16:41:01",336.0,7243426.0,2,And on your agent daemonset:,0.0,0.0,1.0,0.0
Datadog,58270568,58266541,0,"2019/10/07, 16:28:46",False,"2019/10/07, 16:28:46",336.0,7243426.0,0,"At first glance, this seems to be a use case more suitable for the APM part of Datadog which would measure the execution time and could be  instrumented  to measure the execution time of the smaller functions (if it does not picked up this data automatically).",0.0,0.0,1.0,0.0
Datadog,58270568,58266541,0,"2019/10/07, 16:28:46",False,"2019/10/07, 16:28:46",336.0,7243426.0,0,You can then create some nice charts with  Trace Search and Analytics .,0.0,0.329,0.671,0.5994
Datadog,58270568,58266541,0,"2019/10/07, 16:28:46",False,"2019/10/07, 16:28:46",336.0,7243426.0,0,You could also use a custom metric with tags such as  opsize:large  and  opsize:small  which would represent the execution time (a gauge).,0.0,0.0,1.0,0.0
Datadog,58270568,58266541,0,"2019/10/07, 16:28:46",False,"2019/10/07, 16:28:46",336.0,7243426.0,0,You can find more details  here .,0.0,0.0,1.0,0.0
Datadog,58270568,58266541,0,"2019/10/07, 16:28:46",False,"2019/10/07, 16:28:46",336.0,7243426.0,0,"At the moment, the log module of Datadog does not seem to support the calculation you expect to see.",0.111,0.0,0.889,-0.3089
Datadog,58270568,58266541,0,"2019/10/07, 16:28:46",False,"2019/10/07, 16:28:46",336.0,7243426.0,0,"However, the two solutions above and the related logs can be made visible in a dashboard side by side.",0.0,0.091,0.909,0.1779
Datadog,55956522,55953321,1,"2019/05/02, 19:18:34",False,"2019/05/02, 19:18:34",2574.0,4162641.0,0,Have you tried using the  start  and  end  tags with a 24 hour window?,0.0,0.0,1.0,0.0
Datadog,54938650,54920625,0,"2019/03/01, 07:49:07",True,"2019/03/01, 07:49:07",9.0,11043786.0,0,"As I posted the issue on GitHub, they give an answer the issue was in source code for dd-trace-php and they will fix and new release.",0.0,0.0,1.0,0.0
Datadog,54938650,54920625,0,"2019/03/01, 07:49:07",True,"2019/03/01, 07:49:07",9.0,11043786.0,0,https://github.com/DataDog/dd-trace-php/issues/334,0.0,0.0,1.0,0.0
Datadog,54938650,54920625,0,"2019/03/01, 07:49:07",True,"2019/03/01, 07:49:07",9.0,11043786.0,0,Below response of DatDog in github:,0.0,0.0,1.0,0.0
Datadog,54938650,54920625,0,"2019/03/01, 07:49:07",True,"2019/03/01, 07:49:07",9.0,11043786.0,0,"Ah now this is much more clear, thanks for sharing.",0.0,0.559,0.441,0.8347
Datadog,54938650,54920625,0,"2019/03/01, 07:49:07",True,"2019/03/01, 07:49:07",9.0,11043786.0,0,This is a known problem that we are currently and actively working on.,0.18,0.153,0.667,-0.1027
Datadog,54938650,54920625,0,"2019/03/01, 07:49:07",True,"2019/03/01, 07:49:07",9.0,11043786.0,0,"As I cannot commit to that, the fix will be probably come out with the next release.",0.112,0.0,0.888,-0.2235
Datadog,54938650,54920625,0,"2019/03/01, 07:49:07",True,"2019/03/01, 07:49:07",9.0,11043786.0,0,"At an higher level, the cause is an issue we have in some specific cases while invoking private/protected methods and parent::* invocations.",0.0,0.0,1.0,0.0
Datadog,54938650,54920625,0,"2019/03/01, 07:49:07",True,"2019/03/01, 07:49:07",9.0,11043786.0,0,"In the meantime, if you are still interested in testing/using the other integrations, the only thing I can recommend is to disable the pdo integration:  fastcgi_param DD_INTEGRATIONS_DISABLED pdo .",0.0,0.172,0.828,0.6369
Datadog,54938650,54920625,0,"2019/03/01, 07:49:07",True,"2019/03/01, 07:49:07",9.0,11043786.0,0,"Again, the fix to this is currently in development and will be released very soon.",0.0,0.0,1.0,0.0
Datadog,53624609,53459133,0,"2018/12/05, 05:10:35",True,"2018/12/05, 05:10:35",3223.0,43973.0,1,Use the Query Value widget.,0.0,0.375,0.625,0.34
Datadog,53624609,53459133,0,"2018/12/05, 05:10:35",True,"2018/12/05, 05:10:35",3223.0,43973.0,1,"It can only show a single value, which is the average for the current time window that has been chosen.",0.0,0.118,0.882,0.34
Datadog,53383195,53382882,0,"2018/11/19, 23:55:24",False,"2018/11/19, 23:55:24",1371.0,5540166.0,1,Maybe you could mod the process check to also tag the process number metric by PID ( this is probly where you'd change that ).,0.0,0.053,0.947,0.0772
Datadog,53383195,53382882,0,"2018/11/19, 23:55:24",False,"2018/11/19, 23:55:24",1371.0,5540166.0,1,That way you could group your monitor by your pid tag and the no-data alerts would tell you when the pid switched.,0.0,0.0,1.0,0.0
Datadog,53383195,53382882,0,"2018/11/19, 23:55:24",False,"2018/11/19, 23:55:24",1371.0,5540166.0,1,"But this would also alert on expected pid changes, so maybe you'd have to schedule downtimes too aggressively for this to be a good idea?",0.096,0.217,0.686,0.5719
Datadog,53383195,53382882,0,"2018/11/19, 23:55:24",False,"2018/11/19, 23:55:24",1371.0,5540166.0,1,Maybe monitoring some crash logs with  their Log Management tool  would be a better approach?,0.153,0.165,0.682,0.0516
Datadog,53232465,53231545,0,"2018/11/09, 21:54:22",False,"2018/11/09, 21:54:22",1710.0,2779323.0,2,Lambda is serverless.,0.0,0.0,1.0,0.0
Datadog,53232465,53231545,0,"2018/11/09, 21:54:22",False,"2018/11/09, 21:54:22",1710.0,2779323.0,2,Datadog agent is for the host.,0.0,0.0,1.0,0.0
Datadog,53232465,53231545,0,"2018/11/09, 21:54:22",False,"2018/11/09, 21:54:22",1710.0,2779323.0,2,While running lambda you have absolutely no control over the host as you are not managing it.,0.135,0.0,0.865,-0.3597
Datadog,53232465,53231545,0,"2018/11/09, 21:54:22",False,"2018/11/09, 21:54:22",1710.0,2779323.0,2,"Hence, You can monitor application running on lambda using datadog integration of lambda for the different application.",0.0,0.0,1.0,0.0
Datadog,53232465,53231545,0,"2018/11/09, 21:54:22",False,"2018/11/09, 21:54:22",1710.0,2779323.0,2,You may follow below link for AWS Integration of datadog.,0.0,0.0,1.0,0.0
Datadog,53232465,53231545,0,"2018/11/09, 21:54:22",False,"2018/11/09, 21:54:22",1710.0,2779323.0,2,Ref:  https://docs.datadoghq.com/integrations/amazon_lambda/,0.0,0.0,1.0,0.0
Datadog,53400457,53226600,0,"2018/11/20, 21:44:45",False,"2018/11/20, 21:44:45",156.0,2676108.0,0,You can monitor a database from a different host as long as the host the agent is running on has access.,0.0,0.0,1.0,0.0
Datadog,53400457,53226600,0,"2018/11/20, 21:44:45",False,"2018/11/20, 21:44:45",156.0,2676108.0,0,So for this section in the config file:,0.0,0.0,1.0,0.0
Datadog,53400457,53226600,0,"2018/11/20, 21:44:45",False,"2018/11/20, 21:44:45",156.0,2676108.0,0,https://github.com/DataDog/integrations-core/blob/master/postgres/datadog_checks/postgres/data/conf.yaml.example#L4,0.0,0.0,1.0,0.0
Datadog,53400457,53226600,0,"2018/11/20, 21:44:45",False,"2018/11/20, 21:44:45",156.0,2676108.0,0,instead of using  localhost  you can set the IP.,0.0,0.0,1.0,0.0
Datadog,53400457,53226600,0,"2018/11/20, 21:44:45",False,"2018/11/20, 21:44:45",156.0,2676108.0,0,This will allow you to monitor your database without adding a new agent.,0.0,0.147,0.853,0.2263
Datadog,53400457,53226600,0,"2018/11/20, 21:44:45",False,"2018/11/20, 21:44:45",156.0,2676108.0,0,Then you would just follow the normal  postrgres setup  and you will have access to all the metrics and  service checks  including  postgres.can_connect  which is probably what you care about.,0.0,0.099,0.901,0.4939
Datadog,53114821,52311463,0,"2018/11/02, 10:10:01",False,"2018/11/02, 10:10:01",1976.0,43842.0,1,"It seems to be an intentional ""feature""  https://github.com/kamon-io/kamon-datadog/issues/19  introduced in 1.x.",0.0,0.0,1.0,0.0
Datadog,53114821,52311463,0,"2018/11/02, 10:10:01",False,"2018/11/02, 10:10:01",1976.0,43842.0,1,They have chosen approach to put service name in tag instead.,0.0,0.0,1.0,0.0
Datadog,51841340,51840681,1,"2018/08/14, 15:22:29",False,"2018/08/14, 15:22:29",1978.0,7217896.0,0,"Even though datadog is being run from the same machine, it is setting up a separate server on your machine.",0.0,0.0,1.0,0.0
Datadog,51841340,51840681,1,"2018/08/14, 15:22:29",False,"2018/08/14, 15:22:29",1978.0,7217896.0,0,"Because of that, it sounds like the datadog agent doesn't have access to your z:/ driver.",0.0,0.143,0.857,0.3612
Datadog,51841340,51840681,1,"2018/08/14, 15:22:29",False,"2018/08/14, 15:22:29",1978.0,7217896.0,0,"Try to put the ""TaskResults"" folder in your root directory (when running from datadog - where the mycheck.yaml file is) and change the path accordingly.",0.0,0.0,1.0,0.0
Datadog,51841340,51840681,1,"2018/08/14, 15:22:29",False,"2018/08/14, 15:22:29",1978.0,7217896.0,0,"If this works and you still want to have a common drive to be able to share files from your computer to datadog's agent, you have to find a way to mount a drive\folder to the agent.",0.0,0.099,0.901,0.3612
Datadog,51841340,51840681,1,"2018/08/14, 15:22:29",False,"2018/08/14, 15:22:29",1978.0,7217896.0,0,They probably have a way to do that in the  documentation,0.0,0.0,1.0,0.0
Datadog,51894917,51840681,0,"2018/08/17, 14:54:34",True,"2018/08/17, 14:54:34",410.0,831608.0,0,The solution to this is to create a file share on the network drive and use that path instead of the full network drive path.,0.0,0.239,0.761,0.6808
Datadog,51894917,51840681,0,"2018/08/17, 14:54:34",True,"2018/08/17, 14:54:34",410.0,831608.0,0,May be obvious to some but it didn't occur to me right away since the normal Python code worked without any issue outside of Datadog.,0.0,0.0,1.0,0.0
Datadog,51894917,51840681,0,"2018/08/17, 14:54:34",True,"2018/08/17, 14:54:34",410.0,831608.0,0,So instead of:,0.0,0.0,1.0,0.0
Datadog,51894917,51840681,0,"2018/08/17, 14:54:34",True,"2018/08/17, 14:54:34",410.0,831608.0,0,use,0.0,0.0,1.0,0.0
Datadog,51603701,51377442,0,"2018/07/31, 02:52:59",False,"2018/07/31, 02:52:59",1474.0,10109833.0,0,Have a look at the documentation for  Dogstream .,0.0,0.0,1.0,0.0
Datadog,51603701,51377442,0,"2018/07/31, 02:52:59",False,"2018/07/31, 02:52:59",1474.0,10109833.0,0,It allows you to send metrics to datadog from log files (including summarised metrics).,0.0,0.0,1.0,0.0
Datadog,51603701,51377442,0,"2018/07/31, 02:52:59",False,"2018/07/31, 02:52:59",1474.0,10109833.0,0,You may need to write a custom parser for any data that is not in the datadog canonical format in order for datadog to recognize the data.,0.0,0.0,1.0,0.0
Datadog,51603701,51377442,0,"2018/07/31, 02:52:59",False,"2018/07/31, 02:52:59",1474.0,10109833.0,0,Checkout the example  here .,0.0,0.0,1.0,0.0
Datadog,50155727,50148430,3,"2018/05/03, 15:59:44",False,"2018/05/04, 19:46:07",11561.0,970308.0,0,This sounds like a bug.,0.0,0.455,0.545,0.3612
Datadog,50155727,50148430,3,"2018/05/03, 15:59:44",False,"2018/05/04, 19:46:07",11561.0,970308.0,0,It is possible the Datadog exporter is running in a non-daemon thread.,0.0,0.0,1.0,0.0
Datadog,50155727,50148430,3,"2018/05/03, 15:59:44",False,"2018/05/04, 19:46:07",11561.0,970308.0,0,The JVM views non-daemon threads as application critical work.,0.223,0.0,0.777,-0.3182
Datadog,50155727,50148430,3,"2018/05/03, 15:59:44",False,"2018/05/04, 19:46:07",11561.0,970308.0,0,So essentially the JVM thinks it shouldn't shutdown until the non-daemon thread finishes.,0.0,0.0,1.0,0.0
Datadog,50155727,50148430,3,"2018/05/03, 15:59:44",False,"2018/05/04, 19:46:07",11561.0,970308.0,0,"In the case of the Datadog exporter thread, that probably won't happen.",0.0,0.0,1.0,0.0
Datadog,50155727,50148430,3,"2018/05/03, 15:59:44",False,"2018/05/04, 19:46:07",11561.0,970308.0,0,"To verify there are non-daemon threads, use  jstack  to generate a thread dump.",0.191,0.0,0.809,-0.3818
Datadog,50155727,50148430,3,"2018/05/03, 15:59:44",False,"2018/05/04, 19:46:07",11561.0,970308.0,0,(command:  jstack &lt;pid&gt; ) or dump all threads in your  close  method:,0.206,0.0,0.794,-0.3818
Datadog,50155727,50148430,3,"2018/05/03, 15:59:44",False,"2018/05/04, 19:46:07",11561.0,970308.0,0,An example thread dump output is below.,0.302,0.0,0.698,-0.3818
Datadog,50155727,50148430,3,"2018/05/03, 15:59:44",False,"2018/05/04, 19:46:07",11561.0,970308.0,0,Notice the word 'daemon' on the first line:,0.0,0.0,1.0,0.0
Datadog,49818706,49381672,0,"2018/04/13, 16:56:49",False,"2018/04/13, 16:56:49",939.0,2506172.0,3,Gauge metric  types will do the job here given that your query does not run more than once within 10 seconds.,0.0,0.0,1.0,0.0
Datadog,49818706,49381672,0,"2018/04/13, 16:56:49",False,"2018/04/13, 16:56:49",939.0,2506172.0,3,"If that is not the case, go for  count metric",0.0,0.0,1.0,0.0
Datadog,49818706,49381672,0,"2018/04/13, 16:56:49",False,"2018/04/13, 16:56:49",939.0,2506172.0,3,"The flush interval in datadog by default is 10 seconds, if you use a  gauge metric  and the metric is reported more than once in a flush interval, datadog agent only sends the last value ignoring the previous ones.",0.067,0.06,0.873,-0.0772
Datadog,49818706,49381672,0,"2018/04/13, 16:56:49",False,"2018/04/13, 16:56:49",939.0,2506172.0,3,"For  count metric  in contrast, the agent sums up all the values reported in the flush interval.",0.0,0.144,0.856,0.4019
Datadog,49818706,49381672,0,"2018/04/13, 16:56:49",False,"2018/04/13, 16:56:49",939.0,2506172.0,3,More details about flush interval  here .,0.0,0.0,1.0,0.0
Datadog,65533013,49381672,0,"2021/01/01, 21:35:13",False,"2021/01/01, 21:35:13",1718.0,555329.0,0,The best metric type would be a  histogram  metric.,0.0,0.375,0.625,0.6369
Datadog,65533013,49381672,0,"2021/01/01, 21:35:13",False,"2021/01/01, 21:35:13",1718.0,555329.0,0,"This will take multiple values, and pre-aggregate them within a flush window, so you will be able to get things like min/max/sum/avg and various percentiles.",0.0,0.191,0.809,0.6369
Datadog,65533013,49381672,0,"2021/01/01, 21:35:13",False,"2021/01/01, 21:35:13",1718.0,555329.0,0,If you run multiple times within a flush window:,0.0,0.0,1.0,0.0
Datadog,48946203,48922641,0,"2018/02/23, 12:55:16",True,"2018/02/23, 12:55:16",1365.0,4894399.0,0,"As I have mentioned in the comment, you are affected by  two pass model .",0.118,0.0,0.882,-0.1531
Datadog,48946203,48922641,0,"2018/02/23, 12:55:16",True,"2018/02/23, 12:55:16",1365.0,4894399.0,0,You should remove the keys in the resource added to the end of the chef run or triggered by the DD cookbook resources invoked as the last one in the run.,0.0,0.0,1.0,0.0
Datadog,48946203,48922641,0,"2018/02/23, 12:55:16",True,"2018/02/23, 12:55:16",1365.0,4894399.0,0,"However, it may not work with all versions of DD cookbook.",0.0,0.0,1.0,0.0
Datadog,48946203,48922641,0,"2018/02/23, 12:55:16",True,"2018/02/23, 12:55:16",1365.0,4894399.0,0,"From few DD cookbook versions, it is possible to store keys in node's run state which is not written to the Chef server.",0.0,0.0,1.0,0.0
Datadog,48946203,48922641,0,"2018/02/23, 12:55:16",True,"2018/02/23, 12:55:16",1365.0,4894399.0,0,The above example is preferred solution to your issue.,0.0,0.223,0.777,0.3182
Datadog,47003630,47003531,0,"2017/10/29, 20:17:50",True,"2017/10/29, 20:17:50",2010.0,6020610.0,0,I noticed that the API key of Datadog changes everytime whenever in close the site and open a new instance.,0.0,0.0,1.0,0.0
Datadog,47003630,47003531,0,"2017/10/29, 20:17:50",True,"2017/10/29, 20:17:50",2010.0,6020610.0,0,"so after entering new API key , the issue was solved",0.0,0.189,0.811,0.2732
Datadog,46985445,46984902,0,"2017/10/28, 04:36:56",True,"2017/10/28, 04:36:56",1371.0,5540166.0,2,Two approaches that may work:,0.0,0.0,1.0,0.0
Datadog,46985445,46984902,0,"2017/10/28, 04:36:56",True,"2017/10/28, 04:36:56",1371.0,5540166.0,2,"It looks like flink has an  HTTP connector  to send metrics to Datadog, which at first glance looks to send over the Datadog metrics API instead of dogstatsd.",0.0,0.085,0.915,0.3612
Datadog,46985445,46984902,0,"2017/10/28, 04:36:56",True,"2017/10/28, 04:36:56",1371.0,5540166.0,2,"Dogstatsd is not very different from statsd otherwise, so it's often easy to modify statsd libraries to work for dogstatsd.",0.0,0.143,0.857,0.4877
Datadog,46985445,46984902,0,"2017/10/28, 04:36:56",True,"2017/10/28, 04:36:56",1371.0,5540166.0,2,"This project on GitHub  seems to be such a project, and may come in handy.",0.0,0.0,1.0,0.0
Datadog,46572247,46449330,1,"2017/10/04, 21:57:04",True,"2017/10/04, 21:57:04",1371.0,5540166.0,2,"Well, you  could  use the Datadog API to script the creation of 20 unique dashboards that all share the same content, but with different hosts.",0.0,0.176,0.824,0.4019
Datadog,46572247,46449330,1,"2017/10/04, 21:57:04",True,"2017/10/04, 21:57:04",1371.0,5540166.0,2,This is the part of the API docs that would help (with examples!),0.0,0.2,0.8,0.4574
Datadog,46572247,46449330,1,"2017/10/04, 21:57:04",True,"2017/10/04, 21:57:04",1371.0,5540166.0,2,"for Timeboards , and this one  for Screenboards .",0.0,0.0,1.0,0.0
Datadog,46572247,46449330,1,"2017/10/04, 21:57:04",True,"2017/10/04, 21:57:04",1371.0,5540166.0,2,"That said, I'd personally find 20 dashboards a bit cluttered / unwieldy in my own Datadog account.",0.0,0.0,1.0,0.0
Datadog,46572247,46449330,1,"2017/10/04, 21:57:04",True,"2017/10/04, 21:57:04",1371.0,5540166.0,2,"Instead, if it was me, I'd try to (A) find clever uses of dashboard template variables (on e.g, cluster tags, host tags, etc.",0.0,0.12,0.88,0.4588
Datadog,46572247,46449330,1,"2017/10/04, 21:57:04",True,"2017/10/04, 21:57:04",1371.0,5540166.0,2,"), or (B) group out by each host tag and apply  the ""top()"" function  in some way so that I'd be able to see just the most extreme-value hosts.",0.0,0.0,1.0,0.0
Datadog,46572247,46449330,1,"2017/10/04, 21:57:04",True,"2017/10/04, 21:57:04",1371.0,5540166.0,2,But that's certainly up to you :),0.0,0.587,0.413,0.7964
Datadog,45988104,45974396,0,"2017/08/31, 21:40:40",False,"2017/08/31, 21:40:40",148.0,6815223.0,0,Your answer appears to be there in the text -- you're missing a Python package.,0.145,0.0,0.855,-0.296
Datadog,45988104,45974396,0,"2017/08/31, 21:40:40",False,"2017/08/31, 21:40:40",148.0,6815223.0,0,"Try running  sudo pip install psutil , then restarting the agent.",0.0,0.0,1.0,0.0
Datadog,45988104,45974396,0,"2017/08/31, 21:40:40",False,"2017/08/31, 21:40:40",148.0,6815223.0,0,"Can you add your agent version, OS and version, and how you installed the agent to your text as well?",0.0,0.1,0.9,0.2732
Datadog,45988104,45974396,0,"2017/08/31, 21:40:40",False,"2017/08/31, 21:40:40",148.0,6815223.0,0,It looks like you're also using a  very  old version of the agent (it's up to 5.17.,0.0,0.143,0.857,0.3612
Datadog,45988104,45974396,0,"2017/08/31, 21:40:40",False,"2017/08/31, 21:40:40",148.0,6815223.0,0,* for a number of OS's) so there may be better package bundling or critical updates since v. 4.4.0.,0.112,0.205,0.683,0.2263
Datadog,45988104,45974396,0,"2017/08/31, 21:40:40",False,"2017/08/31, 21:40:40",148.0,6815223.0,0,Try installing a newer version as well.,0.0,0.296,0.704,0.2732
Datadog,45996096,45974396,8,"2017/09/01, 11:27:23",False,"2017/09/01, 12:04:48",1.0,8547108.0,-1,Please find the required,0.0,0.434,0.566,0.3182
Datadog,45239352,45232107,0,"2017/07/21, 16:46:50",True,"2017/07/21, 16:46:50",10159.0,4907630.0,1,In the code you posted:,0.0,0.0,1.0,0.0
Datadog,45239352,45232107,0,"2017/07/21, 16:46:50",True,"2017/07/21, 16:46:50",10159.0,4907630.0,1,This seems to be creating an empty client object.,0.164,0.2,0.636,0.1027
Datadog,45239352,45232107,0,"2017/07/21, 16:46:50",True,"2017/07/21, 16:46:50",10159.0,4907630.0,1,"Shouldn't you be creating a client with your keys using  datadog.NewClient(""..."", ""..."")  as in the first code snippet you posted?",0.095,0.0,0.905,-0.2235
Datadog,45239352,45232107,0,"2017/07/21, 16:46:50",True,"2017/07/21, 16:46:50",10159.0,4907630.0,1,"Also, you should check the error returned as that will give you more hints to troubleshoot the issue:",0.13,0.099,0.771,-0.1621
Datadog,45239352,45232107,0,"2017/07/21, 16:46:50",True,"2017/07/21, 16:46:50",10159.0,4907630.0,1,`,0.0,0.0,0.0,0.0
Datadog,44937902,44933565,0,"2017/07/06, 03:17:35",False,"2017/07/06, 03:26:50",1.0,6688988.0,0,The solution:,0.0,0.697,0.303,0.3182
Datadog,44937902,44933565,0,"2017/07/06, 03:17:35",False,"2017/07/06, 03:26:50",1.0,6688988.0,0,docker container 0,0.0,0.0,1.0,0.0
Datadog,44937902,44933565,0,"2017/07/06, 03:17:35",False,"2017/07/06, 03:26:50",1.0,6688988.0,0,"#!/bin/sh 
    java -Djava.util.logging.config.file=logging.properties -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.rmi.port=9998 -Dcom.sun.management.jmxremote.port=9998 -Djava.rmi.server.hostname=$HOSTNAME -Dcom.sun.management.jmxremote.host=$HOSTNAME -Dcom.sun.management.jmxremote.local.only=false -jar /app/my-streams.jar",0.0,0.0,1.0,0.0
Datadog,44937902,44933565,0,"2017/07/06, 03:17:35",False,"2017/07/06, 03:26:50",1.0,6688988.0,0,docker container 1,0.0,0.0,1.0,0.0
Datadog,44937902,44933565,0,"2017/07/06, 03:17:35",False,"2017/07/06, 03:26:50",1.0,6688988.0,0,way more stuff was done that is available from from stack overflow posts.,0.0,0.0,1.0,0.0
Datadog,44937902,44933565,0,"2017/07/06, 03:17:35",False,"2017/07/06, 03:26:50",1.0,6688988.0,0,but the above fixes the metrics finding error from datadog-agent.,0.283,0.0,0.717,-0.5499
Datadog,44937902,44933565,0,"2017/07/06, 03:17:35",False,"2017/07/06, 03:26:50",1.0,6688988.0,0,Here is how to run each component:,0.0,0.0,1.0,0.0
Datadog,44937902,44933565,0,"2017/07/06, 03:17:35",False,"2017/07/06, 03:26:50",1.0,6688988.0,0,"docker container 0 
* my-streams 
* spin up dependent services in tab 
** mvn clean package docker:build 
** docker-compose up",0.0,0.144,0.856,0.4019
Datadog,44937902,44933565,0,"2017/07/06, 03:17:35",False,"2017/07/06, 03:26:50",1.0,6688988.0,0,"docker container 1 
* docker build -t dd-agent-my-streams .",0.0,0.0,1.0,0.0
Datadog,44937902,44933565,0,"2017/07/06, 03:17:35",False,"2017/07/06, 03:26:50",1.0,6688988.0,0,* docker run -v /var/run/docker.sock:/var/run/docker.sock:ro   -v /proc/:/host/proc/:ro   -v /sys/fs/cgroup/:/host/sys/fs/cgroup:ro -e LOG_LEVEL=DEBUG -e SD_BACKEND=docker --network=mystreams_default,0.0,0.0,1.0,0.0
Datadog,44937902,44933565,0,"2017/07/06, 03:17:35",False,"2017/07/06, 03:26:50",1.0,6688988.0,0,"ssh into docker container 1 to verify if metrics work 
* docker ps // to find the name of the container to log into 
* docker exec -it  /bin/bash 
root@904e6561cc97:/# service datadog-agent configcheck 
root@904e6561cc97:/# service datadog-agent jmx list_everything 
root@904e6561cc97:/# service datadog-agent jmx collect",0.0,0.0,1.0,0.0
Datadog,44336405,44335719,4,"2017/06/02, 22:48:19",False,"2017/06/02, 22:48:19",1371.0,5540166.0,1,I think what you actually want is the metrics-query API endpoint?,0.0,0.126,0.874,0.0772
Datadog,44336405,44335719,4,"2017/06/02, 22:48:19",False,"2017/06/02, 22:48:19",1371.0,5540166.0,1,http://docs.datadoghq.com/api/#metrics-query,0.0,0.0,1.0,0.0
Datadog,44336405,44335719,4,"2017/06/02, 22:48:19",False,"2017/06/02, 22:48:19",1371.0,5540166.0,1,There are also a few Node.JS libraries that may be able to handle this kind of metric querying for you:  http://docs.datadoghq.com/libraries/#community-node,0.0,0.0,1.0,0.0
Datadog,44034370,44034151,0,"2017/05/17, 23:37:34",True,"2017/05/17, 23:37:34",261.0,4172512.0,2,The recommendation of:,0.0,0.0,1.0,0.0
Datadog,44034370,44034151,0,"2017/05/17, 23:37:34",True,"2017/05/17, 23:37:34",261.0,4172512.0,2,"""Please don't include endlessly growing tags in your metrics, like timestamps or user ids.",0.088,0.277,0.635,0.5076
Datadog,44034370,44034151,0,"2017/05/17, 23:37:34",True,"2017/05/17, 23:37:34",261.0,4172512.0,2,"Please limit each metric to 1000 tags.""",0.0,0.277,0.723,0.3182
Datadog,44034370,44034151,0,"2017/05/17, 23:37:34",True,"2017/05/17, 23:37:34",261.0,4172512.0,2,Is more of a warning against using infinitely expanding values as they can drastically increase your custom metric usage.,0.118,0.22,0.661,0.323
Datadog,44034370,44034151,0,"2017/05/17, 23:37:34",True,"2017/05/17, 23:37:34",261.0,4172512.0,2,As mentioned in the following article:,0.0,0.0,1.0,0.0
Datadog,44034370,44034151,0,"2017/05/17, 23:37:34",True,"2017/05/17, 23:37:34",261.0,4172512.0,2,https://help.datadoghq.com/hc/en-us/articles/204271775-What-is-a-custom-metric-and-what-is-the-limit-on-the-number-of-custom-metrics-I-can-have-,0.0,0.0,1.0,0.0
Datadog,44034370,44034151,0,"2017/05/17, 23:37:34",True,"2017/05/17, 23:37:34",261.0,4172512.0,2,"""By default customers are allotted 100 custom metrics per host across their entire infrastructure rather than on a per-host basis.",0.0,0.0,1.0,0.0
Datadog,44034370,44034151,0,"2017/05/17, 23:37:34",True,"2017/05/17, 23:37:34",261.0,4172512.0,2,"For example if you were licensed for 3 hosts, you would have 300 custom metrics by default - these 300 metrics may be divided equally amongst each individual host, or all 300 metrics could be sent from a single host.""",0.0,0.0,1.0,0.0
Datadog,44034370,44034151,0,"2017/05/17, 23:37:34",True,"2017/05/17, 23:37:34",261.0,4172512.0,2,You will want to keep in mind when configuring your metrics/tags of your current allotment of custom metrics and any billing implications that may have.,0.0,0.051,0.949,0.0772
Datadog,44034370,44034151,0,"2017/05/17, 23:37:34",True,"2017/05/17, 23:37:34",261.0,4172512.0,2,"That said, if having these tags is important to your team please reach out to support@datadoghq.com and we can sync up with the Sales Team to determine what is best for your team and use case.",0.0,0.227,0.773,0.8126
Datadog,66357269,66354474,1,"2021/02/24, 21:03:12",False,"2021/02/24, 21:03:12",60467.0,86611.0,0,"There is a preview feature that allows you to graph your SNAT port usage and allocation, see:",0.0,0.0,1.0,0.0
Datadog,66357269,66354474,1,"2021/02/24, 21:03:12",False,"2021/02/24, 21:03:12",60467.0,86611.0,0,https://docs.microsoft.com/en-us/azure/load-balancer/load-balancer-standard-diagnostics#how-do-i-check-my-snat-port-usage-and-allocation,0.0,0.0,1.0,0.0
Datadog,65396545,65396200,1,"2020/12/21, 18:34:57",False,"2020/12/21, 18:34:57",64.0,6668901.0,1,You can look in to other datadog kube metrics like kubernetes.replicas.available / total to alert if no of available - total &lt; 0.,0.088,0.189,0.723,0.3612
Datadog,65396545,65396200,1,"2020/12/21, 18:34:57",False,"2020/12/21, 18:34:57",64.0,6668901.0,1,Same can be done or for daemonset pods also there is a specific metric exposed.,0.091,0.0,0.909,-0.0772
Datadog,65396545,65396200,1,"2020/12/21, 18:34:57",False,"2020/12/21, 18:34:57",64.0,6668901.0,1,"[Datadog docs-kube metrics][1]
[1]: https://docs.datadoghq.com/agent/kubernetes/data_collected/",0.0,0.0,1.0,0.0
Datadog,64825884,64720852,0,"2020/11/13, 20:07:25",True,"2020/11/24, 23:46:04",177.0,872145.0,0,"An issue with IE11 is fixed in v1.26.1
See the fix here:  [RUMF-791] prevent IE11 performance entry error #633",0.13,0.053,0.817,-0.3818
Datadog,66445632,64611252,0,"2021/03/02, 21:08:29",False,"2021/03/02, 21:08:29",2834.0,444794.0,0,Datadog's Ruby library keeps this info on the struct  Datadog.tracer.active_correlation .,0.0,0.0,1.0,0.0
Datadog,66445632,64611252,0,"2021/03/02, 21:08:29",False,"2021/03/02, 21:08:29",2834.0,444794.0,0,You can call  Datadog.tracer.active_correlation.trace_id  to grab the trace ID.,0.0,0.0,1.0,0.0
Datadog,62074774,62074299,0,"2020/05/29, 00:23:05",False,"2020/05/29, 00:23:05",156.0,2676108.0,0,"You probably want to be using the MySQL integration, and configure the 'custom queries' option:  https://docs.datadoghq.com/integrations/faq/how-to-collect-metrics-from-custom-mysql-queries",0.0,0.08,0.92,0.0772
Datadog,62074774,62074299,0,"2020/05/29, 00:23:05",False,"2020/05/29, 00:23:05",156.0,2676108.0,0,You can follow those instructions after you configure the base integration  https://docs.datadoghq.com/integrations/mysql/#pagetitle  (This will give you a lot of use metrics in addition to the custom queries you want to run),0.0,0.043,0.957,0.0772
Datadog,62074774,62074299,0,"2020/05/29, 00:23:05",False,"2020/05/29, 00:23:05",156.0,2676108.0,0,"As you mentioned, DogStatsD is a library you can import to whatever script or application in order to submit metrics.",0.0,0.0,1.0,0.0
Datadog,62074774,62074299,0,"2020/05/29, 00:23:05",False,"2020/05/29, 00:23:05",156.0,2676108.0,0,But it really isn't a common practice in the slightest to modify the underlying code of your database.,0.0,0.0,1.0,0.0
Datadog,62074774,62074299,0,"2020/05/29, 00:23:05",False,"2020/05/29, 00:23:05",156.0,2676108.0,0,"So instead it makes more sense to externally run a query on the database, take those results, and send them to datadog.",0.0,0.0,1.0,0.0
Datadog,62074774,62074299,0,"2020/05/29, 00:23:05",False,"2020/05/29, 00:23:05",156.0,2676108.0,0,You could totally write a python script or something to do this.,0.0,0.0,1.0,0.0
Datadog,62074774,62074299,0,"2020/05/29, 00:23:05",False,"2020/05/29, 00:23:05",156.0,2676108.0,0,"However the Datadog agent already has this capability built in, so it's probably easier to just use that.",0.0,0.153,0.847,0.4703
Datadog,62074774,62074299,0,"2020/05/29, 00:23:05",False,"2020/05/29, 00:23:05",156.0,2676108.0,0,"I am also just assuming SQL refers to MySQL, there are other integration for things like SQL Server, and PostgreSQL, and pretty much every implementation of sql.",0.0,0.192,0.808,0.6908
Datadog,62074774,62074299,0,"2020/05/29, 00:23:05",False,"2020/05/29, 00:23:05",156.0,2676108.0,0,"And the same pattern applies where you would configure the integration, and then add an extra line to the config file where you have the check run your queries.",0.0,0.0,1.0,0.0
Datadog,60749201,60748902,0,"2020/03/19, 02:12:25",False,"2020/03/19, 02:12:25",251.0,5477963.0,0,"There is  dogapi  which wraps the entire Datadog API and should be able to do the above use case, probably using a mix of  metric.query ,  infrastructure.search ,  search.query  and  monitor.getAll .",0.0,0.0,1.0,0.0
Datadog,60749201,60748902,0,"2020/03/19, 02:12:25",False,"2020/03/19, 02:12:25",251.0,5477963.0,0,"For example, to get the list of monitors, it would look something like this:",0.0,0.161,0.839,0.3612
Datadog,60749201,60748902,0,"2020/03/19, 02:12:25",False,"2020/03/19, 02:12:25",251.0,5477963.0,0,Please keep in mind that I didn't test the above code.,0.0,0.204,0.796,0.3182
Datadog,60749201,60748902,0,"2020/03/19, 02:12:25",False,"2020/03/19, 02:12:25",251.0,5477963.0,0,"If you need something that is not in the library, it should also be fairly easy to wrap the API directly since it's a simple HTTP endpoint.",0.0,0.104,0.896,0.4404
Datadog,60749201,60748902,0,"2020/03/19, 02:12:25",False,"2020/03/19, 02:12:25",251.0,5477963.0,0,I hope this helps!,0.0,0.853,0.147,0.6996
Datadog,55188839,55187016,0,"2019/03/15, 20:36:45",True,"2019/03/15, 20:36:45",46.0,2089382.0,3,"For each web application that you want to configure with a different  Datadog APM service name , you need to set the environment variable  DD_SERVICE_NAME .",0.0,0.056,0.944,0.0772
Datadog,55188839,55187016,0,"2019/03/15, 20:36:45",True,"2019/03/15, 20:36:45",46.0,2089382.0,3,"If they're all running under the same IIS process, that's not possible.",0.0,0.0,1.0,0.0
Datadog,55188839,55187016,0,"2019/03/15, 20:36:45",True,"2019/03/15, 20:36:45",46.0,2089382.0,3,"In IIS there's a feature named  Application Pool , which can be used to isolate multiple web applications by running them under different processes.",0.079,0.0,0.921,-0.2023
Datadog,55188839,55187016,0,"2019/03/15, 20:36:45",True,"2019/03/15, 20:36:45",46.0,2089382.0,3,The first thing you need to do is to create a separate application pool for each web application.,0.0,0.116,0.884,0.2732
Datadog,55188839,55187016,0,"2019/03/15, 20:36:45",True,"2019/03/15, 20:36:45",46.0,2089382.0,3,"Once you're done with that, you can set a different  DD_SERVICE_NAME  for each application pool.",0.0,0.0,1.0,0.0
Datadog,55188839,55187016,0,"2019/03/15, 20:36:45",True,"2019/03/15, 20:36:45",46.0,2089382.0,3,The  command  to set an environment variable scoped to a specific application pool is,0.0,0.0,1.0,0.0
Datadog,55188839,55187016,0,"2019/03/15, 20:36:45",True,"2019/03/15, 20:36:45",46.0,2089382.0,3,"where  MyAppPool  is the name of the application pool, and  my-service  is the service name that you want to use for the Datadog APM.",0.0,0.053,0.947,0.0772
Datadog,55188839,55187016,0,"2019/03/15, 20:36:45",True,"2019/03/15, 20:36:45",46.0,2089382.0,3,"After running the above command, you have to restart IIS for the changes to take effect:",0.0,0.0,1.0,0.0
Datadog,55543982,55187016,3,"2019/04/06, 01:10:37",False,"2019/04/06, 01:10:37",16566.0,24231.0,2,"Starting with version 1.0 of Datadog's .NET Tracer, you can set most settings in your application's  app.config / web.config  file.",0.0,0.0,1.0,0.0
Datadog,55543982,55187016,3,"2019/04/06, 01:10:37",False,"2019/04/06, 01:10:37",16566.0,24231.0,2,"For example, to set  DD_SERVICE_NAME :",0.0,0.0,1.0,0.0
Datadog,55543982,55187016,3,"2019/04/06, 01:10:37",False,"2019/04/06, 01:10:37",16566.0,24231.0,2,[Disclaimer: I am a Datadog employee],0.0,0.0,1.0,0.0
Datadog,52066429,52066057,0,"2018/08/29, 00:28:16",False,"2018/08/29, 00:28:16",1371.0,5540166.0,0,Maybe you could use the Datadog respective dashboard apis to update your metric names in all dashboards?,0.0,0.149,0.851,0.4215
Datadog,52066429,52066057,0,"2018/08/29, 00:28:16",False,"2018/08/29, 00:28:16",1371.0,5540166.0,0,Should theoretically be not too complicated to script out.,0.0,0.0,1.0,0.0
Datadog,52066429,52066057,0,"2018/08/29, 00:28:16",False,"2018/08/29, 00:28:16",1371.0,5540166.0,0,"https://docs.datadoghq.com/api/?lang=python#screenboards 
 https://docs.datadoghq.com/api/?lang=python#timeboards",0.0,0.0,1.0,0.0
Datadog,50827960,50827869,0,"2018/06/13, 04:31:37",False,"2018/06/13, 04:31:37",470.0,951739.0,0,Use the  requests  library its a lot simpler,0.0,0.0,1.0,0.0
Datadog,50827960,50827869,0,"2018/06/13, 04:31:37",False,"2018/06/13, 04:31:37",470.0,951739.0,0,Generate a request header like this,0.0,0.385,0.615,0.3612
Datadog,50827960,50827869,0,"2018/06/13, 04:31:37",False,"2018/06/13, 04:31:37",470.0,951739.0,0,Send the request like this,0.0,0.385,0.615,0.3612
Datadog,50827969,50827869,0,"2018/06/13, 04:33:31",True,"2018/06/13, 04:33:31",631.0,9933041.0,1,"While searching through this  other issue , I found that all that is needed to fix this issue is to specify the API key and the application key within the URL.",0.0,0.0,1.0,0.0
Datadog,50827969,50827869,0,"2018/06/13, 04:33:31",True,"2018/06/13, 04:33:31",631.0,9933041.0,1,Consider the following.,0.0,0.0,1.0,0.0
Datadog,50957805,50460608,0,"2018/06/21, 01:13:11",True,"2018/06/21, 01:13:11",3956.0,2417043.0,1,"Yes, kind of.",0.0,0.574,0.426,0.4019
Datadog,50957805,50460608,0,"2018/06/21, 01:13:11",True,"2018/06/21, 01:13:11",3956.0,2417043.0,1,"It's possible to show single value on a dashboard (just use ""Query Value"" visualization), but it must be based on some metric reported to Datadog.",0.0,0.134,0.866,0.34
Datadog,50957805,50460608,0,"2018/06/21, 01:13:11",True,"2018/06/21, 01:13:11",3956.0,2417043.0,1,This is how it looks like:,0.0,0.333,0.667,0.3612
Datadog,49465784,49443977,0,"2018/03/24, 16:23:41",False,"2018/03/24, 16:23:41",2077.0,2796894.0,-1,It only applies to produce requests.,0.0,0.0,1.0,0.0
Datadog,49328630,49328198,2,"2018/03/16, 21:59:02",False,"2018/03/16, 21:59:02",1371.0,5540166.0,1,First you may need to download the MSI file:,0.0,0.0,1.0,0.0
Datadog,49328630,49328198,2,"2018/03/16, 21:59:02",False,"2018/03/16, 21:59:02",1371.0,5540166.0,1,The actual powershell command for installation (with extra optional arguments included as arguments):,0.184,0.0,0.816,-0.4019
Datadog,49328630,49328198,2,"2018/03/16, 21:59:02",False,"2018/03/16, 21:59:02",1371.0,5540166.0,1,It's been a while since i've done this (8 months or so?,0.0,0.298,0.702,0.6428
Datadog,49328630,49328198,2,"2018/03/16, 21:59:02",False,"2018/03/16, 21:59:02",1371.0,5540166.0,1,"), so it could be outdated, but it used to work :).",0.0,0.0,1.0,0.0
Datadog,49328630,49328198,2,"2018/03/16, 21:59:02",False,"2018/03/16, 21:59:02",1371.0,5540166.0,1,"Note, if you're running this from a remote provisioning script, you'll probly have to schedule this to be executed not-remotely so that the installation command can be run with heightened permissions, which i believe is required.",0.0,0.0,1.0,0.0
Datadog,49328630,49328198,2,"2018/03/16, 21:59:02",False,"2018/03/16, 21:59:02",1371.0,5540166.0,1,"And you  may  need to make sure the computer is plugged into the power source (i remember hitting some infuriating issue where that was an arbitrary requirement for Windows scheduled tasks to run, and Windows didn't allow me to configure around that).",0.109,0.05,0.841,-0.4149
Datadog,48325027,48314382,2,"2018/01/18, 17:39:30",False,"2018/01/18, 17:39:30",1371.0,5540166.0,0,is your  activemq_58.yaml  all in one line like that?,0.0,0.238,0.762,0.3612
Datadog,48325027,48314382,2,"2018/01/18, 17:39:30",False,"2018/01/18, 17:39:30",1371.0,5540166.0,0,You probably want it to be more like this:,0.0,0.369,0.631,0.4754
Datadog,47014511,47003909,0,"2017/10/30, 13:47:57",True,"2017/10/30, 13:47:57",1035.0,5825844.0,1,There are a variety of issues here.,0.0,0.0,1.0,0.0
Datadog,47014511,47003909,0,"2017/10/30, 13:47:57",True,"2017/10/30, 13:47:57",1035.0,5825844.0,1,1.,0.0,0.0,1.0,0.0
Datadog,47014511,47003909,0,"2017/10/30, 13:47:57",True,"2017/10/30, 13:47:57",1035.0,5825844.0,1,You've misconfigured the scope formats.,0.0,0.0,1.0,0.0
Datadog,47014511,47003909,0,"2017/10/30, 13:47:57",True,"2017/10/30, 13:47:57",1035.0,5825844.0,1,(metrics.scope.operator),0.0,0.0,1.0,0.0
Datadog,47014511,47003909,0,"2017/10/30, 13:47:57",True,"2017/10/30, 13:47:57",1035.0,5825844.0,1,"For one the configuration doesn't make sense since you specify ""metrics.scope.operator"" multiple times; only the last config entry is honored.",0.0,0.167,0.833,0.5859
Datadog,47014511,47003909,0,"2017/10/30, 13:47:57",True,"2017/10/30, 13:47:57",1035.0,5825844.0,1,"Second, and more importantly, you have misunderstood for scope formats are used for.",0.15,0.162,0.688,0.0498
Datadog,47014511,47003909,0,"2017/10/30, 13:47:57",True,"2017/10/30, 13:47:57",1035.0,5825844.0,1,Scope formats configure which context information (like the ID of the task) is included in the reported metric's name.,0.0,0.0,1.0,0.0
Datadog,47014511,47003909,0,"2017/10/30, 13:47:57",True,"2017/10/30, 13:47:57",1035.0,5825844.0,1,"By setting it to a constant (""latency"") you've told Flink to not include anything.",0.0,0.0,1.0,0.0
Datadog,47014511,47003909,0,"2017/10/30, 13:47:57",True,"2017/10/30, 13:47:57",1035.0,5825844.0,1,"As a result, the numRecordsIn metrics for every operator is reported as ""latency.numRecordsIn"".",0.0,0.0,1.0,0.0
Datadog,47014511,47003909,0,"2017/10/30, 13:47:57",True,"2017/10/30, 13:47:57",1035.0,5825844.0,1,I suggest to just remove your scope configuration.,0.0,0.0,1.0,0.0
Datadog,47014511,47003909,0,"2017/10/30, 13:47:57",True,"2017/10/30, 13:47:57",1035.0,5825844.0,1,2.,0.0,0.0,1.0,0.0
Datadog,47014511,47003909,0,"2017/10/30, 13:47:57",True,"2017/10/30, 13:47:57",1035.0,5825844.0,1,You've misconfigured the Datadog Tags,0.0,0.0,1.0,0.0
Datadog,47014511,47003909,0,"2017/10/30, 13:47:57",True,"2017/10/30, 13:47:57",1035.0,5825844.0,1,I do not understand what you were trying to do with your tags configuration.,0.0,0.0,1.0,0.0
Datadog,47014511,47003909,0,"2017/10/30, 13:47:57",True,"2017/10/30, 13:47:57",1035.0,5825844.0,1,"The tags configuration option can only be used to provide  global  tags, i.e.",0.0,0.0,1.0,0.0
Datadog,47014511,47003909,0,"2017/10/30, 13:47:57",True,"2017/10/30, 13:47:57",1035.0,5825844.0,1,"tags that are attached to every single metrics, like ""Flink"".",0.0,0.217,0.783,0.3612
Datadog,47014511,47003909,0,"2017/10/30, 13:47:57",True,"2017/10/30, 13:47:57",1035.0,5825844.0,1,By  default  every metric that the Datadog reports has tags attached to it for every available scope variable available.,0.0,0.0,1.0,0.0
Datadog,47014511,47003909,0,"2017/10/30, 13:47:57",True,"2017/10/30, 13:47:57",1035.0,5825844.0,1,"So, if you have an operator name A, then the numRecordsIn metric will be reported with a tag ""operator_name:A"".",0.0,0.0,1.0,0.0
Datadog,47014511,47003909,0,"2017/10/30, 13:47:57",True,"2017/10/30, 13:47:57",1035.0,5825844.0,1,"Again, I would suggest to just remove your configuration.",0.0,0.0,1.0,0.0
Datadog,46846772,46846059,0,"2017/10/20, 13:03:32",False,"2017/10/20, 13:03:32",148.0,6815223.0,1,"For the container agent, you'll want to run  sudo docker exec -it dd-agent /etc/init.d/datadog-agent status  from your unix based box.",0.0,0.064,0.936,0.0772
Datadog,46846772,46846059,0,"2017/10/20, 13:03:32",False,"2017/10/20, 13:03:32",148.0,6815223.0,1,"If, however, you are using the alpine image the command is:  docker exec -it dd-agent /opt/datadog-agent/bin/agent status  (different path).",0.0,0.0,1.0,0.0
Datadog,46846772,46846059,0,"2017/10/20, 13:03:32",False,"2017/10/20, 13:03:32",148.0,6815223.0,1,More here in this KB from Datadog:  https://help.datadoghq.com/hc/en-us/articles/203764635-Agent-Status-and-Information,0.0,0.0,1.0,0.0
Datadog,45742428,45370569,1,"2017/08/17, 21:19:43",True,"2017/08/17, 21:19:43",3223.0,43973.0,1,I had issues with it too.,0.0,0.0,1.0,0.0
Datadog,45742428,45370569,1,"2017/08/17, 21:19:43",True,"2017/08/17, 21:19:43",3223.0,43973.0,1,"The agent is most likely sending a value for 100 incorrectly as 10000 instead, as that's the highest I've seen it go.",0.0,0.118,0.882,0.3947
Datadog,45742428,45370569,1,"2017/08/17, 21:19:43",True,"2017/08/17, 21:19:43",3223.0,43973.0,1,"To make it usable, ensure the graph type is 'line', click ""Advanced"" on the metric, and make the equation  a / 100 .",0.0,0.12,0.88,0.3818
Datadog,45742428,45370569,1,"2017/08/17, 21:19:43",True,"2017/08/17, 21:19:43",3223.0,43973.0,1,"I didn't find any guidance on this either in my search, but this seems to go along with what I've seen logged locally into a Windows box and watching the performance counters locally.",0.0,0.0,1.0,0.0
Datadog,33961640,33960552,0,"2015/11/27, 18:39:07",True,"2015/11/27, 18:39:07",5244.0,1991579.0,1,Just should set hostname for  org.coursera.metrics.datadog.DatadogReporter.Builder :,0.0,0.0,1.0,0.0
Datadog,66430636,66416385,1,"2021/03/02, 00:30:18",False,"2021/03/02, 00:30:18",156.0,2676108.0,0,Set is almost never the right custom metric type to use.,0.0,0.0,1.0,0.0
Datadog,66430636,66416385,1,"2021/03/02, 00:30:18",False,"2021/03/02, 00:30:18",156.0,2676108.0,0,It will send a count of the number of unique items per a given tag.,0.0,0.098,0.902,0.0772
Datadog,66430636,66416385,1,"2021/03/02, 00:30:18",False,"2021/03/02, 00:30:18",156.0,2676108.0,0,"The underlying items details will be stripped from the metric, meaning that from one time slice to the next, you will have no idea that actual true number of items over time.",0.062,0.116,0.822,0.2263
Datadog,66430636,66416385,1,"2021/03/02, 00:30:18",False,"2021/03/02, 00:30:18",156.0,2676108.0,0,For example,0.0,0.0,1.0,0.0
Datadog,66430636,66416385,1,"2021/03/02, 00:30:18",False,"2021/03/02, 00:30:18",156.0,2676108.0,0,"Your time series to datadog will report  3 , and then  2 .",0.0,0.0,1.0,0.0
Datadog,66430636,66416385,1,"2021/03/02, 00:30:18",False,"2021/03/02, 00:30:18",156.0,2676108.0,0,But because the underlying device info is stripped you have no idea how to combine that 2 and 3 if you to zoom out in time and roll up the numbers to show 1 data point per minute.,0.076,0.0,0.924,-0.4215
Datadog,66430636,66416385,1,"2021/03/02, 00:30:18",False,"2021/03/02, 00:30:18",156.0,2676108.0,0,"It could be any number from 3 to 5, but the Datadog backend has no idea.",0.165,0.068,0.767,-0.3919
Datadog,66430636,66416385,1,"2021/03/02, 00:30:18",False,"2021/03/02, 00:30:18",156.0,2676108.0,0,(even though we know that across those 30 seconds there were 4 unique values total),0.0,0.172,0.828,0.4019
Datadog,66430636,66416385,1,"2021/03/02, 00:30:18",False,"2021/03/02, 00:30:18",156.0,2676108.0,0,"Plus even if it was accurate somehow, you can't create an alert of it or notify anyone, because you won't know which device is having issues if you see a spike of devices in the 60 second bucket.",0.096,0.0,0.904,-0.4023
Datadog,66430636,66416385,1,"2021/03/02, 00:30:18",False,"2021/03/02, 00:30:18",156.0,2676108.0,0,So let's go through other metric options.,0.0,0.0,1.0,0.0
Datadog,66430636,66416385,1,"2021/03/02, 00:30:18",False,"2021/03/02, 00:30:18",156.0,2676108.0,0,"The only metric types that are ever worth using are usually  distributions  or  gauges , or [counts].",0.0,0.112,0.888,0.2263
Datadog,66430636,66416385,1,"2021/03/02, 00:30:18",False,"2021/03/02, 00:30:18",156.0,2676108.0,0,"A gauge metric is just a measurement of the latency at a point in time, it's usually good for things like CPU or Memory of a computer, or temperature in a room.",0.0,0.178,0.822,0.6597
Datadog,66430636,66416385,1,"2021/03/02, 00:30:18",False,"2021/03/02, 00:30:18",156.0,2676108.0,0,"Numbers that are impossible to actually collect all dat a points for so you just take measurements every 10 seconds, or every minute, or however often you never to get an idea of the behavior.",0.0,0.0,1.0,0.0
Datadog,66430636,66416385,1,"2021/03/02, 00:30:18",False,"2021/03/02, 00:30:18",156.0,2676108.0,0,"A count metric is more exact, it's the number of things that happened.",0.0,0.106,0.894,0.0772
Datadog,66430636,66416385,1,"2021/03/02, 00:30:18",False,"2021/03/02, 00:30:18",156.0,2676108.0,0,"Usually good for number of requests to a server, or number of files processed.",0.0,0.355,0.645,0.5423
Datadog,66430636,66416385,1,"2021/03/02, 00:30:18",False,"2021/03/02, 00:30:18",156.0,2676108.0,0,"Even something like the amount of bytes flowing through something, although that usually is treated like a gauge by most people.",0.0,0.217,0.783,0.6124
Datadog,66430636,66416385,1,"2021/03/02, 00:30:18",False,"2021/03/02, 00:30:18",156.0,2676108.0,0,"Distributions are good for when you want to create a gauge metric, but you need detailed measurements for every single event that happens.",0.0,0.197,0.803,0.3919
Datadog,66430636,66416385,1,"2021/03/02, 00:30:18",False,"2021/03/02, 00:30:18",156.0,2676108.0,0,For example a web server is handling hundreds of requests per second and we need to know the latency metrics of that server.,0.0,0.0,1.0,0.0
Datadog,66430636,66416385,1,"2021/03/02, 00:30:18",False,"2021/03/02, 00:30:18",156.0,2676108.0,0,It's not possible to send a latency metric for every request as a gauge.,0.0,0.0,1.0,0.0
Datadog,66430636,66416385,1,"2021/03/02, 00:30:18",False,"2021/03/02, 00:30:18",156.0,2676108.0,0,Gauges have a built in limit of 1 data point per second (in Datadog).,0.0,0.0,1.0,0.0
Datadog,66430636,66416385,1,"2021/03/02, 00:30:18",False,"2021/03/02, 00:30:18",156.0,2676108.0,0,Anything more sent in a 1 second interval gets dropped.,0.0,0.0,1.0,0.0
Datadog,66430636,66416385,1,"2021/03/02, 00:30:18",False,"2021/03/02, 00:30:18",156.0,2676108.0,0,"But we need stats for every request, so a distribution will summarize the data, it keep a running count, min, max, average, and optionally several percentiles (p50, p75, p99).",0.0,0.0,1.0,0.0
Datadog,66430636,66416385,1,"2021/03/02, 00:30:18",False,"2021/03/02, 00:30:18",156.0,2676108.0,0,I haven't seen many good use cases for metric types outside of those 3.,0.167,0.0,0.833,-0.3412
Datadog,66430636,66416385,1,"2021/03/02, 00:30:18",False,"2021/03/02, 00:30:18",156.0,2676108.0,0,"For your scenario, it seems like you would want to be sending a distribution metric for that device interval.",0.0,0.192,0.808,0.4215
Datadog,66430636,66416385,1,"2021/03/02, 00:30:18",False,"2021/03/02, 00:30:18",156.0,2676108.0,0,So device 1 sends a value of 10.14 and device 3 sends a value of 2.3 and so on.,0.0,0.291,0.709,0.6517
Datadog,66430636,66416385,1,"2021/03/02, 00:30:18",False,"2021/03/02, 00:30:18",156.0,2676108.0,0,Then you can use a  distribution widget  in a dashboard to show the number of devices for each interval bucket.,0.0,0.071,0.929,0.0772
Datadog,66430636,66416385,1,"2021/03/02, 00:30:18",False,"2021/03/02, 00:30:18",156.0,2676108.0,0,Of course make sure you tag each metric by the device that is generating the metric.,0.0,0.133,0.867,0.3182
Datadog,53458998,53044670,0,"2018/11/24, 16:09:27",True,"2018/11/24, 16:09:27",347.0,2650254.0,0,"I was able to do that by using this api call:  https://docs.datadoghq.com/api/?lang=python#get-a-screenboard 
and then get it as a son file, which can be passed to cloud formation later.",0.0,0.0,1.0,0.0
Datadog,50451247,50134871,0,"2018/05/21, 17:41:57",False,"2018/05/21, 17:41:57",2352.0,8870132.0,0,Actually what you need to do is create a generic method.,0.0,0.189,0.811,0.2732
Datadog,50451247,50134871,0,"2018/05/21, 17:41:57",False,"2018/05/21, 17:41:57",2352.0,8870132.0,0,Now when you are hitting the different different end points just call the updateCounter method will will capture your metric with the specific name of your route.,0.0,0.0,1.0,0.0
Datadog,50451247,50134871,0,"2018/05/21, 17:41:57",False,"2018/05/21, 17:41:57",2352.0,8870132.0,0,"For example you have route like add and subtract
Then call the update counter method with metric name add and subtract.",0.0,0.111,0.889,0.3612
Datadog,41618026,41605761,0,"2017/01/12, 18:15:50",False,"2017/01/12, 18:15:50",261.0,4172512.0,1,You can leverage Datadog's Agent to collect metrics via a JMX connection.,0.0,0.0,1.0,0.0
Datadog,41618026,41605761,0,"2017/01/12, 18:15:50",False,"2017/01/12, 18:15:50",261.0,4172512.0,1,There is documentation found here:,0.0,0.0,1.0,0.0
Datadog,41618026,41605761,0,"2017/01/12, 18:15:50",False,"2017/01/12, 18:15:50",261.0,4172512.0,1,http://docs.datadoghq.com/integrations/java/,0.0,0.0,1.0,0.0
Datadog,41618026,41605761,0,"2017/01/12, 18:15:50",False,"2017/01/12, 18:15:50",261.0,4172512.0,1,https://www.datadoghq.com/blog/monitoring-jmx-metrics-with-datadog/,0.0,0.0,1.0,0.0
Datadog,41618026,41605761,0,"2017/01/12, 18:15:50",False,"2017/01/12, 18:15:50",261.0,4172512.0,1,https://help.datadoghq.com/hc/en-us/articles/204501525-Custom-JMX-Integration-s-,0.0,0.0,1.0,0.0
Datadog,41618026,41605761,0,"2017/01/12, 18:15:50",False,"2017/01/12, 18:15:50",261.0,4172512.0,1,https://help.datadoghq.com/hc/en-us/articles/207525586-View-jmx-data-in-jConsole-and-set-up-your-jmx-yaml-to-collect-them,0.0,0.0,1.0,0.0
Datadog,41618026,41605761,0,"2017/01/12, 18:15:50",False,"2017/01/12, 18:15:50",261.0,4172512.0,1,That should help you get setup and collecting the necessary metrics exposed by your JMX port.,0.072,0.15,0.778,0.34
Datadog,41618026,41605761,0,"2017/01/12, 18:15:50",False,"2017/01/12, 18:15:50",261.0,4172512.0,1,"That said, if you encounter any issues reach out to support@datadoghq.com and they can assist you.",0.0,0.068,0.932,0.0258
Datadog,36459921,36459827,3,"2016/04/06, 21:52:23",False,"2016/04/06, 21:52:23",12279.0,596041.0,2,What you have is a nil pointer dereference.,0.0,0.0,1.0,0.0
Datadog,36459921,36459827,3,"2016/04/06, 21:52:23",False,"2016/04/06, 21:52:23",12279.0,596041.0,2,"(Unless you are using package  unsafe , which you probably shouldn't touch, so I'm assuming you're not.)",0.0,0.0,1.0,0.0
Datadog,36459921,36459827,3,"2016/04/06, 21:52:23",False,"2016/04/06, 21:52:23",12279.0,596041.0,2,It looks like the  e  argument to  func (c *Client) Event(e *Event) error  is  nil  when called from  github.com/some/path/server/http.go:86 .,0.229,0.11,0.661,-0.4019
Datadog,36461477,36459827,1,"2016/04/06, 23:17:02",True,"2016/04/06, 23:17:02",1631.0,97094.0,19,"Thanks to a comment from @twotwotwo, I think I figured this out.",0.0,0.266,0.734,0.4404
Datadog,36461477,36459827,1,"2016/04/06, 23:17:02",True,"2016/04/06, 23:17:02",1631.0,97094.0,19,In this line,0.0,0.0,1.0,0.0
Datadog,36461477,36459827,1,"2016/04/06, 23:17:02",True,"2016/04/06, 23:17:02",1631.0,97094.0,19,I wrote the following program to demonstrate to myself how different function signatures appear in a stack trace:,0.0,0.0,1.0,0.0
Datadog,36461477,36459827,1,"2016/04/06, 23:17:02",True,"2016/04/06, 23:17:02",1631.0,97094.0,19,"When  bam  is called, which acts on  *Y  but has no arguments or return value, the output contains:",0.26,0.127,0.613,-0.5023
Datadog,36461477,36459827,1,"2016/04/06, 23:17:02",True,"2016/04/06, 23:17:02",1631.0,97094.0,19,"When  foo  is called, which acts on  *Y  and takes a  *X  as argument, but has no return value, the output contains:",0.177,0.121,0.702,-0.1154
Datadog,36461477,36459827,1,"2016/04/06, 23:17:02",True,"2016/04/06, 23:17:02",1631.0,97094.0,19,"When  bar  is called, which acts on  *Y , takes a  *X  as argument, and returns a  *Y , the output contains:",0.128,0.0,0.872,-0.3612
Datadog,36461477,36459827,1,"2016/04/06, 23:17:02",True,"2016/04/06, 23:17:02",1631.0,97094.0,19,"When  baz  is called, which acts on  *Y , takes  *X  as argument, and returns an  error  (which is an interface), the output contains:",0.198,0.0,0.802,-0.6369
Datadog,45359315,45358188,3,"2017/07/27, 22:20:09",False,"2018/10/29, 09:50:33",6670.0,3625317.0,7,Think differently :),0.0,0.6,0.4,0.4588
Datadog,45359315,45358188,3,"2017/07/27, 22:20:09",False,"2018/10/29, 09:50:33",6670.0,3625317.0,7,Do bind a nginx-server (vhost) on port 10080  in addition  - that server does offer the status location and what you need.,0.0,0.0,1.0,0.0
Datadog,45359315,45358188,3,"2017/07/27, 22:20:09",False,"2018/10/29, 09:50:33",6670.0,3625317.0,7,Server on 80/443 is also there and ONLY that one is bound/exposed to host ( exposed to the outer world ).,0.064,0.0,0.936,-0.0772
Datadog,45359315,45358188,3,"2017/07/27, 22:20:09",False,"2018/10/29, 09:50:33",6670.0,3625317.0,7,"Since datadog is part of your docker-network / service network, it can still access 10080 in the internal network, but nobody else from the outer network.",0.0,0.0,1.0,0.0
Datadog,45359315,45358188,3,"2017/07/27, 22:20:09",False,"2018/10/29, 09:50:33",6670.0,3625317.0,7,"Bulletproof, easy - no strings attached.",0.272,0.358,0.37,0.1779
Datadog,45359614,45358188,0,"2017/07/27, 22:38:08",True,"2017/07/27, 22:38:08",123680.0,2830850.0,4,Since we are running the service through  docker-compose  and our issue being we don't know the IP of the agent.,0.0,0.0,1.0,0.0
Datadog,45359614,45358188,0,"2017/07/27, 22:38:08",True,"2017/07/27, 22:38:08",123680.0,2830850.0,4,So the simple solution is to know the IP before starting.,0.0,0.204,0.796,0.3744
Datadog,45359614,45358188,0,"2017/07/27, 22:38:08",True,"2017/07/27, 22:38:08",123680.0,2830850.0,4,And that means assigning our agent a specific IP,0.0,0.0,1.0,0.0
Datadog,45359614,45358188,0,"2017/07/27, 22:38:08",True,"2017/07/27, 22:38:08",123680.0,2830850.0,4,Here is a update  docker-compose  to do that,0.0,0.0,1.0,0.0
Datadog,45359614,45358188,0,"2017/07/27, 22:38:08",True,"2017/07/27, 22:38:08",123680.0,2830850.0,4,Now you can do two possible things,0.0,0.0,1.0,0.0
Datadog,45359614,45358188,0,"2017/07/27, 22:38:08",True,"2017/07/27, 22:38:08",123680.0,2830850.0,4,You can listen only on  172.25.0.101  which is accessible only container running on agent network.,0.0,0.0,1.0,0.0
Datadog,45359614,45358188,0,"2017/07/27, 22:38:08",True,"2017/07/27, 22:38:08",123680.0,2830850.0,4,Also you can add  allow 172.25.0.100  to only allow the agent container to be able to access this.,0.0,0.192,0.808,0.4215
Datadog,61277277,45358188,1,"2020/04/17, 20:27:04",False,"2020/04/17, 20:27:04",515.0,2075004.0,3,There are two (easier) ways to go about it.,0.0,0.0,1.0,0.0
Datadog,61277277,45358188,1,"2020/04/17, 20:27:04",False,"2020/04/17, 20:27:04",515.0,2075004.0,3,"First one is  docker-compose  but since I already have a setup running since 2 years which doesn't use docker-compose, I went for the 2nd way.",0.0,0.0,1.0,0.0
Datadog,61277277,45358188,1,"2020/04/17, 20:27:04",False,"2020/04/17, 20:27:04",515.0,2075004.0,3,Second way is  Allow  Directive with a range of IPs.,0.0,0.192,0.808,0.2263
Datadog,61277277,45358188,1,"2020/04/17, 20:27:04",False,"2020/04/17, 20:27:04",515.0,2075004.0,3,Eg:,0.0,0.0,1.0,0.0
Datadog,61277277,45358188,1,"2020/04/17, 20:27:04",False,"2020/04/17, 20:27:04",515.0,2075004.0,3,"I am not security expert, but mostly  192.168.",0.202,0.0,0.798,-0.1326
Datadog,61277277,45358188,1,"2020/04/17, 20:27:04",False,"2020/04/17, 20:27:04",515.0,2075004.0,3,"*  IP range is for local networks, not sure about  172.18.",0.179,0.0,0.821,-0.2411
Datadog,61277277,45358188,1,"2020/04/17, 20:27:04",False,"2020/04/17, 20:27:04",515.0,2075004.0,3,*  range though.,0.0,0.0,1.0,0.0
Datadog,61277277,45358188,1,"2020/04/17, 20:27:04",False,"2020/04/17, 20:27:04",515.0,2075004.0,3,"To get more idea about this IP range thing and CIDR stuff, refer below links
 http://nginx.org/en/docs/http/ngx_http_access_module.html",0.0,0.0,1.0,0.0
Datadog,61277277,45358188,1,"2020/04/17, 20:27:04",False,"2020/04/17, 20:27:04",515.0,2075004.0,3,https://www.ripe.net/about-us/press-centre/understanding-ip-addressing,0.0,0.0,1.0,0.0
Datadog,34031282,33929394,0,"2015/12/02, 00:18:15",False,"2015/12/02, 00:18:15",117.0,5379516.0,0,"As the err info said dh key is too small, a larger one might help.",0.0,0.172,0.828,0.4019
Datadog,34031282,33929394,0,"2015/12/02, 00:18:15",False,"2015/12/02, 00:18:15",117.0,5379516.0,0,Replace the default dh512.pem file with dh4096.pem,0.0,0.0,1.0,0.0
Datadog,34031282,33929394,0,"2015/12/02, 00:18:15",False,"2015/12/02, 00:18:15",117.0,5379516.0,0,"sudo wget ""https://git.openssl.org/gitweb/?p=openssl.git;a=blob_plain;f=apps/dh4096.pem"" -O dh4096.pem",0.0,0.0,1.0,0.0
Datadog,34031282,33929394,0,"2015/12/02, 00:18:15",False,"2015/12/02, 00:18:15",117.0,5379516.0,0,Ref:  http://www.alexrhino.net/jekyll/update/2015/07/14/dh-params-test-fail.html,0.0,0.0,1.0,0.0
Datadog,60844436,60842629,1,"2020/03/25, 09:24:11",False,"2020/03/25, 09:59:29",71285.0,3558960.0,3,This is actually a lot harder than it seems.,0.0,0.0,1.0,0.0
Datadog,60844436,60842629,1,"2020/03/25, 09:24:11",False,"2020/03/25, 09:59:29",71285.0,3558960.0,3,"Representing big integers in JavaScript can be done using the  BigInt  data type (by suffixing the number with  n ), which is fairly widely supported at this point.",0.0,0.126,0.874,0.3818
Datadog,60844436,60842629,1,"2020/03/25, 09:24:11",False,"2020/03/25, 09:59:29",71285.0,3558960.0,3,This would make your object look like this:,0.0,0.263,0.737,0.3612
Datadog,60844436,60842629,1,"2020/03/25, 09:24:11",False,"2020/03/25, 09:59:29",71285.0,3558960.0,3,"The problem presents itself in the JSON serialization, as there is currently no support for the serialization of  BigInt  objects.",0.199,0.11,0.691,-0.296
Datadog,60844436,60842629,1,"2020/03/25, 09:24:11",False,"2020/03/25, 09:59:29",71285.0,3558960.0,3,"And when it comes to JSON serialization, your options for customization are very limited:",0.144,0.0,0.856,-0.2944
Datadog,60844436,60842629,1,"2020/03/25, 09:24:11",False,"2020/03/25, 09:59:29",71285.0,3558960.0,3,So the only option that I can find is to (at least partially) implement your own JSON serialization mechanism.,0.0,0.0,1.0,0.0
Datadog,60844436,60842629,1,"2020/03/25, 09:24:11",False,"2020/03/25, 09:59:29",71285.0,3558960.0,3,"This is a  very  poor man's implementation that calls  toString()  for object properties that are of type  BigInt , and delegates to  JSON.stringify()  otherwise:",0.139,0.0,0.861,-0.5256
Datadog,60844436,60842629,1,"2020/03/25, 09:24:11",False,"2020/03/25, 09:59:29",71285.0,3558960.0,3,"
 
 const o = {
  ""span_id"": 16956440953342013954n,
  ""trace_id"": 13756071592735822010n
};

const stringify = (o) =&gt; '{'
  + Object.entries(o).reduce((a, [k, v]) =&gt; ([
      ...a, 
      `""${k}"": ${typeof v === 'bigint' ?",0.0,0.0,1.0,0.0
Datadog,60844436,60842629,1,"2020/03/25, 09:24:11",False,"2020/03/25, 09:59:29",71285.0,3558960.0,3,"v.toString() : JSON.stringify(v)}`
    ])).join(', ')
  + '}';

console.log(stringify(o));",0.0,0.0,1.0,0.0
Datadog,60844436,60842629,1,"2020/03/25, 09:24:11",False,"2020/03/25, 09:59:29",71285.0,3558960.0,3,"Note that the above will not work correctly in a number of cases, most prominently nested objects and arrays.",0.0,0.071,0.929,0.0772
Datadog,60844436,60842629,1,"2020/03/25, 09:24:11",False,"2020/03/25, 09:59:29",71285.0,3558960.0,3,"If I were to do this for real-world usage, I would probably base myself on  Douglas Crockford's JSON implementation .",0.0,0.0,1.0,0.0
Datadog,60844436,60842629,1,"2020/03/25, 09:24:11",False,"2020/03/25, 09:59:29",71285.0,3558960.0,3,It should be sufficient to add an additional case around  this line :,0.0,0.0,1.0,0.0
Datadog,59354863,58897099,0,"2019/12/16, 12:36:35",False,"2019/12/16, 12:36:35",394.0,1602433.0,-1,You could try to use  clinic  in order to debug and profile the app.,0.0,0.0,1.0,0.0
Datadog,59354863,58897099,0,"2019/12/16, 12:36:35",False,"2019/12/16, 12:36:35",394.0,1602433.0,-1,pretty good tool for nodeJS.,0.0,0.67,0.33,0.7269
Datadog,59406939,58897099,1,"2019/12/19, 11:41:00",False,"2019/12/19, 11:41:00",388.0,4546641.0,-1,You could user  node-memwatch  to detect where is memory leak.,0.211,0.0,0.789,-0.34
Datadog,59406939,58897099,1,"2019/12/19, 11:41:00",False,"2019/12/19, 11:41:00",388.0,4546641.0,-1,"It also might be a known issue, here is the  link  with a similar issue.",0.0,0.0,1.0,0.0
Datadog,59433027,58897099,0,"2019/12/21, 04:16:20",False,"2019/12/21, 04:16:20",3025.0,3154872.0,0,You are on the right path.,0.0,0.0,1.0,0.0
Datadog,59433027,58897099,0,"2019/12/21, 04:16:20",False,"2019/12/21, 04:16:20",3025.0,3154872.0,0,The guide I'm about to link to begins by following a similar approach to the one you've taken.,0.0,0.0,1.0,0.0
Datadog,59433027,58897099,0,"2019/12/21, 04:16:20",False,"2019/12/21, 04:16:20",3025.0,3154872.0,0,"I'll link to the section that talks about monitoring memory in real time, which is available when you  Record allocation timeline  in chrome://inspect",0.0,0.0,1.0,0.0
Datadog,59433027,58897099,0,"2019/12/21, 04:16:20",False,"2019/12/21, 04:16:20",3025.0,3154872.0,0,https://marmelab.com/blog/2018/04/03/how-to-track-and-fix-memory-leak-with-nodejs.html#watching-memory-allocation-in-real-time,0.0,0.0,1.0,0.0
Datadog,65950961,47299307,0,"2021/01/29, 10:19:44",False,"2021/01/29, 10:19:44",21.0,6351378.0,2,"This question is quite old, however, still might be useful for new users of Google Cloud.",0.0,0.162,0.838,0.4404
Datadog,65950961,47299307,0,"2021/01/29, 10:19:44",False,"2021/01/29, 10:19:44",21.0,6351378.0,2,In 'Metrics Explorer' in Google Cloud Console there is an option to write a query with MQL (click  Query Editor  button).,0.0,0.0,1.0,0.0
Datadog,65950961,47299307,0,"2021/01/29, 10:19:44",False,"2021/01/29, 10:19:44",21.0,6351378.0,2,MQL supports expressions which are described in detail  here .,0.0,0.238,0.762,0.3612
Datadog,65950961,47299307,0,"2021/01/29, 10:19:44",False,"2021/01/29, 10:19:44",21.0,6351378.0,2,The simplest example for dividing one metric by another would look like this:,0.0,0.172,0.828,0.3612
Datadog,63340327,63049334,1,"2020/08/10, 15:35:10",True,"2020/08/10, 15:35:10",632.0,5197466.0,2,"It is probably caused by a regression between .NET Core 2.2 and .NET Core 3.0
Apparently it will be fixed in version 3.1.7",0.0,0.0,1.0,0.0
Datadog,63340327,63049334,1,"2020/08/10, 15:35:10",True,"2020/08/10, 15:35:10",632.0,5197466.0,2,"Just starting the process causes the memory leak on linux, because of a non released handle",0.146,0.0,0.854,-0.34
Datadog,63340327,63049334,1,"2020/08/10, 15:35:10",True,"2020/08/10, 15:35:10",632.0,5197466.0,2,Issue has been tracked here  https://github.com/dotnet/runtime/issues/36661,0.0,0.0,1.0,0.0
Datadog,60253636,60237664,2,"2020/02/17, 00:04:53",True,"2020/02/17, 23:43:26",37971.0,1237575.0,2,The problem is that the advice class will be loaded on the system class loader as a part of the agent whereas the actual application code is loaded on a sub-class loader that is not visible to the system class loader.,0.066,0.0,0.934,-0.4019
Datadog,60253636,60237664,2,"2020/02/17, 00:04:53",True,"2020/02/17, 23:43:26",37971.0,1237575.0,2,This situation does not change if you load your agent on the boot loader either.,0.0,0.0,1.0,0.0
Datadog,60253636,60237664,2,"2020/02/17, 00:04:53",True,"2020/02/17, 23:43:26",37971.0,1237575.0,2,"Therefore, the agent cannot load the  HttpServletRequest  class which is part of the uber-jar.",0.0,0.0,1.0,0.0
Datadog,60253636,60237664,2,"2020/02/17, 00:04:53",True,"2020/02/17, 23:43:26",37971.0,1237575.0,2,This is a typical problem with agents and Byte Buddy has a standard way to circumvent it by using a  Transformer.ForAdvice  instance instead of using the  Advice  class directly.,0.097,0.0,0.903,-0.4019
Datadog,60253636,60237664,2,"2020/02/17, 00:04:53",True,"2020/02/17, 23:43:26",37971.0,1237575.0,2,Byte Buddy then creates a virtual class loader hierarchy that considers classes represented by both class loaders.,0.0,0.123,0.877,0.2732
Datadog,60253636,60237664,2,"2020/02/17, 00:04:53",True,"2020/02/17, 23:43:26",37971.0,1237575.0,2,Update : The problem is that you are calling down to your interceptor that is defined in the system class loader where the class in question is not available.,0.091,0.0,0.909,-0.4019
Datadog,60253636,60237664,2,"2020/02/17, 00:04:53",True,"2020/02/17, 23:43:26",37971.0,1237575.0,2,The annotated code will be inlined but the invoked method will not.,0.0,0.0,1.0,0.0
Datadog,60253636,60237664,2,"2020/02/17, 00:04:53",True,"2020/02/17, 23:43:26",37971.0,1237575.0,2,"If you copy-pasted the code into the annotated method, the behavior is as you'd expect it.",0.0,0.0,1.0,0.0
Datadog,60253636,60237664,2,"2020/02/17, 00:04:53",True,"2020/02/17, 23:43:26",37971.0,1237575.0,2,Byte Buddy uses the annotated code as template and reuses a lot of information emitted by javac to guarantee a speedy conversion.,0.0,0.095,0.905,0.25
Datadog,60253636,60237664,2,"2020/02/17, 00:04:53",True,"2020/02/17, 23:43:26",37971.0,1237575.0,2,"Therefore, the library cannot simply copy the method and should rather feed the entire method body to javac.",0.0,0.0,1.0,0.0
Datadog,56645177,54185496,0,"2019/06/18, 11:49:21",True,"2019/06/18, 11:49:21",36.0,11663155.0,2,The reason for this error is because apache is listening to port 80 on IPv4 &amp; IPv6.,0.163,0.0,0.837,-0.481
Datadog,56645177,54185496,0,"2019/06/18, 11:49:21",True,"2019/06/18, 11:49:21",36.0,11663155.0,2,This will explicitly tell apache to listen to IPv4.,0.0,0.0,1.0,0.0
Datadog,56645177,54185496,0,"2019/06/18, 11:49:21",True,"2019/06/18, 11:49:21",36.0,11663155.0,2,In apache config change:,0.0,0.0,1.0,0.0
Datadog,56645177,54185496,0,"2019/06/18, 11:49:21",True,"2019/06/18, 11:49:21",36.0,11663155.0,2,Listen 80,0.0,0.0,1.0,0.0
Datadog,56645177,54185496,0,"2019/06/18, 11:49:21",True,"2019/06/18, 11:49:21",36.0,11663155.0,2,to,0.0,0.0,1.0,0.0
Datadog,56645177,54185496,0,"2019/06/18, 11:49:21",True,"2019/06/18, 11:49:21",36.0,11663155.0,2,Listen 0.0.0.0:80,0.0,0.0,1.0,0.0
Datadog,56645177,54185496,0,"2019/06/18, 11:49:21",True,"2019/06/18, 11:49:21",36.0,11663155.0,2,Make sure the file is being copied in to your docker container and being used in apache.,0.0,0.126,0.874,0.3182
Datadog,56645177,54185496,0,"2019/06/18, 11:49:21",True,"2019/06/18, 11:49:21",36.0,11663155.0,2,Or add an extra step in the Dockerfile:,0.0,0.0,1.0,0.0
Datadog,56645177,54185496,0,"2019/06/18, 11:49:21",True,"2019/06/18, 11:49:21",36.0,11663155.0,2,&amp;&amp; sed -i 's/^Listen 80$/Listen 0.0.0.0:80/' /etc/apache2/httpd.conf,0.0,0.0,1.0,0.0
Datadog,52392480,52390678,0,"2018/09/18, 21:18:13",True,"2018/09/18, 21:18:13",3537.0,8993347.0,3,"Containers are about isolation so in container ""localhost"" means inside container  so ddtrace-test cannot find ddagent inside his container.",0.13,0.0,0.87,-0.4019
Datadog,52392480,52390678,0,"2018/09/18, 21:18:13",True,"2018/09/18, 21:18:13",3537.0,8993347.0,3,You have 2 ways to fix that:,0.0,0.0,1.0,0.0
Datadog,50097560,50096290,2,"2018/04/30, 12:24:42",True,"2018/04/30, 12:52:06",18198.0,1765189.0,6,"I'm guessing you're talking about ""metrics"" instead of matrix!",0.0,0.0,1.0,0.0
Datadog,50097560,50096290,2,"2018/04/30, 12:24:42",True,"2018/04/30, 12:52:06",18198.0,1765189.0,6,"On the Producer, you have  kafka.producer:type=producer-metrics,client-id=""{client-id}"" .",0.0,0.0,1.0,0.0
Datadog,50097560,50096290,2,"2018/04/30, 12:24:42",True,"2018/04/30, 12:52:06",18198.0,1765189.0,6,That metric has 2 interesting attributes:,0.0,0.403,0.597,0.4019
Datadog,50097560,50096290,2,"2018/04/30, 12:24:42",True,"2018/04/30, 12:52:06",18198.0,1765189.0,6,request-latency-avg: The average request latency in ms,0.0,0.0,1.0,0.0
Datadog,50097560,50096290,2,"2018/04/30, 12:24:42",True,"2018/04/30, 12:52:06",18198.0,1765189.0,6,request-latency-max: The maximum request latency in ms,0.0,0.0,1.0,0.0
Datadog,50097560,50096290,2,"2018/04/30, 12:24:42",True,"2018/04/30, 12:52:06",18198.0,1765189.0,6,"On the broker side, there are a few metrics you want to check to investigate your issue:",0.0,0.08,0.92,0.0772
Datadog,50097560,50096290,2,"2018/04/30, 12:24:42",True,"2018/04/30, 12:52:06",18198.0,1765189.0,6,Request total time: Total time Kafka took to process the request.,0.0,0.0,1.0,0.0
Datadog,50097560,50096290,2,"2018/04/30, 12:24:42",True,"2018/04/30, 12:52:06",18198.0,1765189.0,6,"kafka.network:type=RequestMetrics,name=TotalTimeMs,request=Produce",0.0,0.0,1.0,0.0
Datadog,50097560,50096290,2,"2018/04/30, 12:24:42",True,"2018/04/30, 12:52:06",18198.0,1765189.0,6,"In case this is high, you can check the break down metrics:",0.0,0.0,1.0,0.0
Datadog,50097560,50096290,2,"2018/04/30, 12:24:42",True,"2018/04/30, 12:52:06",18198.0,1765189.0,6,These are all listed in the metrics recommended to monitor list in the Kafka documentation:  http://kafka.apache.org/documentation/#monitoring,0.0,0.107,0.893,0.2023
Datadog,44857071,44855220,2,"2017/07/01, 06:28:14",False,"2017/07/01, 06:28:14",19457.0,684908.0,3,Some preliminary Google searching lands me on  https://github.com/kubernetes/kubernetes/pull/42717  by way of  https://github.com/kubernetes/kubernetes/issues/24657 .,0.0,0.0,1.0,0.0
Datadog,44857071,44855220,2,"2017/07/01, 06:28:14",False,"2017/07/01, 06:28:14",19457.0,684908.0,3,It looks like the pull request was merged in time to be in Kubernetes 1.7.,0.0,0.152,0.848,0.3612
Datadog,44857071,44855220,2,"2017/07/01, 06:28:14",False,"2017/07/01, 06:28:14",19457.0,684908.0,3,This should mean that you can use the Downward API to expose  status.hostIP  as an environment variable ( https://kubernetes.io/docs/tasks/inject-data-application/environment-variable-expose-pod-information/ ) or a file in a volume ( https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/ ).,0.065,0.0,0.935,-0.1531
Datadog,44857071,44855220,2,"2017/07/01, 06:28:14",False,"2017/07/01, 06:28:14",19457.0,684908.0,3,Your application would then need to read the environment variable or file to get the value of the actual host IP address.,0.0,0.103,0.897,0.34
Datadog,44867146,44855220,2,"2017/07/02, 06:15:59",False,"2017/07/04, 10:35:08",9153.0,172265.0,0,"If you agent is written by yourself, you can open and listen on a Unix domain socket and let the other pod send data through it.",0.0,0.0,1.0,0.0
Datadog,44867146,44855220,2,"2017/07/02, 06:15:59",False,"2017/07/04, 10:35:08",9153.0,172265.0,0,"If not, you can write a small data proxy that listens on a Unix socket for data.",0.0,0.0,1.0,0.0
Datadog,44867146,44855220,2,"2017/07/02, 06:15:59",False,"2017/07/04, 10:35:08",9153.0,172265.0,0,"On the other end, by sharing a pod with the daemon, you can easily send data to the local container",0.0,0.234,0.766,0.6369
Datadog,44927321,44855220,3,"2017/07/05, 16:14:08",True,"2017/07/05, 16:14:08",1123.0,1595197.0,6,"I use the exact same setup,  dd-agent  running as a DaemonSet in my kubernetes cluster.",0.0,0.0,1.0,0.0
Datadog,44927321,44855220,3,"2017/07/05, 16:14:08",True,"2017/07/05, 16:14:08",1123.0,1595197.0,6,"Using the same port mapping you commented  here , you can just send metrics to the hostname of the node an application is running on.",0.0,0.0,1.0,0.0
Datadog,44927321,44855220,3,"2017/07/05, 16:14:08",True,"2017/07/05, 16:14:08",1123.0,1595197.0,6,You can add the node name to the pods environment using the downward api in your pod spec:,0.0,0.0,1.0,0.0
Datadog,44927321,44855220,3,"2017/07/05, 16:14:08",True,"2017/07/05, 16:14:08",1123.0,1595197.0,6,"Then, you can just open an UDP connection to  ${NODE_NAME}:8125  to connect to the datadog agent.",0.0,0.0,1.0,0.0
Datadog,59890601,59881685,0,"2020/01/24, 07:03:49",False,"2020/01/24, 07:03:49",390.0,1467579.0,1,There are a number of different ways you could handle this:,0.0,0.126,0.874,0.0772
Datadog,59890601,59881685,0,"2020/01/24, 07:03:49",False,"2020/01/24, 07:03:49",390.0,1467579.0,1,These are just some of your options.,0.0,0.0,1.0,0.0
Datadog,59890601,59881685,0,"2020/01/24, 07:03:49",False,"2020/01/24, 07:03:49",390.0,1467579.0,1,Since the Airflow webserver is just a Flask app you can really expose metrics in whatever way you see fit.,0.088,0.117,0.795,0.1548
Datadog,63754701,59881685,0,"2020/09/05, 16:57:34",False,"2020/09/05, 18:28:32",65.0,11246025.0,0,"As I understand, you can monitor running tasks in DAGs using DataDog, refer the integration with Airflow  docs",0.0,0.0,1.0,0.0
Datadog,63754701,59881685,0,"2020/09/05, 16:57:34",False,"2020/09/05, 18:28:32",65.0,11246025.0,0,You may refer metrics via DogStatD  docs .,0.0,0.0,1.0,0.0
Datadog,63754701,59881685,0,"2020/09/05, 16:57:34",False,"2020/09/05, 18:28:32",65.0,11246025.0,0,"Also, look at this  page  would be useful to understand what to monitor.",0.0,0.195,0.805,0.4404
Datadog,63754701,59881685,0,"2020/09/05, 16:57:34",False,"2020/09/05, 18:28:32",65.0,11246025.0,0,"E.g., the metrics as below:",0.0,0.0,1.0,0.0
Datadog,65918401,59879331,0,"2021/01/27, 13:40:25",False,"2021/01/27, 13:40:25",2115.0,2247740.0,0,HikariCP is a connection pool and JDBC is the API for managing a connection.,0.0,0.0,1.0,0.0
Datadog,65918401,59879331,0,"2021/01/27, 13:40:25",False,"2021/01/27, 13:40:25",2115.0,2247740.0,0,So it can be thought that Spring thinks about separating connection-pool-manager metrics from connection metrics.,0.0,0.0,1.0,0.0
Datadog,59679096,59678757,7,"2020/01/10, 11:44:26",True,"2020/01/10, 12:04:43",39071.0,2291321.0,3,You need to set the  aws_ecs_task_definition 's  network_mode  to  awsvpc  if you are defining the  network_configuration  of the service that uses that task definition.,0.0,0.0,1.0,0.0
Datadog,59679096,59678757,7,"2020/01/10, 11:44:26",True,"2020/01/10, 12:04:43",39071.0,2291321.0,3,This is mentioned in the  documentation for the  network_configuration  parameter of the  aws_ecs_service  resource :,0.0,0.0,1.0,0.0
Datadog,59679096,59678757,7,"2020/01/10, 11:44:26",True,"2020/01/10, 12:04:43",39071.0,2291321.0,3,"network_configuration  - (Optional) The network configuration for the
  service.",0.0,0.0,1.0,0.0
Datadog,59679096,59678757,7,"2020/01/10, 11:44:26",True,"2020/01/10, 12:04:43",39071.0,2291321.0,3,"This parameter is required for task definitions that use the
   awsvpc  network mode to receive their own Elastic Network Interface,
  and it is not supported for other network modes.",0.065,0.0,0.935,-0.2411
Datadog,59679096,59678757,7,"2020/01/10, 11:44:26",True,"2020/01/10, 12:04:43",39071.0,2291321.0,3,In your case you've added the  network_mode  parameter to the  container  definition instead of the  task  definition (a task is a collection of n containers and are grouped together to share some resources).,0.0,0.068,0.932,0.296
Datadog,59679096,59678757,7,"2020/01/10, 11:44:26",True,"2020/01/10, 12:04:43",39071.0,2291321.0,3,The  container definition schema  doesn't allow for a  network_mode  parameter.,0.172,0.0,0.828,-0.1695
Datadog,52935718,52934832,1,"2018/10/22, 21:35:34",False,"2018/10/22, 21:35:34",922.0,8622323.0,2,You can use a conditional with count to override if a resource is to be created.,0.0,0.133,0.867,0.25
Datadog,52935718,52934832,1,"2018/10/22, 21:35:34",False,"2018/10/22, 21:35:34",922.0,8622323.0,2,The example below will only create the resource when the variable environment is not = production.,0.0,0.13,0.87,0.2732
Datadog,52935718,52934832,1,"2018/10/22, 21:35:34",False,"2018/10/22, 21:35:34",922.0,8622323.0,2,"If Count = 0 then the resource won't be created,",0.199,0.0,0.801,-0.1877
Datadog,52935718,52934832,1,"2018/10/22, 21:35:34",False,"2018/10/22, 21:35:34",922.0,8622323.0,2,"Regards,",0.0,0.0,1.0,0.0
Datadog,48836870,48807945,0,"2018/02/17, 02:52:20",False,"2018/02/17, 02:52:20",3131.0,1314028.0,-1,I don't think there is something that does this for you automatically.,0.0,0.0,1.0,0.0
Datadog,48836870,48807945,0,"2018/02/17, 02:52:20",False,"2018/02/17, 02:52:20",3131.0,1314028.0,-1,You have to reset the counter yourself at each reporting interval.,0.0,0.0,1.0,0.0
Datadog,48836870,48807945,0,"2018/02/17, 02:52:20",False,"2018/02/17, 02:52:20",3131.0,1314028.0,-1,Something like this should work:,0.0,0.385,0.615,0.3612
Datadog,45324581,45324189,1,"2017/07/26, 13:41:18",False,"2017/07/26, 13:41:18",173.0,8047168.0,6,"Actually, It is quite simple.",0.0,0.0,1.0,0.0
Datadog,45324581,45324189,1,"2017/07/26, 13:41:18",False,"2017/07/26, 13:41:18",173.0,8047168.0,6,This is called  Packaging namespace packages .,0.0,0.0,1.0,0.0
Datadog,45324581,45324189,1,"2017/07/26, 13:41:18",False,"2017/07/26, 13:41:18",173.0,8047168.0,6,https://packaging.python.org/guides/packaging-namespace-packages/,0.0,0.0,1.0,0.0
Datadog,45324581,45324189,1,"2017/07/26, 13:41:18",False,"2017/07/26, 13:41:18",173.0,8047168.0,6,All you need is to separate all packages to sub - packages and after install it with a namespace.,0.0,0.0,1.0,0.0
Datadog,42717056,42708258,1,"2017/03/10, 13:15:07",False,"2017/03/10, 13:15:07",136.0,3403240.0,0,2 questions that'll be helpful:,0.0,0.483,0.517,0.4215
Datadog,42717056,42708258,1,"2017/03/10, 13:15:07",False,"2017/03/10, 13:15:07",136.0,3403240.0,0,Now some clarification on where I think you're going wrong here:,0.256,0.0,0.744,-0.4767
Datadog,42718833,42708258,2,"2017/03/10, 14:48:54",False,"2017/03/10, 14:57:25",761.0,2521248.0,0,I suggest tracing the problematic query to see what cassandra was doing.,0.225,0.0,0.775,-0.4404
Datadog,42718833,42708258,2,"2017/03/10, 14:48:54",False,"2017/03/10, 14:57:25",761.0,2521248.0,0,https://docs.datastax.com/en/cql/3.1/cql/cql_reference/tracing_r.html,0.0,0.0,1.0,0.0
Datadog,42718833,42708258,2,"2017/03/10, 14:48:54",False,"2017/03/10, 14:57:25",761.0,2521248.0,0,"Open cql shell, type  TRACING ON  and execute your query.",0.0,0.0,1.0,0.0
Datadog,42718833,42708258,2,"2017/03/10, 14:48:54",False,"2017/03/10, 14:57:25",761.0,2521248.0,0,"If everything seems fine, there is a chance that this problem happens occasionally, in which case I'd suggest tracing the queries using nodetool settraceprobablilty for some time, until you manage to catch the problem.",0.141,0.099,0.759,-0.3818
Datadog,42718833,42708258,2,"2017/03/10, 14:48:54",False,"2017/03/10, 14:57:25",761.0,2521248.0,0,You enable it on each node separately using  nodetool settraceprobability &lt;param&gt;  where param is the probability (between 0 and 1) that the query will get traced.,0.0,0.0,1.0,0.0
Datadog,42718833,42708258,2,"2017/03/10, 14:48:54",False,"2017/03/10, 14:57:25",761.0,2521248.0,0,"Careful: this WILL cause increased load, so start with a very low number and go up.",0.128,0.283,0.589,0.2228
Datadog,42718833,42708258,2,"2017/03/10, 14:48:54",False,"2017/03/10, 14:57:25",761.0,2521248.0,0,"If this problem is occasional there is a chance that this might be caused by long garbage collections, in which case you need to analyse the GC logs.",0.091,0.067,0.842,-0.1779
Datadog,42718833,42708258,2,"2017/03/10, 14:48:54",False,"2017/03/10, 14:57:25",761.0,2521248.0,0,Check how long your GC's are.,0.0,0.0,1.0,0.0
Datadog,42718833,42708258,2,"2017/03/10, 14:48:54",False,"2017/03/10, 14:57:25",761.0,2521248.0,0,"edit: just to be clear, if this problem is caused by GC's you will NOT see it with tracing.",0.121,0.117,0.762,-0.0258
Datadog,42718833,42708258,2,"2017/03/10, 14:48:54",False,"2017/03/10, 14:57:25",761.0,2521248.0,0,"So first check your GC's, and if its not the problem then move on to tracing.",0.0,0.131,0.869,0.3089
Datadog,28831358,28436960,0,"2015/03/03, 14:03:18",False,"2015/03/03, 14:03:18",579.0,4531116.0,1,Here you can find how to add a new webhook to your mandrill account:  https://mandrillapp.com/api/docs/webhooks.php.html#method=add,0.0,0.0,1.0,0.0
Datadog,28831358,28436960,0,"2015/03/03, 14:03:18",False,"2015/03/03, 14:03:18",579.0,4531116.0,1,"tha main thing here is this:
 $url = 'http://example/webhook-url'; 
this is your webhook URL what will process the data sent by mandrill and forward the information to Datadog.",0.0,0.0,1.0,0.0
Datadog,28831358,28436960,0,"2015/03/03, 14:03:18",False,"2015/03/03, 14:03:18",579.0,4531116.0,1,and this is a description about what mandrill will send to your webhook URL:  http://help.mandrill.com/entries/21738186-Introduction-to-Webhooks,0.0,0.0,1.0,0.0
Datadog,28835739,28436960,0,"2015/03/03, 17:35:33",False,"2015/03/03, 17:35:33",516.0,1988301.0,1,a listener for webhooks is nothing else then a website/app which triggers an action if a request comes in.,0.0,0.0,1.0,0.0
Datadog,28835739,28436960,0,"2015/03/03, 17:35:33",False,"2015/03/03, 17:35:33",516.0,1988301.0,1,Usually you keep it secret or secure it with (http basic) authentication.,0.0,0.179,0.821,0.34
Datadog,28835739,28436960,0,"2015/03/03, 17:35:33",False,"2015/03/03, 17:35:33",516.0,1988301.0,1,E.g.,0.0,0.0,1.0,0.0
Datadog,28835739,28436960,0,"2015/03/03, 17:35:33",False,"2015/03/03, 17:35:33",516.0,1988301.0,1,create a website called  http://yourdomain.com/hooklistener.php .,0.0,0.412,0.588,0.2732
Datadog,28835739,28436960,0,"2015/03/03, 17:35:33",False,"2015/03/03, 17:35:33",516.0,1988301.0,1,You can then call it with HTTP POST or GET and pass some data like hooklistener.php?event=triggerDataDog or with POST and send data along with the body.,0.0,0.091,0.909,0.3612
Datadog,28835739,28436960,0,"2015/03/03, 17:35:33",False,"2015/03/03, 17:35:33",516.0,1988301.0,1,You then run a script or anything you want to process that event.,0.0,0.106,0.894,0.0772
Datadog,28919358,28436960,0,"2015/03/07, 22:02:50",False,"2015/03/07, 22:02:50",544.0,1428713.0,0,"A ""listener"" is just any URL that you host where you can receive data that is posted to it.",0.0,0.0,1.0,0.0
Datadog,28919358,28436960,0,"2015/03/07, 22:02:50",False,"2015/03/07, 22:02:50",544.0,1428713.0,0,"Keep in mind, since you mentioned Zapier, you can set up a trigger that receives the webhook data - in this case the listener URL is provided by Zapier, and you can then send that data into any application (or even post to another webhook).",0.0,0.0,1.0,0.0
Datadog,28919358,28436960,0,"2015/03/07, 22:02:50",False,"2015/03/07, 22:02:50",544.0,1428713.0,0,Using Zapier is nice because it doesn't require you to write the listener code that receives the hook data and does something with it.,0.0,0.109,0.891,0.4215
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,I believe that Amazon actually offers a service that would accomplish your goal -  CloudWatch   (pricing) .,0.0,0.29,0.71,0.5423
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,I'm going to take your points one by one.,0.0,0.0,1.0,0.0
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,"Note that I haven't actually  used  it before, but the documentation is fairly clear.",0.0,0.221,0.779,0.5267
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,One server had high CPU for over 5 mins when the alert should be triggered after 1 minute,0.0,0.128,0.872,0.296
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,It looks like CloudWatch can be configured to send an alert (which I'll get to) after one minute of a condition being met:,0.0,0.19,0.81,0.5719
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,"One can actually set conditions for many other metrics as well - this is what I see on one of my instances, and I think that detailed monitoring (I use free), might have even more:",0.0,0.063,0.937,0.2732
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,What else is out there that can do the same job and  will also integrate with Pager Duty?,0.0,0.0,1.0,0.0
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,I'm assuming you're talking about  this .,0.0,0.0,1.0,0.0
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,It turns out the Pager Duty has a  helpful guide  just for integrating CloudWatch.,0.0,0.189,0.811,0.4215
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,How nice!,0.0,0.756,0.244,0.4753
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,"Here's the pricing page , as you would probably like to parse it instead of me telling you.",0.0,0.135,0.865,0.3612
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,"I'll give a brief overview, though:",0.0,0.0,1.0,0.0
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,"You don't want basic monitoring, as it only gives you metrics once per five minutes (which you've indicated is unacceptable.)",0.06,0.0,0.94,-0.0572
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,"Instead, you want detailed monitoring (once every minute).",0.0,0.157,0.843,0.0772
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,"For an EC2 instance, the price for detailed monitoring is $3.50 per instance  per month .",0.0,0.0,1.0,0.0
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,"Additionally, every alarm you make is $0.10 per month.",0.231,0.0,0.769,-0.34
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,This is actually very cheap if compared to  CopperEgg's pricing  - $70/mo versus  maybe  $30 per month for 9 instances and copious amounts of alarms.,0.087,0.0,0.913,-0.2732
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,"In reality, you'll probably be paying more like $10/mo.",0.0,0.259,0.741,0.4201
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,"Pager Duty's tutorial suggests you use SNS, which is another cost.",0.0,0.0,1.0,0.0
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,The good thing:  it's dirt cheap .,0.258,0.312,0.43,0.128
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,$0.60 per million notifications.,0.0,0.0,1.0,0.0
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,"If you ever get above a dollar in a year for SNS, you need to perform some serious reliability improvements on your servers.",0.058,0.102,0.841,0.25
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,You're not just limited to Amazon's pre-packaged metrics!,0.0,0.219,0.781,0.2401
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,"You can actually send custom metrics (time it took to complete a cronjob, whatever) to Cloudwatch via a PUT request.",0.0,0.0,1.0,0.0
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,Quite handy.,0.0,0.0,1.0,0.0
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,Submit Custom Metrics generated by your own applications (or by AWS resources not mentioned above) and have them monitored by Amazon CloudWatch.,0.0,0.075,0.925,0.1779
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,You can submit these metrics to Amazon CloudWatch via a simple Put API request.,0.0,0.124,0.876,0.1779
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,(from  here ),0.0,0.0,1.0,0.0
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180.0,1849664.0,7,"So all in all: CloudWatch is quite cheap, can do 1-minute frequency stats, and will integrate with Pager Duty.",0.0,0.0,1.0,0.0
Datadog,29150848,21527387,0,"2015/03/19, 19:10:30",False,"2015/03/19, 19:10:30",7448.0,554903.0,3,In short Server Density is a monitoring tool that will monitor all the relevant server metrics.,0.0,0.0,1.0,0.0
Datadog,29150848,21527387,0,"2015/03/19, 19:10:30",False,"2015/03/19, 19:10:30",7448.0,554903.0,3,You can take a look at this page  where it’s all described .,0.0,0.0,1.0,0.0
Datadog,29150848,21527387,0,"2015/03/19, 19:10:30",False,"2015/03/19, 19:10:30",7448.0,554903.0,3,One server had high CPU for over 5 mins when the alert should be triggered after 1 minute,0.0,0.128,0.872,0.296
Datadog,29150848,21527387,0,"2015/03/19, 19:10:30",False,"2015/03/19, 19:10:30",7448.0,554903.0,3,Server Density’s open source agent collects and posts the data to their server every minute and you can decide yourself when that alert should be triggered.,0.0,0.081,0.919,0.296
Datadog,29150848,21527387,0,"2015/03/19, 19:10:30",False,"2015/03/19, 19:10:30",7448.0,554903.0,3,In the alert below you can see that the alert will alert 1 person after 1 minute and then repeatedly alert every 5 minutes.,0.0,0.341,0.659,0.7783
Datadog,29150848,21527387,0,"2015/03/19, 19:10:30",False,"2015/03/19, 19:10:30",7448.0,554903.0,3,There is a lot of other metrics that you can alert on too.,0.0,0.167,0.833,0.296
Datadog,29150848,21527387,0,"2015/03/19, 19:10:30",False,"2015/03/19, 19:10:30",7448.0,554903.0,3,What else is out there that can do the same job and will also integrate with Pager Duty?,0.0,0.0,1.0,0.0
Datadog,29150848,21527387,0,"2015/03/19, 19:10:30",False,"2015/03/19, 19:10:30",7448.0,554903.0,3,Server Density also integrates with PagerDuty.,0.0,0.0,1.0,0.0
Datadog,29150848,21527387,0,"2015/03/19, 19:10:30",False,"2015/03/19, 19:10:30",7448.0,554903.0,3,The only thing you need to do is to  generate an api key at PagerDuty  and then provide that in the settings.,0.0,0.0,1.0,0.0
Datadog,29150848,21527387,0,"2015/03/19, 19:10:30",False,"2015/03/19, 19:10:30",7448.0,554903.0,3,Just provide the API key in the settings and you can then in check pagerduty as one of the alert recipients.,0.0,0.099,0.901,0.296
Datadog,29150848,21527387,0,"2015/03/19, 19:10:30",False,"2015/03/19, 19:10:30",7448.0,554903.0,3,You can find the  pricing page here .,0.0,0.0,1.0,0.0
Datadog,29150848,21527387,0,"2015/03/19, 19:10:30",False,"2015/03/19, 19:10:30",7448.0,554903.0,3,I’ll give you a brief overview of it.,0.0,0.0,1.0,0.0
Datadog,29150848,21527387,0,"2015/03/19, 19:10:30",False,"2015/03/19, 19:10:30",7448.0,554903.0,3,The pricing starts at $10 for one server plus one web check and then get’s cheaper per server the more servers you add.,0.0,0.0,1.0,0.0
Datadog,29150848,21527387,0,"2015/03/19, 19:10:30",False,"2015/03/19, 19:10:30",7448.0,554903.0,3,"Everything will be monitored once every minute and there is no fees added for the amount of alerts added or triggered, even if that is an SMS to your phone number.",0.068,0.04,0.892,-0.2263
Datadog,29150848,21527387,0,"2015/03/19, 19:10:30",False,"2015/03/19, 19:10:30",7448.0,554903.0,3,"The cost is slightly more expensive than the Cloudwatch example, but the support is good.",0.0,0.363,0.637,0.8126
Datadog,29150848,21527387,0,"2015/03/19, 19:10:30",False,"2015/03/19, 19:10:30",7448.0,554903.0,3,If you used copperegg before they have a  migration tool  too.,0.0,0.0,1.0,0.0
Datadog,29150848,21527387,0,"2015/03/19, 19:10:30",False,"2015/03/19, 19:10:30",7448.0,554903.0,3,Server Density allows you to monitor all the things!,0.0,0.0,1.0,0.0
Datadog,29150848,21527387,0,"2015/03/19, 19:10:30",False,"2015/03/19, 19:10:30",7448.0,554903.0,3,Then only thing you need to do is to send us custom metrics which you can do with a plugin written by yourself or by someone else.,0.0,0.0,1.0,0.0
Datadog,29150848,21527387,0,"2015/03/19, 19:10:30",False,"2015/03/19, 19:10:30",7448.0,554903.0,3,I have to say that the graphs that Server Density provides is somewhat akin to eye candy too.,0.0,0.0,1.0,0.0
Datadog,29150848,21527387,0,"2015/03/19, 19:10:30",False,"2015/03/19, 19:10:30",7448.0,554903.0,3,Most other monitoring solutions I’ve seen out there have quite dull dashboards.,0.2,0.131,0.669,-0.2568
Datadog,29150848,21527387,0,"2015/03/19, 19:10:30",False,"2015/03/19, 19:10:30",7448.0,554903.0,3,It will do the job for you.,0.0,0.0,1.0,0.0
Datadog,29150848,21527387,0,"2015/03/19, 19:10:30",False,"2015/03/19, 19:10:30",7448.0,554903.0,3,"Not as cheap as CloudWatch, but doesn’t lock you in into AWS.",0.0,0.0,1.0,0.0
Datadog,29150848,21527387,0,"2015/03/19, 19:10:30",False,"2015/03/19, 19:10:30",7448.0,554903.0,3,It’ll give you 1 minute frequency metrics and integrate with pagerduty + a lot more stuff.,0.0,0.0,1.0,0.0
Datadog,62818348,62818107,3,"2020/07/09, 18:25:43",True,"2020/07/13, 17:33:38",6777.0,7875623.0,3,Reference,0.0,0.0,1.0,0.0
Datadog,62818348,62818107,3,"2020/07/09, 18:25:43",True,"2020/07/13, 17:33:38",6777.0,7875623.0,3,https://people.apache.org/~dkulp/camel/eventnotifier-to-log-details-about-all-sent-exchanges.html,0.0,0.0,1.0,0.0
Datadog,62818348,62818107,3,"2020/07/09, 18:25:43",True,"2020/07/13, 17:33:38",6777.0,7875623.0,3,Update,0.0,0.0,1.0,0.0
Datadog,62818348,62818107,3,"2020/07/09, 18:25:43",True,"2020/07/13, 17:33:38",6777.0,7875623.0,3,"The  Exchange.CREATED_TIMESTAMP  is no longer stored as exchange property, but you should use the  getCreated  method on Exchange.",0.086,0.0,0.914,-0.1531
Datadog,63516418,62298190,0,"2020/08/21, 07:12:56",False,"2020/08/21, 07:12:56",11.0,4392334.0,1,"If you put in ECS Task Definition (sample from json version, but in UI also possible to setup), you should be able to configure container logs:",0.0,0.0,1.0,0.0
Datadog,62069822,62056153,2,"2020/05/28, 19:31:43",False,"2020/05/28, 19:38:33",1119.0,6490744.0,0,Since your question says  is there a way to inspect inside/after each task completes  - I'm assuming you haven't tried this celery-result-backend stuff.,0.0,0.0,1.0,0.0
Datadog,62069822,62056153,2,"2020/05/28, 19:31:43",False,"2020/05/28, 19:38:33",1119.0,6490744.0,0,So you could check out this feature which is provided by Celery itself :  Celery-Result-Backend / Task-result-Backend  .,0.0,0.0,1.0,0.0
Datadog,62069822,62056153,2,"2020/05/28, 19:31:43",False,"2020/05/28, 19:38:33",1119.0,6490744.0,0,It is very useful for storing results of your celery tasks.,0.0,0.242,0.758,0.4927
Datadog,62069822,62056153,2,"2020/05/28, 19:31:43",False,"2020/05/28, 19:38:33",1119.0,6490744.0,0,Read through this =   https://docs.celeryproject.org/en/stable/userguide/configuration.html#task-result-backend-settings,0.0,0.0,1.0,0.0
Datadog,62069822,62056153,2,"2020/05/28, 19:31:43",False,"2020/05/28, 19:38:33",1119.0,6490744.0,0,"Once you get an idea of how to setup this result-backend, Search for  result_extended  key (in the same link) to be able to add  queue-names  in your task return values.",0.0,0.085,0.915,0.4019
Datadog,62069822,62056153,2,"2020/05/28, 19:31:43",False,"2020/05/28, 19:38:33",1119.0,6490744.0,0,Number of options are available - Like you can setup these results to go to any of these :,0.0,0.202,0.798,0.4215
Datadog,62069822,62056153,2,"2020/05/28, 19:31:43",False,"2020/05/28, 19:38:33",1119.0,6490744.0,0,I have made use of this  Result-Backend  feature with  Elasticsearch  and this how my task results are stored :,0.0,0.0,1.0,0.0
Datadog,62069822,62056153,2,"2020/05/28, 19:31:43",False,"2020/05/28, 19:38:33",1119.0,6490744.0,0,It is just a matter of adding few configurations in  settings.py  file as per your requirements.,0.0,0.073,0.927,0.0258
Datadog,62069822,62056153,2,"2020/05/28, 19:31:43",False,"2020/05/28, 19:38:33",1119.0,6490744.0,0,Worked really well for my application.,0.0,0.324,0.676,0.3384
Datadog,62069822,62056153,2,"2020/05/28, 19:31:43",False,"2020/05/28, 19:38:33",1119.0,6490744.0,0,And I have a weekly cron that clears only  successful results  of tasks - since we don't need the results anymore - and I can see only  failed results   (like the one in image).,0.096,0.148,0.756,0.2023
Datadog,62069822,62056153,2,"2020/05/28, 19:31:43",False,"2020/05/28, 19:38:33",1119.0,6490744.0,0,These were main keys for my requirement :  task_track_started  and  task_acks_late  along with  result_backend,0.0,0.0,1.0,0.0
Datadog,60517002,60516923,1,"2020/03/04, 01:27:31",True,"2020/03/04, 01:27:31",25945.0,11923999.0,1,Try  recover  to catch all panics and log them.,0.266,0.0,0.734,-0.4404
Datadog,60517002,60516923,1,"2020/03/04, 01:27:31",True,"2020/03/04, 01:27:31",25945.0,11923999.0,1,"Without that, it'll write the panic msg to stderr:",0.292,0.0,0.708,-0.5106
Datadog,59605166,59549130,2,"2020/01/06, 01:37:29",True,"2020/01/06, 01:37:29",5837.0,9090751.0,3,You can override the default  TracingInstrumentation  with your own implementation.,0.0,0.0,1.0,0.0
Datadog,59605166,59549130,2,"2020/01/06, 01:37:29",True,"2020/01/06, 01:37:29",5837.0,9090751.0,3,It will be picked automatically due to the @ConditionalOnMissingBean annotation in the  GraphQLInstrumentationAutoConfiguration  class.,0.0,0.0,1.0,0.0
Datadog,59605166,59549130,2,"2020/01/06, 01:37:29",True,"2020/01/06, 01:37:29",5837.0,9090751.0,3,Here is a simple example that adds two custom metrics:  graphql.counter.query.success  and  graphql.counter.query.error :,0.0,0.0,1.0,0.0
Datadog,59605166,59549130,2,"2020/01/06, 01:37:29",True,"2020/01/06, 01:37:29",5837.0,9090751.0,3,"My application.yaml, just in case:",0.0,0.0,1.0,0.0
Datadog,59605166,59549130,2,"2020/01/06, 01:37:29",True,"2020/01/06, 01:37:29",5837.0,9090751.0,3,"I'm using spring-boot-starter-parent:2.2.2.RELEASE, graphql-spring-boot-starter:6.0.0",0.0,0.0,1.0,0.0
Datadog,59605166,59549130,2,"2020/01/06, 01:37:29",True,"2020/01/06, 01:37:29",5837.0,9090751.0,3,I hope it helps.,0.0,0.846,0.154,0.6705
Datadog,66324527,59102943,0,"2021/02/23, 00:44:33",False,"2021/02/23, 00:44:33",1.0,15263108.0,0,Avg CPU usage may not give better view.,0.256,0.0,0.744,-0.3412
Datadog,66324527,59102943,0,"2021/02/23, 00:44:33",False,"2021/02/23, 00:44:33",1.0,15263108.0,0,Check if max CPU utilization is getting around 100%.,0.0,0.0,1.0,0.0
Datadog,66324527,59102943,0,"2021/02/23, 00:44:33",False,"2021/02/23, 00:44:33",1.0,15263108.0,0,"If so, you may need to optimize on ES side.",0.0,0.262,0.738,0.4939
Datadog,55834007,55832300,2,"2019/04/24, 18:48:15",False,"2019/04/24, 18:48:15",5656.0,11374921.0,1,You should understand how cluster autoscaler works.,0.0,0.0,1.0,0.0
Datadog,55834007,55832300,2,"2019/04/24, 18:48:15",False,"2019/04/24, 18:48:15",5656.0,11374921.0,1,It is responsible  only  for adding or removing nodes.,0.0,0.223,0.777,0.3182
Datadog,55834007,55832300,2,"2019/04/24, 18:48:15",False,"2019/04/24, 18:48:15",5656.0,11374921.0,1,It is not responsible for creating or destroying pods.,0.554,0.0,0.446,-0.7543
Datadog,55834007,55832300,2,"2019/04/24, 18:48:15",False,"2019/04/24, 18:48:15",5656.0,11374921.0,1,So in your case cluster autoscaler is not doing anything because it's useless.,0.189,0.0,0.811,-0.4215
Datadog,55834007,55832300,2,"2019/04/24, 18:48:15",False,"2019/04/24, 18:48:15",5656.0,11374921.0,1,Even if you add one more node - there will be still a requirement to run DaemonSet pods on nodes where is not enough CPU.,0.0,0.0,1.0,0.0
Datadog,55834007,55832300,2,"2019/04/24, 18:48:15",False,"2019/04/24, 18:48:15",5656.0,11374921.0,1,That's why it is not adding nodes.,0.0,0.0,1.0,0.0
Datadog,55834007,55832300,2,"2019/04/24, 18:48:15",False,"2019/04/24, 18:48:15",5656.0,11374921.0,1,What you should do is to manually remove some pods from occupied nodes.,0.0,0.0,1.0,0.0
Datadog,55834007,55832300,2,"2019/04/24, 18:48:15",False,"2019/04/24, 18:48:15",5656.0,11374921.0,1,Then it will be able to schedule DaemonSet pods.,0.0,0.0,1.0,0.0
Datadog,55834007,55832300,2,"2019/04/24, 18:48:15",False,"2019/04/24, 18:48:15",5656.0,11374921.0,1,"Alternatively you can reduce CPU requests of Datadog to, for example, 100m or 50m.",0.0,0.0,1.0,0.0
Datadog,55834007,55832300,2,"2019/04/24, 18:48:15",False,"2019/04/24, 18:48:15",5656.0,11374921.0,1,This should be enough to start those pods.,0.0,0.0,1.0,0.0
Datadog,57509622,55832300,3,"2019/08/15, 15:24:55",True,"2019/08/19, 13:40:01",1425.0,1837991.0,4,You can add priorityClassName to point to a high priority PriorityClass to your DaemonSet.,0.0,0.0,1.0,0.0
Datadog,57509622,55832300,3,"2019/08/15, 15:24:55",True,"2019/08/19, 13:40:01",1425.0,1837991.0,4,Kubernetes will then remove other pods in order to run the DaemonSet's pods.,0.0,0.0,1.0,0.0
Datadog,57509622,55832300,3,"2019/08/15, 15:24:55",True,"2019/08/19, 13:40:01",1425.0,1837991.0,4,"If that results in unschedulable pods, cluster-autoscaler should add a node to schedule them on.",0.0,0.0,1.0,0.0
Datadog,57509622,55832300,3,"2019/08/15, 15:24:55",True,"2019/08/19, 13:40:01",1425.0,1837991.0,4,"See  the docs  (Most examples based on that) (For some pre-1.14 versions, the apiVersion is likely a beta (1.11-1.13) or alpha version (1.8 - 1.10) instead)",0.0,0.0,1.0,0.0
Datadog,57509622,55832300,3,"2019/08/15, 15:24:55",True,"2019/08/19, 13:40:01",1425.0,1837991.0,4,Apply it to your workload,0.0,0.0,1.0,0.0
Datadog,54660782,54660692,0,"2019/02/13, 02:35:57",True,"2019/02/13, 03:15:24",185548.0,152948.0,1,Here are two ways that work:,0.0,0.0,1.0,0.0
Datadog,54660782,54660692,0,"2019/02/13, 02:35:57",True,"2019/02/13, 03:15:24",185548.0,152948.0,1,1.,0.0,0.0,1.0,0.0
Datadog,54660782,54660692,0,"2019/02/13, 02:35:57",True,"2019/02/13, 03:15:24",185548.0,152948.0,1,"(Drill down to  .services , remember it as  $services  for later use, get the list of keys, and select the ones such that the corresponding value in  $services  has a  build  key).",0.0,0.076,0.924,0.34
Datadog,54660782,54660692,0,"2019/02/13, 02:35:57",True,"2019/02/13, 03:15:24",185548.0,152948.0,1,2.,0.0,0.0,1.0,0.0
Datadog,54660782,54660692,0,"2019/02/13, 02:35:57",True,"2019/02/13, 03:15:24",185548.0,152948.0,1,"(Drill down to  .services , convert to a list of  {""key"": ..., ""value"": ...}  objects, select the ones where the  .value  has a  build  key, and return the  .key  for each).",0.0,0.082,0.918,0.34
Datadog,54660782,54660692,0,"2019/02/13, 02:35:57",True,"2019/02/13, 03:15:24",185548.0,152948.0,1,"The second is probably more idiomatic jq, but the first provides an interesting way to think about the problem as well.",0.128,0.223,0.649,0.3919
Datadog,54660903,54660692,0,"2019/02/13, 02:51:04",False,"2019/02/13, 08:06:26",70657.0,997358.0,2,"Here's a third approach, notable for being oblivious to the upper reaches:",0.0,0.107,0.893,0.0516
Datadog,53328483,53323761,1,"2018/11/15, 23:58:41",False,"2018/11/15, 23:58:41",1283.0,10657880.0,0,Have you tried to mock the  datalog  module inside your function  test ?,0.203,0.0,0.797,-0.4215
Datadog,53328483,53323761,1,"2018/11/15, 23:58:41",False,"2018/11/15, 23:58:41",1283.0,10657880.0,0,"As long as your other scripts are not running concurrently with your test, this may work.",0.0,0.0,1.0,0.0
Datadog,53328483,53323761,1,"2018/11/15, 23:58:41",False,"2018/11/15, 23:58:41",1283.0,10657880.0,0,"That way the mock itself will be set only when the function is called, instead of being set in your script scope.",0.118,0.0,0.882,-0.4215
Datadog,53363259,53323761,0,"2018/11/18, 18:50:17",False,"2018/11/18, 18:50:17",1223.0,1939996.0,0,You could use  unittest.mock.patch .,0.0,0.0,1.0,0.0
Datadog,53363259,53323761,0,"2018/11/18, 18:50:17",False,"2018/11/18, 18:50:17",1223.0,1939996.0,0,If you are using pytest you can do the same with the  monkeypatch  fixture.,0.0,0.0,1.0,0.0
Datadog,52703220,52703024,0,"2018/10/08, 16:16:52",False,"2018/10/08, 16:16:52",9507.0,1312478.0,0,I have used many of solutions you mentioned.,0.0,0.221,0.779,0.1779
Datadog,52703220,52703024,0,"2018/10/08, 16:16:52",False,"2018/10/08, 16:16:52",9507.0,1312478.0,0,Splunk is good but it becomes really expensive if you have huge amount of data.,0.0,0.274,0.726,0.5994
Datadog,52703220,52703024,0,"2018/10/08, 16:16:52",False,"2018/10/08, 16:16:52",9507.0,1312478.0,0,You could have always used Cloudwatch Logs but it doesn't give you so much on visual part..,0.0,0.0,1.0,0.0
Datadog,52703220,52703024,0,"2018/10/08, 16:16:52",False,"2018/10/08, 16:16:52",9507.0,1312478.0,0,"I will recommend ELK (ElasticSearch, Logstash, Kibana) stack.",0.0,0.294,0.706,0.3612
Datadog,52703220,52703024,0,"2018/10/08, 16:16:52",False,"2018/10/08, 16:16:52",9507.0,1312478.0,0,It is a very standard solution; in which logs are stored in Elastic Search.,0.0,0.177,0.823,0.3774
Datadog,52703220,52703024,0,"2018/10/08, 16:16:52",False,"2018/10/08, 16:16:52",9507.0,1312478.0,0,Kibana is used for visualization of logs.,0.0,0.0,1.0,0.0
Datadog,52703220,52703024,0,"2018/10/08, 16:16:52",False,"2018/10/08, 16:16:52",9507.0,1312478.0,0,This works in almost real time.,0.0,0.0,1.0,0.0
Datadog,52703220,52703024,0,"2018/10/08, 16:16:52",False,"2018/10/08, 16:16:52",9507.0,1312478.0,0,If you have very specific dashboards; then you can always create custom dashboards using some front end technologies like AngularJS etc.,0.0,0.195,0.805,0.5574
Datadog,52703220,52703024,0,"2018/10/08, 16:16:52",False,"2018/10/08, 16:16:52",9507.0,1312478.0,0,but if visual part is really huge and very flexible then I feel ELK is better.,0.0,0.455,0.545,0.8758
Datadog,52703399,52703024,0,"2018/10/08, 16:27:05",False,"2018/10/08, 16:27:05",1508.0,1623047.0,0,"ELK (ElasticSearch, Logstash, Kibana) stack is a really good solution for what you are looking for, but in some cases ELK is not going to be able to get some metrics, in this case you have some solutions like create your own  beat  program to get the information or use another program to gather this metrics like Apache NiFi.",0.0,0.225,0.775,0.9199
Datadog,52703411,52703024,0,"2018/10/08, 16:27:49",False,"2018/10/08, 16:27:49",3533.0,6619626.0,1,"You can use AWS CloudWatch, create a log stream for each of your application or service.",0.0,0.13,0.87,0.2732
Datadog,52703411,52703024,0,"2018/10/08, 16:27:49",False,"2018/10/08, 16:27:49",3533.0,6619626.0,1,"Define your custom metrics, create a dashboard and alert.",0.0,0.417,0.583,0.5106
Datadog,52703411,52703024,0,"2018/10/08, 16:27:49",False,"2018/10/08, 16:27:49",3533.0,6619626.0,1,It's not limited to AWS things; you can use CloudWatch log agent for On-premises services or software on your local network.,0.0,0.077,0.923,0.1695
Datadog,52703411,52703024,0,"2018/10/08, 16:27:49",False,"2018/10/08, 16:27:49",3533.0,6619626.0,1,For more information read the following article by Jeff Barr,0.0,0.0,1.0,0.0
Datadog,52703411,52703024,0,"2018/10/08, 16:27:49",False,"2018/10/08, 16:27:49",3533.0,6619626.0,1,https://aws.amazon.com/blogs/aws/cloudwatch-log-service/,0.0,0.0,1.0,0.0
Datadog,52703411,52703024,0,"2018/10/08, 16:27:49",False,"2018/10/08, 16:27:49",3533.0,6619626.0,1,and,0.0,0.0,1.0,0.0
Datadog,52703411,52703024,0,"2018/10/08, 16:27:49",False,"2018/10/08, 16:27:49",3533.0,6619626.0,1,https://aws.amazon.com/blogs/aws/improvements-to-cloudwatch-logs-dashboards/,0.0,0.0,1.0,0.0
Datadog,52703411,52703024,0,"2018/10/08, 16:27:49",False,"2018/10/08, 16:27:49",3533.0,6619626.0,1,"FYI: we already monitor a lot of application and service inside and outside of AWS by CloudWatch, and it works like a charm.",0.0,0.301,0.699,0.7739
Datadog,52692334,52689144,5,"2018/10/07, 22:57:28",True,"2018/10/08, 13:07:16",3260.0,950762.0,0,It seems normal because  Ansible  does not refer to Python virtual environment in your case:,0.0,0.0,1.0,0.0
Datadog,52692334,52689144,5,"2018/10/07, 22:57:28",True,"2018/10/08, 13:07:16",3260.0,950762.0,0,In  virtualenv  non-installed packages are initialized from real system environment.,0.0,0.0,1.0,0.0
Datadog,52692334,52689144,5,"2018/10/07, 22:57:28",True,"2018/10/08, 13:07:16",3260.0,950762.0,0,So you can achieve it by setting up  Ansible  within  virtualenv,0.0,0.0,1.0,0.0
Datadog,52692334,52689144,5,"2018/10/07, 22:57:28",True,"2018/10/08, 13:07:16",3260.0,950762.0,0,Have a look at this example:,0.0,0.0,1.0,0.0
Datadog,52692334,52689144,5,"2018/10/07, 22:57:28",True,"2018/10/08, 13:07:16",3260.0,950762.0,0,After installation of  Ansible  in  virtualenv,0.0,0.0,1.0,0.0
Datadog,52692334,52689144,5,"2018/10/07, 22:57:28",True,"2018/10/08, 13:07:16",3260.0,950762.0,0,Ansible  refers to the paths of Python virtual environment:,0.0,0.0,1.0,0.0
Datadog,52692334,52689144,5,"2018/10/07, 22:57:28",True,"2018/10/08, 13:07:16",3260.0,950762.0,0,ps: Need to deactivate and activate again the  virtualenv  once to load the  Ansible  from virtual environment after the installation.,0.0,0.0,1.0,0.0
Datadog,52834860,51690297,0,"2018/10/16, 14:52:14",False,"2018/10/16, 14:52:14",160.0,9716385.0,2,To get you started: Create a timelion expression:,0.0,0.259,0.741,0.2732
Datadog,52959391,51690297,1,"2018/10/24, 03:10:30",False,"2018/10/24, 03:10:30",2567.0,7983309.0,4,What you are looking for is achievable using Visual Builder visualization,0.0,0.187,0.813,0.3182
Datadog,52959391,51690297,1,"2018/10/24, 03:10:30",False,"2018/10/24, 03:10:30",2567.0,7983309.0,4,See  https://www.elastic.co/guide/en/kibana/6.1/time-series-visualizations.html,0.0,0.0,1.0,0.0
Datadog,52959391,51690297,1,"2018/10/24, 03:10:30",False,"2018/10/24, 03:10:30",2567.0,7983309.0,4,Aggregate  Max  or  Avg  on  system.cpu.total.pct,0.0,0.0,1.0,0.0
Datadog,52959391,51690297,1,"2018/10/24, 03:10:30",False,"2018/10/24, 03:10:30",2567.0,7983309.0,4,Group By  Terms  By  beat.hostname.keyword,0.0,0.0,1.0,0.0
Datadog,52959391,51690297,1,"2018/10/24, 03:10:30",False,"2018/10/24, 03:10:30",2567.0,7983309.0,4,The visualization will show CPU usage in % for all hosts sending metrics to your cluster.,0.0,0.0,1.0,0.0
Datadog,52959391,51690297,1,"2018/10/24, 03:10:30",False,"2018/10/24, 03:10:30",2567.0,7983309.0,4,If you add more hosts those will show up too!,0.0,0.0,1.0,0.0
Datadog,55875339,51690297,0,"2019/04/27, 00:53:24",False,"2019/04/27, 00:53:24",12414.0,33204.0,0,This is just to illustrate @ben5556's answer with an image.,0.0,0.0,1.0,0.0
Datadog,55875339,51690297,0,"2019/04/27, 00:53:24",False,"2019/04/27, 00:53:24",12414.0,33204.0,0,"NOTE:  The ""Term"" is  beat.hostname .",0.0,0.0,1.0,0.0
Datadog,53874603,50794695,2,"2018/12/20, 21:06:35",False,"2018/12/20, 21:06:35",101.0,4771526.0,0,sorry for the delay.,0.643,0.0,0.357,-0.3818
Datadog,53874603,50794695,2,"2018/12/20, 21:06:35",False,"2018/12/20, 21:06:35",101.0,4771526.0,0,From your error log I can't see any issues on recovery being thrown but I either don't see any connection attempts.,0.093,0.0,0.907,-0.2144
Datadog,53874603,50794695,2,"2018/12/20, 21:06:35",False,"2018/12/20, 21:06:35",101.0,4771526.0,0,I wonder if you have some issues with data in the group replication relay logs...,0.0,0.0,1.0,0.0
Datadog,53874603,50794695,2,"2018/12/20, 21:06:35",False,"2018/12/20, 21:06:35",101.0,4771526.0,0,I suggest you open a bug if the problem still persists.,0.252,0.0,0.748,-0.4019
Datadog,53874603,50794695,2,"2018/12/20, 21:06:35",False,"2018/12/20, 21:06:35",101.0,4771526.0,0,"As a workaround you can try to reset the applier channel before ""START GROUP_REPLICATION""",0.0,0.0,1.0,0.0
Datadog,53874603,50794695,2,"2018/12/20, 21:06:35",False,"2018/12/20, 21:06:35",101.0,4771526.0,0,"RESET SLAVE ALL FOR CHANNEL ""group_replication_applier"";",0.0,0.0,1.0,0.0
Datadog,47014916,47013899,0,"2017/10/30, 14:10:13",False,"2017/10/30, 14:10:13",3148.0,7925197.0,0,The telegraf/influxdb/grafana stack can monitor space left on disc.,0.0,0.0,1.0,0.0
Datadog,47014916,47013899,0,"2017/10/30, 14:10:13",False,"2017/10/30, 14:10:13",3148.0,7925197.0,0,Kapacitor can also be added if you want alerts.,0.0,0.14,0.86,0.0772
Datadog,47014916,47013899,0,"2017/10/30, 14:10:13",False,"2017/10/30, 14:10:13",3148.0,7925197.0,0,"If you want to specify a limit, you have to use a dedicated partition / mount point or a btrfs subvolume with quotas.",0.0,0.202,0.798,0.5106
Datadog,47016362,47013899,0,"2017/10/30, 15:22:49",False,"2017/10/30, 15:22:49",21.0,7688163.0,0,"Another option is to make cron job to clean up unused docker images, unused docker volume, and exited docker container.",0.0,0.124,0.876,0.4019
Datadog,47016362,47013899,0,"2017/10/30, 15:22:49",False,"2017/10/30, 15:22:49",21.0,7688163.0,0,I use this method myself.,0.0,0.0,1.0,0.0
Datadog,46739062,46716705,0,"2017/10/14, 01:56:40",False,"2017/10/14, 01:56:40",3994.0,505196.0,0,A better approach than using DaemonSets to run your application would be to use a Deployment so that you don't tie your application to the number of nodes in your cluster.,0.0,0.135,0.865,0.4939
Datadog,46739062,46716705,0,"2017/10/14, 01:56:40",False,"2017/10/14, 01:56:40",3994.0,505196.0,0,You can then deploy the datadog agent image as a DaemonSet with a set  spec.template.spec.affinity  that selects nodes with a pod of your application running.,0.0,0.0,1.0,0.0
Datadog,46739062,46716705,0,"2017/10/14, 01:56:40",False,"2017/10/14, 01:56:40",3994.0,505196.0,0,This will make sure you have a datadog agent in every node where your application runs.,0.0,0.141,0.859,0.3182
Datadog,46739062,46716705,0,"2017/10/14, 01:56:40",False,"2017/10/14, 01:56:40",3994.0,505196.0,0,Another option is to deploy the datadog agent container in the same pod as your application container.,0.0,0.0,1.0,0.0
Datadog,46739062,46716705,0,"2017/10/14, 01:56:40",False,"2017/10/14, 01:56:40",3994.0,505196.0,0,"In this case you can reach the agent through localhost and scale together, but might end up with more than an agent per node, hence my preference for a DaemonSet with an affinity.",0.0,0.033,0.967,0.0129
Datadog,46740957,46716705,0,"2017/10/14, 07:44:41",False,"2017/10/14, 07:44:41",31231.0,242493.0,0,"My team ran it as a daemon set for the purposes of collecting node metrics, but only exposed it as a normal cluster IP service for the purposes of programmatically sending it data from other apps in the cluster.",0.039,0.0,0.961,-0.1154
Datadog,46740957,46716705,0,"2017/10/14, 07:44:41",False,"2017/10/14, 07:44:41",31231.0,242493.0,0,You don't need to expose it on a node port unless you need to access it from outside the cluster and don't have a service-aware load balancer like an ingress controller.,0.0,0.127,0.873,0.4486
Datadog,46740957,46716705,0,"2017/10/14, 07:44:41",False,"2017/10/14, 07:44:41",31231.0,242493.0,0,"(That would be quite a strange use case, so chances are you don't need to expose it on a node port.)",0.096,0.174,0.73,0.1821
Datadog,46411356,46407772,1,"2017/09/25, 20:56:47",True,"2017/09/25, 20:56:47",17772.0,1494519.0,3,is the name of the default queue.,0.0,0.0,1.0,0.0
Datadog,46411356,46407772,1,"2017/09/25, 20:56:47",True,"2017/09/25, 20:56:47",17772.0,1494519.0,3,"As you state, it is ""queue:$NAME"" but namespaces (if you use them (please don't)) will also prefix the key.",0.0,0.0,1.0,0.0
Datadog,44550799,44550138,0,"2017/06/14, 20:06:35",True,"2017/06/14, 20:06:35",1028.0,8020142.0,0,You can not set a name on the instances of docker that manages amazon.,0.0,0.124,0.876,0.1779
Datadog,44550799,44550138,0,"2017/06/14, 20:06:35",True,"2017/06/14, 20:06:35",1028.0,8020142.0,0,The namespaces it uses are to be able to handle the scaling of the service.,0.0,0.0,1.0,0.0
Datadog,44550799,44550138,0,"2017/06/14, 20:06:35",True,"2017/06/14, 20:06:35",1028.0,8020142.0,0,"Think that if you write the name and then the service you ask for more than one instance of your application, amazon could not instantiate it on the same node.",0.0,0.055,0.945,0.1779
Datadog,44550799,44550138,0,"2017/06/14, 20:06:35",True,"2017/06/14, 20:06:35",1028.0,8020142.0,0,I hope the explanation has served.,0.0,0.42,0.58,0.4404
Datadog,44551679,44550138,0,"2017/06/14, 21:02:08",False,"2017/06/14, 21:02:08",3575.0,112821.0,4,"No, there is not a way to control the name used for the container in Amazon ECS.",0.123,0.095,0.782,-0.128
Datadog,44551679,44550138,0,"2017/06/14, 21:02:08",False,"2017/06/14, 21:02:08",3575.0,112821.0,4,ECS picks a random name designed to avoid conflicts (since names must be unique in Docker; you can't have two containers with the same name) and you can see the code  here .,0.142,0.0,0.858,-0.5859
Datadog,44551679,44550138,0,"2017/06/14, 21:02:08",False,"2017/06/14, 21:02:08",3575.0,112821.0,4,"However, ECS does give you a few things that might be able to help you.",0.0,0.172,0.828,0.4019
Datadog,44551679,44550138,0,"2017/06/14, 21:02:08",False,"2017/06/14, 21:02:08",3575.0,112821.0,4,"There are automatically-assigned Docker labels for the task ARN, the container name in your task definition, the task definition family, the task definition revision, and the cluster; see  here .",0.0,0.0,1.0,0.0
Datadog,44551679,44550138,0,"2017/06/14, 21:02:08",False,"2017/06/14, 21:02:08",3575.0,112821.0,4,"Additionally, you can assign your own custom Docker labels through the task definition.",0.0,0.0,1.0,0.0
Datadog,39333248,39323306,9,"2016/09/05, 17:58:10",True,"2016/09/05, 17:58:10",2479.0,5027078.0,2,I just had some difficulties to determine what is exactly your second separator.,0.167,0.0,0.833,-0.296
Datadog,39333248,39323306,9,"2016/09/05, 17:58:10",True,"2016/09/05, 17:58:10",2479.0,5027078.0,2,"you text example shows '·', but when I checked what is just after 'Elberg"" and before '2nd...', I found 4 characters : code 32 (space), code 194 (¬), code 183 (∑), code 32 (space).",0.0,0.0,1.0,0.0
Datadog,39333248,39323306,9,"2016/09/05, 17:58:10",True,"2016/09/05, 17:58:10",2479.0,5027078.0,2,"In the script bellow, I have used the code 194. it works when I cut/paste your text example into a file.",0.0,0.0,1.0,0.0
Datadog,39333248,39323306,9,"2016/09/05, 17:58:10",True,"2016/09/05, 17:58:10",2479.0,5027078.0,2,Here is the script :,0.0,0.0,1.0,0.0
Datadog,39333248,39323306,9,"2016/09/05, 17:58:10",True,"2016/09/05, 17:58:10",2479.0,5027078.0,2,"Note : if the text does not contain ""Job posted by "", then myAuthor is ''.",0.0,0.0,1.0,0.0
Datadog,39334390,39323306,0,"2016/09/05, 19:17:04",False,"2016/09/05, 19:17:04",1031.0,5390105.0,0,"You had the right idea to use  AppleScript's text item delimiters , but the way you tried to extract the name was giving you trouble.",0.124,0.108,0.768,-0.1154
Datadog,39334390,39323306,0,"2016/09/05, 19:17:04",False,"2016/09/05, 19:17:04",1031.0,5390105.0,0,"First, though, I'll go through some things you can do to improve your script:",0.0,0.182,0.818,0.4404
Datadog,39334390,39323306,0,"2016/09/05, 19:17:04",False,"2016/09/05, 19:17:04",1031.0,5390105.0,0,"There's no need to break the file contents into lines; AppleScript can operate on entire paragraphs or more, if desired.",0.097,0.105,0.797,0.046
Datadog,39334390,39323306,0,"2016/09/05, 19:17:04",False,"2016/09/05, 19:17:04",1031.0,5390105.0,0,Removing these unnecessary steps (and adding new ones to make it work on the entire file) shrinks the script considerably:,0.0,0.0,1.0,0.0
Datadog,39334390,39323306,0,"2016/09/05, 19:17:04",False,"2016/09/05, 19:17:04",1031.0,5390105.0,0,This right here is what's giving you wrong output:,0.248,0.192,0.56,-0.1779
Datadog,39334390,39323306,0,"2016/09/05, 19:17:04",False,"2016/09/05, 19:17:04",1031.0,5390105.0,0,This is incorrect.,0.0,0.0,1.0,0.0
Datadog,39334390,39323306,0,"2016/09/05, 19:17:04",False,"2016/09/05, 19:17:04",1031.0,5390105.0,0,It's not the  last  word you want; that's the last word of the file!,0.0,0.109,0.891,0.1511
Datadog,39334390,39323306,0,"2016/09/05, 19:17:04",False,"2016/09/05, 19:17:04",1031.0,5390105.0,0,"To extract the poster of the job listing, change it to the following:",0.0,0.0,1.0,0.0
Datadog,39334390,39323306,0,"2016/09/05, 19:17:04",False,"2016/09/05, 19:17:04",1031.0,5390105.0,0,"Due to AppleScript's weird Unicode handling, for whatever reason the dot (·) that separates the name from the other text is converted to ""¬∑"" when run though the script.",0.057,0.0,0.943,-0.1779
Datadog,39334390,39323306,0,"2016/09/05, 19:17:04",False,"2016/09/05, 19:17:04",1031.0,5390105.0,0,"So, we look for ""¬"" instead.",0.0,0.0,1.0,0.0
Datadog,39334390,39323306,0,"2016/09/05, 19:17:04",False,"2016/09/05, 19:17:04",1031.0,5390105.0,0,Some last code fixes:,0.0,0.0,1.0,0.0
Datadog,39334390,39323306,0,"2016/09/05, 19:17:04",False,"2016/09/05, 19:17:04",1031.0,5390105.0,0,"Some of your variable names use  the_snake_case , while others use  theCamelCase .",0.0,0.0,1.0,0.0
Datadog,39334390,39323306,0,"2016/09/05, 19:17:04",False,"2016/09/05, 19:17:04",1031.0,5390105.0,0,"It's generally a good idea to use one convention or another, so I fixed that, too.",0.0,0.182,0.818,0.4404
Datadog,39334390,39323306,0,"2016/09/05, 19:17:04",False,"2016/09/05, 19:17:04",1031.0,5390105.0,0,"I assumed you wanted that dollar sign in the output for whatever reason, so I kept it in.",0.0,0.0,1.0,0.0
Datadog,39334390,39323306,0,"2016/09/05, 19:17:04",False,"2016/09/05, 19:17:04",1031.0,5390105.0,0,"If you don't want it, just replace  set output to ""$ ""  with  set output to """" .",0.075,0.0,0.925,-0.0572
Datadog,39334390,39323306,0,"2016/09/05, 19:17:04",False,"2016/09/05, 19:17:04",1031.0,5390105.0,0,"So, your final, working script looks like this:",0.0,0.263,0.737,0.3612
Datadog,38986680,38986190,0,"2016/08/17, 04:08:24",False,"2016/08/17, 04:08:24",2571.0,2149974.0,0,"I agree it's hard to find, the closest one I can find is this",0.101,0.18,0.719,0.2732
Datadog,38986680,38986190,0,"2016/08/17, 04:08:24",False,"2016/08/17, 04:08:24",2571.0,2149974.0,0,"Return the number of instances that are set for the given module
  version.",0.0,0.098,0.902,0.0772
Datadog,38986680,38986190,0,"2016/08/17, 04:08:24",False,"2016/08/17, 04:08:24",2571.0,2149974.0,0,"This is only valid for fixed modules, an error will be raised for automatically-scaled modules.",0.162,0.0,0.838,-0.4019
Datadog,38986680,38986190,0,"2016/08/17, 04:08:24",False,"2016/08/17, 04:08:24",2571.0,2149974.0,0,Support for automatically-scaled modules may be supported in the future.,0.0,0.385,0.615,0.6124
Datadog,38986680,38986190,0,"2016/08/17, 04:08:24",False,"2016/08/17, 04:08:24",2571.0,2149974.0,0,https://cloud.google.com/appengine/docs/python/refdocs/google.appengine.api.modules.modules,0.0,0.0,1.0,0.0
Datadog,38986680,38986190,0,"2016/08/17, 04:08:24",False,"2016/08/17, 04:08:24",2571.0,2149974.0,0,Btw you can also have monitoring from the StackDriver which has metric for total instance,0.0,0.0,1.0,0.0
Datadog,38999762,38986190,5,"2016/08/17, 17:42:16",False,"2016/08/17, 17:42:16",7229.0,178236.0,4,You should also be able to use the recently GA'd App Engine Admin API to figure this out.,0.0,0.0,1.0,0.0
Datadog,38999762,38986190,5,"2016/08/17, 17:42:16",False,"2016/08/17, 17:42:16",7229.0,178236.0,4,"The nice thing about the admin API is that it's going to work for both standard and flexible:
 https://cloud.google.com/appengine/docs/admin-api/",0.0,0.217,0.783,0.5719
Datadog,38999762,38986190,5,"2016/08/17, 17:42:16",False,"2016/08/17, 17:42:16",7229.0,178236.0,4,Here's the endpoint that returns all of the instances for a given service/version:,0.0,0.0,1.0,0.0
Datadog,38999762,38986190,5,"2016/08/17, 17:42:16",False,"2016/08/17, 17:42:16",7229.0,178236.0,4,https://cloud.google.com/appengine/docs/admin-api/reference/rest/v1/apps.services.versions.instances/list,0.0,0.0,1.0,0.0
Datadog,38999762,38986190,5,"2016/08/17, 17:42:16",False,"2016/08/17, 17:42:16",7229.0,178236.0,4,"Depending on the language you're using, there's usually a nice wrapper in the form of a ""Google API client"" + language library.",0.0,0.135,0.865,0.4215
Datadog,38999762,38986190,5,"2016/08/17, 17:42:16",False,"2016/08/17, 17:42:16",7229.0,178236.0,4,Hope this helps!,0.0,0.853,0.147,0.6996
Datadog,39003441,38986190,0,"2016/08/17, 21:02:18",False,"2016/08/17, 21:02:18",2646.0,1543502.0,0,"If you're trying to collect stats, you might want to use the  Stackdriver Monitoring API  to collect the timeseries values that Google has already aggregated.",0.0,0.148,0.852,0.4588
Datadog,39003441,38986190,0,"2016/08/17, 21:02:18",False,"2016/08/17, 21:02:18",2646.0,1543502.0,0,"In particular, the list of  App Engine Metrics is here .",0.0,0.0,1.0,0.0
Datadog,39003441,38986190,0,"2016/08/17, 21:02:18",False,"2016/08/17, 21:02:18",2646.0,1543502.0,0,"For example,  system/instance_count  is the metric indicating the number of instances App Engine is running.",0.0,0.085,0.915,0.0772
Datadog,39007294,38986190,0,"2016/08/18, 01:36:25",True,"2016/08/19, 08:04:25",10698.0,30997.0,1,"I hate it when SO questions only end up with partial answers, so here's a complete, working example.",0.198,0.0,0.802,-0.5719
Datadog,39007294,38986190,0,"2016/08/18, 01:36:25",True,"2016/08/19, 08:04:25",10698.0,30997.0,1,"If you paste it into your interactive console, it should work for you.",0.0,0.0,1.0,0.0
Datadog,39007294,38986190,0,"2016/08/18, 01:36:25",True,"2016/08/19, 08:04:25",10698.0,30997.0,1,(Don't forget to set the  versionsId  to whatever your default app version is.,0.0,0.122,0.878,0.1695
Datadog,39007294,38986190,0,"2016/08/18, 01:36:25",True,"2016/08/19, 08:04:25",10698.0,30997.0,1,"If you know how I can get it to use the default version, please post a comment.",0.0,0.141,0.859,0.3182
Datadog,39007294,38986190,0,"2016/08/18, 01:36:25",True,"2016/08/19, 08:04:25",10698.0,30997.0,1,"'default', '*', 'any', etc.",0.0,0.0,1.0,0.0
Datadog,39007294,38986190,0,"2016/08/18, 01:36:25",True,"2016/08/19, 08:04:25",10698.0,30997.0,1,all no da workie.),0.423,0.0,0.577,-0.296
Datadog,39007294,38986190,0,"2016/08/18, 01:36:25",True,"2016/08/19, 08:04:25",10698.0,30997.0,1,Strictly achieved by trial and error:,0.351,0.0,0.649,-0.4019
Datadog,35230909,35219961,0,"2016/02/05, 20:20:41",True,"2016/02/05, 20:20:41",94.0,2207886.0,2,The easiest way to proceed is to create a custom check.,0.0,0.38,0.62,0.5994
Datadog,35230909,35219961,0,"2016/02/05, 20:20:41",True,"2016/02/05, 20:20:41",94.0,2207886.0,2,You can read up on this here:  http://docs.datadoghq.com/guides/agent_checks/ .,0.0,0.0,1.0,0.0
Datadog,35230909,35219961,0,"2016/02/05, 20:20:41",True,"2016/02/05, 20:20:41",94.0,2207886.0,2,"There isn't a way to take a pre-existing Nagios or Sensu plugin and have it work as is with Datadog, but looking at one of the delayed_job plugins on Github, looks like it should be pretty easy to convert to a Datadog check.",0.0,0.236,0.764,0.9081
Datadog,35230909,35219961,0,"2016/02/05, 20:20:41",True,"2016/02/05, 20:20:41",94.0,2207886.0,2,"If you have any issues, reach out to support either via email or #datadog on IRC.",0.0,0.213,0.787,0.4215
Datadog,62680072,62436021,3,"2020/07/01, 18:20:46",True,"2020/07/01, 18:20:46",844.0,11921495.0,3,This was a issue with deployed DataDog daemonset for me:,0.0,0.0,1.0,0.0
Datadog,62680072,62436021,3,"2020/07/01, 18:20:46",True,"2020/07/01, 18:20:46",844.0,11921495.0,3,What I did to resolve:,0.0,0.464,0.536,0.3818
Datadog,62680072,62436021,3,"2020/07/01, 18:20:46",True,"2020/07/01, 18:20:46",844.0,11921495.0,3,Check daemonset if it exists or not:,0.0,0.0,1.0,0.0
Datadog,62680072,62436021,3,"2020/07/01, 18:20:46",True,"2020/07/01, 18:20:46",844.0,11921495.0,3,Edit the datadog daemonset:,0.0,0.0,1.0,0.0
Datadog,62680072,62436021,3,"2020/07/01, 18:20:46",True,"2020/07/01, 18:20:46",844.0,11921495.0,3,"In the opened yaml, add",0.0,0.0,1.0,0.0
Datadog,62680072,62436021,3,"2020/07/01, 18:20:46",True,"2020/07/01, 18:20:46",844.0,11921495.0,3,Add this in  env:  tag for all places.,0.0,0.0,1.0,0.0
Datadog,62680072,62436021,3,"2020/07/01, 18:20:46",True,"2020/07/01, 18:20:46",844.0,11921495.0,3,For me there were 4 places which are having DD tags in the yaml.,0.0,0.0,1.0,0.0
Datadog,62680072,62436021,3,"2020/07/01, 18:20:46",True,"2020/07/01, 18:20:46",844.0,11921495.0,3,Save and close it.,0.0,0.516,0.484,0.4939
Datadog,62680072,62436021,3,"2020/07/01, 18:20:46",True,"2020/07/01, 18:20:46",844.0,11921495.0,3,The daemonset will restart.,0.0,0.0,1.0,0.0
Datadog,62680072,62436021,3,"2020/07/01, 18:20:46",True,"2020/07/01, 18:20:46",844.0,11921495.0,3,And the application will start getting traced.,0.0,0.0,1.0,0.0
Datadog,67104589,62436021,0,"2021/04/15, 11:08:06",False,"2021/04/15, 11:08:06",1929.0,2179157.0,0,"If you are using the Helm chart, you can overwrite on the values:",0.0,0.184,0.816,0.4019
Datadog,64803224,60491872,0,"2020/11/12, 13:39:22",False,"2020/11/12, 13:39:22",129.0,6103623.0,1,hey!,0.0,0.0,1.0,0.0
Datadog,64803224,60491872,0,"2020/11/12, 13:39:22",False,"2020/11/12, 13:39:22",129.0,6103623.0,1,"Sorry in advance if my answer isn't correct because  I'm a complete newby  in kuber and helm and I can't make sure that it will help, but maybe it helps.",0.083,0.165,0.753,0.5602
Datadog,64803224,60491872,0,"2020/11/12, 13:39:22",False,"2020/11/12, 13:39:22",129.0,6103623.0,1,"So, the problem, as I can understand, in the resulting  ConfigMap  configuration.",0.229,0.0,0.771,-0.4549
Datadog,64803224,60491872,0,"2020/11/12, 13:39:22",False,"2020/11/12, 13:39:22",129.0,6103623.0,1,"From my expirience, I faced the same with the following config:",0.0,0.0,1.0,0.0
Datadog,64803224,60491872,0,"2020/11/12, 13:39:22",False,"2020/11/12, 13:39:22",129.0,6103623.0,1,And I could solve it only by surrounding with quotes all the values:,0.0,0.31,0.69,0.5423
Datadog,59159712,59145932,0,"2019/12/03, 16:50:16",False,"2019/12/03, 16:50:16",1038.0,796064.0,2,"Per Yuri's suggestion, I found the culprit, and this is how (thanks to Google Support for walking me through this):",0.0,0.13,0.87,0.4019
Datadog,59159712,59145932,0,"2019/12/03, 16:50:16",False,"2019/12/03, 16:50:16",1038.0,796064.0,2,"This showed me a graph making it clear just about all of my requests were coming from a credential named  datadog-metrics-collection , a service account I'd set up previously to collect GCP metrics and emit to Datadog.",0.0,0.075,0.925,0.3818
Datadog,59164889,59145932,2,"2019/12/03, 22:32:13",False,"2019/12/03, 22:32:13",136.0,11980517.0,0,"Considering the answer posted and question, If we think we do not need Stackdriver monitoring, we can disable stackdriver monitoring API using bellow steps:",0.0,0.0,1.0,0.0
Datadog,59164889,59145932,2,"2019/12/03, 22:32:13",False,"2019/12/03, 22:32:13",136.0,11980517.0,0,In addition you can view Stackdriver usage by billing account and also can estimate cost using Stackdriver pricing calculator [a] [b].,0.0,0.0,1.0,0.0
Datadog,59164889,59145932,2,"2019/12/03, 22:32:13",False,"2019/12/03, 22:32:13",136.0,11980517.0,0,View Stackdriver usage by billing account:,0.0,0.0,1.0,0.0
Datadog,59164889,59145932,2,"2019/12/03, 22:32:13",False,"2019/12/03, 22:32:13",136.0,11980517.0,0,4.Select Group By   SKU.,0.0,0.0,1.0,0.0
Datadog,59164889,59145932,2,"2019/12/03, 22:32:13",False,"2019/12/03, 22:32:13",136.0,11980517.0,0,"This menu might be hidden; you can access it by clicking Show 
  Filters.",0.0,0.0,1.0,0.0
Datadog,59164889,59145932,2,"2019/12/03, 22:32:13",False,"2019/12/03, 22:32:13",136.0,11980517.0,0,You can also select just one or some of these SKUs if you don't want to group your usage data.,0.06,0.0,0.94,-0.0572
Datadog,59164889,59145932,2,"2019/12/03, 22:32:13",False,"2019/12/03, 22:32:13",136.0,11980517.0,0,"Note: If your usage of any of these SKUs is 0, they don't appear in the Group By   SKU pull-down menu.",0.0,0.0,1.0,0.0
Datadog,59164889,59145932,2,"2019/12/03, 22:32:13",False,"2019/12/03, 22:32:13",136.0,11980517.0,0,"For example, who use only the Cloud console might never generate API requests, so Monitoring API Requests doesn't appear in the list.",0.0,0.0,1.0,0.0
Datadog,59164889,59145932,2,"2019/12/03, 22:32:13",False,"2019/12/03, 22:32:13",136.0,11980517.0,0,Use the Stackdriver pricing calculator [b]:,0.0,0.0,1.0,0.0
Datadog,59164889,59145932,2,"2019/12/03, 22:32:13",False,"2019/12/03, 22:32:13",136.0,11980517.0,0,[a]  https://cloud.google.com/stackdriver/estimating-bills#billing-acct-usage,0.0,0.0,1.0,0.0
Datadog,59164889,59145932,2,"2019/12/03, 22:32:13",False,"2019/12/03, 22:32:13",136.0,11980517.0,0,[b]  https://cloud.google.com/products/calculator/#tab=google-stackdriver,0.0,0.0,1.0,0.0
Datadog,59055123,59046195,0,"2019/11/26, 18:14:07",True,"2019/11/26, 18:14:07",11561.0,970308.0,2,You need to  start  the publishing.,0.0,0.0,1.0,0.0
Datadog,59055123,59046195,0,"2019/11/26, 18:14:07",True,"2019/11/26, 18:14:07",11561.0,970308.0,2,Compare with the  LoggingMeterRegistry,0.0,0.0,1.0,0.0
Datadog,59055123,59046195,0,"2019/11/26, 18:14:07",True,"2019/11/26, 18:14:07",11561.0,970308.0,2,In your constructor something like:,0.0,0.385,0.615,0.3612
Datadog,38089941,38088122,0,"2016/06/29, 06:35:15",False,"2016/06/29, 06:35:15",5780.0,6163736.0,3,Why not try?,0.0,0.0,1.0,0.0
Datadog,38089941,38088122,0,"2016/06/29, 06:35:15",False,"2016/06/29, 06:35:15",5780.0,6163736.0,3,It will return the docker container's hostname.,0.0,0.0,1.0,0.0
Datadog,38089941,38088122,0,"2016/06/29, 06:35:15",False,"2016/06/29, 06:35:15",5780.0,6163736.0,3,"If you haven't set a hostname explicitly, using something like  docker run -h hostname image command  then it will return the docker host's hostname.",0.0,0.102,0.898,0.3612
Datadog,38089941,38088122,0,"2016/06/29, 06:35:15",False,"2016/06/29, 06:35:15",5780.0,6163736.0,3,"Alternatively, you could do this using a deployment tool like puppet, ansible, etc.",0.0,0.185,0.815,0.3612
Datadog,38089941,38088122,0,"2016/06/29, 06:35:15",False,"2016/06/29, 06:35:15",5780.0,6163736.0,3,and template the file when you deploy the container.,0.0,0.0,1.0,0.0
Datadog,66775560,66775454,2,"2021/03/24, 08:05:50",False,"2021/03/24, 08:05:50",2752.0,2672947.0,2,See  HTTP response status codes,0.0,0.0,1.0,0.0
Datadog,66775560,66775454,2,"2021/03/24, 08:05:50",False,"2021/03/24, 08:05:50",2752.0,2672947.0,2,The categories are generally:,0.0,0.0,1.0,0.0
Datadog,66775560,66775454,2,"2021/03/24, 08:05:50",False,"2021/03/24, 08:05:50",2752.0,2672947.0,2,"So 4XX errors are errors, but they indicate the client is likely at fault.",0.397,0.0,0.603,-0.7374
Datadog,66775560,66775454,2,"2021/03/24, 08:05:50",False,"2021/03/24, 08:05:50",2752.0,2672947.0,2,E.g.,0.0,0.0,1.0,0.0
Datadog,66775560,66775454,2,"2021/03/24, 08:05:50",False,"2021/03/24, 08:05:50",2752.0,2672947.0,2,The user went to a page or user agent made a request to a page that does not exist.,0.0,0.0,1.0,0.0
Datadog,66775560,66775454,2,"2021/03/24, 08:05:50",False,"2021/03/24, 08:05:50",2752.0,2672947.0,2,"The server responds with 404 because &quot;Everything on my end is fine, but that page isn't real.&quot;",0.0,0.08,0.92,0.1027
Datadog,66775560,66775454,2,"2021/03/24, 08:05:50",False,"2021/03/24, 08:05:50",2752.0,2672947.0,2,Is it an error?,0.474,0.0,0.526,-0.4019
Datadog,66775560,66775454,2,"2021/03/24, 08:05:50",False,"2021/03/24, 08:05:50",2752.0,2672947.0,2,Sure.,0.0,1.0,0.0,0.3182
Datadog,66775560,66775454,2,"2021/03/24, 08:05:50",False,"2021/03/24, 08:05:50",2752.0,2672947.0,2,Could you potentially identify issues (e.g.,0.0,0.0,1.0,0.0
Datadog,66775560,66775454,2,"2021/03/24, 08:05:50",False,"2021/03/24, 08:05:50",2752.0,2672947.0,2,"typos in links, missing pages, misspellings, malformed API requests, etc..) by routing these to your logs?",0.128,0.0,0.872,-0.296
Datadog,66775560,66775454,2,"2021/03/24, 08:05:50",False,"2021/03/24, 08:05:50",2752.0,2672947.0,2,Sure.,0.0,1.0,0.0,0.3182
Datadog,66775560,66775454,2,"2021/03/24, 08:05:50",False,"2021/03/24, 08:05:50",2752.0,2672947.0,2,Are you obligated to take action on it?,0.0,0.0,1.0,0.0
Datadog,66775560,66775454,2,"2021/03/24, 08:05:50",False,"2021/03/24, 08:05:50",2752.0,2672947.0,2,Not if you don't want to.,0.196,0.0,0.804,-0.0572
Datadog,66775560,66775454,2,"2021/03/24, 08:05:50",False,"2021/03/24, 08:05:50",2752.0,2672947.0,2,You're probably best to determine why you feel they are not actionable.,0.0,0.276,0.724,0.6369
Datadog,66775560,66775454,2,"2021/03/24, 08:05:50",False,"2021/03/24, 08:05:50",2752.0,2672947.0,2,Most likely are actionable.,0.0,0.0,1.0,0.0
Datadog,65953918,65951118,0,"2021/01/29, 13:51:14",True,"2021/01/29, 13:51:14",84586.0,1384297.0,2,Micrometer uses  MeterFilter s registered with a  MeterRegistry  to modified the meters that are registered.,0.0,0.0,1.0,0.0
Datadog,65953918,65951118,0,"2021/01/29, 13:51:14",True,"2021/01/29, 13:51:14",84586.0,1384297.0,2,The modifications include the ability to map a meter's ID to something different.,0.0,0.173,0.827,0.3182
Datadog,65953918,65951118,0,"2021/01/29, 13:51:14",True,"2021/01/29, 13:51:14",84586.0,1384297.0,2,"In Spring Boot, you can use a  MeterRegistryCustomizer  bean to add a  MeterFilter  to a registry.",0.0,0.0,1.0,0.0
Datadog,65953918,65951118,0,"2021/01/29, 13:51:14",True,"2021/01/29, 13:51:14",84586.0,1384297.0,2,"You can use generics to work with a registry of a specific type, for example  MeterRegistryCustomizer&lt;DatadogMeterRegistry&gt;  for a customizer that is only interested in customizing the Datadog registry.",0.0,0.101,0.899,0.4019
Datadog,65953918,65951118,0,"2021/01/29, 13:51:14",True,"2021/01/29, 13:51:14",84586.0,1384297.0,2,"Putting this together, you can map the ID of the  http.server.request  meter to  i.want.to.be.different  using the following bean:",0.0,0.0,1.0,0.0
Datadog,63631475,63626179,0,"2020/08/28, 12:52:58",True,"2020/08/28, 12:52:58",4922.0,11560878.0,1,There are some options you should consider:,0.0,0.0,1.0,0.0
Datadog,63631475,63626179,0,"2020/08/28, 12:52:58",True,"2020/08/28, 12:52:58",4922.0,11560878.0,1,don't update anything and just stick to Kubernetes 1.15 (not recommended as it is 4 main versions behind the latest one),0.0,0.087,0.913,0.2023
Datadog,63631475,63626179,0,"2020/08/28, 12:52:58",True,"2020/08/28, 12:52:58",4922.0,11560878.0,1,git clone  your repo and change  apiVersion  to  apps/v1  in all your resources,0.0,0.0,1.0,0.0
Datadog,63631475,63626179,0,"2020/08/28, 12:52:58",True,"2020/08/28, 12:52:58",4922.0,11560878.0,1,"use  kubectl convert  in order to change the  apiVersion , for example:  kubectl convert -f deployment.yaml --output-version apps/v1",0.0,0.0,1.0,0.0
Datadog,63631475,63626179,0,"2020/08/28, 12:52:58",True,"2020/08/28, 12:52:58",4922.0,11560878.0,1,It is worth to mention that stuff gets deprecated for a reason and it is strongly not recommended to stick to old ways if they are not supported anymore.,0.113,0.127,0.761,0.1144
Datadog,63266155,63259337,0,"2020/08/05, 16:22:30",False,"2020/08/05, 16:22:30",434.0,13958041.0,0,"If you are doing the setup in an organisation, datadog or prometheus is probably the way to go.",0.0,0.0,1.0,0.0
Datadog,63266155,63259337,0,"2020/08/05, 16:22:30",False,"2020/08/05, 16:22:30",434.0,13958041.0,0,You can capture other Kafka related metrics as well.,0.0,0.208,0.792,0.2732
Datadog,63266155,63259337,0,"2020/08/05, 16:22:30",False,"2020/08/05, 16:22:30",434.0,13958041.0,0,These agents also have integrations with many other tools beside Kafka and will be a good common choice for monitoring.,0.0,0.139,0.861,0.4404
Datadog,63266155,63259337,0,"2020/08/05, 16:22:30",False,"2020/08/05, 16:22:30",434.0,13958041.0,0,"If you are just doing it for personal POC type of a project and you just want to  view  the lag, I find CMAK very useful ( https://github.com/yahoo/CMAK ).",0.08,0.15,0.769,0.2716
Datadog,63266155,63259337,0,"2020/08/05, 16:22:30",False,"2020/08/05, 16:22:30",434.0,13958041.0,0,"This does  not  have historical data, but provides a good  current  visual state of Kafka cluster including lag.",0.141,0.175,0.683,0.1901
Datadog,63279458,63259337,0,"2020/08/06, 11:15:49",False,"2020/08/06, 11:15:49",106.0,12612778.0,0,For cluster wide metrics you can use kafka_exporter ( https://github.com/danielqsj/kafka_exporter ) which exposes some very useful cluster metrics(including consumer lag) and is easy to integrate with prometheus and visualize using grafana.,0.045,0.181,0.774,0.6801
Datadog,65170331,63259337,0,"2020/12/06, 18:24:17",False,"2020/12/06, 18:31:54",81.0,5501217.0,0,Burrow is extremely effective and specialised in monitoring consumer lag.Burrow is good at caliberating consumer offset and more importantly validate if the lag is malicious or not.,0.067,0.323,0.61,0.8506
Datadog,65170331,63259337,0,"2020/12/06, 18:24:17",False,"2020/12/06, 18:31:54",81.0,5501217.0,0,It has integrations with pagerduty so that the alerts are pushed to the necessary parties.,0.0,0.162,0.838,0.4019
Datadog,65170331,63259337,0,"2020/12/06, 18:24:17",False,"2020/12/06, 18:31:54",81.0,5501217.0,0,https://community.cloudera.com/t5/Community-Articles/Monitoring-Kafka-with-Burrow-Part-1/ta-p/245987,0.0,0.0,1.0,0.0
Datadog,65170331,63259337,0,"2020/12/06, 18:24:17",False,"2020/12/06, 18:31:54",81.0,5501217.0,0,What burrow has:,0.0,0.0,1.0,0.0
Datadog,65170331,63259337,0,"2020/12/06, 18:24:17",False,"2020/12/06, 18:31:54",81.0,5501217.0,0,If you are looking for quick solution you can deploy burrow followed by the burrow front end  https://github.com/GeneralMills/BurrowUI,0.0,0.119,0.881,0.3182
Datadog,62246112,62177611,1,"2020/06/07, 16:32:56",False,"2020/06/09, 00:40:01",927.0,2188893.0,0,You could use the Java Admin Kafka API quite easily to expose this over command line or as an HTTP call quite quickly with Spring Boot (could avoid any metrics for each app individually since Admin API could do it for any group).,0.087,0.058,0.855,-0.0953
Datadog,62246112,62177611,1,"2020/06/07, 16:32:56",False,"2020/06/09, 00:40:01",927.0,2188893.0,0,See link for an example:  https://gquintana.github.io/2020/01/16/Retrieving-Kafka-lag.html,0.0,0.0,1.0,0.0
Datadog,62246112,62177611,1,"2020/06/07, 16:32:56",False,"2020/06/09, 00:40:01",927.0,2188893.0,0,Code taken from example above.,0.0,0.0,1.0,0.0
Datadog,62246112,62177611,1,"2020/06/07, 16:32:56",False,"2020/06/09, 00:40:01",927.0,2188893.0,0,Get all groups:,0.0,0.0,1.0,0.0
Datadog,62246112,62177611,1,"2020/06/07, 16:32:56",False,"2020/06/09, 00:40:01",927.0,2188893.0,0,Get highest offset in a set of partitions:,0.0,0.0,1.0,0.0
Datadog,62246112,62177611,1,"2020/06/07, 16:32:56",False,"2020/06/09, 00:40:01",927.0,2188893.0,0,High level on connecting them together to get lag:,0.231,0.0,0.769,-0.34
Datadog,62137876,62123685,0,"2020/06/01, 20:43:26",False,"2020/06/01, 20:43:26",471.0,967088.0,0,"Use the supplied jconsole.sh script in bin, don't try and build up the classpath by hand.",0.0,0.176,0.824,0.4939
Datadog,62137876,62123685,0,"2020/06/01, 20:43:26",False,"2020/06/01, 20:43:26",471.0,967088.0,0,You also need to use the custom service url.,0.0,0.0,1.0,0.0
Datadog,62137876,62123685,0,"2020/06/01, 20:43:26",False,"2020/06/01, 20:43:26",471.0,967088.0,0,See the docs for details,0.0,0.0,1.0,0.0
Datadog,63924760,61119886,0,"2020/09/16, 19:57:11",False,"2020/09/16, 19:57:11",460.0,2810489.0,0,You can run datadog tracing on AWS Elastic Beanstalk with Flask by configure tracing manually as defined  here :,0.0,0.0,1.0,0.0
Datadog,61063468,61043878,0,"2020/04/06, 18:37:38",False,"2020/04/06, 18:37:38",9394.0,573153.0,2,"Custom metrics are on the roadmap for the APM agent, but we're still working on the exact schedule.",0.0,0.0,1.0,0.0
Datadog,61063468,61043878,0,"2020/04/06, 18:37:38",False,"2020/04/06, 18:37:38",9394.0,573153.0,2,In the meantime you could either use the  JMX config options  of the agent with custom JMX key properties.,0.0,0.0,1.0,0.0
Datadog,61063468,61043878,0,"2020/04/06, 18:37:38",False,"2020/04/06, 18:37:38",9394.0,573153.0,2,Or use the Elasticsearch output of Micrometer.,0.0,0.0,1.0,0.0
Datadog,61063468,61043878,0,"2020/04/06, 18:37:38",False,"2020/04/06, 18:37:38",9394.0,573153.0,2,Maybe just change the Micrometer output as an interim solution and potentially switch to custom APM metrics once they are available?,0.0,0.103,0.897,0.3182
Datadog,61063468,61043878,0,"2020/04/06, 18:37:38",False,"2020/04/06, 18:37:38",9394.0,573153.0,2,"There's also the option to get metrics with  Metricbeat from JMX / Jolokia , but that sounds like an even bigger change and not really a long-term upside.",0.0,0.119,0.881,0.5023
Datadog,60050555,59961483,0,"2020/02/04, 06:24:29",False,"2020/02/05, 03:52:39",1201.0,8673695.0,2,One straight forward way I can think of in this case is to use the  .env  file for your docker-compose.,0.0,0.095,0.905,0.2263
Datadog,60050555,59961483,0,"2020/02/04, 06:24:29",False,"2020/02/05, 03:52:39",1201.0,8673695.0,2,docker-compose.yaml  file will look something like this,0.0,0.294,0.706,0.3612
Datadog,60050555,59961483,0,"2020/02/04, 06:24:29",False,"2020/02/05, 03:52:39",1201.0,8673695.0,2,.env  file for each stack will look something like this,0.0,0.217,0.783,0.3612
Datadog,60050555,59961483,0,"2020/02/04, 06:24:29",False,"2020/02/05, 03:52:39",1201.0,8673695.0,2,and,0.0,0.0,1.0,0.0
Datadog,60050555,59961483,0,"2020/02/04, 06:24:29",False,"2020/02/05, 03:52:39",1201.0,8673695.0,2,for different projects.,0.0,0.0,1.0,0.0
Datadog,60050555,59961483,0,"2020/02/04, 06:24:29",False,"2020/02/05, 03:52:39",1201.0,8673695.0,2,Note:,0.0,0.0,1.0,0.0
Datadog,60068032,59961483,1,"2020/02/05, 04:00:43",False,"2020/02/05, 04:00:43",7900.0,2259934.0,0,You're making things harder than they have to be.,0.0,0.0,1.0,0.0
Datadog,60068032,59961483,1,"2020/02/05, 04:00:43",False,"2020/02/05, 04:00:43",7900.0,2259934.0,0,Your app is containerized- use a container system.,0.0,0.0,1.0,0.0
Datadog,60068032,59961483,1,"2020/02/05, 04:00:43",False,"2020/02/05, 04:00:43",7900.0,2259934.0,0,ECS is  very  easy to get going with.,0.0,0.313,0.687,0.4927
Datadog,60068032,59961483,1,"2020/02/05, 04:00:43",False,"2020/02/05, 04:00:43",7900.0,2259934.0,0,"It's a json file that defines your deployment- basically analogous to docker-compose (they actually supported compose files at some point, not sure if that feature stayed around).",0.069,0.081,0.849,0.0869
Datadog,60068032,59961483,1,"2020/02/05, 04:00:43",False,"2020/02/05, 04:00:43",7900.0,2259934.0,0,You can deploy an arbitrary number of services with different container images.,0.0,0.106,0.894,0.0772
Datadog,60068032,59961483,1,"2020/02/05, 04:00:43",False,"2020/02/05, 04:00:43",7900.0,2259934.0,0,"We like to use a terraform module with the image tag as a parameter, but easy enough to write a shell script or whatever.",0.0,0.228,0.772,0.6808
Datadog,60068032,59961483,1,"2020/02/05, 04:00:43",False,"2020/02/05, 04:00:43",7900.0,2259934.0,0,"Since you're trying to save money, create a single application load balancer.",0.0,0.371,0.629,0.6486
Datadog,60068032,59961483,1,"2020/02/05, 04:00:43",False,"2020/02/05, 04:00:43",7900.0,2259934.0,0,"each app gets a hostname, and each container gets a subpath.",0.0,0.0,1.0,0.0
Datadog,60068032,59961483,1,"2020/02/05, 04:00:43",False,"2020/02/05, 04:00:43",7900.0,2259934.0,0,"For short lived feature branch deployments, you can even deploy on Fargate and not have an ongoing server cost.",0.0,0.0,1.0,0.0
Datadog,60405918,59961483,0,"2020/02/26, 04:28:04",False,"2020/02/26, 04:28:04",971.0,8379207.0,0,It turns out the solution involved capabilities from docker-compose.,0.0,0.223,0.777,0.3182
Datadog,60405918,59961483,0,"2020/02/26, 04:28:04",False,"2020/02/26, 04:28:04",971.0,8379207.0,0,In docker docs the concept is called  Multiple Isolated environments on a single host,0.161,0.0,0.839,-0.3182
Datadog,60405918,59961483,0,"2020/02/26, 04:28:04",False,"2020/02/26, 04:28:04",971.0,8379207.0,0,to achieve this:,0.0,0.0,1.0,0.0
Datadog,60405918,59961483,0,"2020/02/26, 04:28:04",False,"2020/02/26, 04:28:04",971.0,8379207.0,0,I used an .env file with so many env vars.,0.0,0.0,1.0,0.0
Datadog,60405918,59961483,0,"2020/02/26, 04:28:04",False,"2020/02/26, 04:28:04",971.0,8379207.0,0,The main one is  CONTAINER_IMAGE_TAG  that defines the git branch ID to identify the stack.,0.0,0.0,1.0,0.0
Datadog,60405918,59961483,0,"2020/02/26, 04:28:04",False,"2020/02/26, 04:28:04",971.0,8379207.0,0,"A separate docker-compose-dev file defines ports, image tags, extra metadata that is dev related",0.0,0.0,1.0,0.0
Datadog,60405918,59961483,0,"2020/02/26, 04:28:04",False,"2020/02/26, 04:28:04",971.0,8379207.0,0,Finally the use of  --project-name  in the docker-compose command allows to have different stacks.,0.0,0.0,1.0,0.0
Datadog,60405918,59961483,0,"2020/02/26, 04:28:04",False,"2020/02/26, 04:28:04",971.0,8379207.0,0,an example docker-compose Bash function that uses the docker-compose command,0.0,0.0,1.0,0.0
Datadog,60405918,59961483,0,"2020/02/26, 04:28:04",False,"2020/02/26, 04:28:04",971.0,8379207.0,0,"The separation should be done in the image tags, container names, network names, volume names and project name.",0.0,0.0,1.0,0.0
Datadog,58576495,58576189,1,"2019/10/27, 05:54:22",False,"2019/10/27, 05:54:22",1190.0,5621569.0,1,The issue is that your library depends on  gcc  to run.,0.0,0.0,1.0,0.0
Datadog,58576495,58576189,1,"2019/10/27, 05:54:22",False,"2019/10/27, 05:54:22",1190.0,5621569.0,1,"If you are running in a container, you can try two options:",0.0,0.0,1.0,0.0
Datadog,58576495,58576189,1,"2019/10/27, 05:54:22",False,"2019/10/27, 05:54:22",1190.0,5621569.0,1,"You could also need  musl-dev  package, but you should try without it first.",0.0,0.0,1.0,0.0
Datadog,58576495,58576189,1,"2019/10/27, 05:54:22",False,"2019/10/27, 05:54:22",1190.0,5621569.0,1,"Since MacOS and most Linux distros come with GCC, I guess you could be using Windows.",0.0,0.0,1.0,0.0
Datadog,58576495,58576189,1,"2019/10/27, 05:54:22",False,"2019/10/27, 05:54:22",1190.0,5621569.0,1,"In this case, you need to install  MinGW .",0.0,0.0,1.0,0.0
Datadog,59060540,58576189,0,"2019/11/27, 00:50:03",False,"2019/11/27, 01:25:23",671.0,445891.0,1,"I know this is old but I ran into this problem too, About Alexey answer, on windows, you should install MinGW and add the path to win environment.",0.125,0.156,0.719,0.2529
Datadog,59060540,58576189,0,"2019/11/27, 00:50:03",False,"2019/11/27, 01:25:23",671.0,445891.0,1,You should follow  this .,0.0,0.0,1.0,0.0
Datadog,59060540,58576189,0,"2019/11/27, 00:50:03",False,"2019/11/27, 01:25:23",671.0,445891.0,1,"In case MinGW did not work, you can install  this  one which worked perfectly for me on windows.",0.0,0.198,0.802,0.6369
Datadog,59958891,57207906,0,"2020/01/29, 02:05:05",False,"2020/01/29, 02:05:05",1.0,12802667.0,0,I had the same error and installing the .NET Framework 4.6.1 SDK ( https://dotnet.microsoft.com/download/visual-studio-sdks ) and restarting the Datadog Agent solved the problem,0.22,0.086,0.694,-0.5106
Datadog,56117644,56116856,0,"2019/05/13, 21:01:29",False,"2019/05/13, 21:01:29",4287.0,9073130.0,1,Use  context.Request.Path  conditionally if your  routeData  is null.,0.0,0.0,1.0,0.0
Datadog,56117644,56116856,0,"2019/05/13, 21:01:29",False,"2019/05/13, 21:01:29",4287.0,9073130.0,1,It is the closest I can think of since Identity Server 4 middleware has internal routing logic for the standard OAuth protocol routes.,0.0,0.0,1.0,0.0
Datadog,55054296,55009193,0,"2019/03/08, 01:11:52",True,"2019/03/08, 01:11:52",923.0,1181073.0,1,"After a few more days of research, discovered that:",0.0,0.0,1.0,0.0
Datadog,55159583,54729245,0,"2019/03/14, 12:00:21",False,"2019/03/14, 12:00:21",523.0,145725.0,0,"The errors you are getting are coming from the remote computer, that is, the Heroku dyno.",0.138,0.0,0.862,-0.34
Datadog,55159583,54729245,0,"2019/03/14, 12:00:21",False,"2019/03/14, 12:00:21",523.0,145725.0,0,You can't follow the instructions in the warning (to update bundler) as you can't run arbitrary instructions on their servers.,0.112,0.0,0.888,-0.34
Datadog,55159583,54729245,0,"2019/03/14, 12:00:21",False,"2019/03/14, 12:00:21",523.0,145725.0,0,"Heroku only support limited "" carefully curated "" versions of bundler.",0.157,0.347,0.496,0.3182
Datadog,55159583,54729245,0,"2019/03/14, 12:00:21",False,"2019/03/14, 12:00:21",523.0,145725.0,0,"Normally when the bundler versions don't match it just gives a warning, not an error, so you can  potentially  just ignore it.",0.195,0.09,0.715,-0.3903
Datadog,55159583,54729245,0,"2019/03/14, 12:00:21",False,"2019/03/14, 12:00:21",523.0,145725.0,0,Personally I like to eliminate warnings (or supress them if elimination isn't possible) so that when new warnings pop up I am more likely to notice them and deal with them.,0.134,0.076,0.79,-0.2263
Datadog,55159583,54729245,0,"2019/03/14, 12:00:21",False,"2019/03/14, 12:00:21",523.0,145725.0,0,"That being said, I was not able to ""downgrade"" my Gemfile.lock from 2.0.1 to 1.15.2.",0.0,0.0,1.0,0.0
Datadog,55159583,54729245,0,"2019/03/14, 12:00:21",False,"2019/03/14, 12:00:21",523.0,145725.0,0,I had to first delete Gemfile.lock and then recreate it (presumably there are potentially breaking changes across these major versions).,0.0,0.0,1.0,0.0
Datadog,55159583,54729245,0,"2019/03/14, 12:00:21",False,"2019/03/14, 12:00:21",523.0,145725.0,0,I suspect this is the second problem you encountered.,0.45,0.0,0.55,-0.5994
Datadog,55159583,54729245,0,"2019/03/14, 12:00:21",False,"2019/03/14, 12:00:21",523.0,145725.0,0,The best way around these warnings/errors is to match your local version of Bundler to Heroku's carefully curated version.,0.0,0.251,0.749,0.6908
Datadog,55159583,54729245,0,"2019/03/14, 12:00:21",False,"2019/03/14, 12:00:21",523.0,145725.0,0,"That page above links to another page with the currently supported versions: 
 https://devcenter.heroku.com/articles/ruby-support#libraries",0.0,0.161,0.839,0.3182
Datadog,55159583,54729245,0,"2019/03/14, 12:00:21",False,"2019/03/14, 12:00:21",523.0,145725.0,0,As of today that's version 2.0.1 for Gemfile.locks bundled with 2.x and 1.15.2 for everything else.,0.0,0.0,1.0,0.0
Datadog,56572059,54717464,0,"2019/06/13, 03:54:58",False,"2019/06/13, 03:54:58",2578.0,1235057.0,0,Could be merge conflicts in the Gemfile.lock.,0.302,0.0,0.698,-0.3818
Datadog,56572059,54717464,0,"2019/06/13, 03:54:58",False,"2019/06/13, 03:54:58",2578.0,1235057.0,0,Try running  bundle install  locally and see if it works before committing and pushing to heroku.,0.0,0.08,0.92,0.0772
Datadog,52758714,52755069,3,"2018/10/11, 14:16:24",False,"2018/10/11, 14:16:24",1.0,7850738.0,-2,add bash script as userparameters in zabbix-agent.,0.0,0.0,1.0,0.0
Datadog,52590134,52588302,4,"2018/10/01, 14:29:23",True,"2018/10/01, 14:29:23",941.0,5488567.0,2,"Since it requires admin permissions, we can not give out UAA clients for the firehose.",0.0,0.0,1.0,0.0
Datadog,52590134,52588302,4,"2018/10/01, 14:29:23",True,"2018/10/01, 14:29:23",941.0,5488567.0,2,"However, there are different ways to get metrics in context of a user.",0.0,0.0,1.0,0.0
Datadog,52590134,52588302,4,"2018/10/01, 14:29:23",True,"2018/10/01, 14:29:23",941.0,5488567.0,2,CF API,0.0,0.0,1.0,0.0
Datadog,52590134,52588302,4,"2018/10/01, 14:29:23",True,"2018/10/01, 14:29:23",941.0,5488567.0,2,"You can obtain basic metrics of a specific app by polling the CF API:
 https://apidocs.cloudfoundry.org/5.0.0/apps/get_detailed_stats_for_a_started_app.html",0.0,0.0,1.0,0.0
Datadog,52590134,52588302,4,"2018/10/01, 14:29:23",True,"2018/10/01, 14:29:23",941.0,5488567.0,2,"However, since you have to poll (and for each app), it's not the recommended way.",0.102,0.0,0.898,-0.1511
Datadog,52590134,52588302,4,"2018/10/01, 14:29:23",True,"2018/10/01, 14:29:23",941.0,5488567.0,2,Metrics in syslog drain,0.0,0.0,1.0,0.0
Datadog,52590134,52588302,4,"2018/10/01, 14:29:23",True,"2018/10/01, 14:29:23",941.0,5488567.0,2,"CF allows devs to forward their logs to syslog drains; in more recent versions, CF also sends metrics to this syslog drain (see  https://docs.cloudfoundry.org/devguide/deploy-apps/streaming-logs.html#container-metrics ).",0.0,0.0,1.0,0.0
Datadog,52590134,52588302,4,"2018/10/01, 14:29:23",True,"2018/10/01, 14:29:23",941.0,5488567.0,2,"For example, you could use Swisscom's Elasticsearch service to store these metrics and then analyze it using Kibana.",0.0,0.0,1.0,0.0
Datadog,52590134,52588302,4,"2018/10/01, 14:29:23",True,"2018/10/01, 14:29:23",941.0,5488567.0,2,Metrics using loggregator (firehose),0.0,0.0,1.0,0.0
Datadog,52590134,52588302,4,"2018/10/01, 14:29:23",True,"2018/10/01, 14:29:23",941.0,5488567.0,2,"The firehose allows streaming logs to clients for two types of roles:
Streaming  all  logs to admins (which requires a UAA client with admin permissions) and streaming  app  logs and metrics to devs with permissions in the app's space.",0.0,0.0,1.0,0.0
Datadog,52590134,52588302,4,"2018/10/01, 14:29:23",True,"2018/10/01, 14:29:23",941.0,5488567.0,2,This is also what the  cf logs  command uses.,0.0,0.0,1.0,0.0
Datadog,52590134,52588302,4,"2018/10/01, 14:29:23",True,"2018/10/01, 14:29:23",941.0,5488567.0,2,cf top  also works this way  (it enumerates all apps and streams the logs of each app).,0.0,0.101,0.899,0.2023
Datadog,52590134,52588302,4,"2018/10/01, 14:29:23",True,"2018/10/01, 14:29:23",941.0,5488567.0,2,"However, you will find out that most open source tools that leverage the firehose only work in admin mode, since they're written for the platform operator.",0.0,0.0,1.0,0.0
Datadog,52590134,52588302,4,"2018/10/01, 14:29:23",True,"2018/10/01, 14:29:23",941.0,5488567.0,2,"Of course you also have the possibility to monitor your app by instrumenting it (white box approach), for example by configuring Spring actuator in a Spring boot app or by including an agent of your favourite APM vendor (Dynatrace, AppDynamics, ...)",0.0,0.0,1.0,0.0
Datadog,52590134,52588302,4,"2018/10/01, 14:29:23",True,"2018/10/01, 14:29:23",941.0,5488567.0,2,I guess this is the most common approach; we've seen a lot of teams having success by instrumenting their applications.,0.0,0.179,0.821,0.5719
Datadog,52590134,52588302,4,"2018/10/01, 14:29:23",True,"2018/10/01, 14:29:23",941.0,5488567.0,2,Especially since advanced monitoring anyway requires you to create your own metrics as the firehose provided cpu/memory metrics are not that powerful in a microservice world.,0.081,0.153,0.766,0.2608
Datadog,52590134,52588302,4,"2018/10/01, 14:29:23",True,"2018/10/01, 14:29:23",941.0,5488567.0,2,"However, option 2. would be worth a try as well, especially since the ELK's stack metric support is getting better and better.",0.0,0.439,0.561,0.8885
Datadog,51209988,51097821,2,"2018/07/06, 14:58:20",False,"2018/07/08, 06:33:21",4349.0,5030709.0,0,How are you spinning up ecs-agent container?,0.0,0.0,1.0,0.0
Datadog,51209988,51097821,2,"2018/07/06, 14:58:20",False,"2018/07/08, 06:33:21",4349.0,5030709.0,0,What is docker run command?.,0.0,0.0,1.0,0.0
Datadog,51209988,51097821,2,"2018/07/06, 14:58:20",False,"2018/07/08, 06:33:21",4349.0,5030709.0,0,Did you try like below?.,0.0,0.385,0.615,0.3612
Datadog,50266462,50266331,4,"2018/05/10, 09:02:11",False,"2018/05/10, 09:02:11",162400.0,174777.0,0,"Yes, you can pass a script to the instance that will be executed on the first boot (but not thereafter).",0.0,0.13,0.87,0.4019
Datadog,50266462,50266331,4,"2018/05/10, 09:02:11",False,"2018/05/10, 09:02:11",162400.0,174777.0,0,It is often referred to as a  User Data script .,0.0,0.0,1.0,0.0
Datadog,50266462,50266331,4,"2018/05/10, 09:02:11",False,"2018/05/10, 09:02:11",162400.0,174777.0,0,See:,0.0,0.0,1.0,0.0
Datadog,50266462,50266331,4,"2018/05/10, 09:02:11",False,"2018/05/10, 09:02:11",162400.0,174777.0,0,"If you wish to install  after  the instance has started, use the  AWS Systems Manager Run Command .",0.0,0.144,0.856,0.4019
Datadog,54622604,49892339,0,"2019/02/11, 02:38:36",False,"2019/02/11, 02:38:36",385.0,912643.0,1,The simplest way to get access to data in Cloudyn is to configure a report and schedule data to be pushed to a storage account.,0.0,0.0,1.0,0.0
Datadog,54622604,49892339,0,"2019/02/11, 02:38:36",False,"2019/02/11, 02:38:36",385.0,912643.0,1,"From there, you can use standard storage account APIs to access the data.",0.0,0.0,1.0,0.0
Datadog,54622604,49892339,0,"2019/02/11, 02:38:36",False,"2019/02/11, 02:38:36",385.0,912643.0,1,"Instead of using Cloudyn APIs, however, I would recommend using the  Cost Management Query API  for aggregated cost/usage data or  UsageDetails API  for raw usage.",0.0,0.098,0.902,0.3612
Datadog,48502902,48475616,0,"2018/01/29, 15:54:07",False,"2018/01/29, 15:54:07",914.0,1679567.0,0,"If you want to secure access to docker socket,  this docker documents  is a good start.",0.0,0.355,0.645,0.6808
Datadog,48576285,48457894,0,"2018/02/02, 07:34:44",True,"2019/03/29, 04:35:10",253.0,607808.0,1,Spoke with Datadog support.,0.0,0.474,0.526,0.4019
Datadog,48576285,48457894,0,"2018/02/02, 07:34:44",True,"2019/03/29, 04:35:10",253.0,607808.0,1,Very helpful but the short answer is that there is currently no option to add additional tags to specify the specific proc_name in the individual  gunicorn.yaml  file.,0.094,0.069,0.838,-0.191
Datadog,48576285,48457894,0,"2018/02/02, 07:34:44",True,"2019/03/29, 04:35:10",253.0,607808.0,1,As a workaround to enable grouping we enabled unique prefixes for each application but the trade-off is that the metrics are no longer sharing the same namespace.,0.092,0.121,0.787,0.2263
Datadog,48576285,48457894,0,"2018/02/02, 07:34:44",True,"2019/03/29, 04:35:10",253.0,607808.0,1,I've submitted a new feature request on the Github project which will hopefully be considered.,0.0,0.172,0.828,0.4019
Datadog,46154565,46154416,0,"2017/09/11, 14:24:24",False,"2017/09/11, 14:24:24",31125.0,460802.0,0,Response time is in  time  field.,0.0,0.0,1.0,0.0
Datadog,46154565,46154416,0,"2017/09/11, 14:24:24",False,"2017/09/11, 14:24:24",31125.0,460802.0,0,There is an additional metric  latency  which provides time to first byte.,0.0,0.0,1.0,0.0
Datadog,46154565,46154416,0,"2017/09/11, 14:24:24",False,"2017/09/11, 14:24:24",31125.0,460802.0,0,See:,0.0,0.0,1.0,0.0
Datadog,46154565,46154416,0,"2017/09/11, 14:24:24",False,"2017/09/11, 14:24:24",31125.0,460802.0,0,You might also want to read :,0.0,0.206,0.794,0.0772
Datadog,46079393,46079056,1,"2017/09/06, 18:42:48",True,"2017/09/06, 18:42:48",2688.0,2718295.0,4,Shared buffers are used for postgres memory cache (at a lower level closer to postgres as compared to OS cache).,0.102,0.111,0.787,0.0516
Datadog,46079393,46079056,1,"2017/09/06, 18:42:48",True,"2017/09/06, 18:42:48",2688.0,2718295.0,4,Setting it to 7gb means that pg will cache to 7gb of data.,0.0,0.0,1.0,0.0
Datadog,46079393,46079056,1,"2017/09/06, 18:42:48",True,"2017/09/06, 18:42:48",2688.0,2718295.0,4,So if you are doing a lot of full table scans or (recursive) CTEs that may improve performance.,0.0,0.153,0.847,0.4404
Datadog,46079393,46079056,1,"2017/09/06, 18:42:48",True,"2017/09/06, 18:42:48",2688.0,2718295.0,4,"Note that  postgres  master process will allocate this entire amount at database startup, which is why you are seeing your OS use 10GB of ram now.",0.0,0.0,1.0,0.0
Datadog,46079393,46079056,1,"2017/09/06, 18:42:48",True,"2017/09/06, 18:42:48",2688.0,2718295.0,4,work_mem  is memory used for sorts and  each  concurrent sort allocates a bucket of this size.,0.0,0.0,1.0,0.0
Datadog,46079393,46079056,1,"2017/09/06, 18:42:48",True,"2017/09/06, 18:42:48",2688.0,2718295.0,4,"Therefore this is only bounded by  max_connections  * concurrent sorts, so effectively it is  only  bounded by the sort complexity of your queries, so increasing this poses the most risk to system stability.",0.066,0.104,0.83,0.3288
Datadog,46079393,46079056,1,"2017/09/06, 18:42:48",True,"2017/09/06, 18:42:48",2688.0,2718295.0,4,"(That is, if you have a single query that the query planner executes with 8 merge sorts, you will use 8* work_mem  every time the query is executed).",0.0,0.0,1.0,0.0
Datadog,46079393,46079056,1,"2017/09/06, 18:42:48",True,"2017/09/06, 18:42:48",2688.0,2718295.0,4,maintenance_work_mem  is the memory used by  VACUUM  and friends (including  ALTER TABLE ADD FOREIGN KEY !,0.0,0.195,0.805,0.5255
Datadog,46079393,46079056,1,"2017/09/06, 18:42:48",True,"2017/09/06, 18:42:48",2688.0,2718295.0,4,Increasing this may increase VACUUM speed.,0.0,0.315,0.685,0.3182
Datadog,46079393,46079056,1,"2017/09/06, 18:42:48",True,"2017/09/06, 18:42:48",2688.0,2718295.0,4,"wal_buffers  has no benefit beyond 16MB, which is the largest WAL chunk the server will write at one time.",0.099,0.135,0.766,0.2023
Datadog,46079393,46079056,1,"2017/09/06, 18:42:48",True,"2017/09/06, 18:42:48",2688.0,2718295.0,4,This can help with slow write i/o.,0.0,0.31,0.69,0.4019
Datadog,46079393,46079056,1,"2017/09/06, 18:42:48",True,"2017/09/06, 18:42:48",2688.0,2718295.0,4,See also:  https://wiki.postgresql.org/wiki/Tuning_Your_PostgreSQL_Server,0.0,0.0,1.0,0.0
Datadog,45147281,45147143,2,"2017/07/17, 17:47:26",False,"2017/07/17, 17:47:26",677.0,6803997.0,1,It depends.,0.0,0.0,1.0,0.0
Datadog,45147281,45147143,2,"2017/07/17, 17:47:26",False,"2017/07/17, 17:47:26",677.0,6803997.0,1,"Also, your image doesn't display as of my answer.",0.0,0.0,1.0,0.0
Datadog,45147281,45147143,2,"2017/07/17, 17:47:26",False,"2017/07/17, 17:47:26",677.0,6803997.0,1,"If your machines are super memory hungry and you are an individual without unlimited income, I think your approach would be fine.",0.0,0.231,0.769,0.6908
Datadog,45147281,45147143,2,"2017/07/17, 17:47:26",False,"2017/07/17, 17:47:26",677.0,6803997.0,1,"I would recommend a slightly higher arbitrary percentage to start with, such as 50%, to provide a bit of wiggle room.",0.0,0.128,0.872,0.3612
Datadog,45147281,45147143,2,"2017/07/17, 17:47:26",False,"2017/07/17, 17:47:26",677.0,6803997.0,1,Continue to analyze the memory usage and adjust your maximum accordingly.,0.0,0.0,1.0,0.0
Datadog,45147281,45147143,2,"2017/07/17, 17:47:26",False,"2017/07/17, 17:47:26",677.0,6803997.0,1,I don't see any reason to set memory usage below default.,0.0,0.0,1.0,0.0
Datadog,45147281,45147143,2,"2017/07/17, 17:47:26",False,"2017/07/17, 17:47:26",677.0,6803997.0,1,"Otherwise, you can be much more gratuitous and provide 100-200% extra memory, in case your application experiences sudden heavy load.",0.0,0.0,1.0,0.0
Datadog,41028352,41025255,0,"2016/12/08, 00:14:48",False,"2016/12/08, 00:14:48",643.0,4214037.0,1,"As you can see  here , Ansible provides role dependecies.",0.0,0.0,1.0,0.0
Datadog,41028352,41025255,0,"2016/12/08, 00:14:48",False,"2016/12/08, 00:14:48",643.0,4214037.0,1,You may create in  Datadog.datadog  role new directory named meta with main.yml file.,0.0,0.149,0.851,0.2732
Datadog,41028352,41025255,0,"2016/12/08, 00:14:48",False,"2016/12/08, 00:14:48",643.0,4214037.0,1,In  meta/main.yml  write,0.0,0.0,1.0,0.0
Datadog,41028352,41025255,0,"2016/12/08, 00:14:48",False,"2016/12/08, 00:14:48",643.0,4214037.0,1,"After that, when you call  Datadog.datadog  role, Ansible will run  role1  automatically before  Datadog.datadog  role.",0.0,0.0,1.0,0.0
Datadog,41028352,41025255,0,"2016/12/08, 00:14:48",False,"2016/12/08, 00:14:48",643.0,4214037.0,1,"If you create another role named  Datadog.datadog1  with the same  meta/main.yml  file and call roles  Datadog.datadog  and  Datadog.datadog1 , then Ansible will run  role1  only once, before running Datadogs roles.",0.0,0.07,0.93,0.2732
Datadog,38387725,38359717,0,"2016/07/15, 07:08:42",True,"2016/07/15, 07:08:42",1048.0,3778976.0,1,"For your data model, I would suggest adding   time   as a clustering column:",0.0,0.0,1.0,0.0
Datadog,38387725,38359717,0,"2016/07/15, 07:08:42",True,"2016/07/15, 07:08:42",1048.0,3778976.0,1,Use descending order to keep the latest metrics first.,0.0,0.0,1.0,0.0
Datadog,38387725,38359717,0,"2016/07/15, 07:08:42",True,"2016/07/15, 07:08:42",1048.0,3778976.0,1,You can then query using the LIMIT clause to get the most recent hour:,0.0,0.0,1.0,0.0
Datadog,38387725,38359717,0,"2016/07/15, 07:08:42",True,"2016/07/15, 07:08:42",1048.0,3778976.0,1,Or day:,0.0,0.0,1.0,0.0
Datadog,38387725,38359717,0,"2016/07/15, 07:08:42",True,"2016/07/15, 07:08:42",1048.0,3778976.0,1,"Depending upon how long you plan to keep the data, you may want to add a  column for year, month, or days to the table to limit your partition size.",0.0,0.044,0.956,0.0772
Datadog,38387725,38359717,0,"2016/07/15, 07:08:42",True,"2016/07/15, 07:08:42",1048.0,3778976.0,1,"For example, if you wish to keep data for 3 months,  a  month  column can be added to partition your keys by id and month:",0.0,0.109,0.891,0.4019
Datadog,38387725,38359717,0,"2016/07/15, 07:08:42",True,"2016/07/15, 07:08:42",1048.0,3778976.0,1,"If you keep data for several years, use year + month or a date value.",0.0,0.167,0.833,0.34
Datadog,38387725,38359717,0,"2016/07/15, 07:08:42",True,"2016/07/15, 07:08:42",1048.0,3778976.0,1,"Regarding your final question, about separate tables or a single table.",0.0,0.0,1.0,0.0
Datadog,38387725,38359717,0,"2016/07/15, 07:08:42",True,"2016/07/15, 07:08:42",1048.0,3778976.0,1,"Cassandra supports sparse columns, so you can make multiple inserts in a common table for each metric without updating any data.",0.0,0.116,0.884,0.3612
Datadog,38387725,38359717,0,"2016/07/15, 07:08:42",True,"2016/07/15, 07:08:42",1048.0,3778976.0,1,"However, it's always faster to write just once per row.",0.0,0.0,1.0,0.0
Datadog,38387725,38359717,0,"2016/07/15, 07:08:42",True,"2016/07/15, 07:08:42",1048.0,3778976.0,1,You may need separate tables if you have to query for different metrics by an alternative key.,0.0,0.0,1.0,0.0
Datadog,38387725,38359717,0,"2016/07/15, 07:08:42",True,"2016/07/15, 07:08:42",1048.0,3778976.0,1,"For example, query for disk usage by id and disk name.",0.0,0.0,1.0,0.0
Datadog,38387725,38359717,0,"2016/07/15, 07:08:42",True,"2016/07/15, 07:08:42",1048.0,3778976.0,1,You'd need a separate table or a materialized view to support that query pattern.,0.0,0.197,0.803,0.4019
Datadog,38387725,38359717,0,"2016/07/15, 07:08:42",True,"2016/07/15, 07:08:42",1048.0,3778976.0,1,"Finally, your schema defines an  assetid , but this isn't defined in your primary key so with your current schema you can't query using assetid.",0.0,0.0,1.0,0.0
Datadog,36528782,35866315,0,"2016/04/10, 13:51:31",True,"2016/04/10, 13:51:31",56.0,355476.0,3,Try,0.0,0.0,1.0,0.0
Datadog,66513856,66005706,0,"2021/03/07, 08:31:28",False,"2021/03/07, 08:31:28",11.0,15128440.0,0,"Finally, found the solution by examining services logs.",0.0,0.247,0.753,0.3182
Datadog,66513856,66005706,0,"2021/03/07, 08:31:28",False,"2021/03/07, 08:31:28",11.0,15128440.0,0,w32time service was configured as &quot;manual start&quot; instead of delay-auto or auto.,0.0,0.0,1.0,0.0
Datadog,66513856,66005706,0,"2021/03/07, 08:31:28",False,"2021/03/07, 08:31:28",11.0,15128440.0,0,"And though &quot;Set time automatically&quot; was &quot;On&quot;, clock synchronization only happened after starting the service and resyncing.",0.0,0.0,1.0,0.0
Datadog,66513856,66005706,0,"2021/03/07, 08:31:28",False,"2021/03/07, 08:31:28",11.0,15128440.0,0,So I've changed the Startup type of the Windows time service from Manual to Automatic (Delayed Start) using the following command:,0.0,0.0,1.0,0.0
Datadog,66513856,66005706,0,"2021/03/07, 08:31:28",False,"2021/03/07, 08:31:28",11.0,15128440.0,0,&amp;sc.exe config w32time start= delayed-auto,0.0,0.0,1.0,0.0
Datadog,66513856,66005706,0,"2021/03/07, 08:31:28",False,"2021/03/07, 08:31:28",11.0,15128440.0,0,Disabled Time Synchronization (service task) in Task Scheduler.,0.0,0.0,1.0,0.0
Datadog,66513856,66005706,0,"2021/03/07, 08:31:28",False,"2021/03/07, 08:31:28",11.0,15128440.0,0,Disable-ScheduledTask -TaskName &quot;SynchronizeTime&quot; -TaskPath &quot;\Microsoft\Windows\Time Synchronization&quot;,0.0,0.0,1.0,0.0
Datadog,66513856,66005706,0,"2021/03/07, 08:31:28",False,"2021/03/07, 08:31:28",11.0,15128440.0,0,Time Skews are being fixed automatically and occur less often.,0.0,0.0,1.0,0.0
Datadog,65786188,65785913,6,"2021/01/19, 07:57:08",True,"2021/01/19, 08:18:40",5019.0,10186808.0,1,Typical work flow will look like this (there are other methods),0.0,0.2,0.8,0.3612
Datadog,65786188,65785913,6,"2021/01/19, 07:57:08",True,"2021/01/19, 08:18:40",5019.0,10186808.0,1,"This can be achieved in multiple ways, easiest is to create a template with index pattern , alias and mapping.",0.0,0.234,0.766,0.5994
Datadog,65786188,65785913,6,"2021/01/19, 07:57:08",True,"2021/01/19, 08:18:40",5019.0,10186808.0,1,Example: Any new index created matching the pattern  staff-*  will be assigned with given mapping and attached to alias  staff   and we can query  staff  instead of individual indexes and setup alerts.,0.0,0.061,0.939,0.25
Datadog,65786188,65785913,6,"2021/01/19, 07:57:08",True,"2021/01/19, 08:18:40",5019.0,10186808.0,1,We can use cwl--aws-containerinsights-eks-cluster-for-test-host to run queries.,0.0,0.0,1.0,0.0
Datadog,65786188,65785913,6,"2021/01/19, 07:57:08",True,"2021/01/19, 08:18:40",5019.0,10186808.0,1,"Note: If unsure of mapping, we can remove mapping section.",0.182,0.0,0.818,-0.25
Datadog,66059109,63338416,0,"2021/02/05, 09:17:00",False,"2021/02/05, 09:17:00",1.0,15150258.0,0,FetchFollower acting like &quot;long poll&quot; request.,0.0,0.333,0.667,0.3612
Datadog,66059109,63338416,0,"2021/02/05, 09:17:00",False,"2021/02/05, 09:17:00",1.0,15150258.0,0,It waits till it gets  replica.fetch.min.bytes  data for replication or  replica.fetch.wait.max.ms  timeout (which is by default 500ms).,0.0,0.0,1.0,0.0
Datadog,66059109,63338416,0,"2021/02/05, 09:17:00",False,"2021/02/05, 09:17:00",1.0,15150258.0,0,"So it's basically ok, it's just means that most of FetchFollower requests are waiting for data",0.0,0.141,0.859,0.3535
Datadog,58132394,58094024,3,"2019/09/27, 13:06:50",True,"2019/09/27, 13:06:50",26.0,5036870.0,1,This is likely related to this  issue in the portmap plugin .,0.0,0.0,1.0,0.0
Datadog,58132394,58094024,3,"2019/09/27, 13:06:50",True,"2019/09/27, 13:06:50",26.0,5036870.0,1,"The current working theory is that a conntrack entry is created when the client pod reaches out for the UDP host port, and that entry becomes stale when the server pod is deleted, but it's not deleted, so clients keep hitting it, essentially blackholing the traffic.",0.0,0.057,0.943,0.1531
Datadog,58132394,58094024,3,"2019/09/27, 13:06:50",True,"2019/09/27, 13:06:50",26.0,5036870.0,1,You can try removing the conntrack entry with something like  conntrack -D -p udp --dport 8125  on one of the impacted host.,0.0,0.106,0.894,0.3612
Datadog,58132394,58094024,3,"2019/09/27, 13:06:50",True,"2019/09/27, 13:06:50",26.0,5036870.0,1,If that solves the issue then that was the root cause of your problem.,0.161,0.125,0.714,-0.1531
Datadog,58132394,58094024,3,"2019/09/27, 13:06:50",True,"2019/09/27, 13:06:50",26.0,5036870.0,1,This workaround described in the GitHub issue should mitigate the issue until a fix is merged:,0.0,0.0,1.0,0.0
Datadog,58132394,58094024,3,"2019/09/27, 13:06:50",True,"2019/09/27, 13:06:50",26.0,5036870.0,1,You can add an initContainer to the server's pod to run the conntrack command when it starts:,0.0,0.0,1.0,0.0
Datadog,51727825,51727504,2,"2018/08/07, 16:19:11",False,"2018/08/07, 16:19:11",554.0,6529322.0,2,Some things I'd consider indicative of the health of the cluster are as follows:,0.0,0.0,1.0,0.0
Datadog,51727825,51727504,2,"2018/08/07, 16:19:11",False,"2018/08/07, 16:19:11",554.0,6529322.0,2,Offline/Under Replicated Partitions : This is a good indicator as to whether all the nodes in a cluster are even online.,0.0,0.146,0.854,0.4404
Datadog,51727825,51727504,2,"2018/08/07, 16:19:11",False,"2018/08/07, 16:19:11",554.0,6529322.0,2,"If one goes offline, you will almost certainly see some under-replication, and if several are offline, you might even see some offline partitions.",0.176,0.082,0.742,-0.101
Datadog,51727825,51727504,2,"2018/08/07, 16:19:11",False,"2018/08/07, 16:19:11",554.0,6529322.0,2,"Active Controller : If this keeps changing, then it means that the cluster is potentially unstable.",0.137,0.148,0.714,0.0516
Datadog,51727825,51727504,2,"2018/08/07, 16:19:11",False,"2018/08/07, 16:19:11",554.0,6529322.0,2,"The controller should not change regularly; if it does, then something is wrong with your cluster.",0.171,0.0,0.829,-0.4767
Datadog,51727825,51727504,2,"2018/08/07, 16:19:11",False,"2018/08/07, 16:19:11",554.0,6529322.0,2,Bytes In/Out : These show that your cluster is able to send and receive data.,0.0,0.0,1.0,0.0
Datadog,51727825,51727504,2,"2018/08/07, 16:19:11",False,"2018/08/07, 16:19:11",554.0,6529322.0,2,"If these are lower than you'd expect, then it might imply that the cluster is undergoing some sort of network issue which would possibly impact the cluster health.",0.075,0.0,0.925,-0.296
Datadog,51727825,51727504,2,"2018/08/07, 16:19:11",False,"2018/08/07, 16:19:11",554.0,6529322.0,2,Hope this helps!,0.0,0.853,0.147,0.6996
Datadog,41252693,41252657,4,"2016/12/21, 01:13:34",False,"2016/12/21, 01:13:34",110.0,6938240.0,-1,.strip()  for removing whitespace characters.,0.0,0.0,1.0,0.0
Datadog,41252693,41252657,4,"2016/12/21, 01:13:34",False,"2016/12/21, 01:13:34",110.0,6938240.0,-1,".replace('offset=', '')  for removing that string.",0.0,0.0,1.0,0.0
Datadog,41252693,41252657,4,"2016/12/21, 01:13:34",False,"2016/12/21, 01:13:34",110.0,6938240.0,-1,You should be able to chain them too.,0.0,0.0,1.0,0.0
Datadog,41252760,41252657,0,"2016/12/21, 01:20:55",False,"2016/12/21, 01:31:54",26687.0,5352399.0,0,How to extract the numeric value appeared after  offset= ?,0.0,0.231,0.769,0.34
Datadog,41252760,41252657,0,"2016/12/21, 01:20:55",False,"2016/12/21, 01:31:54",26687.0,5352399.0,0,Why i prefer regular expression?,0.0,0.0,1.0,0.0
Datadog,41252760,41252657,0,"2016/12/21, 01:20:55",False,"2016/12/21, 01:31:54",26687.0,5352399.0,0,"Because even if the string contains other keywords, regular expression will extract the numeric value which appeared after the  offset=  expression.",0.0,0.107,0.893,0.34
Datadog,41252760,41252657,0,"2016/12/21, 01:20:55",False,"2016/12/21, 01:31:54",26687.0,5352399.0,0,"For example, check for the following cases with my given example.",0.0,0.0,1.0,0.0
Datadog,41252760,41252657,0,"2016/12/21, 01:20:55",False,"2016/12/21, 01:31:54",26687.0,5352399.0,0,How to remove leading and trailing whitespace characters?,0.0,0.0,1.0,0.0
Datadog,41252760,41252657,0,"2016/12/21, 01:20:55",False,"2016/12/21, 01:31:54",26687.0,5352399.0,0,"will remove all the leading and trailing whitespace characters such as \n, \r, \t, \f, space.",0.0,0.0,1.0,0.0
Datadog,41252760,41252657,0,"2016/12/21, 01:20:55",False,"2016/12/21, 01:31:54",26687.0,5352399.0,0,For more flexibility use the following,0.0,0.35,0.65,0.4005
Datadog,41252760,41252657,0,"2016/12/21, 01:20:55",False,"2016/12/21, 01:31:54",26687.0,5352399.0,0,Reference: see this SO  answer .,0.0,0.0,1.0,0.0
Datadog,41252788,41252657,0,"2016/12/21, 01:23:06",False,"2016/12/21, 01:23:06",17322.0,763269.0,1,The straightforward way is:,0.0,0.0,1.0,0.0
Datadog,41252788,41252657,0,"2016/12/21, 01:23:06",False,"2016/12/21, 01:23:06",17322.0,763269.0,1,For example:,0.0,0.0,1.0,0.0
Datadog,41252788,41252657,0,"2016/12/21, 01:23:06",False,"2016/12/21, 01:23:06",17322.0,763269.0,1,Example:,0.0,0.0,1.0,0.0
Datadog,41252788,41252657,0,"2016/12/21, 01:23:06",False,"2016/12/21, 01:23:06",17322.0,763269.0,1,Up to you if you need to convert the output.,0.0,0.0,1.0,0.0
Datadog,39060683,39060624,1,"2016/08/21, 06:47:56",True,"2016/08/21, 06:47:56",95948.0,1079354.0,1,"Since it looks like  StatsDClient  is an interface of some kind, it would make your testing effort easier to simply inject this dependency into your object.",0.0,0.274,0.726,0.8271
Datadog,39060683,39060624,1,"2016/08/21, 06:47:56",True,"2016/08/21, 06:47:56",95948.0,1079354.0,1,"Even if you're not using an IoC container like Spring or Guice, you can still somewhat control this simply by passing an instance of it in through the constructor.",0.0,0.082,0.918,0.3612
Datadog,39060683,39060624,1,"2016/08/21, 06:47:56",True,"2016/08/21, 06:47:56",95948.0,1079354.0,1,This will make your testing simpler since all you realistically need to do is mock the object passed in during test.,0.123,0.0,0.877,-0.4215
Datadog,39060683,39060624,1,"2016/08/21, 06:47:56",True,"2016/08/21, 06:47:56",95948.0,1079354.0,1,"Right now, the reason it's failing is because you're  new ing up the instance, and Mockito (in this current configuration) isn't equipped to mock the newed instance.",0.108,0.076,0.816,-0.2425
Datadog,39060683,39060624,1,"2016/08/21, 06:47:56",True,"2016/08/21, 06:47:56",95948.0,1079354.0,1,"In all honesty, this set up will make testing simpler to conduct, and you should only need your client configured in one area.",0.0,0.127,0.873,0.4939
Datadog,39061963,39060624,0,"2016/08/21, 10:42:11",False,"2016/08/21, 10:42:11",126469.0,1531124.0,0,You are getting things wrong here.,0.383,0.0,0.617,-0.4767
Datadog,39061963,39060624,0,"2016/08/21, 10:42:11",False,"2016/08/21, 10:42:11",126469.0,1531124.0,0,"You don't use a  mocking  framework to test your ""class under test"".",0.0,0.184,0.816,0.3089
Datadog,39061963,39060624,0,"2016/08/21, 10:42:11",False,"2016/08/21, 10:42:11",126469.0,1531124.0,0,"You use the mocking framework to create  mocked  objects; which you then pass to your ""class under test"" within a test case.",0.199,0.084,0.717,-0.4404
Datadog,39061963,39060624,0,"2016/08/21, 10:42:11",False,"2016/08/21, 10:42:11",126469.0,1531124.0,0,"Then your ""code under test"" calls methods on the mocked object; and by controlling returned values (or by verifying what happens to your mock); that is how you write your testcases.",0.068,0.079,0.853,0.1027
Datadog,39061963,39060624,0,"2016/08/21, 10:42:11",False,"2016/08/21, 10:42:11",126469.0,1531124.0,0,"So, your testcase for a MetricRecorder doesn't mock a MetricRecorder; it should mock the StatsDClient class; and as Makoto suggests; use  dependency  injection to put an object of that class into MetricRecorder.",0.0,0.143,0.857,0.5667
Datadog,39061963,39060624,0,"2016/08/21, 10:42:11",False,"2016/08/21, 10:42:11",126469.0,1531124.0,0,"Besides: basically writing ""test-able"" code is something that needs to be practiced.",0.0,0.0,1.0,0.0
Datadog,39061963,39060624,0,"2016/08/21, 10:42:11",False,"2016/08/21, 10:42:11",126469.0,1531124.0,0,I wholeheartedly recommend you to watch these  videos  if you are serious about getting in this business.,0.073,0.14,0.787,0.296
Datadog,39061963,39060624,0,"2016/08/21, 10:42:11",False,"2016/08/21, 10:42:11",126469.0,1531124.0,0,All of them; really (worth each second!,0.0,0.0,1.0,0.0
Datadog,39061963,39060624,0,"2016/08/21, 10:42:11",False,"2016/08/21, 10:42:11",126469.0,1531124.0,0,).,0.0,0.0,1.0,0.0
Datadog,24537072,24536971,1,"2014/07/02, 20:08:18",True,"2014/07/07, 19:45:33",29.0,3794458.0,0,Kamon was being built for Java 1.7 by default.,0.0,0.0,1.0,0.0
Datadog,24537072,24536971,1,"2014/07/02, 20:08:18",True,"2014/07/07, 19:45:33",29.0,3794458.0,0,"Now, it will support 1.6",0.0,0.403,0.597,0.4019
Datadog,67184949,67161917,0,"2021/04/20, 22:09:14",False,"2021/04/20, 22:09:14",116.0,9839284.0,0,"As told by @TRW in the comments, using this should do the trick:",0.091,0.0,0.909,-0.0516
Datadog,66871585,66761816,0,"2021/03/30, 16:09:45",True,"2021/03/30, 16:09:45",329.0,6301287.0,0,I had to open a ticket asking the Heroku CS team to apply the &quot;pg_monitor&quot; role to my user.,0.0,0.0,1.0,0.0
Datadog,66871585,66761816,0,"2021/03/30, 16:09:45",True,"2021/03/30, 16:09:45",329.0,6301287.0,0,They've granted the role and now everything is working fine,0.0,0.322,0.678,0.4215
Datadog,65746430,65744466,0,"2021/01/16, 06:12:41",False,"2021/01/16, 06:12:41",13378.0,1430810.0,1,Maybe it's a complicated idea but I think you can make your own cache store wrapper that decides which cache store to use if I understand your question correctly.,0.0,0.0,1.0,0.0
Datadog,65746430,65744466,0,"2021/01/16, 06:12:41",False,"2021/01/16, 06:12:41",13378.0,1430810.0,1,"When calling the cache method, it eventually calls  read_fragment  and  write_fragment  on your controller  https://apidock.com/rails/AbstractController/Caching/Fragments/read_fragment  and those methods call  cache_store.read  and  cache_store.write .",0.0,0.0,1.0,0.0
Datadog,65746430,65744466,0,"2021/01/16, 06:12:41",False,"2021/01/16, 06:12:41",13378.0,1430810.0,1,"Then you could have a custom cache store class with custom  read  and  write  method that, depending on an option, delegates the read and write to real cache stores.",0.0,0.0,1.0,0.0
Datadog,65746430,65744466,0,"2021/01/16, 06:12:41",False,"2021/01/16, 06:12:41",13378.0,1430810.0,1,Then you use it like...,0.0,0.0,1.0,0.0
Datadog,65746430,65744466,0,"2021/01/16, 06:12:41",False,"2021/01/16, 06:12:41",13378.0,1430810.0,1,I'm not sure if that's what you are asking sorry.,0.29,0.0,0.71,-0.3098
Datadog,65026062,65009092,2,"2020/11/26, 18:39:43",True,"2020/11/26, 18:39:43",782.0,1953079.0,1,Easy solution is to fetch this library directly and do  add_subdirectory .,0.0,0.366,0.634,0.6369
Datadog,65026062,65009092,2,"2020/11/26, 18:39:43",True,"2020/11/26, 18:39:43",782.0,1953079.0,1,But this requires cmake &gt;= 3.11.,0.0,0.0,1.0,0.0
Datadog,65026062,65009092,2,"2020/11/26, 18:39:43",True,"2020/11/26, 18:39:43",782.0,1953079.0,1,Create dir  cmake  and file  cmake/cpp-datadogstatsd.cmake,0.0,0.296,0.704,0.2732
Datadog,65026062,65009092,2,"2020/11/26, 18:39:43",True,"2020/11/26, 18:39:43",782.0,1953079.0,1,cpp-datadogstatsd.cmake :,0.0,0.0,1.0,0.0
Datadog,65026062,65009092,2,"2020/11/26, 18:39:43",True,"2020/11/26, 18:39:43",782.0,1953079.0,1,"Then, include this cmake file, and link  DataDogStatsD_static  to your lib/exe:",0.0,0.0,1.0,0.0
Datadog,62864281,62819086,0,"2020/07/12, 20:27:18",False,"2020/07/12, 20:27:18",449.0,12318748.0,0,"As you have limited requirements, you could achieve this without a bot.",0.16,0.0,0.84,-0.2263
Datadog,62864281,62819086,0,"2020/07/12, 20:27:18",False,"2020/07/12, 20:27:18",449.0,12318748.0,0,MS Teams has income and outgoing webhooks.,0.0,0.268,0.732,0.296
Datadog,62864281,62819086,0,"2020/07/12, 20:27:18",False,"2020/07/12, 20:27:18",449.0,12318748.0,0,You could create a  Incoming webhook  inside a Teams channel.,0.0,0.231,0.769,0.2732
Datadog,62864281,62819086,0,"2020/07/12, 20:27:18",False,"2020/07/12, 20:27:18",449.0,12318748.0,0,It provides an URL which you could use inside the monitoring remote server and POST the message in JSON format to the webhook url.,0.0,0.0,1.0,0.0
Datadog,62864281,62819086,0,"2020/07/12, 20:27:18",False,"2020/07/12, 20:27:18",449.0,12318748.0,0,It will be posted in teams channel like below,0.0,0.238,0.762,0.3612
Datadog,62864281,62819086,0,"2020/07/12, 20:27:18",False,"2020/07/12, 20:27:18",449.0,12318748.0,0,For sending message back to the server you need to configure the  Outgoing webhook  in the channel.,0.0,0.121,0.879,0.296
Datadog,62710050,62699424,6,"2020/07/03, 09:57:59",True,"2020/07/03, 09:57:59",3786.0,2509773.0,1,Spring Cloud Data Flow and Skipper servers are  Spring Boot  applications and hence you can configure/customize logging system based on your requirements.,0.0,0.0,1.0,0.0
Datadog,62710050,62699424,6,"2020/07/03, 09:57:59",True,"2020/07/03, 09:57:59",3786.0,2509773.0,1,Here are some of the references to configure logging system for a Spring Boot app:,0.0,0.0,1.0,0.0
Datadog,62448235,62448098,4,"2020/06/18, 13:49:16",True,"2020/06/18, 13:49:16",64309.0,10008173.0,2,docker logs  and similar just collect the stdout and stderr streams from the main process running inside the container.,0.0,0.0,1.0,0.0
Datadog,62448235,62448098,4,"2020/06/18, 13:49:16",True,"2020/06/18, 13:49:16",64309.0,10008173.0,2,"There's not a ""log level"" associated with that, though some systems might treat or highlight the two streams differently.",0.0,0.242,0.758,0.6249
Datadog,62448235,62448098,4,"2020/06/18, 13:49:16",True,"2020/06/18, 13:49:16",64309.0,10008173.0,2,"As a basic example, you could run",0.0,0.0,1.0,0.0
Datadog,62448235,62448098,4,"2020/06/18, 13:49:16",True,"2020/06/18, 13:49:16",64309.0,10008173.0,2,"The resulting file listing isn't especially ""error"" or ""debug"" level.",0.0,0.0,1.0,0.0
Datadog,62448235,62448098,4,"2020/06/18, 13:49:16",True,"2020/06/18, 13:49:16",64309.0,10008173.0,2,"The production-oriented setups I'm used to include the log level in log messages (in a Node context, I've used the  Winston  logging library), and then use a tool like  fluentd  to collect and parse those messages.",0.0,0.07,0.93,0.3612
Datadog,59973857,59973631,8,"2020/01/29, 21:00:41",False,"2020/01/29, 21:00:41",126283.0,14660.0,0,Make sure  CreatedDate  is indexed.,0.0,0.365,0.635,0.3182
Datadog,59973857,59973631,8,"2020/01/29, 21:00:41",False,"2020/01/29, 21:00:41",126283.0,14660.0,0,Make sure  CreatedDate  is using the  date  column type .,0.0,0.223,0.777,0.3182
Datadog,59973857,59973631,8,"2020/01/29, 21:00:41",False,"2020/01/29, 21:00:41",126283.0,14660.0,0,"This will be more efficient on storage (just 4 bytes), performance, and you can use all the built in  date formatting  and  functions .",0.0,0.128,0.872,0.4754
Datadog,59973857,59973631,8,"2020/01/29, 21:00:41",False,"2020/01/29, 21:00:41",126283.0,14660.0,0,Avoid  select *  and only select the columns you need.,0.216,0.0,0.784,-0.296
Datadog,59973857,59973631,8,"2020/01/29, 21:00:41",False,"2020/01/29, 21:00:41",126283.0,14660.0,0,Use  YYYY-MM-DD  ISO 8601 format .,0.0,0.0,1.0,0.0
Datadog,59973857,59973631,8,"2020/01/29, 21:00:41",False,"2020/01/29, 21:00:41",126283.0,14660.0,0,"This has nothing to do with performance, but it will avoid a lot of ambiguity.",0.177,0.0,0.823,-0.4215
Datadog,59973857,59973631,8,"2020/01/29, 21:00:41",False,"2020/01/29, 21:00:41",126283.0,14660.0,0,The real problem is likely that you have thousands of tables with which you regularly make unions of hundreds of tables.,0.119,0.0,0.881,-0.4019
Datadog,59973857,59973631,8,"2020/01/29, 21:00:41",False,"2020/01/29, 21:00:41",126283.0,14660.0,0,This indicates a need to redesign your schema to simplify your queries and get better performance.,0.0,0.172,0.828,0.4404
Datadog,59973857,59973631,8,"2020/01/29, 21:00:41",False,"2020/01/29, 21:00:41",126283.0,14660.0,0,Unions and date change checks suggest a lot of redundancy.,0.0,0.0,1.0,0.0
Datadog,59973857,59973631,8,"2020/01/29, 21:00:41",False,"2020/01/29, 21:00:41",126283.0,14660.0,0,Perhaps you've partitioned your tables by date.,0.0,0.0,1.0,0.0
Datadog,59973857,59973631,8,"2020/01/29, 21:00:41",False,"2020/01/29, 21:00:41",126283.0,14660.0,0,Postgres has its own built in  table partitioning  which might help.,0.0,0.213,0.787,0.4019
Datadog,59973857,59973631,8,"2020/01/29, 21:00:41",False,"2020/01/29, 21:00:41",126283.0,14660.0,0,Without more detail that's all I can say.,0.0,0.0,1.0,0.0
Datadog,59973857,59973631,8,"2020/01/29, 21:00:41",False,"2020/01/29, 21:00:41",126283.0,14660.0,0,Perhaps ask another question about your schema.,0.0,0.0,1.0,0.0
Datadog,59974435,59973631,7,"2020/01/29, 21:40:37",False,"2020/01/29, 21:40:37",18091.0,1721239.0,0,"Without seeing  EXPLAIN (ANALYZE, BUFFERS) , all we can do is speculate.",0.0,0.0,1.0,0.0
Datadog,59974435,59973631,7,"2020/01/29, 21:40:37",False,"2020/01/29, 21:40:37",18091.0,1721239.0,0,But we can do some pretty good speculation.,0.0,0.576,0.424,0.8462
Datadog,59974435,59973631,7,"2020/01/29, 21:40:37",False,"2020/01/29, 21:40:37",18091.0,1721239.0,0,Cluster the tables on the index on CreatedDate.,0.0,0.0,1.0,0.0
Datadog,59974435,59973631,7,"2020/01/29, 21:40:37",False,"2020/01/29, 21:40:37",18091.0,1721239.0,0,"This will allow the data to be accessed more sequentially, allowing more read-ahead (but this might not help much for some kinds of storage).",0.086,0.073,0.841,-0.092
Datadog,59974435,59973631,7,"2020/01/29, 21:40:37",False,"2020/01/29, 21:40:37",18091.0,1721239.0,0,"If the tables have high write load, they may not stay clustered and so you would have recluster them occasionally.",0.0,0.0,1.0,0.0
Datadog,59974435,59973631,7,"2020/01/29, 21:40:37",False,"2020/01/29, 21:40:37",18091.0,1721239.0,0,"If they are static, this could be a one-time event.",0.0,0.0,1.0,0.0
Datadog,59974435,59973631,7,"2020/01/29, 21:40:37",False,"2020/01/29, 21:40:37",18091.0,1721239.0,0,Get more RAM.,0.0,0.0,1.0,0.0
Datadog,59974435,59973631,7,"2020/01/29, 21:40:37",False,"2020/01/29, 21:40:37",18091.0,1721239.0,0,"If you want to perform as if all the data was in memory, then get all the data into memory.",0.0,0.064,0.936,0.0772
Datadog,59974435,59973631,7,"2020/01/29, 21:40:37",False,"2020/01/29, 21:40:37",18091.0,1721239.0,0,"Get faster storage, like top-notch SSD.",0.0,0.333,0.667,0.3612
Datadog,59974435,59973631,7,"2020/01/29, 21:40:37",False,"2020/01/29, 21:40:37",18091.0,1721239.0,0,"It isn't as fast as RAM, but much faster than HDD.",0.0,0.0,1.0,0.0
Datadog,58858157,58625365,0,"2019/11/14, 15:40:57",True,"2019/11/14, 15:40:57",14336.0,1251219.0,0,The answer to the question is found in the comments to it.,0.0,0.0,1.0,0.0
Datadog,58858157,58625365,0,"2019/11/14, 15:40:57",True,"2019/11/14, 15:40:57",14336.0,1251219.0,0,"Hence, this question should not go unanswered.",0.0,0.0,1.0,0.0
Datadog,58858157,58625365,0,"2019/11/14, 15:40:57",True,"2019/11/14, 15:40:57",14336.0,1251219.0,0,"The code from the question works as expected, however, the path where the named pipe resides is a special path and this is the reason why the data that is being sent to it never reaches the script.",0.03,0.07,0.901,0.372
Datadog,58858157,58625365,0,"2019/11/14, 15:40:57",True,"2019/11/14, 15:40:57",14336.0,1251219.0,0,The corresponding special casing in Bash for instance can be found in  redir.c .,0.0,0.184,0.816,0.4019
Datadog,58858157,58625365,0,"2019/11/14, 15:40:57",True,"2019/11/14, 15:40:57",14336.0,1251219.0,0,The solution to the problem is to use a real UDP server on that port:,0.159,0.135,0.706,-0.1027
Datadog,57463587,57423102,0,"2019/08/12, 17:56:44",False,"2019/08/12, 17:56:44",149.0,5059173.0,0,It turns out that someone had turned on a scheduled job that was sending a super expensive query that was supposed to be a singleton onto the query queue every 5 min.,0.0,0.126,0.874,0.5994
Datadog,57463587,57423102,0,"2019/08/12, 17:56:44",False,"2019/08/12, 17:56:44",149.0,5059173.0,0,"The query takes 20 min to run, so eventually the system bogs down and falls over.",0.0,0.0,1.0,0.0
Datadog,57014014,56993743,0,"2019/07/13, 00:03:23",False,"2019/07/13, 00:03:23",8484.0,1224827.0,0,"Apparently this was / is an issue with  rpy2 , which was a dependency of our project.",0.0,0.0,1.0,0.0
Datadog,57014014,56993743,0,"2019/07/13, 00:03:23",False,"2019/07/13, 00:03:23",8484.0,1224827.0,0,It was being imported by a utility module that was imported on startup.,0.0,0.0,1.0,0.0
Datadog,57014014,56993743,0,"2019/07/13, 00:03:23",False,"2019/07/13, 00:03:23",8484.0,1224827.0,0,This caused it to be called on every single request to our REST API endpoints.,0.0,0.0,1.0,0.0
Datadog,57014014,56993743,0,"2019/07/13, 00:03:23",False,"2019/07/13, 00:03:23",8484.0,1224827.0,0,Putting the import inside the actual function that was using it fixed this issue.,0.0,0.0,1.0,0.0
Datadog,56173299,56172624,0,"2019/05/16, 19:55:12",True,"2019/05/16, 19:55:12",1371.0,5540166.0,2,"What you want is either the  is_match  or  is_exact_match  conditional variable, which are  documented here  (with examples).",0.0,0.075,0.925,0.0772
Datadog,56173299,56172624,0,"2019/05/16, 19:55:12",True,"2019/05/16, 19:55:12",1371.0,5540166.0,2,"The idea is that you can nest your messages  and notifications  in conditional logic arguments so that only when the monitor alerts/warns/resolves, or only when the evaluated tag scope matches certain conditions, will certain messages or notification channels be part of the alert.",0.056,0.133,0.811,0.4019
Datadog,56173299,56172624,0,"2019/05/16, 19:55:12",True,"2019/05/16, 19:55:12",1371.0,5540166.0,2,So in your case you want your message to include something like this:,0.0,0.257,0.743,0.4215
Datadog,56173299,56172624,0,"2019/05/16, 19:55:12",True,"2019/05/16, 19:55:12",1371.0,5540166.0,2,"{{#is_exact_match ""environment.name"" ""prod""}}",0.0,0.0,1.0,0.0
Datadog,56173299,56172624,0,"2019/05/16, 19:55:12",True,"2019/05/16, 19:55:12",1371.0,5540166.0,2,Add special prod message here,0.0,0.403,0.597,0.4019
Datadog,56173299,56172624,0,"2019/05/16, 19:55:12",True,"2019/05/16, 19:55:12",1371.0,5540166.0,2,and @pagerduty or @pagerduty-foo,0.0,0.0,1.0,0.0
Datadog,56173299,56172624,0,"2019/05/16, 19:55:12",True,"2019/05/16, 19:55:12",1371.0,5540166.0,2,{{/is_exact_match}},0.0,0.0,1.0,0.0
Datadog,56173299,56172624,0,"2019/05/16, 19:55:12",True,"2019/05/16, 19:55:12",1371.0,5540166.0,2,Add message that should always show up here,0.0,0.0,1.0,0.0
Datadog,56173299,56172624,0,"2019/05/16, 19:55:12",True,"2019/05/16, 19:55:12",1371.0,5540166.0,2,and @slack-bar,0.0,0.0,1.0,0.0
Datadog,56173299,56172624,0,"2019/05/16, 19:55:12",True,"2019/05/16, 19:55:12",1371.0,5540166.0,2,"In this case, only when the ""environment"" tag's value is ""prod"" will the bracketed content be included (which includes the pagerduty notification).",0.0,0.103,0.897,0.34
Datadog,56173299,56172624,0,"2019/05/16, 19:55:12",True,"2019/05/16, 19:55:12",1371.0,5540166.0,2,The non-bracketed part will always be included (which includes the slack notification).,0.0,0.0,1.0,0.0
Datadog,53732835,53732707,0,"2018/12/11, 23:49:57",False,"2018/12/11, 23:49:57",1455.0,2301088.0,0,(?,0.0,0.0,1.0,0.0
Datadog,53732835,53732707,0,"2018/12/11, 23:49:57",False,"2018/12/11, 23:49:57",1455.0,2301088.0,0,":\/private\/toolbox\/)(.+)  ought to match your route path, capturing the wildcard as the first group:",0.0,0.0,1.0,0.0
Datadog,53732835,53732707,0,"2018/12/11, 23:49:57",False,"2018/12/11, 23:49:57",1455.0,2301088.0,0,"I cannot speak to that RegExp's performance, however.",0.0,0.0,1.0,0.0
Datadog,53624583,53469205,1,"2018/12/05, 05:07:49",False,"2018/12/05, 05:07:49",3223.0,43973.0,0,"I can't speak specifically for the Java implementation, but in the CSharp client, the ability to send this data to Datadog is done to 127.0.0.1 via UDP port 8125.",0.0,0.098,0.902,0.4497
Datadog,53624583,53469205,1,"2018/12/05, 05:07:49",False,"2018/12/05, 05:07:49",3223.0,43973.0,0,It's on the same thread as your executing code and not asynchronous.,0.0,0.0,1.0,0.0
Datadog,53624583,53469205,1,"2018/12/05, 05:07:49",False,"2018/12/05, 05:07:49",3223.0,43973.0,0,The whole effort by your process is finished once the UDP message is sent - it's fired and immediately forgotten.,0.244,0.0,0.756,-0.6705
Datadog,53624583,53469205,1,"2018/12/05, 05:07:49",False,"2018/12/05, 05:07:49",3223.0,43973.0,0,"The thread overhead you mention occurs in the separate Datadog agent process which is listening on the other end of UDP 8125, and has it's own thread pool and ability to buffer some data before sending up to Datadog's servers.",0.0,0.056,0.944,0.3182
Datadog,53624583,53469205,1,"2018/12/05, 05:07:49",False,"2018/12/05, 05:07:49",3223.0,43973.0,0,Do you have additional information that shows this behavior?,0.0,0.0,1.0,0.0
Datadog,53624583,53469205,1,"2018/12/05, 05:07:49",False,"2018/12/05, 05:07:49",3223.0,43973.0,0,"Based on what I know, this doesn't sound like a side effect of the Datadog/StatsD stuff.",0.14,0.0,0.86,-0.2755
Datadog,54224944,53469205,0,"2019/01/16, 22:34:23",True,"2019/01/16, 22:34:23",2723.0,5885013.0,0,"I found the answer on Datadog's help forum:  ""How to graph percentiles in Datadog"" .",0.0,0.184,0.816,0.4019
Datadog,54224944,53469205,0,"2019/01/16, 22:34:23",True,"2019/01/16, 22:34:23",2723.0,5885013.0,0,"So the gist is that the latency itself didn't go up, but aggregating over multiple streams (where each stream corresponds to each custom tag) caused the graph to display a different shape.",0.0,0.0,1.0,0.0
Datadog,51552249,51524214,0,"2018/07/27, 09:49:35",True,"2018/07/27, 10:22:48",11822.0,193178.0,1,"Just use  NLog.MappedDiagnosticsLogicalContext.Set(""userid"", ""someValue"")  together with  ${mdlc:item=userid}  where needed.",0.0,0.0,1.0,0.0
Datadog,51552249,51524214,0,"2018/07/27, 09:49:35",True,"2018/07/27, 10:22:48",11822.0,193178.0,1,See also  http://github.com/NLog/NLog/wiki/MDLC-Layout-Renderer,0.0,0.0,1.0,0.0
Datadog,51552249,51524214,0,"2018/07/27, 09:49:35",True,"2018/07/27, 10:22:48",11822.0,193178.0,1,MappedDiagnosticsLogicalContext  uses  CallContext  (and  AsyncLocal  on NetCore) which are thread-safe.,0.0,0.0,1.0,0.0
Datadog,51552249,51524214,0,"2018/07/27, 09:49:35",True,"2018/07/27, 10:22:48",11822.0,193178.0,1,"Settings will also support async Task and follow to the chained tasks, but if scheduling a Time-callback based on a user-request, then the Timer-callback will not see the userid.",0.0,0.066,0.934,0.2144
Datadog,51552249,51524214,0,"2018/07/27, 09:49:35",True,"2018/07/27, 10:22:48",11822.0,193178.0,1,"You should avoid changing  LogManager.Configuration.Variable  at runtime, they are global for all concurrent requests, and might get lost during configuration-reload (If autoreload configured).",0.176,0.0,0.824,-0.5423
Datadog,52038935,51521838,0,"2018/08/27, 15:02:48",True,"2018/08/27, 15:02:48",1.0,10134087.0,0,"If anyone will need the answer, this is how I did this.",0.0,0.0,1.0,0.0
Datadog,52038935,51521838,0,"2018/08/27, 15:02:48",True,"2018/08/27, 15:02:48",1.0,10134087.0,0,"It shows the biggest tables that were not last vacuumed in that past 2 weeks, but limits the list for 20 results.",0.0,0.0,1.0,0.0
Datadog,52038935,51521838,0,"2018/08/27, 15:02:48",True,"2018/08/27, 15:02:48",1.0,10134087.0,0,If you want to you can change LIMIT 20 or delete it for shorter/longer list.,0.0,0.085,0.915,0.0772
Datadog,52038935,51521838,0,"2018/08/27, 15:02:48",True,"2018/08/27, 15:02:48",1.0,10134087.0,0,Also change pg.last_autovacuum for analyze or anything else from pg.stat table you want to check and also 2 week can be changed to whatever time period you want.,0.0,0.094,0.906,0.1531
Datadog,52039061,51521838,0,"2018/08/27, 15:09:40",False,"2018/08/27, 15:09:40",1.0,10134087.0,0,In datadog under postgres.yaml I added this:,0.0,0.0,1.0,0.0
Datadog,52039061,51521838,0,"2018/08/27, 15:09:40",False,"2018/08/27, 15:09:40",1.0,10134087.0,0,Then I added this as a top-list inside a dashboard.,0.0,0.0,1.0,0.0
Datadog,52039061,51521838,0,"2018/08/27, 15:09:40",False,"2018/08/27, 15:09:40",1.0,10134087.0,0,"You can also make it as an alert and sum up the counts inside the metrics and decide what's your limit that about it you want to start vacuuming, although it's just recommended to run it regularly and not just when it's just too big, that's why we use this only as a list in our dashboard.",0.0,0.091,0.909,0.5106
Datadog,52039061,51521838,0,"2018/08/27, 15:09:40",False,"2018/08/27, 15:09:40",1.0,10134087.0,0,To get eyes only.,0.0,0.0,1.0,0.0
Datadog,49291336,49280325,0,"2018/03/15, 05:58:25",True,"2018/03/15, 05:58:25",1390.0,8748848.0,1,"Datadog, like OMS and other monitoring software uses the Azure VM agent to steam the information.",0.0,0.143,0.857,0.3612
Datadog,49291336,49280325,0,"2018/03/15, 05:58:25",True,"2018/03/15, 05:58:25",1390.0,8748848.0,1,Once this agent is installed on the system we are able to gather the info needed.,0.0,0.0,1.0,0.0
Datadog,49291336,49280325,0,"2018/03/15, 05:58:25",True,"2018/03/15, 05:58:25",1390.0,8748848.0,1,The VM agent is not something that goes out over the internet like other connections.,0.0,0.152,0.848,0.3612
Datadog,49291336,49280325,0,"2018/03/15, 05:58:25",True,"2018/03/15, 05:58:25",1390.0,8748848.0,1,"Hence, you should still see the reporting available.",0.0,0.0,1.0,0.0
Datadog,49291336,49280325,0,"2018/03/15, 05:58:25",True,"2018/03/15, 05:58:25",1390.0,8748848.0,1,"Rather, it should be a direct connection from the Hyper-V manager and the VM itself.",0.0,0.0,1.0,0.0
Datadog,49291336,49280325,0,"2018/03/15, 05:58:25",True,"2018/03/15, 05:58:25",1390.0,8748848.0,1,"This therefore, bypassing any NSG rules you would have in place.",0.0,0.0,1.0,0.0
Datadog,49292913,49280325,0,"2018/03/15, 08:32:47",False,"2018/03/15, 08:44:04",12857.0,6851908.0,0,I have installed data dog agent on one of my virtual machines,0.0,0.0,1.0,0.0
Datadog,49292913,49280325,0,"2018/03/15, 08:32:47",False,"2018/03/15, 08:44:04",12857.0,6851908.0,0,Datadog agent will collect system metrics and  forward  to Datadog.,0.0,0.0,1.0,0.0
Datadog,49292913,49280325,0,"2018/03/15, 08:32:47",False,"2018/03/15, 08:44:04",12857.0,6851908.0,0,Datadog agent works like this:,0.0,0.385,0.615,0.3612
Datadog,49292913,49280325,0,"2018/03/15, 08:32:47",False,"2018/03/15, 08:44:04",12857.0,6851908.0,0,"Also you can try to perform a network capture on your Azure VM, then we are able to find the detailed of the agent behavior.",0.0,0.0,1.0,0.0
Datadog,49292913,49280325,0,"2018/03/15, 08:32:47",False,"2018/03/15, 08:44:04",12857.0,6851908.0,0,Here is the network capture in my test VM:,0.0,0.0,1.0,0.0
Datadog,49292913,49280325,0,"2018/03/15, 08:32:47",False,"2018/03/15, 08:44:04",12857.0,6851908.0,0,We can find that  Datadog agent forward over HTTPS(443) to Datadog HQ .,0.0,0.0,1.0,0.0
Datadog,49292913,49280325,0,"2018/03/15, 08:32:47",False,"2018/03/15, 08:44:04",12857.0,6851908.0,0,"After you deny port 443 in NSG outbound rules, the datadog will not get your metrics:",0.138,0.0,0.862,-0.34
Datadog,49292913,49280325,0,"2018/03/15, 08:32:47",False,"2018/03/15, 08:44:04",12857.0,6851908.0,0,"More information about datadog agent, please refer to this official  article .",0.0,0.187,0.813,0.3182
Datadog,48265003,48262289,0,"2018/01/15, 16:28:30",False,"2018/01/15, 16:28:30",9230.0,63308.0,0,Anything printed to STDOUT will be sent to your logging addons like SumoLogic.,0.0,0.172,0.828,0.3612
Datadog,48265003,48262289,0,"2018/01/15, 16:28:30",False,"2018/01/15, 16:28:30",9230.0,63308.0,0,The options you've shown should take care of that.,0.0,0.286,0.714,0.4939
Datadog,48265003,48262289,0,"2018/01/15, 16:28:30",False,"2018/01/15, 16:28:30",9230.0,63308.0,0,"The mechanism that addons like SumoLogic use is call  Log Drains , and you can tap into that your self to get your log stream over HTTP.",0.0,0.091,0.909,0.3612
Datadog,45557292,45554437,2,"2017/08/08, 02:56:57",False,"2017/08/08, 02:56:57",1371.0,5540166.0,1,"hmm, so you're trying to use autodiscovery to find which container the dd-agent should be running the etcd check on?",0.0,0.0,1.0,0.0
Datadog,45557292,45554437,2,"2017/08/08, 02:56:57",False,"2017/08/08, 02:56:57",1371.0,5540166.0,1,and you're using the auto_conf files approach?,0.0,0.0,1.0,0.0
Datadog,45557292,45554437,2,"2017/08/08, 02:56:57",False,"2017/08/08, 02:56:57",1371.0,5540166.0,1,"And there, you're wondering how to apply the  %%host%%  template variable?",0.0,0.0,1.0,0.0
Datadog,45557292,45554437,2,"2017/08/08, 02:56:57",False,"2017/08/08, 02:56:57",1371.0,5540166.0,1,"If that's what you're interested in, I think you'll want to add it into your  etcd.yaml  on the  url  line, as shown in  the example file  like so:",0.0,0.213,0.787,0.6705
Datadog,43724984,43721365,3,"2017/05/01, 22:11:46",False,"2017/05/01, 22:11:46",261.0,4172512.0,1,When submitting histograms via dogstatsD you should be automatically creating 5 metrics as shown here:,0.0,0.145,0.855,0.296
Datadog,43724984,43721365,3,"2017/05/01, 22:11:46",False,"2017/05/01, 22:11:46",261.0,4172512.0,1,dog.histogram(...),0.0,0.0,1.0,0.0
Datadog,43724984,43721365,3,"2017/05/01, 22:11:46",False,"2017/05/01, 22:11:46",261.0,4172512.0,1,Usage: Used to track the statistical distribution of a set of values over a statsd flush period.,0.0,0.162,0.838,0.4019
Datadog,43724984,43721365,3,"2017/05/01, 22:11:46",False,"2017/05/01, 22:11:46",261.0,4172512.0,1,Actually submits as multiple metrics:,0.0,0.0,1.0,0.0
Datadog,43724984,43721365,3,"2017/05/01, 22:11:46",False,"2017/05/01, 22:11:46",261.0,4172512.0,1,Additional details on metric types and their submission sources can be found here:,0.0,0.0,1.0,0.0
Datadog,43724984,43721365,3,"2017/05/01, 22:11:46",False,"2017/05/01, 22:11:46",261.0,4172512.0,1,https://help.datadoghq.com/hc/en-us/articles/206955236-Metric-types-in-Datadog,0.0,0.0,1.0,0.0
Datadog,43724984,43721365,3,"2017/05/01, 22:11:46",False,"2017/05/01, 22:11:46",261.0,4172512.0,1,It appears for your use case  metric.count  would be the closest match for calculating the total length of your word.,0.0,0.0,1.0,0.0
Datadog,43724984,43721365,3,"2017/05/01, 22:11:46",False,"2017/05/01, 22:11:46",261.0,4172512.0,1,"Once selected, you can make use of the  as_count()  modifier which will calculate the total count rather than the average over the flushing period.",0.0,0.0,1.0,0.0
Datadog,43724984,43721365,3,"2017/05/01, 22:11:46",False,"2017/05/01, 22:11:46",261.0,4172512.0,1,More information on this use case can be found here:,0.0,0.0,1.0,0.0
Datadog,43724984,43721365,3,"2017/05/01, 22:11:46",False,"2017/05/01, 22:11:46",261.0,4172512.0,1,https://help.datadoghq.com/hc/en-us/articles/204271195-Why-is-a-counter-metric-being-displayed-as-a-decimal-value-,0.0,0.0,1.0,0.0
Datadog,43724984,43721365,3,"2017/05/01, 22:11:46",False,"2017/05/01, 22:11:46",261.0,4172512.0,1,If you find yourself still running into any issues with this submission feel free to reach out to support@datadoghq.com,0.0,0.206,0.794,0.5267
Datadog,39513221,39513085,0,"2016/09/15, 17:06:29",False,"2016/09/15, 17:06:29",13863.0,6162307.0,0,You can modify the following according to your needs.,0.0,0.0,1.0,0.0
Datadog,39513221,39513085,0,"2016/09/15, 17:06:29",False,"2016/09/15, 17:06:29",13863.0,6162307.0,0,"What this basically does, is that it prevents  print()  from writing the default end character ( end='' ) and at the same time, it write a carriage return ( '\r' ) before anything else.",0.0,0.044,0.956,0.0772
Datadog,39513221,39513085,0,"2016/09/15, 17:06:29",False,"2016/09/15, 17:06:29",13863.0,6162307.0,0,"In simple terms, you are overwriting the previous  print()  statement.",0.0,0.0,1.0,0.0
Datadog,39513348,39513085,1,"2016/09/15, 17:12:09",False,"2016/09/15, 17:16:06",254.0,3785588.0,0,"the naive solution would be to just use the total amount of rows in your dataset and the index your are at, then calculate the progress:",0.07,0.169,0.762,0.4588
Datadog,39513348,39513085,1,"2016/09/15, 17:12:09",False,"2016/09/15, 17:16:06",254.0,3785588.0,0,This will only be somewhat reliable if every row takes around the same time to complete.,0.0,0.0,1.0,0.0
Datadog,39513348,39513085,1,"2016/09/15, 17:12:09",False,"2016/09/15, 17:16:06",254.0,3785588.0,0,"Because you have a large dataset, it might average out over time, but if some rows take a millisecond, and another takes 10 minutes, the percentage will be garbage.",0.0,0.0,1.0,0.0
Datadog,39513348,39513085,1,"2016/09/15, 17:12:09",False,"2016/09/15, 17:16:06",254.0,3785588.0,0,Also consider rounding the percentage to one decimal:,0.0,0.0,1.0,0.0
Datadog,39513348,39513085,1,"2016/09/15, 17:12:09",False,"2016/09/15, 17:16:06",254.0,3785588.0,0,Printing for every row might slow your task down significantly so consider this improvement:,0.0,0.228,0.772,0.5899
Datadog,39513348,39513085,1,"2016/09/15, 17:12:09",False,"2016/09/15, 17:16:06",254.0,3785588.0,0,"There are, of course, also modules for this:",0.0,0.0,1.0,0.0
Datadog,39513348,39513085,1,"2016/09/15, 17:12:09",False,"2016/09/15, 17:16:06",254.0,3785588.0,0,progressbar,0.0,0.0,1.0,0.0
Datadog,39513348,39513085,1,"2016/09/15, 17:12:09",False,"2016/09/15, 17:16:06",254.0,3785588.0,0,progress,0.0,1.0,0.0,0.4215
Datadog,43644895,43641450,0,"2017/04/27, 00:34:11",False,"2017/04/27, 00:34:11",261.0,4172512.0,2,You could use Datadog's Outlier detection to identify instances which exhibit behavior outside the normal for it's peer set.,0.0,0.0,1.0,0.0
Datadog,43644895,43641450,0,"2017/04/27, 00:34:11",False,"2017/04/27, 00:34:11",261.0,4172512.0,2,"As an example, you could create an outlier detection monitor:",0.0,0.189,0.811,0.2732
Datadog,43644895,43641450,0,"2017/04/27, 00:34:11",False,"2017/04/27, 00:34:11",261.0,4172512.0,2,http://docs.datadoghq.com/guides/outliers/#alerts,0.0,0.0,1.0,0.0
Datadog,43644895,43641450,0,"2017/04/27, 00:34:11",False,"2017/04/27, 00:34:11",261.0,4172512.0,2,Which would be scoped to a system metric like  aws.ec2.cpuutilization  and be alerted if any host spiked abnormally or had very low utilization in comparison to its group.,0.08,0.084,0.836,0.0276
Datadog,43644895,43641450,0,"2017/04/27, 00:34:11",False,"2017/04/27, 00:34:11",261.0,4172512.0,2,There are some additional blog posts which discuss the use of the algorithms that can be found here:,0.0,0.0,1.0,0.0
Datadog,43644895,43641450,0,"2017/04/27, 00:34:11",False,"2017/04/27, 00:34:11",261.0,4172512.0,2,https://www.datadoghq.com/blog/introducing-outlier-detection-in-datadog/,0.0,0.0,1.0,0.0
Datadog,43644895,43641450,0,"2017/04/27, 00:34:11",False,"2017/04/27, 00:34:11",261.0,4172512.0,2,https://www.datadoghq.com/blog/outlier-detection-algorithms-at-datadog/,0.0,0.0,1.0,0.0
Datadog,43644895,43641450,0,"2017/04/27, 00:34:11",False,"2017/04/27, 00:34:11",261.0,4172512.0,2,https://www.datadoghq.com/blog/scaling-outlier-algorithms/,0.0,0.0,1.0,0.0
Datadog,43644895,43641450,0,"2017/04/27, 00:34:11",False,"2017/04/27, 00:34:11",261.0,4172512.0,2,"That said, if you find yourself needing additional assistance with outlier detection you can always reach out to the Support team at support@datadoghq.com or by using the internal support features found here:",0.0,0.183,0.817,0.6705
Datadog,43644895,43641450,0,"2017/04/27, 00:34:11",False,"2017/04/27, 00:34:11",261.0,4172512.0,2,https://app.datadoghq.com/help,0.0,0.0,1.0,0.0
Datadog,43644895,43641450,0,"2017/04/27, 00:34:11",False,"2017/04/27, 00:34:11",261.0,4172512.0,2,Hope this helps!,0.0,0.853,0.147,0.6996
Datadog,66203307,66202271,0,"2021/02/15, 07:52:17",False,"2021/02/15, 07:52:17",124.0,1534712.0,0,I have found that  Blackfire  is doing the trick.,0.146,0.0,0.854,-0.0516
Datadog,66203307,66202271,0,"2021/02/15, 07:52:17",False,"2021/02/15, 07:52:17",124.0,1534712.0,0,Seems to be relatively easy to install and can run it free locally.,0.0,0.36,0.64,0.7351
Datadog,65834510,65790261,0,"2021/01/21, 21:32:18",True,"2021/01/21, 21:32:18",493.0,244037.0,0,"Datadog Tags are generally strings, but also support &quot;key:value&quot; strings, which is most useful, since then the  key  can act as a dimension.",0.0,0.282,0.718,0.8334
Datadog,65834510,65790261,0,"2021/01/21, 21:32:18",True,"2021/01/21, 21:32:18",493.0,244037.0,0,"There's no support that I know of that allows for a single key with multiple values, so I don't think Datadog will support the syntax you're attempting.",0.073,0.267,0.66,0.7096
Datadog,65834510,65790261,0,"2021/01/21, 21:32:18",True,"2021/01/21, 21:32:18",493.0,244037.0,0,You  may  want to try:,0.0,0.245,0.755,0.0772
Datadog,65834510,65790261,0,"2021/01/21, 21:32:18",True,"2021/01/21, 21:32:18",493.0,244037.0,0,in your config.,0.0,0.0,1.0,0.0
Datadog,65834510,65790261,0,"2021/01/21, 21:32:18",True,"2021/01/21, 21:32:18",493.0,244037.0,0,General reference here:  https://docs.datadoghq.com/getting_started/tagging/,0.0,0.0,1.0,0.0
Datadog,65153895,65151286,0,"2020/12/05, 06:52:20",False,"2020/12/05, 06:52:20",17233.0,9576186.0,1,"You're welcome to go look through the source code yourself , but generally my comment is correct.",0.0,0.118,0.882,0.25
Datadog,65153895,65151286,0,"2020/12/05, 06:52:20",False,"2020/12/05, 06:52:20",17233.0,9576186.0,1,"Nest binds all route handlers and enhancers (guards, interceptors, pipes, and filters) as a large anonymous function, in a very abstract way (does the same thing for Fastify as far as I can tell).",0.0,0.0,1.0,0.0
Datadog,64842339,64821299,4,"2020/11/15, 10:08:00",False,"2020/11/15, 10:08:00",327.0,2882619.0,0,You can use Fluentd as a  daemonset  on your cluster.,0.0,0.0,1.0,0.0
Datadog,64842339,64821299,4,"2020/11/15, 10:08:00",False,"2020/11/15, 10:08:00",327.0,2882619.0,0,see this repo and docker images -&gt;  fluent/fluentd-docker-image,0.0,0.0,1.0,0.0
Datadog,64842339,64821299,4,"2020/11/15, 10:08:00",False,"2020/11/15, 10:08:00",327.0,2882619.0,0,and use this filter to add  Kubernetes metadata  to every log collected by Fluentd and then use a grep filter to exclude logs that are not in your namespaces.,0.066,0.0,0.934,-0.2263
Datadog,64842339,64821299,4,"2020/11/15, 10:08:00",False,"2020/11/15, 10:08:00",327.0,2882619.0,0,something like this:,0.0,0.556,0.444,0.3612
Datadog,60992355,60987316,3,"2020/04/02, 16:08:59",False,"2020/04/02, 16:08:59",239.0,2268923.0,0,did u try bulk publish?,0.0,0.0,1.0,0.0
Datadog,60992355,60987316,3,"2020/04/02, 16:08:59",False,"2020/04/02, 16:08:59",239.0,2268923.0,0,"publish(topic,[]Message{1,2,3,4,.....})",0.0,0.0,1.0,0.0
Datadog,59738942,59734858,0,"2020/01/14, 19:30:22",False,"2020/01/14, 19:30:22",2803.0,10809602.0,1,I believe your requirement can be accomplished using cmdlet  Invoke-AzVMRunCommand  /  Invoke-AzureRmVMRunCommand  or  Set-AzVMCustomScriptExtension  /  Set-AzureRmVMCustomScriptExtension .,0.0,0.195,0.805,0.4404
Datadog,59738942,59734858,0,"2020/01/14, 19:30:22",False,"2020/01/14, 19:30:22",2803.0,10809602.0,1,Related scripts can be found  here  and  here .,0.0,0.0,1.0,0.0
Datadog,59738942,59734858,0,"2020/01/14, 19:30:22",False,"2020/01/14, 19:30:22",2803.0,10809602.0,1,"Just FYI,  this  and  this  are actual references for the above information.",0.0,0.187,0.813,0.368
Datadog,59738942,59734858,0,"2020/01/14, 19:30:22",False,"2020/01/14, 19:30:22",2803.0,10809602.0,1,Hope this update helps!,0.0,0.743,0.257,0.6996
Datadog,60614784,59519717,0,"2020/03/10, 11:34:48",False,"2020/03/10, 11:34:48",57.0,339202.0,0,This is possible by adding the annotation below to nginx ingress:,0.0,0.0,1.0,0.0
Datadog,60614784,59519717,0,"2020/03/10, 11:34:48",False,"2020/03/10, 11:34:48",57.0,339202.0,0,See full answer at  https://github.com/DataDog/dd-opentracing-cpp/issues/118,0.0,0.0,1.0,0.0
Datadog,57763275,57762582,0,"2019/09/03, 01:43:07",True,"2019/09/03, 01:43:07",104968.0,1766831.0,1,"Turn on the ""general log"" and have it write to a file.",0.0,0.0,1.0,0.0
Datadog,57763275,57762582,0,"2019/09/03, 01:43:07",True,"2019/09/03, 01:43:07",104968.0,1766831.0,1,Wait a finite amount of time.,0.0,0.0,1.0,0.0
Datadog,57763275,57762582,0,"2019/09/03, 01:43:07",True,"2019/09/03, 01:43:07",104968.0,1766831.0,1,Then use  pt-query-digest  to summarize the results.,0.0,0.0,1.0,0.0
Datadog,57763275,57762582,0,"2019/09/03, 01:43:07",True,"2019/09/03, 01:43:07",104968.0,1766831.0,1,Turn off the general log before it fills up disk.,0.0,0.0,1.0,0.0
Datadog,57763275,57762582,0,"2019/09/03, 01:43:07",True,"2019/09/03, 01:43:07",104968.0,1766831.0,1,The slowlog (with a small value in  long_query_time ) is more useful for finding naughty queries.,0.0,0.318,0.682,0.6801
Datadog,55752352,55737494,0,"2019/04/18, 22:06:04",False,"2019/04/18, 22:06:04",175.0,2356037.0,0,The issue was the size of the HTTPRequest was to large the higher the parallelism which makes sense.,0.0,0.0,1.0,0.0
Datadog,55752352,55737494,0,"2019/04/18, 22:06:04",False,"2019/04/18, 22:06:04",175.0,2356037.0,0,"I was getting back ""Request Entity Too Large"" however the exception wasn't logging out correctly so I missed it.",0.152,0.0,0.848,-0.4341
Datadog,55752352,55737494,0,"2019/04/18, 22:06:04",False,"2019/04/18, 22:06:04",175.0,2356037.0,0,It seems that the Flink  DatadogHttpReporter  does not take the size of the request into consideration when building it.,0.0,0.0,1.0,0.0
Datadog,55752352,55737494,0,"2019/04/18, 22:06:04",False,"2019/04/18, 22:06:04",175.0,2356037.0,0,I modified the Reporter to limit the number of metrics per request to 1000.,0.0,0.098,0.902,0.0772
Datadog,55752352,55737494,0,"2019/04/18, 22:06:04",False,"2019/04/18, 22:06:04",175.0,2356037.0,0,Now the metrics are showing up just fine.,0.0,0.205,0.795,0.2023
Datadog,54751580,54735683,2,"2019/02/18, 18:30:16",False,"2019/02/18, 18:30:16",1371.0,5540166.0,0,"This becomes pretty easy with Datadog's Log Management product -- you can measure lots of things by endpoint, including hits, unique client-ip count, latency (if you add response time to your nginx logs).",0.0,0.164,0.836,0.7269
Datadog,54751580,54735683,2,"2019/02/18, 18:30:16",False,"2019/02/18, 18:30:16",1371.0,5540166.0,0,More info on the setup and these use cases  in this blogpost .,0.0,0.0,1.0,0.0
Datadog,54751580,54735683,2,"2019/02/18, 18:30:16",False,"2019/02/18, 18:30:16",1371.0,5540166.0,0,Documentation on the logs part of the nginx integration  here .,0.0,0.0,1.0,0.0
Datadog,53449811,53441210,1,"2018/11/23, 18:04:49",True,"2018/11/23, 18:04:49",1371.0,5540166.0,1,"This is definitely possible, but you will want to change your tag setup a little.",0.0,0.216,0.784,0.3182
Datadog,53449811,53441210,1,"2018/11/23, 18:04:49",True,"2018/11/23, 18:04:49",1371.0,5540166.0,1,"You want to take advantage of  key:value  syntax with your tags, so that you can group out the tags by their common  key .",0.0,0.136,0.864,0.3182
Datadog,53449811,53441210,1,"2018/11/23, 18:04:49",True,"2018/11/23, 18:04:49",1371.0,5540166.0,1,"So in your case, instead of tagging by  entity.count.payment , you would want to tag by  entity.count:payment  or better yet  entity:payment .",0.0,0.189,0.811,0.4939
Datadog,53449811,53441210,1,"2018/11/23, 18:04:49",True,"2018/11/23, 18:04:49",1371.0,5540166.0,1,That way you can write one query of your metric and use the  group by  functionality on the shared  entity  tag key to see it's values for all the different  entity  tags.,0.0,0.145,0.855,0.6249
Datadog,53449811,53441210,1,"2018/11/23, 18:04:49",True,"2018/11/23, 18:04:49",1371.0,5540166.0,1,"From there, you can use the  top  function to always see just the top n values, whether that be  payment  or  cart  or  visit  etc.",0.0,0.231,0.769,0.6486
Datadog,53449811,53441210,1,"2018/11/23, 18:04:49",True,"2018/11/23, 18:04:49",1371.0,5540166.0,1,This doc here about tags  is definitely worth a read!,0.0,0.411,0.589,0.5983
Datadog,53449811,53441210,1,"2018/11/23, 18:04:49",True,"2018/11/23, 18:04:49",1371.0,5540166.0,1,Tags can make graphing and monitoring much easier and more scalable.,0.0,0.219,0.781,0.4215
Datadog,52971077,50468823,0,"2018/10/24, 17:06:01",False,"2018/10/24, 17:06:01",338.0,8335684.0,0,It is necessary to  add AspectJ weaver as Java Agent  when you're starting your Akka aplication:  -javaagent:aspectjweaver.jar,0.0,0.0,1.0,0.0
Datadog,52971077,50468823,0,"2018/10/24, 17:06:01",False,"2018/10/24, 17:06:01",338.0,8335684.0,0,You can add the following settings in your project SBT configuration:,0.0,0.0,1.0,0.0
Datadog,52971077,50468823,0,"2018/10/24, 17:06:01",False,"2018/10/24, 17:06:01",338.0,8335684.0,0,So AspectJ weaver JAR will be copied to  ./lib_managed/jars/org.aspectj/aspectjweaver/aspectjweaver-[aspectJWeaverV].jar  in your project root.,0.0,0.0,1.0,0.0
Datadog,52971077,50468823,0,"2018/10/24, 17:06:01",False,"2018/10/24, 17:06:01",338.0,8335684.0,0,Then you can refer this JAR in your Dockerfile:,0.0,0.0,1.0,0.0
Datadog,49608019,49607708,6,"2018/04/02, 11:42:22",False,"2018/04/02, 11:42:22",1443.0,4333853.0,1,Few things to debug,0.0,0.0,1.0,0.0
Datadog,49277969,49274395,0,"2018/03/14, 14:36:12",True,"2018/04/29, 00:26:33",2024.0,698082.0,1,You can easily get all needed data via querying dmv and other resources inside SQL Server.,0.0,0.138,0.862,0.34
Datadog,49277969,49274395,0,"2018/03/14, 14:36:12",True,"2018/04/29, 00:26:33",2024.0,698082.0,1,Good start is  here .,0.0,0.492,0.508,0.4404
Datadog,46871722,46867883,2,"2017/10/22, 10:49:32",False,"2017/10/22, 10:49:32",2208.0,5259881.0,1,I have 35 Cassandra nodes (different clusters) monitored without any problems with graphite + carbon + whisper + grafana.,0.0,0.139,0.861,0.3089
Datadog,46871722,46867883,2,"2017/10/22, 10:49:32",False,"2017/10/22, 10:49:32",2208.0,5259881.0,1,But i have to tell that re-configuring collection and aggregations windows with whisper is a pain.,0.255,0.0,0.745,-0.6652
Datadog,46871722,46867883,2,"2017/10/22, 10:49:32",False,"2017/10/22, 10:49:32",2208.0,5259881.0,1,"There's many alternatives today for this job, you can use influxdb (+ telegraf) stack for example.",0.0,0.0,1.0,0.0
Datadog,46871722,46867883,2,"2017/10/22, 10:49:32",False,"2017/10/22, 10:49:32",2208.0,5259881.0,1,"Also with datadog you don't need grafana, they're also a visualizing platform.",0.0,0.0,1.0,0.0
Datadog,46871722,46867883,2,"2017/10/22, 10:49:32",False,"2017/10/22, 10:49:32",2208.0,5259881.0,1,"I've worked with it some time ago, but they have some misleading names for some metrics in their plugin, and some metrics were just missing.",0.216,0.0,0.784,-0.7469
Datadog,46871722,46867883,2,"2017/10/22, 10:49:32",False,"2017/10/22, 10:49:32",2208.0,5259881.0,1,"As a pros for this platform, it's really easy to install and use.",0.0,0.225,0.775,0.4927
Datadog,46949745,46867883,2,"2017/10/26, 11:49:53",False,"2017/10/26, 11:49:53",341.0,529321.0,1,"We have a cassandra cluster of 36 nodes in production right now (we had 51 but migrated the instance type since then so we need less C* servers now), monitored using a single graphite server.",0.0,0.0,1.0,0.0
Datadog,46949745,46867883,2,"2017/10/26, 11:49:53",False,"2017/10/26, 11:49:53",341.0,529321.0,1,We are also saving data for 30 days but in a 60s resolution.,0.0,0.0,1.0,0.0
Datadog,46949745,46867883,2,"2017/10/26, 11:49:53",False,"2017/10/26, 11:49:53",341.0,529321.0,1,We excluded the internode metrics (e.g.,0.324,0.0,0.676,-0.34
Datadog,46949745,46867883,2,"2017/10/26, 11:49:53",False,"2017/10/26, 11:49:53",341.0,529321.0,1,"open connections from a to b) because of the scaling of the metric count, but keep all other.",0.0,0.0,1.0,0.0
Datadog,46949745,46867883,2,"2017/10/26, 11:49:53",False,"2017/10/26, 11:49:53",341.0,529321.0,1,"This totals to ~510k metrics, each whisper file being ~500kb in size =  ~250GB.",0.0,0.0,1.0,0.0
Datadog,46949745,46867883,2,"2017/10/26, 11:49:53",False,"2017/10/26, 11:49:53",341.0,529321.0,1,"iostat tells me, that we have write peaks to ~70k writes/s.",0.0,0.0,1.0,0.0
Datadog,46949745,46867883,2,"2017/10/26, 11:49:53",False,"2017/10/26, 11:49:53",341.0,529321.0,1,This all is done on a single AWS i3.2xlarge instance which include 1.9TB nvme instance storage and 61GB of RAM.,0.0,0.0,1.0,0.0
Datadog,46949745,46867883,2,"2017/10/26, 11:49:53",False,"2017/10/26, 11:49:53",341.0,529321.0,1,To fully utilize the power of the this disk type we increased the number of carbon caches.,0.0,0.185,0.815,0.34
Datadog,46949745,46867883,2,"2017/10/26, 11:49:53",False,"2017/10/26, 11:49:53",341.0,529321.0,1,The cpu usage is very low (&lt;20%) and so is the iowait (&lt;1%).,0.166,0.0,0.834,-0.3384
Datadog,46949745,46867883,2,"2017/10/26, 11:49:53",False,"2017/10/26, 11:49:53",341.0,529321.0,1,"I guess we could get away with a less beefy machine, but this gives us a lot of headroom for growing the cluster and we are constantly adding new servers.",0.0,0.073,0.927,0.2617
Datadog,46949745,46867883,2,"2017/10/26, 11:49:53",False,"2017/10/26, 11:49:53",341.0,529321.0,1,"For the monitoring: Be prepared that AWS will terminate these machines more often than others, so backup and restore are more likely a regular operation.",0.0,0.166,0.834,0.5209
Datadog,46949745,46867883,2,"2017/10/26, 11:49:53",False,"2017/10/26, 11:49:53",341.0,529321.0,1,I hope this little insight helped you.,0.0,0.367,0.633,0.4404
Datadog,41809571,39172174,0,"2017/01/23, 17:14:15",False,"2017/01/23, 17:14:15",89.0,4256613.0,1,It looks like that you have not set your JMX_PORT for kafka from where your datadog agent can listen information abouot the metrics.,0.0,0.102,0.898,0.3612
Datadog,41809571,39172174,0,"2017/01/23, 17:14:15",False,"2017/01/23, 17:14:15",89.0,4256613.0,1,"Restart your Kafka with the following additional key/value pair parameter:
'JMX_PORT=9999'",0.0,0.0,1.0,0.0
Datadog,41809571,39172174,0,"2017/01/23, 17:14:15",False,"2017/01/23, 17:14:15",89.0,4256613.0,1,$ JMX_PORT=9999 ./kafka-server-start.sh ../config/server.properties,0.0,0.0,1.0,0.0
Datadog,50008438,39172174,0,"2018/04/24, 21:19:55",False,"2018/04/24, 21:19:55",3040.0,4460737.0,0,This error essentially means that the Datadog Agent is unable to connect to the Kafka instance to retrieve metrics from the exposed mBeans over the RMI protocol.,0.138,0.0,0.862,-0.4588
Datadog,50008438,39172174,0,"2018/04/24, 21:19:55",False,"2018/04/24, 21:19:55",3040.0,4460737.0,0,"This error can be resolved by including the following JVM (Java Virtual Machine) arguments when starting the Kafka instance (required for Producer, Consumer, and Broker as they are all separate Java instances)
please",0.141,0.104,0.755,-0.34
Datadog,50008438,39172174,0,"2018/04/24, 21:19:55",False,"2018/04/24, 21:19:55",3040.0,4460737.0,0,Please read this article,0.0,0.434,0.566,0.3182
Datadog,39174290,39163880,0,"2016/08/26, 23:35:25",True,"2016/08/26, 23:35:25",961.0,247700.0,2,"Nexus 3.0.1 exposes authenticated access to metrics using  http://metrics.dropwizard.io/3.1.0/manual/servlets/ 
You have these endpoints available for different purposes:
 
        {host:port}/service/metrics/healthcheck
        {host:port}/service/metrics/data
        {host:port}/service/metrics/ping
        {host:port}/service/metrics/threads",0.07,0.0,0.93,-0.128
Datadog,30168310,30155496,3,"2015/05/11, 15:58:22",True,"2015/05/11, 15:58:22",1802.0,1368626.0,2,"I recall the default behavior being that each gear can handle 16 concurrent connections, then auto-scaling would kick in and you would get a new gear.",0.0,0.0,1.0,0.0
Datadog,30168310,30155496,3,"2015/05/11, 15:58:22",True,"2015/05/11, 15:58:22",1802.0,1368626.0,2,Therefore I would think it makes sense to start by testing that a gear works well with 16 users at once.,0.0,0.104,0.896,0.2732
Datadog,30168310,30155496,3,"2015/05/11, 15:58:22",True,"2015/05/11, 15:58:22",1802.0,1368626.0,2,"If not, then you can  change the scaling policy  to what works best for you application.",0.0,0.219,0.781,0.6369
Datadog,30168310,30155496,3,"2015/05/11, 15:58:22",True,"2015/05/11, 15:58:22",1802.0,1368626.0,2,BlazeMeter  is a tool that could probably help with creating the connections.,0.0,0.353,0.647,0.5994
Datadog,30168310,30155496,3,"2015/05/11, 15:58:22",True,"2015/05/11, 15:58:22",1802.0,1368626.0,2,"They mention 100,000 concurrent users on that main page so I don't think you have to worry about getting banned for this sort of test.",0.211,0.0,0.789,-0.7096
Datadog,66554834,66554063,2,"2021/03/09, 23:11:35",False,"2021/03/09, 23:11:35",1693.0,971735.0,2,spring.sleuth.baggage.correlation-fields  automatically sets baggage values to Slf4j’s MDC so you only need to set the baggage field.,0.0,0.144,0.856,0.4019
Datadog,66554834,66554063,2,"2021/03/09, 23:11:35",False,"2021/03/09, 23:11:35",1693.0,971735.0,2,I suppose you use Sleuth out of the box (uses Brave):,0.0,0.0,1.0,0.0
Datadog,66554834,66554063,2,"2021/03/09, 23:11:35",False,"2021/03/09, 23:11:35",1693.0,971735.0,2,The  spring.sleuth.baggage.correlation-fields  property automatically sets baggage values to Slf4j’s MDC so you only need to set the baggage field.,0.0,0.13,0.87,0.4019
Datadog,66554834,66554063,2,"2021/03/09, 23:11:35",False,"2021/03/09, 23:11:35",1693.0,971735.0,2,"Also, using  MDCScopeDecorator , you can set the baggage values to Slf4j’s MDC programmatically, you can see how to do it in  Sleuth docs :",0.0,0.109,0.891,0.4019
Datadog,66538157,66533080,0,"2021/03/09, 00:29:39",True,"2021/03/09, 00:29:39",1693.0,971735.0,1,"We figured this out in the comments, I'm posting an answer that summarizes it all up: it seems the root cause was using different versions of different spring-boot modules.",0.0,0.0,1.0,0.0
Datadog,66538157,66533080,0,"2021/03/09, 00:29:39",True,"2021/03/09, 00:29:39",1693.0,971735.0,1,"It is a good rule of thumb to not define the versions yourself but use BOMs and let them define the versions for you, e.g.",0.0,0.078,0.922,0.2382
Datadog,66538157,66533080,0,"2021/03/09, 00:29:39",True,"2021/03/09, 00:29:39",1693.0,971735.0,1,see:  spring-boot-dependencies .,0.0,0.0,1.0,0.0
Datadog,66538157,66533080,0,"2021/03/09, 00:29:39",True,"2021/03/09, 00:29:39",1693.0,971735.0,1,This way you will use the compatible (and tested) versions.,0.0,0.0,1.0,0.0
Datadog,66538157,66533080,0,"2021/03/09, 00:29:39",True,"2021/03/09, 00:29:39",1693.0,971735.0,1,management.metrics.tags.your-tag  is the way to add tags to all of your metrics.,0.0,0.0,1.0,0.0
Datadog,66538157,66533080,0,"2021/03/09, 00:29:39",True,"2021/03/09, 00:29:39",1693.0,971735.0,1,A good way to check this is looking at  /actuator/metrics .,0.0,0.266,0.734,0.4404
Datadog,65636279,65619895,3,"2021/01/08, 22:56:19",True,"2021/01/08, 22:56:19",385.0,2051074.0,2,"I ended up going with the  sbt-javaagent  plugin to avoid extra code to exclude the agent jar from the classpath, which the plugin handles automatically.",0.157,0.0,0.843,-0.4767
Datadog,65636279,65619895,3,"2021/01/08, 22:56:19",True,"2021/01/08, 22:56:19",385.0,2051074.0,2,"The trick/hack was to filter out the default  addJava -javaagent  line the  sbt-javaagent  plugin adds automatically , and then appending a new script snippent to only enable the javaagent when a certain env.",0.0,0.068,0.932,0.2732
Datadog,65636279,65619895,3,"2021/01/08, 22:56:19",True,"2021/01/08, 22:56:19",385.0,2051074.0,2,variable is set.,0.0,0.0,1.0,0.0
Datadog,63061286,63054587,2,"2020/07/23, 21:56:23",False,"2020/07/23, 21:56:23",2661.0,1184752.0,2,You can use simple  jcmd  command line tool,0.0,0.0,1.0,0.0
Datadog,63061286,63054587,2,"2020/07/23, 21:56:23",False,"2020/07/23, 21:56:23",2661.0,1184752.0,2,As an example of running this on my simple Clojure application:,0.0,0.0,1.0,0.0
Datadog,62333814,62331584,0,"2020/06/12, 00:23:58",False,"2020/06/12, 00:23:58",3.0,5605817.0,0,"As shown in the documentation in  your link , WHL files are also supported.",0.0,0.161,0.839,0.3182
Datadog,62333814,62331584,0,"2020/06/12, 00:23:58",False,"2020/06/12, 00:23:58",3.0,5605817.0,0,It says:,0.0,0.0,1.0,0.0
Datadog,62333814,62331584,0,"2020/06/12, 00:23:58",False,"2020/06/12, 00:23:58",3.0,5605817.0,0,You might already have one or more Python libraries packaged as an .egg or a .whl file.,0.0,0.0,1.0,0.0
Datadog,62333814,62331584,0,"2020/06/12, 00:23:58",False,"2020/06/12, 00:23:58",3.0,5605817.0,0,There is a .whl file available for the DataDog python library here:  https://pypi.org/project/datadog/#files .,0.0,0.0,1.0,0.0
Datadog,62333814,62331584,0,"2020/06/12, 00:23:58",False,"2020/06/12, 00:23:58",3.0,5605817.0,0,"You might try downloading that file, uploading it to your S3 bucket, and using that as your Python library for your Glue job.",0.0,0.0,1.0,0.0
Datadog,62333814,62331584,0,"2020/06/12, 00:23:58",False,"2020/06/12, 00:23:58",3.0,5605817.0,0,You might be more successful using that than trying to build your own .egg file.,0.0,0.226,0.774,0.624
Datadog,62032453,62030485,0,"2020/05/27, 01:30:00",False,"2020/05/27, 01:30:00",101971.0,248823.0,0,The AWS API calls to start a task are:,0.0,0.0,1.0,0.0
Datadog,62032453,62030485,0,"2020/05/27, 01:30:00",False,"2020/05/27, 01:30:00",101971.0,248823.0,0,StartTask :,0.0,0.0,1.0,0.0
Datadog,62032453,62030485,0,"2020/05/27, 01:30:00",False,"2020/05/27, 01:30:00",101971.0,248823.0,0,Starts a new task from the specified task definition on the specified container instance or instances.,0.0,0.0,1.0,0.0
Datadog,62032453,62030485,0,"2020/05/27, 01:30:00",False,"2020/05/27, 01:30:00",101971.0,248823.0,0,RunTask :,0.0,0.0,1.0,0.0
Datadog,62032453,62030485,0,"2020/05/27, 01:30:00",False,"2020/05/27, 01:30:00",101971.0,248823.0,0,Starts a new task using the specified task definition.,0.0,0.0,1.0,0.0
Datadog,62032453,62030485,0,"2020/05/27, 01:30:00",False,"2020/05/27, 01:30:00",101971.0,248823.0,0,"You can allow Amazon ECS to place tasks for you, or you can customize how Amazon ECS places tasks using placement constraints and placement strategies.",0.0,0.194,0.806,0.5106
Datadog,62032453,62030485,0,"2020/05/27, 01:30:00",False,"2020/05/27, 01:30:00",101971.0,248823.0,0,"Since this is AWS API calls, there are equivalent calls in CLI and SDK.",0.0,0.0,1.0,0.0
Datadog,61873777,61872153,0,"2020/05/18, 18:57:42",False,"2020/05/18, 18:57:42",19520.0,2332336.0,0,"A colleaque of mine informed me that, since we're using docker, we can by-pass supervisor and just run the horizon artisan command directly as the entrypoint of the container.",0.0,0.0,1.0,0.0
Datadog,61873777,61872153,0,"2020/05/18, 18:57:42",False,"2020/05/18, 18:57:42",19520.0,2332336.0,0,"So, I removed all things related to supervisor and my service yaml is simplified to the following and the logs are coming into datadog:",0.0,0.0,1.0,0.0
Datadog,61912692,61644174,0,"2020/05/20, 15:05:31",False,"2020/05/20, 15:05:31",23.0,12932875.0,0,"you can open it by navigating to the directory it is in, and then typing",0.0,0.0,1.0,0.0
Datadog,61912692,61644174,0,"2020/05/20, 15:05:31",False,"2020/05/20, 15:05:31",23.0,12932875.0,0,You need root permissions to view the file as far as I know.,0.0,0.0,1.0,0.0
Datadog,61590972,61588585,6,"2020/05/04, 14:18:01",True,"2020/05/04, 14:34:48",204.0,11981028.0,0,Replace  key  and  value  to what you want to use.,0.0,0.316,0.684,0.4019
Datadog,61590972,61588585,6,"2020/05/04, 14:18:01",True,"2020/05/04, 14:34:48",204.0,11981028.0,0,"In my case  key  is ""testKey"" and  value  is ""testValue""",0.0,0.211,0.789,0.34
Datadog,61590972,61588585,6,"2020/05/04, 14:18:01",True,"2020/05/04, 14:34:48",204.0,11981028.0,0,it it my full sample code and xml configuration info.,0.0,0.0,1.0,0.0
Datadog,61590972,61588585,6,"2020/05/04, 14:18:01",True,"2020/05/04, 14:34:48",204.0,11981028.0,0,code,0.0,0.0,1.0,0.0
Datadog,61590972,61588585,6,"2020/05/04, 14:18:01",True,"2020/05/04, 14:34:48",204.0,11981028.0,0,log4j2.xml,0.0,0.0,1.0,0.0
Datadog,61590972,61588585,6,"2020/05/04, 14:18:01",True,"2020/05/04, 14:34:48",204.0,11981028.0,0,output,0.0,0.0,1.0,0.0
Datadog,60951103,60933143,2,"2020/03/31, 15:56:50",False,"2020/03/31, 15:56:50",11561.0,970308.0,1,You are configuring the filter on a new  StatsdMeterRegistry .,0.0,0.0,1.0,0.0
Datadog,60951103,60933143,2,"2020/03/31, 15:56:50",False,"2020/03/31, 15:56:50",11561.0,970308.0,1,When using a  MeterRegistryCustomizer  you need to operate on the registry that was passed in.,0.0,0.0,1.0,0.0
Datadog,60951103,60933143,2,"2020/03/31, 15:56:50",False,"2020/03/31, 15:56:50",11561.0,970308.0,1,"Since the customizer will be used against all registries, you also would need to add an if statement to only filter against the registry you want filtered.",0.0,0.048,0.952,0.0772
Datadog,62521519,60896119,0,"2020/06/22, 21:52:24",False,"2020/06/22, 21:52:24",374.0,5117002.0,0,It doesn't have Grafana support yet (coming in a week or 2) but Questdb supports traditional SQL on a Time Series database and might be able to do what you want.,0.05,0.145,0.804,0.4715
Datadog,62521519,60896119,0,"2020/06/22, 21:52:24",False,"2020/06/22, 21:52:24",374.0,5117002.0,0,https://questdb.io  or  https://github.com/questdb  on GitHub.,0.0,0.0,1.0,0.0
Datadog,60471660,60465436,0,"2020/03/01, 05:14:06",False,"2020/03/01, 05:14:06",4659.0,3709060.0,0,Datadog can process logs through their pipeline fitering feature,0.0,0.0,1.0,0.0
Datadog,60471660,60465436,0,"2020/03/01, 05:14:06",False,"2020/03/01, 05:14:06",4659.0,3709060.0,0,https://docs.datadoghq.com/logs/processing/pipelines/,0.0,0.0,1.0,0.0
Datadog,60493178,60465436,0,"2020/03/02, 18:42:31",True,"2020/03/02, 18:42:31",1371.0,5540166.0,1,"If you already have an attribute that contains the url, a really easy way to do this would be to use the  processing pipelines  and add a processor of the "" url parser "" type to these logs.",0.0,0.088,0.912,0.4927
Datadog,60493178,60465436,0,"2020/03/02, 18:42:31",True,"2020/03/02, 18:42:31",1371.0,5540166.0,1,"You just plug in the attribute that contains the url and an attribute path that you'd like to contain all the outputs from it (usually  http.url_details ), and then all new logs will get the extra url parsing applied.",0.0,0.062,0.938,0.3612
Datadog,60493178,60465436,0,"2020/03/02, 18:42:31",True,"2020/03/02, 18:42:31",1371.0,5540166.0,1,"If your logs have the ""source:nginx"" applied to them (configured in the log shipper), then you'll already have an out-of-the-box Nginx processing pipeline that Datadog has for structuring standard Nginx syntax logs.",0.0,0.0,1.0,0.0
Datadog,60493178,60465436,0,"2020/03/02, 18:42:31",True,"2020/03/02, 18:42:31",1371.0,5540166.0,1,You can clone that and then just add your new url parser there.,0.0,0.0,1.0,0.0
Datadog,60493178,60465436,0,"2020/03/02, 18:42:31",True,"2020/03/02, 18:42:31",1371.0,5540166.0,1,"Or, if your syntax is similar to the standard syntax, you can just modify their default suggested parsers (in the cloned pipeline).",0.0,0.0,1.0,0.0
Datadog,60493178,60465436,0,"2020/03/02, 18:42:31",True,"2020/03/02, 18:42:31",1371.0,5540166.0,1,"In any case, it'd be worth looking at that default pipeline for inspiration for other valuable things to do beyond url parsing.",0.0,0.307,0.693,0.8126
Datadog,59633846,59632924,0,"2020/01/07, 20:08:11",True,"2020/01/07, 20:08:11",13615.0,1061413.0,1,Assuming you'd like the output to look like the following:,0.0,0.385,0.615,0.6124
Datadog,59633846,59632924,0,"2020/01/07, 20:08:11",True,"2020/01/07, 20:08:11",13615.0,1061413.0,1,"you need to escape the  {  and and use  \""  instead of  \' :",0.0,0.134,0.866,0.1779
Datadog,57978792,57978390,9,"2019/09/17, 19:52:43",False,"2019/09/17, 19:52:43",2237.0,11486670.0,0,Unfortuanetly I can not propose an exact solution/workaround to you but you might have a look at the following documentations/API's:,0.0,0.0,1.0,0.0
Datadog,57978792,57978390,9,"2019/09/17, 19:52:43",False,"2019/09/17, 19:52:43",2237.0,11486670.0,0,Indices Stats API,0.0,0.0,1.0,0.0
Datadog,57978792,57978390,9,"2019/09/17, 19:52:43",False,"2019/09/17, 19:52:43",2237.0,11486670.0,0,Cluster Stats API,0.0,0.0,1.0,0.0
Datadog,57978792,57978390,9,"2019/09/17, 19:52:43",False,"2019/09/17, 19:52:43",2237.0,11486670.0,0,Nodes Stats API,0.0,0.0,1.0,0.0
Datadog,57978792,57978390,9,"2019/09/17, 19:52:43",False,"2019/09/17, 19:52:43",2237.0,11486670.0,0,The cpu usage is not included in the exported fields but maybe you can derive a high cpu usage behaviour from the other fields.,0.0,0.0,1.0,0.0
Datadog,57978792,57978390,9,"2019/09/17, 19:52:43",False,"2019/09/17, 19:52:43",2237.0,11486670.0,0,I hope I could help you in some way.,0.0,0.528,0.472,0.6808
Datadog,55661352,55621848,0,"2019/04/13, 04:58:31",True,"2019/04/13, 04:58:31",1371.0,5540166.0,1,You can reference  this doc  to find where the default logging path is for Jenkins depending on your OS.,0.0,0.0,1.0,0.0
Datadog,55661352,55621848,0,"2019/04/13, 04:58:31",True,"2019/04/13, 04:58:31",1371.0,5540166.0,1,"(For linux, it's  /var/log/jenkins/jenkins.log  if you don't configure it to be something else.",0.0,0.0,1.0,0.0
Datadog,55661352,55621848,0,"2019/04/13, 04:58:31",True,"2019/04/13, 04:58:31",1371.0,5540166.0,1,Then as long as your  Datadog agent  is v6+ you can use the Datadog agent to tail your jenkins.log file by following  this doc .,0.0,0.0,1.0,0.0
Datadog,55661352,55621848,0,"2019/04/13, 04:58:31",True,"2019/04/13, 04:58:31",1371.0,5540166.0,1,"Specifically, you'd add this line to your  dadatod.yaml :",0.0,0.0,1.0,0.0
Datadog,55661352,55621848,0,"2019/04/13, 04:58:31",True,"2019/04/13, 04:58:31",1371.0,5540166.0,1,"and add this content to any old  conf.yaml  file nested in your  conf.d/  directory, such as  conf.d/jenkins.d/conf.yaml :",0.0,0.0,1.0,0.0
Datadog,55661352,55621848,0,"2019/04/13, 04:58:31",True,"2019/04/13, 04:58:31",1371.0,5540166.0,1,"Then the agent will tail your log file as it's written to, and will forward it to your Datadog account so you can query, graph, and monitor on your log data there.",0.0,0.0,1.0,0.0
Datadog,55661352,55621848,0,"2019/04/13, 04:58:31",True,"2019/04/13, 04:58:31",1371.0,5540166.0,1,"Once you have the logs coming in, you may want to write a  processing pipeline  to get the critical attributes parsed out, but that would be material for a new question :) .",0.049,0.152,0.799,0.5423
Datadog,54284969,54280115,2,"2019/01/21, 09:09:17",True,"2019/01/21, 09:14:28",125340.0,6464308.0,1,This command will only take a split second.,0.0,0.0,1.0,0.0
Datadog,54284969,54280115,2,"2019/01/21, 09:09:17",True,"2019/01/21, 09:14:28",125340.0,6464308.0,1,You must have spent 35 minutes waitung for the  ACCESS EXCLUSIVE  lock on the table to be granted (all the while blocking any transaction unfortunate enough to be queued behind you).,0.152,0.115,0.733,-0.3328
Datadog,54284969,54280115,2,"2019/01/21, 09:09:17",True,"2019/01/21, 09:14:28",125340.0,6464308.0,1,You probably have a problem with long transactions.,0.31,0.0,0.69,-0.4019
Datadog,54284969,54280115,2,"2019/01/21, 09:09:17",True,"2019/01/21, 09:14:28",125340.0,6464308.0,1,"Normally they should be as short as possible, otherwise they hold locks for a long time and also keep  VACUUM  from cleaning up dead row versions.",0.152,0.0,0.848,-0.6486
Datadog,54284969,54280115,2,"2019/01/21, 09:09:17",True,"2019/01/21, 09:14:28",125340.0,6464308.0,1,"The lock is necessary, but is should not pose a problem with a well behaved database workload.",0.0,0.299,0.701,0.6744
Datadog,52703484,52594973,0,"2018/10/08, 16:32:20",False,"2018/10/08, 16:32:20",66.0,10121786.0,0,You can create one PowerShell script to execute your Batch scripts remotely.,0.0,0.16,0.84,0.2732
Datadog,52703484,52594973,0,"2018/10/08, 16:32:20",False,"2018/10/08, 16:32:20",66.0,10121786.0,0,And Even you can schedule your PowerShell script using Windows Task Scheduler which will run as per your settings.,0.0,0.0,1.0,0.0
Datadog,52065295,52064818,0,"2018/08/28, 22:53:03",False,"2018/08/28, 22:53:03",892.0,1757329.0,0,Rubber Duck.,0.0,0.0,1.0,0.0
Datadog,52065295,52064818,0,"2018/08/28, 22:53:03",False,"2018/08/28, 22:53:03",892.0,1757329.0,0,"Turns out because we changed  .set  to  .default , we lost the ability to have the variables properly set during the first run.",0.093,0.093,0.813,0.0
Datadog,52065295,52064818,0,"2018/08/28, 22:53:03",False,"2018/08/28, 22:53:03",892.0,1757329.0,0,.normal  will do it for us.,0.0,0.0,1.0,0.0
Datadog,50526751,50526252,1,"2018/05/25, 13:10:38",True,"2018/05/25, 13:10:38",629.0,7202531.0,2,"Lots of threads are in WAITING state, and it's absolutely ok for them.",0.0,0.172,0.828,0.3597
Datadog,50526751,50526252,1,"2018/05/25, 13:10:38",True,"2018/05/25, 13:10:38",629.0,7202531.0,2,"For example, there are thread which have the following stack trace:",0.0,0.0,1.0,0.0
Datadog,50526751,50526252,1,"2018/05/25, 13:10:38",True,"2018/05/25, 13:10:38",629.0,7202531.0,2,This only means threads are waiting for any tasks to do.,0.0,0.0,1.0,0.0
Datadog,50526751,50526252,1,"2018/05/25, 13:10:38",True,"2018/05/25, 13:10:38",629.0,7202531.0,2,"However, other stacks do not look good.",0.286,0.0,0.714,-0.3412
Datadog,50526751,50526252,1,"2018/05/25, 13:10:38",True,"2018/05/25, 13:10:38",629.0,7202531.0,2,Those threads are waiting for connection to be free in the pool.,0.0,0.231,0.769,0.5106
Datadog,50526751,50526252,1,"2018/05/25, 13:10:38",True,"2018/05/25, 13:10:38",629.0,7202531.0,2,C3P0 is a pool of database connections.,0.0,0.0,1.0,0.0
Datadog,50526751,50526252,1,"2018/05/25, 13:10:38",True,"2018/05/25, 13:10:38",629.0,7202531.0,2,"Instead of creating a new connection every time, they are cached in the pool.",0.0,0.155,0.845,0.296
Datadog,50526751,50526252,1,"2018/05/25, 13:10:38",True,"2018/05/25, 13:10:38",629.0,7202531.0,2,"Upon closing, the connection itself is not closed, but only returned to the pool.",0.0,0.0,1.0,0.0
Datadog,50526751,50526252,1,"2018/05/25, 13:10:38",True,"2018/05/25, 13:10:38",629.0,7202531.0,2,"So, if hibernate for some reason (or other user) do not close connection after releasing it, then pool can get exhausted.",0.111,0.0,0.889,-0.3612
Datadog,50526751,50526252,1,"2018/05/25, 13:10:38",True,"2018/05/25, 13:10:38",629.0,7202531.0,2,"In order to resolve an issue, you have to find out why some connections are not closed after using.",0.0,0.126,0.874,0.3818
Datadog,50526751,50526252,1,"2018/05/25, 13:10:38",True,"2018/05/25, 13:10:38",629.0,7202531.0,2,Try to look at your code to do this.,0.0,0.0,1.0,0.0
Datadog,50526751,50526252,1,"2018/05/25, 13:10:38",True,"2018/05/25, 13:10:38",629.0,7202531.0,2,The other option is to temporarily go without C3P0 (pooling).,0.0,0.0,1.0,0.0
Datadog,50526751,50526252,1,"2018/05/25, 13:10:38",True,"2018/05/25, 13:10:38",629.0,7202531.0,2,"This is not forever, but at least you can check whether this guess is right.",0.0,0.0,1.0,0.0
Datadog,49475152,49468467,1,"2018/03/25, 14:11:43",True,"2018/03/25, 14:11:43",16823.0,1075289.0,1,"In Grails 3, You should put the below code to  grails-app/conf/spring/resources.groovy :",0.0,0.0,1.0,0.0
Datadog,48892885,48892401,0,"2018/02/20, 21:33:25",True,"2018/02/20, 21:33:25",43078.0,78722.0,2,"Neither, you want something like this I think:",0.14,0.287,0.573,0.3134
Datadog,48892885,48892401,0,"2018/02/20, 21:33:25",True,"2018/02/20, 21:33:25",43078.0,78722.0,2,Also putting the key into node attributes like that is very unsafe and kind of defeats the point of encrypted bags since node attributes are all written back to the Chef Server and so the key will be sent unencrypted.,0.06,0.058,0.882,-0.024
Datadog,47779901,47779576,0,"2017/12/12, 21:09:55",False,"2017/12/12, 21:09:55",3545.0,1831118.0,0,"I still don't know why it makes a difference, but adding the  -4  option made it work",0.0,0.0,1.0,0.0
Datadog,47779901,47779576,0,"2017/12/12, 21:09:55",False,"2017/12/12, 21:09:55",3545.0,1831118.0,0,Here's the man page on the option:,0.0,0.0,1.0,0.0
Datadog,47779901,47779576,0,"2017/12/12, 21:09:55",False,"2017/12/12, 21:09:55",3545.0,1831118.0,0,-4      Forces nc to use IPv4 addresses only.,0.0,0.0,1.0,0.0
Datadog,36027695,36024447,2,"2016/03/16, 07:49:29",False,"2016/03/16, 19:40:27",824.0,2493338.0,0,Problem in this case is not running scripts via JMeter GUI.,0.213,0.0,0.787,-0.4019
Datadog,36027695,36024447,2,"2016/03/16, 07:49:29",False,"2016/03/16, 19:40:27",824.0,2493338.0,0,Instead it is related to network.,0.0,0.0,1.0,0.0
Datadog,36027695,36024447,2,"2016/03/16, 07:49:29",False,"2016/03/16, 19:40:27",824.0,2493338.0,0,I had a similar distributed setup in EC2-environment and I successfully executed heavy load tests in GUI mode.,0.0,0.186,0.814,0.4939
Datadog,36027695,36024447,2,"2016/03/16, 07:49:29",False,"2016/03/16, 19:40:27",824.0,2493338.0,0,"In my case, all my JMeter (master/slaves) were running on EC2 instances (windows environment).",0.0,0.0,1.0,0.0
Datadog,36027695,36024447,2,"2016/03/16, 07:49:29",False,"2016/03/16, 19:40:27",824.0,2493338.0,0,"So, I will recommend you to setup your  JMeter   (Master)  on EC2 and run scripts via GUI mode.",0.0,0.148,0.852,0.4173
Datadog,36027695,36024447,2,"2016/03/16, 07:49:29",False,"2016/03/16, 19:40:27",824.0,2493338.0,0,If you still want to run in command line mode then you simply need to pass command to create jtl file while the script runs on command line.,0.0,0.116,0.884,0.34
Datadog,36027695,36024447,2,"2016/03/16, 07:49:29",False,"2016/03/16, 19:40:27",824.0,2493338.0,0,Later on you can use this JTL to generate any JMeter report as per requirement.,0.0,0.0,1.0,0.0
Datadog,36027695,36024447,2,"2016/03/16, 07:49:29",False,"2016/03/16, 19:40:27",824.0,2493338.0,0,For more details check..,0.0,0.0,1.0,0.0
Datadog,36027695,36024447,2,"2016/03/16, 07:49:29",False,"2016/03/16, 19:40:27",824.0,2493338.0,0,Jmeter - Run .jmx file through command line and get the summary report in a excel,0.0,0.188,0.812,0.4588
Datadog,36027695,36024447,2,"2016/03/16, 07:49:29",False,"2016/03/16, 19:40:27",824.0,2493338.0,0,jmeter -n -t /path/to/your/test.jmx  -l /path/to/results/file.jtl,0.0,0.0,1.0,0.0
Datadog,36027695,36024447,2,"2016/03/16, 07:49:29",False,"2016/03/16, 19:40:27",824.0,2493338.0,0,Please refer to Dmitri answer in following question to reduce JTL size.,0.0,0.173,0.827,0.3182
Datadog,36027695,36024447,2,"2016/03/16, 07:49:29",False,"2016/03/16, 19:40:27",824.0,2493338.0,0,How can we control size of JTL file while running test from Non GUI Mode,0.0,0.0,1.0,0.0
Datadog,31340312,30227260,0,"2015/07/10, 15:03:45",False,"2015/07/10, 15:03:45",491.0,4923204.0,0,"Before implementing the code, you need to look around in Widows ""registry"" using ""regedit"" and find the exact registry key value for the software.",0.0,0.094,0.906,0.34
Datadog,31340312,30227260,0,"2015/07/10, 15:03:45",False,"2015/07/10, 15:03:45",491.0,4923204.0,0,"Below example shows, how to fetch the version number of ""internet explorer"".",0.0,0.106,0.894,0.0772
Datadog,31340312,30227260,0,"2015/07/10, 15:03:45",False,"2015/07/10, 15:03:45",491.0,4923204.0,0,"Also recommended to have basic knowledge on Ruby array and hash, to understand the code",0.0,0.114,0.886,0.2023
Datadog,31340312,30227260,0,"2015/07/10, 15:03:45",False,"2015/07/10, 15:03:45",491.0,4923204.0,0,I've used registry_key_XXXXX Chef methods.,0.0,0.0,1.0,0.0
Datadog,31340312,30227260,0,"2015/07/10, 15:03:45",False,"2015/07/10, 15:03:45",491.0,4923204.0,0,Note: Registry key entry may differ for Windows 32bit and 64bit,0.0,0.0,1.0,0.0
Datadog,65043190,65042688,8,"2020/11/27, 21:56:03",True,"2020/11/28, 01:23:09",14946.0,190823.0,1,The problem is your  sendData()  function.,0.351,0.0,0.649,-0.4019
Datadog,65043190,65042688,8,"2020/11/27, 21:56:03",True,"2020/11/28, 01:23:09",14946.0,190823.0,1,This function is called in your for loop and has the following line:,0.0,0.0,1.0,0.0
Datadog,65043190,65042688,8,"2020/11/27, 21:56:03",True,"2020/11/28, 01:23:09",14946.0,190823.0,1,"This line will create a new DataDog client, which uses a Unix socket.",0.0,0.174,0.826,0.2732
Datadog,65043190,65042688,8,"2020/11/27, 21:56:03",True,"2020/11/28, 01:23:09",14946.0,190823.0,1,This explains your error message.,0.403,0.0,0.597,-0.4019
Datadog,65043190,65042688,8,"2020/11/27, 21:56:03",True,"2020/11/28, 01:23:09",14946.0,190823.0,1,"With every iteration of your loop, a new socket is &quot;allocated&quot;.",0.0,0.0,1.0,0.0
Datadog,65043190,65042688,8,"2020/11/27, 21:56:03",True,"2020/11/28, 01:23:09",14946.0,190823.0,1,"After a sufficient amount of loops no sockets can be opened, resulting in:",0.167,0.0,0.833,-0.296
Datadog,65043190,65042688,8,"2020/11/27, 21:56:03",True,"2020/11/28, 01:23:09",14946.0,190823.0,1,socket: too many open files,0.0,0.0,1.0,0.0
Datadog,65043190,65042688,8,"2020/11/27, 21:56:03",True,"2020/11/28, 01:23:09",14946.0,190823.0,1,To fix this you should create the client only once and pass it to your method as parameter.,0.0,0.11,0.89,0.2732
Datadog,49457754,49457370,0,"2018/03/23, 22:21:44",True,"2018/03/24, 05:00:40",2191.0,6084559.0,3,The variable  $LASTEXITCODE  will give you the exit code of the last native command (executable) that was run.,0.0,0.0,1.0,0.0
Datadog,44265020,44258173,0,"2017/05/30, 17:28:04",False,"2017/05/30, 17:28:04",11.0,8014459.0,0,"I'm not familiar with the program, but I've had to solve a problem like this before.",0.169,0.26,0.571,0.2263
Datadog,44265020,44258173,0,"2017/05/30, 17:28:04",False,"2017/05/30, 17:28:04",11.0,8014459.0,0,"I'm making the assumption that you're always going to start the output you want with a line 'Dogstatsd', and always end with several equals signs.",0.0,0.053,0.947,0.0772
Datadog,44265020,44258173,0,"2017/05/30, 17:28:04",False,"2017/05/30, 17:28:04",11.0,8014459.0,0,"Based on that, you could script out your output like this:",0.0,0.2,0.8,0.3612
Datadog,44265020,44258173,0,"2017/05/30, 17:28:04",False,"2017/05/30, 17:28:04",11.0,8014459.0,0,"We get the values defining the length of the file, the length until we hit the first line you want, the length where the output ends, and trim accordingly.",0.0,0.129,0.871,0.4588
Datadog,42575657,42556276,4,"2017/03/03, 11:53:34",True,"2017/03/03, 11:53:34",541.0,1153563.0,0,I would strongly suggest to setup a pre-production environment and run load tests (with tools like  JMeter ) in conjunction with server-side monitoring.,0.0,0.204,0.796,0.5574
Datadog,42575657,42556276,4,"2017/03/03, 11:53:34",True,"2017/03/03, 11:53:34",541.0,1153563.0,0,Tomcat backends can be monitored using the JMX protocol.,0.0,0.0,1.0,0.0
Datadog,42575657,42556276,4,"2017/03/03, 11:53:34",True,"2017/03/03, 11:53:34",541.0,1153563.0,0,You have 2 solutions :,0.0,0.459,0.541,0.1779
Datadog,42575657,42556276,4,"2017/03/03, 11:53:34",True,"2017/03/03, 11:53:34",541.0,1153563.0,0,"Like always, free software costs nothing but your time, and paid software gets you straight to the issue in exchanges for some pennies.",0.0,0.238,0.762,0.6428
Elastic APM,55021893,53428661,0,"2019/03/06, 13:22:41",False,"2019/03/06, 13:22:41",89.0,989723.0,2,As it is reported on the official site it could be released in the future.,0.0,0.0,1.0,0.0
Elastic APM,55021893,53428661,0,"2019/03/06, 13:22:41",False,"2019/03/06, 13:22:41",89.0,989723.0,2,"Some of the features we are very excited to provide in the near future
  include distributed tracing and providing framework-specific
  information (e.g., route change times) for some of the frontend
  frameworks such as React, Angular, Vue.js, etc.",0.0,0.07,0.93,0.4005
Elastic APM,55021893,53428661,0,"2019/03/06, 13:22:41",False,"2019/03/06, 13:22:41",89.0,989723.0,2,https://www.elastic.co/blog/elastic-apm-rum-js-agent-is-generally-available,0.0,0.0,1.0,0.0
Elastic APM,55021893,53428661,0,"2019/03/06, 13:22:41",False,"2019/03/06, 13:22:41",89.0,989723.0,2,For now you can rely on Elastic APM RUM JS Agent using JS tags:,0.0,0.0,1.0,0.0
Elastic APM,55021893,53428661,0,"2019/03/06, 13:22:41",False,"2019/03/06, 13:22:41",89.0,989723.0,2,https://www.elastic.co/guide/en/apm/agent/js-base/3.x/getting-started.html#using-script-tags,0.0,0.0,1.0,0.0
Elastic APM,58086472,54623543,0,"2019/09/24, 21:46:42",False,"2019/09/24, 21:46:42",3095.0,1532769.0,3,You can try to increase:,0.0,0.365,0.635,0.3182
Elastic APM,58086472,54623543,0,"2019/09/24, 21:46:42",False,"2019/09/24, 21:46:42",3095.0,1532769.0,3,Please take a look on documentation:  Tune APM Server,0.0,0.247,0.753,0.3182
Elastic APM,60540275,58780229,0,"2020/03/05, 09:37:36",False,"2020/03/05, 09:45:37",1.0,13010745.0,0,You can attach your ElasticApmAttacher.attach() in the Spring Application main class,0.0,0.0,1.0,0.0
Elastic APM,60540275,58780229,0,"2020/03/05, 09:37:36",False,"2020/03/05, 09:45:37",1.0,13010745.0,0,"For a SpringBootApplication packaged as a war file, and deployed to Tomcat server, this can be added to the configure method",0.178,0.0,0.822,-0.5994
Elastic APM,60540275,58780229,0,"2020/03/05, 09:37:36",False,"2020/03/05, 09:45:37",1.0,13010745.0,0,Below code might help:,0.0,0.474,0.526,0.4019
Elastic APM,61101616,61098374,0,"2020/04/08, 16:15:18",True,"2020/04/08, 16:15:18",36.0,13258898.0,2,unfortunately oracle is not supported by elastic apm agent.,0.384,0.0,0.616,-0.5207
Elastic APM,61101616,61098374,0,"2020/04/08, 16:15:18",True,"2020/04/08, 16:15:18",36.0,13258898.0,2,you should wrap your  oracleQueryRunner  in order to start and end agent spans manually.,0.0,0.0,1.0,0.0
Elastic APM,61101616,61098374,0,"2020/04/08, 16:15:18",True,"2020/04/08, 16:15:18",36.0,13258898.0,2,put this code in your  main.ts  file:,0.0,0.0,1.0,0.0
Elastic APM,62516533,62511256,2,"2020/06/22, 17:17:56",True,"2020/06/22, 17:17:56",86.0,6020497.0,5,"It's true, there isn't an Elixir agent for Elastic APM - you can upvote  this issue  to get the topic more attention.",0.0,0.123,0.877,0.4215
Elastic APM,62516533,62511256,2,"2020/06/22, 17:17:56",True,"2020/06/22, 17:17:56",86.0,6020497.0,5,"As you discovered, you can use the OpenTelemetry in the meantime.",0.0,0.0,1.0,0.0
Elastic APM,62516533,62511256,2,"2020/06/22, 17:17:56",True,"2020/06/22, 17:17:56",86.0,6020497.0,5,"To do that, run the OpenTelemetry contrib collector (otel collector) and configure it to export to  elastic  - there is a full explanation  in the docs  along with this sample configuration:",0.0,0.0,1.0,0.0
Elastic APM,62516533,62511256,2,"2020/06/22, 17:17:56",True,"2020/06/22, 17:17:56",86.0,6020497.0,5,"In your application, configure the tracer use the  opentelemetry exporter .",0.0,0.0,1.0,0.0
Elastic APM,62516533,62511256,2,"2020/06/22, 17:17:56",True,"2020/06/22, 17:17:56",86.0,6020497.0,5,At that point you'll have a tracer in your application sending traces to the otel collector.,0.0,0.0,1.0,0.0
Elastic APM,62516533,62511256,2,"2020/06/22, 17:17:56",True,"2020/06/22, 17:17:56",86.0,6020497.0,5,"From there, traces will be exported to the Elastic Stack via APM Server.",0.0,0.0,1.0,0.0
Elastic APM,62516533,62511256,2,"2020/06/22, 17:17:56",True,"2020/06/22, 17:17:56",86.0,6020497.0,5,In summary:  your app -&gt; otel collector -&gt; apm-server -&gt; elasticsearch,0.0,0.0,1.0,0.0
Elastic APM,62516533,62511256,2,"2020/06/22, 17:17:56",True,"2020/06/22, 17:17:56",86.0,6020497.0,5,The  Erlang/Elixir Agent Docs  have sample code for starting and decorating spans.,0.0,0.0,1.0,0.0
Elastic APM,63573627,63573207,4,"2020/08/25, 10:04:20",True,"2020/08/25, 10:04:20",28262.0,45691.0,1,transaction.duration.us  should indeed be what you're looking for.,0.0,0.0,1.0,0.0
Elastic APM,63573627,63573207,4,"2020/08/25, 10:04:20",True,"2020/08/25, 10:04:20",28262.0,45691.0,1,It's the duration in microseconds as an integer.,0.0,0.0,1.0,0.0
Elastic APM,63573627,63573207,4,"2020/08/25, 10:04:20",True,"2020/08/25, 10:04:20",28262.0,45691.0,1,"Divide it by 1000 to get milliseconds, or by 1'000'000 to get seconds.",0.0,0.0,1.0,0.0
Elastic APM,63573627,63573207,4,"2020/08/25, 10:04:20",True,"2020/08/25, 10:04:20",28262.0,45691.0,1,https://www.elastic.co/guide/en/apm/server/7.9/exported-fields-apm-transaction.html#_duration_2,0.0,0.0,1.0,0.0
Elastic APM,50283028,50282965,0,"2018/05/11, 02:51:21",False,"2018/05/11, 02:51:21",7900.0,2259934.0,0,"When you bring up containers using compose, each container has its own networking stack (so they can each talk to themselves on  localhost , but they need an ip address or dns name to talk to a different container!",0.0,0.0,1.0,0.0
Elastic APM,50283028,50282965,0,"2018/05/11, 02:51:21",False,"2018/05/11, 02:51:21",7900.0,2259934.0,0,).,0.0,0.0,1.0,0.0
Elastic APM,50283028,50282965,0,"2018/05/11, 02:51:21",False,"2018/05/11, 02:51:21",7900.0,2259934.0,0,Compose by default connects each of the containers to a default network and gives each a dns name with the name of the service.,0.0,0.0,1.0,0.0
Elastic APM,50283028,50282965,0,"2018/05/11, 02:51:21",False,"2018/05/11, 02:51:21",7900.0,2259934.0,0,If your compose file looks like,0.0,0.333,0.667,0.3612
Elastic APM,50283028,50282965,0,"2018/05/11, 02:51:21",False,"2018/05/11, 02:51:21",7900.0,2259934.0,0,A process in the  apm  container could access elasticsearch at  http://elasticsearch:9200,0.0,0.0,1.0,0.0
Elastic APM,51649953,50282965,0,"2018/08/02, 12:19:26",False,"2018/08/02, 12:19:26",41.0,8848390.0,4,"If you are starting all the services with single docker compose file, the app-server.yaml should have the value like this
 
output:
  elasticsearch:
    hosts: elasticsearch:9200
 
The ""hosts: elasticsearch:9200"" should be service name of the elasticsearch you mentioned in the docker-compose.",0.0,0.117,0.883,0.5994
Elastic APM,51649953,50282965,0,"2018/08/02, 12:19:26",False,"2018/08/02, 12:19:26",41.0,8848390.0,4,Like in the followiing,0.0,0.455,0.545,0.3612
Elastic APM,51649953,50282965,0,"2018/08/02, 12:19:26",False,"2018/08/02, 12:19:26",41.0,8848390.0,4,"
    version: '2'
    services:
      elasticsearch:
          image: elasticsearch:latest",0.0,0.0,1.0,0.0
Elastic APM,56741976,56665160,0,"2019/06/24, 21:20:03",False,"2019/06/24, 21:20:03",528.0,9619535.0,1,Answer to the Errors - I have custom resole.extensions in the  webpack.config.js :,0.211,0.0,0.789,-0.34
Elastic APM,56741976,56665160,0,"2019/06/24, 21:20:03",False,"2019/06/24, 21:20:03",528.0,9619535.0,1,That was missing the default  .json :,0.306,0.0,0.694,-0.296
Elastic APM,56741976,56665160,0,"2019/06/24, 21:20:03",False,"2019/06/24, 21:20:03",528.0,9619535.0,1,Now only the warnings are left:,0.306,0.0,0.694,-0.296
Elastic APM,56741976,56665160,0,"2019/06/24, 21:20:03",False,"2019/06/24, 21:20:03",528.0,9619535.0,1,I addressed them to the developer:  https://github.com/elastic/apm-agent-nodejs/issues/1154,0.0,0.0,1.0,0.0
Elastic APM,58717230,56989849,0,"2019/11/05, 20:16:57",False,"2019/11/05, 20:16:57",552.0,1518708.0,4,To listen on  0.0.0.0  try:,0.0,0.0,1.0,0.0
Elastic APM,57257482,57211703,2,"2019/07/29, 18:58:36",True,"2020/05/16, 21:59:41",445.0,3834445.0,3,Try using a configuration:,0.0,0.0,1.0,0.0
Elastic APM,57257482,57211703,2,"2019/07/29, 18:58:36",True,"2020/05/16, 21:59:41",445.0,3834445.0,3,where the file apm-server/config/apm-server.yml has your config content:,0.0,0.0,1.0,0.0
Elastic APM,57257482,57211703,2,"2019/07/29, 18:58:36",True,"2020/05/16, 21:59:41",445.0,3834445.0,3,Note the rum.allow_origins option that you can configure to resolve the CORS issue.,0.0,0.178,0.822,0.3818
Elastic APM,57257482,57211703,2,"2019/07/29, 18:58:36",True,"2020/05/16, 21:59:41",445.0,3834445.0,3,https://www.elastic.co/guide/en/apm/agent/rum-js/master/configuring-cors.html,0.0,0.0,1.0,0.0
Elastic APM,61340495,57696669,1,"2020/04/21, 12:44:16",True,"2020/04/21, 12:44:16",36.0,2155647.0,1,you have to modify your elastic APM python agent code.,0.0,0.0,1.0,0.0
Elastic APM,61340495,57696669,1,"2020/04/21, 12:44:16",True,"2020/04/21, 12:44:16",36.0,2155647.0,1,"In general, you can add labels to your span example",0.0,0.0,1.0,0.0
Elastic APM,61340495,57696669,1,"2020/04/21, 12:44:16",True,"2020/04/21, 12:44:16",36.0,2155647.0,1,"also, you can add directly to span object.",0.0,0.0,1.0,0.0
Elastic APM,61340495,57696669,1,"2020/04/21, 12:44:16",True,"2020/04/21, 12:44:16",36.0,2155647.0,1,You will get the ip from request object flask,0.0,0.0,1.0,0.0
Elastic APM,61340495,57696669,1,"2020/04/21, 12:44:16",True,"2020/04/21, 12:44:16",36.0,2155647.0,1,request.remote_addr   set this to the desired key.,0.0,0.259,0.741,0.2732
Elastic APM,61340495,57696669,1,"2020/04/21, 12:44:16",True,"2020/04/21, 12:44:16",36.0,2155647.0,1,more details of APIs on elastic APM python agent can be found here -  https://www.elastic.co/guide/en/apm/agent/python/current/api.html,0.0,0.0,1.0,0.0
Elastic APM,61340495,57696669,1,"2020/04/21, 12:44:16",True,"2020/04/21, 12:44:16",36.0,2155647.0,1,Thanks,0.0,1.0,0.0,0.4404
Elastic APM,60827426,60820578,0,"2020/03/24, 10:35:00",True,"2020/03/24, 10:35:00",6576.0,547452.0,2,These metrics come directly from  java.lang.management.GarbageCollectorMXBean .,0.0,0.0,1.0,0.0
Elastic APM,60827426,60820578,0,"2020/03/24, 10:35:00",True,"2020/03/24, 10:35:00",6576.0,547452.0,2,"The value of the  jvm.gc.time  metric is taken from  GarbageCollectorMXBean.getCollectionTime , which is indeed accumulating since the process started.",0.0,0.124,0.876,0.34
Elastic APM,60827426,60820578,0,"2020/03/24, 10:35:00",True,"2020/03/24, 10:35:00",6576.0,547452.0,2,"Assuming you're looking at metrics from a single JVM, there are a couple of possible reasons for why the value would appear to have gone backwards:",0.0,0.094,0.906,0.34
Elastic APM,60827426,60820578,0,"2020/03/24, 10:35:00",True,"2020/03/24, 10:35:00",6576.0,547452.0,2,"If the process had restarted (which I expect you would know about anyway), the metrics documents in Elasticsearch would have different values for the field  agent.ephemeral_id .",0.0,0.101,0.899,0.4019
Elastic APM,60827426,60820578,0,"2020/03/24, 10:35:00",True,"2020/03/24, 10:35:00",6576.0,547452.0,2,"The more likely answer is that you're seeing values for two different memory managers/GC generations, in which case the metrics documents in Elasticsearch would have different values for the field  labels.name .",0.0,0.157,0.843,0.6597
Elastic APM,60892441,60890836,2,"2020/03/27, 20:53:59",True,"2020/03/27, 21:21:59",3264.0,1783306.0,2,You don't end  trans1  and  trans2 .,0.0,0.0,1.0,0.0
Elastic APM,60892441,60890836,2,"2020/03/27, 20:53:59",True,"2020/03/27, 21:21:59",3264.0,1783306.0,2,"Just put these 2 lines to the point where these end, and everything should show up fine:",0.0,0.107,0.893,0.2023
Elastic APM,60892441,60890836,2,"2020/03/27, 20:53:59",True,"2020/03/27, 21:21:59",3264.0,1783306.0,2,"There is the  CaptureTransaction , which is a convenient method that can wrap you any code and makes sure the transaction is ended and all exceptions are captured - so you use that method and it does ""everything"" for you.",0.0,0.06,0.94,0.3182
Elastic APM,60892441,60890836,2,"2020/03/27, 20:53:59",True,"2020/03/27, 21:21:59",3264.0,1783306.0,2,"Then there is the  StartTransaction  method - this is the one you use in your code -, which starts the transaction and does not do anything else.",0.0,0.0,1.0,0.0
Elastic APM,60892441,60890836,2,"2020/03/27, 20:53:59",True,"2020/03/27, 21:21:59",3264.0,1783306.0,2,The advantage here is that you get an  ITransaction  instance which you can use wherever and whenever you want.,0.0,0.163,0.837,0.3182
Elastic APM,60892441,60890836,2,"2020/03/27, 20:53:59",True,"2020/03/27, 21:21:59",3264.0,1783306.0,2,But in this case you need to call  .End()  on it manually once the transaction (aka the code you want to capture) is executed.,0.0,0.059,0.941,0.1154
Elastic APM,60892441,60890836,2,"2020/03/27, 20:53:59",True,"2020/03/27, 21:21:59",3264.0,1783306.0,2,Same with  CaptureSpan  and  StartSpan .,0.0,0.0,1.0,0.0
Elastic APM,60892441,60890836,2,"2020/03/27, 20:53:59",True,"2020/03/27, 21:21:59",3264.0,1783306.0,2,"So you used  CaptureSpan  for your spans, so those where ended automatically when the lambda with  Task.Delay  finished, on the other hand you started your transactions with  StartTransaction  but only called  .End()  on  trans3  and not on the 2 other transactions.",0.0,0.051,0.949,0.2732
Elastic APM,60892441,60890836,2,"2020/03/27, 20:53:59",True,"2020/03/27, 21:21:59",3264.0,1783306.0,2,There is some explanation with a demo  here  - sample code of that demo is  here .,0.0,0.0,1.0,0.0
Elastic APM,61066068,61065570,6,"2020/04/06, 21:03:11",True,"2020/04/06, 21:03:11",3264.0,1783306.0,1,Currently background services are not captured out of the box.,0.0,0.0,1.0,0.0
Elastic APM,61066068,61065570,6,"2020/04/06, 21:03:11",True,"2020/04/06, 21:03:11",3264.0,1783306.0,1,What you can do is to use the  Public Agent API  and with a little bit of an additional code you can capture those also as transactions.,0.0,0.0,1.0,0.0
Elastic APM,61066068,61065570,6,"2020/04/06, 21:03:11",True,"2020/04/06, 21:03:11",3264.0,1783306.0,1,Something like this in the background service:,0.0,0.294,0.706,0.3612
Elastic APM,61196239,61115721,0,"2020/04/13, 23:31:21",False,"2020/04/13, 23:31:21",64.0,12032134.0,1,"I run via docker-compose elasticsearch, apm, kibana and tomcat application in docker.",0.0,0.0,1.0,0.0
Elastic APM,61196239,61115721,0,"2020/04/13, 23:31:21",False,"2020/04/13, 23:31:21",64.0,12032134.0,1,In apm- -transaction-  index exist this meta information:  container.id .,0.0,0.0,1.0,0.0
Elastic APM,61196239,61115721,0,"2020/04/13, 23:31:21",False,"2020/04/13, 23:31:21",64.0,12032134.0,1,And in apm- -metrics-  index this information is also stored.,0.0,0.0,1.0,0.0
Elastic APM,61196239,61115721,0,"2020/04/13, 23:31:21",False,"2020/04/13, 23:31:21",64.0,12032134.0,1,"Try to look at json structure at Discover tab by index pattern ""apm-*""",0.0,0.0,1.0,0.0
Elastic APM,61196239,61115721,0,"2020/04/13, 23:31:21",False,"2020/04/13, 23:31:21",64.0,12032134.0,1,enter image description here,0.0,0.0,1.0,0.0
Elastic APM,61509232,61262895,0,"2020/04/29, 21:38:55",False,"2020/04/29, 21:38:55",64.0,12032134.0,0,did you try to give permissions to folder /opt/elastic ?,0.0,0.0,1.0,0.0
Elastic APM,64329014,64323562,2,"2020/10/13, 08:23:52",False,"2020/10/13, 08:23:52",156911.0,4668.0,1,"It's hard to say without knowing exactly how you're using NestJS and GraphQL together, and without knowing which parts of Apollo Server that NestJS itself uses.",0.053,0.0,0.947,-0.1027
Elastic APM,64329014,64323562,2,"2020/10/13, 08:23:52",False,"2020/10/13, 08:23:52",156911.0,4668.0,1,Seeing a small sample of what  I am using the graphql feature of nestjs  means would be useful.,0.0,0.162,0.838,0.4404
Elastic APM,64329014,64323562,2,"2020/10/13, 08:23:52",False,"2020/10/13, 08:23:52",156911.0,4668.0,1,Here's a few datapoints that might help you narrow things down.,0.0,0.231,0.769,0.4019
Elastic APM,64329014,64323562,2,"2020/10/13, 08:23:52",False,"2020/10/13, 08:23:52",156911.0,4668.0,1,"Also, opening  an issue  in the Agent repository or  a question in their forums  might get more of the right eyes on this.",0.0,0.0,1.0,0.0
Elastic APM,64329014,64323562,2,"2020/10/13, 08:23:52",False,"2020/10/13, 08:23:52",156911.0,4668.0,1,"The Elastic APM instrumentation for Apollo Server works by wrapping the  runHttpQuery  function of the  apollo-server-core  module, and  marking the transaction with  trans._graphqlRoute .",0.0,0.0,1.0,0.0
Elastic APM,64329014,64323562,2,"2020/10/13, 08:23:52",False,"2020/10/13, 08:23:52",156911.0,4668.0,1,"When the agent  sees this  _graphqlRoute  property , it runs some code that will set a default name for the transaction",0.0,0.0,1.0,0.0
Elastic APM,64329014,64323562,2,"2020/10/13, 08:23:52",False,"2020/10/13, 08:23:52",156911.0,4668.0,1,"In your application, either the  _graphqlRoute  property isn't getting set, or the renaming code above is doing something weird, or something about NestJS comes along and renames the transaction after it's been named with the above code.",0.045,0.0,0.955,-0.1779
Elastic APM,64329014,64323562,2,"2020/10/13, 08:23:52",False,"2020/10/13, 08:23:52",156911.0,4668.0,1,Knowing more specifically  what  you're doing would help folks narrow in on your problems.,0.155,0.155,0.69,0.0
Elastic APM,65496630,65488128,2,"2020/12/29, 20:01:53",True,"2020/12/29, 20:01:53",633.0,13659075.0,1,The metrics shown in Kibana are sent by the APM agent that like you said has limited access to your environment.,0.081,0.107,0.812,0.1531
Elastic APM,65496630,65488128,2,"2020/12/29, 20:01:53",True,"2020/12/29, 20:01:53",633.0,13659075.0,1,It basically says anything that is collected by the JVM running your JAR.,0.0,0.0,1.0,0.0
Elastic APM,65496630,65488128,2,"2020/12/29, 20:01:53",True,"2020/12/29, 20:01:53",633.0,13659075.0,1,If you want to get further visibility into the CPU details of your local environment then you must augment your setup using  Elastic MetricBeats  that ships O.S level details about your machine that sees beyond what the JVM can see.,0.0,0.032,0.968,0.0772
Elastic APM,65496630,65488128,2,"2020/12/29, 20:01:53",True,"2020/12/29, 20:01:53",633.0,13659075.0,1,"In the presentation below I show how to configure logs, metrics, and APM altogether.",0.0,0.0,1.0,0.0
Elastic APM,65496630,65488128,2,"2020/12/29, 20:01:53",True,"2020/12/29, 20:01:53",633.0,13659075.0,1,https://www.youtube.com/watch?v=aXbg9pZCjpk,0.0,0.0,1.0,0.0
Elastic APM,54385310,52708201,1,"2019/01/27, 07:21:01",False,"2019/01/27, 07:21:01",1.0,10973722.0,-1,"currently,we run apm-agent for java application, but after 1 day, server cpu has over 100% and java application killed by system",0.248,0.0,0.752,-0.8047
Elastic APM,55333285,55319800,0,"2019/03/25, 09:50:57",False,"2019/03/25, 09:50:57",3915.0,837717.0,0,"I am not familiar with what is Elastic APM, but if it says it supports Spring Boot, then it means it supports any spring-boot-based framework which Spring Cloud Stream is.",0.0,0.194,0.806,0.7579
Elastic APM,57318627,57297752,1,"2019/08/02, 03:41:47",False,"2019/08/02, 03:41:47",9394.0,573153.0,0,"The  JVM GC metrics tracked  right now are  jvm.gc.alloc ,  jvm.gc.time , and  jvm.gc.count .",0.0,0.0,1.0,0.0
Elastic APM,57318627,57297752,1,"2019/08/02, 03:41:47",False,"2019/08/02, 03:41:47",9394.0,573153.0,0,"If you are looking for additional ones, which ones would those be?",0.0,0.0,1.0,0.0
Elastic APM,57318627,57297752,1,"2019/08/02, 03:41:47",False,"2019/08/02, 03:41:47",9394.0,573153.0,0,And could you  open an issue with the details .,0.0,0.0,1.0,0.0
Elastic APM,61072797,57297752,1,"2020/04/07, 07:20:17",False,"2020/04/07, 07:20:17",23.0,6332568.0,0,Please import from saved objects option -  https://github.com/elastic/apm-contrib/blob/master/apm-agent-java/dashboards/java_metrics_dashboard_7.x.json,0.0,0.505,0.495,0.6249
Elastic APM,57298815,57298727,3,"2019/08/01, 00:04:16",False,"2019/08/01, 00:04:16",151.0,639333.0,0,To include spans into the transactions you should start the spans from the transaction object,0.0,0.0,1.0,0.0
Elastic APM,57298815,57298727,3,"2019/08/01, 00:04:16",False,"2019/08/01, 00:04:16",151.0,639333.0,0,"
 
 ...
var span = transaction.startSpan('My custom span')
...",0.0,0.0,1.0,0.0
Elastic APM,57298815,57298727,3,"2019/08/01, 00:04:16",False,"2019/08/01, 00:04:16",151.0,639333.0,0,And ending the parent transaction object all the nested spans will be also ended in cascade,0.0,0.0,1.0,0.0
Elastic APM,57298815,57298727,3,"2019/08/01, 00:04:16",False,"2019/08/01, 00:04:16",151.0,639333.0,0,https://www.elastic.co/guide/en/apm/agent/js-base/4.x/transaction-api.html#transaction-start-span,0.0,0.0,1.0,0.0
Elastic APM,59268282,57782547,8,"2019/12/10, 15:25:10",False,"2019/12/10, 15:25:10",228.0,4349519.0,1,You can start your application with argument  active=false .,0.263,0.0,0.737,-0.3612
Elastic APM,59268282,57782547,8,"2019/12/10, 15:25:10",False,"2019/12/10, 15:25:10",228.0,4349519.0,1,C:\Users\dsm\Documents&gt;java -jar apm-agent-attach-1.9.0-standalone.jar --pid 16832 --args 'service_name=test;server_urls=http://localhost:8200;active=false',0.0,0.0,1.0,0.0
Elastic APM,58075056,58074198,0,"2019/09/24, 10:22:09",True,"2019/09/24, 10:22:09",163369.0,4604579.0,5,You simply need to change your query to this:,0.0,0.0,1.0,0.0
Elastic APM,63339911,58074198,0,"2020/08/10, 15:09:15",False,"2020/08/10, 15:09:15",383.0,10119759.0,3,"The accepted answer no longer works, you can use the following",0.165,0.158,0.677,-0.0258
Elastic APM,58087113,58076441,0,"2019/09/24, 22:36:31",True,"2019/09/24, 22:36:31",9394.0,573153.0,1,The key features of APM agents is normally in their framework integrations.,0.0,0.0,1.0,0.0
Elastic APM,58087113,58076441,0,"2019/09/24, 22:36:31",True,"2019/09/24, 22:36:31",9394.0,573153.0,1,The Java APM agent is mostly focussed on web frameworks — see the list of  supported technologies .,0.0,0.133,0.867,0.3182
Elastic APM,58087113,58076441,0,"2019/09/24, 22:36:31",True,"2019/09/24, 22:36:31",9394.0,573153.0,1,"But you already mentioned the  public API  — if you manually instrument your code with that, you will still be able to use it.",0.0,0.0,1.0,0.0
Elastic APM,58087113,58076441,0,"2019/09/24, 22:36:31",True,"2019/09/24, 22:36:31",9394.0,573153.0,1,It just doesn't automatically understand the framework and you need to help it with that.,0.0,0.162,0.838,0.4019
Elastic APM,58087113,58076441,0,"2019/09/24, 22:36:31",True,"2019/09/24, 22:36:31",9394.0,573153.0,1,"Alternatively, if your tool supports OpenTracing then you could use the  OpenTracing bridge  for that.",0.0,0.152,0.848,0.3612
Elastic APM,58750060,58728047,0,"2019/11/07, 15:43:30",True,"2019/11/07, 15:51:40",26.0,9528014.0,1,When routing to a different subpage you have to set the Route Name manually.,0.0,0.0,1.0,0.0
Elastic APM,58750060,58728047,0,"2019/11/07, 15:43:30",True,"2019/11/07, 15:51:40",26.0,9528014.0,1,You can achieve this via a filter on the 'change-route' type.,0.0,0.0,1.0,0.0
Elastic APM,58750060,58728047,0,"2019/11/07, 15:43:30",True,"2019/11/07, 15:51:40",26.0,9528014.0,1,"See  apm.addFilter()  
docs:  https://www.elastic.co/guide/en/apm/agent/rum-js/current/agent-api.html#apm-add-filter",0.0,0.0,1.0,0.0
Elastic APM,58750060,58728047,0,"2019/11/07, 15:43:30",True,"2019/11/07, 15:51:40",26.0,9528014.0,1,Something like this should work:,0.0,0.385,0.615,0.3612
Elastic APM,60703633,58728047,0,"2020/03/16, 11:47:36",False,"2020/03/16, 11:47:36",55.0,3588136.0,1,Another way to do it would be to observe for transaction events which is fired whenever transaction starts and ends.,0.159,0.0,0.841,-0.5574
Elastic APM,60703633,58728047,0,"2020/03/16, 11:47:36",False,"2020/03/16, 11:47:36",55.0,3588136.0,1,API docs -  https://www.elastic.co/guide/en/apm/agent/rum-js/current/agent-api.html#observe,0.0,0.0,1.0,0.0
Elastic APM,60703633,58728047,0,"2020/03/16, 11:47:36",False,"2020/03/16, 11:47:36",55.0,3588136.0,1,"The agent also supports frameworks such as Vue.js, React and Angular out of the box, so the above code should not be necessary.",0.0,0.102,0.898,0.3612
Elastic APM,60703633,58728047,0,"2020/03/16, 11:47:36",False,"2020/03/16, 11:47:36",55.0,3588136.0,1,Vue docs -  https://www.elastic.co/guide/en/apm/agent/rum-js/current/vue-integration.html,0.0,0.0,1.0,0.0
Elastic APM,58742793,58738337,0,"2019/11/07, 08:00:56",False,"2019/11/07, 08:00:56",9394.0,573153.0,0,"Logging is separate from APM / tracing, but can be integrated.",0.0,0.0,1.0,0.0
Elastic APM,58742793,58738337,0,"2019/11/07, 08:00:56",False,"2019/11/07, 08:00:56",9394.0,573153.0,0,"https://github.com/elastic/ecs-logging-java  is a curated logging library that will also correlate the trace IDs, so you can tie both together.",0.0,0.0,1.0,0.0
Elastic APM,58742793,58738337,0,"2019/11/07, 08:00:56",False,"2019/11/07, 08:00:56",9394.0,573153.0,0,Keep using SLF4J and just add the right logging backend.,0.0,0.0,1.0,0.0
Elastic APM,58742793,58738337,0,"2019/11/07, 08:00:56",False,"2019/11/07, 08:00:56",9394.0,573153.0,0,The output can then be picked up by Filebeat (as described in the repository) and you're ready to go.,0.0,0.122,0.878,0.3612
Elastic APM,61196812,58738337,0,"2020/04/14, 00:09:21",False,"2020/04/14, 00:09:21",64.0,12032134.0,1,"Elastic-apm-agent-java automatically capture exceptions when you use slf4j implementation  Logger#error(""message"", Throwable) .",0.0,0.0,1.0,0.0
Elastic APM,61196812,58738337,0,"2020/04/14, 00:09:21",False,"2020/04/14, 00:09:21",64.0,12032134.0,1,More information you can find  here,0.0,0.0,1.0,0.0
Elastic APM,61801017,59328108,0,"2020/05/14, 18:18:04",False,"2020/05/14, 18:18:04",396.0,4191904.0,0,Try  this  official docker-compose set up:,0.0,0.0,1.0,0.0
Elastic APM,60702327,59352981,0,"2020/03/16, 10:08:57",True,"2020/03/16, 10:08:57",1083.0,3074424.0,0,"The Elastic RUM agent has support for  click user interactions , therefore you shouldn't need to manually start these type of transactions.",0.0,0.119,0.881,0.4019
Elastic APM,60702327,59352981,0,"2020/03/16, 10:08:57",True,"2020/03/16, 10:08:57",1083.0,3074424.0,0,Regarding the failure in your code the correct API call is  getCurrentTransaction  and not  currentTransaction .,0.191,0.0,0.809,-0.5106
Elastic APM,60702327,59352981,0,"2020/03/16, 10:08:57",True,"2020/03/16, 10:08:57",1083.0,3074424.0,0,Hope this helps.,0.0,0.846,0.154,0.6705
Elastic APM,60983706,60920952,0,"2020/04/02, 06:15:04",False,"2020/04/02, 06:15:04",6576.0,547452.0,0,"So, I'm guessing that the handler wrappers are dropping the buffalo.Context information.",0.0,0.0,1.0,0.0
Elastic APM,60983706,60920952,0,"2020/04/02, 06:15:04",False,"2020/04/02, 06:15:04",6576.0,547452.0,0,That's correct.,0.0,0.0,1.0,0.0
Elastic APM,60983706,60920952,0,"2020/04/02, 06:15:04",False,"2020/04/02, 06:15:04",6576.0,547452.0,0,The problem is that  buffalo.WrapHandler  ( Source ) throws away all of the context other than the underlying  http.Request / http.Response :,0.137,0.0,0.863,-0.4019
Elastic APM,60983706,60920952,0,"2020/04/02, 06:15:04",False,"2020/04/02, 06:15:04",6576.0,547452.0,0,"So, what would I need to do to be able to integrate Sentry and Elastic in Buffalo asides from trying to reimplement their wrappers?",0.0,0.0,1.0,0.0
Elastic APM,60983706,60920952,0,"2020/04/02, 06:15:04",False,"2020/04/02, 06:15:04",6576.0,547452.0,0,I can see two options:,0.0,0.0,1.0,0.0
Elastic APM,60983706,60920952,0,"2020/04/02, 06:15:04",False,"2020/04/02, 06:15:04",6576.0,547452.0,0,There's an open issue in the Elastic APM agent for the latter option:  elastic/apm#39 .,0.0,0.0,1.0,0.0
Elastic APM,61067229,61066877,2,"2020/04/06, 22:11:02",True,"2020/04/06, 22:11:02",3264.0,1783306.0,1,There are basically 2 ways the agent captures things:,0.0,0.0,1.0,0.0
Elastic APM,61067229,61066877,2,"2020/04/06, 22:11:02",True,"2020/04/06, 22:11:02",3264.0,1783306.0,1,"In a typical ASP.NET Classic MVC application the agent has auto instrumentation for outgoing HTTP calls with  HttpClient , Database calls with EF6 (Make sure to  add the interceptor ) ( SqlClient  support is already work-in-progress, hopefully released soon).",0.0,0.242,0.758,0.836
Elastic APM,61067229,61066877,2,"2020/04/06, 22:11:02",True,"2020/04/06, 22:11:02",3264.0,1783306.0,1,"So unless you have any of these within those requests, the agent won't capture things out of the box.",0.0,0.0,1.0,0.0
Elastic APM,61067229,61066877,2,"2020/04/06, 22:11:02",True,"2020/04/06, 22:11:02",3264.0,1783306.0,1,"If you want to capture more things, currently the way to go is to place some agent specific code - so basically manual code instrumentation - into your application and use the public agent API.",0.0,0.039,0.961,0.0772
Elastic APM,61171704,61108716,0,"2020/04/12, 15:46:20",True,"2020/04/12, 15:46:20",33.0,9358877.0,0,Finally noticed issue.,0.0,0.0,1.0,0.0
Elastic APM,61171704,61108716,0,"2020/04/12, 15:46:20",True,"2020/04/12, 15:46:20",33.0,9358877.0,0,application API is returning service name as Bootstrap because.,0.0,0.0,1.0,0.0
Elastic APM,61171704,61108716,0,"2020/04/12, 15:46:20",True,"2020/04/12, 15:46:20",33.0,9358877.0,0,Id is not set so it is using default value,0.0,0.211,0.789,0.34
Elastic APM,61171704,61108716,0,"2020/04/12, 15:46:20",True,"2020/04/12, 15:46:20",33.0,9358877.0,0,The Id can be set like this,0.0,0.294,0.706,0.3612
Elastic APM,61195913,61163723,0,"2020/04/13, 23:09:07",True,"2020/04/13, 23:09:07",64.0,12032134.0,1,"You can try the method described here  disscuss-elastic , via ElasticApmAttacher#attach(map of properties).",0.0,0.0,1.0,0.0
Elastic APM,61524739,61524132,3,"2020/04/30, 16:40:12",True,"2020/04/30, 16:40:12",3264.0,1783306.0,1,"Is this not yet included in the .NET agent, or is there additional configuration necessary to get this working?",0.0,0.0,1.0,0.0
Elastic APM,61524739,61524132,3,"2020/04/30, 16:40:12",True,"2020/04/30, 16:40:12",3264.0,1783306.0,1,This is not yet included in the .NET Agent unfortunately.,0.211,0.0,0.789,-0.34
Elastic APM,61652111,61642883,0,"2020/05/07, 10:28:33",False,"2020/05/07, 10:37:54",3606.0,2042827.0,0,WebFlux can run on Servlet containers with support for the Servlet 3.1 Non-Blocking IO API as well as on other async runtimes such as  Netty  and Undertow.,0.0,0.161,0.839,0.5859
Elastic APM,61652111,61642883,0,"2020/05/07, 10:28:33",False,"2020/05/07, 10:37:54",3606.0,2042827.0,0,Each Spring Boot web application includes an embedded web server.,0.0,0.0,1.0,0.0
Elastic APM,61652111,61642883,0,"2020/05/07, 10:28:33",False,"2020/05/07, 10:37:54",3606.0,2042827.0,0,"For reactive stack applications, the  spring-boot-starter-webflux  includes Reactor Netty by default I guess.",0.0,0.0,1.0,0.0
Elastic APM,61652111,61642883,0,"2020/05/07, 10:28:33",False,"2020/05/07, 10:37:54",3606.0,2042827.0,0,"And it does not include  Servlet API  (Netty is non-Servlet runtime), but it looks like your  Elastic APM  expects this API to be present.",0.0,0.124,0.876,0.5023
Elastic APM,61652111,61642883,0,"2020/05/07, 10:28:33",False,"2020/05/07, 10:37:54",3606.0,2042827.0,0,Try to use  spring-boot-starter-tomcat  instead of Netty.,0.0,0.0,1.0,0.0
Elastic APM,61652111,61642883,0,"2020/05/07, 10:28:33",False,"2020/05/07, 10:37:54",3606.0,2042827.0,0,"When  switching to a different HTTP server , you need to exclude the default dependencies in addition to including the one you need.",0.087,0.0,0.913,-0.2263
Elastic APM,61652111,61642883,0,"2020/05/07, 10:28:33",False,"2020/05/07, 10:37:54",3606.0,2042827.0,0,Here is an example:,0.0,0.0,1.0,0.0
Elastic APM,61652111,61642883,0,"2020/05/07, 10:28:33",False,"2020/05/07, 10:37:54",3606.0,2042827.0,0,Tomcat dependency brings Servlet API.,0.0,0.0,1.0,0.0
Elastic APM,61652111,61642883,0,"2020/05/07, 10:28:33",False,"2020/05/07, 10:37:54",3606.0,2042827.0,0,Perhaps it will resolve your issue.,0.0,0.342,0.658,0.3818
Elastic APM,66580927,61642883,0,"2021/03/11, 12:42:08",False,"2021/03/11, 12:42:08",47.0,2800145.0,0,Looks like there is no support from elastic for WebFlux yet,0.143,0.338,0.519,0.4588
Elastic APM,66580927,61642883,0,"2021/03/11, 12:42:08",False,"2021/03/11, 12:42:08",47.0,2800145.0,0,Check here  https://github.com/elastic/apm-agent-java/issues/60,0.0,0.0,1.0,0.0
Elastic APM,66580927,61642883,0,"2021/03/11, 12:42:08",False,"2021/03/11, 12:42:08",47.0,2800145.0,0,"They are currently working on it, but there is not a date to be ready yet",0.0,0.188,0.812,0.5023
Elastic APM,62680831,62677891,0,"2020/07/01, 19:02:17",True,"2020/07/01, 19:02:17",45.0,13409135.0,0,I got the answer after posting the same on  Elastic Support Forum .,0.0,0.213,0.787,0.4019
Elastic APM,62680831,62677891,0,"2020/07/01, 19:02:17",True,"2020/07/01, 19:02:17",45.0,13409135.0,0,It was a very prompt response.,0.0,0.0,1.0,0.0
Elastic APM,62680831,62677891,0,"2020/07/01, 19:02:17",True,"2020/07/01, 19:02:17",45.0,13409135.0,0,"This was not a problem from Elastic APM side, and was more of a silly problem from my side.",0.0,0.296,0.704,0.5986
Elastic APM,62680831,62677891,0,"2020/07/01, 19:02:17",True,"2020/07/01, 19:02:17",45.0,13409135.0,0,Refer the  discussion  to find the problem and solution.,0.225,0.192,0.583,-0.1027
Elastic APM,62903745,62815678,1,"2020/07/14, 23:46:57",True,"2020/07/14, 23:46:57",1274.0,1370767.0,1,"This question was cross-posted to discuss.elastic.co, and you can see the answer that was provided there:  https://discuss.elastic.co/t/elastic-apm-python-system-metrics-dont-show-process-related-metrics-like-memory-on-kibana/240531/2?u=basepi",0.0,0.0,1.0,0.0
Elastic APM,63118018,63116459,4,"2020/07/27, 17:58:27",False,"2020/07/27, 17:58:27",163369.0,4604579.0,1,"In your ES Cloud console, you need to Edit the cluster configuration, scroll to the APM section and then click &quot;User override settings&quot;.",0.0,0.0,1.0,0.0
Elastic APM,63118018,63116459,4,"2020/07/27, 17:58:27",False,"2020/07/27, 17:58:27",163369.0,4604579.0,1,In there you can override the target index by adding the following property:,0.0,0.0,1.0,0.0
Elastic APM,63118018,63116459,4,"2020/07/27, 17:58:27",False,"2020/07/27, 17:58:27",163369.0,4604579.0,1,"Note that if you change this setting, you also need to modify the corresponding index template to match the new index name.",0.0,0.0,1.0,0.0
Elastic APM,63291099,63129247,0,"2020/08/06, 23:12:47",False,"2020/08/06, 23:12:47",64.0,12032134.0,0,currently elastic-apm-agent support natively Quartz framework(since 1.8).,0.0,0.31,0.69,0.4019
Elastic APM,63291099,63129247,0,"2020/08/06, 23:12:47",False,"2020/08/06, 23:12:47",64.0,12032134.0,0,"If you use it, instrumentation should work.",0.0,0.0,1.0,0.0
Elastic APM,63291099,63129247,0,"2020/08/06, 23:12:47",False,"2020/08/06, 23:12:47",64.0,12032134.0,0,But you should add your packages to  application_packages .,0.0,0.0,1.0,0.0
Elastic APM,63291099,63129247,0,"2020/08/06, 23:12:47",False,"2020/08/06, 23:12:47",64.0,12032134.0,0,It would be good if you can share mini-demo project.,0.0,0.389,0.611,0.6249
Elastic APM,63291099,63129247,0,"2020/08/06, 23:12:47",False,"2020/08/06, 23:12:47",64.0,12032134.0,0,And I can reproduce your problem locally.,0.351,0.0,0.649,-0.4019
Elastic APM,63291099,63129247,0,"2020/08/06, 23:12:47",False,"2020/08/06, 23:12:47",64.0,12032134.0,0,Information from  supported-technologies-details,0.0,0.0,1.0,0.0
Elastic APM,63351255,63339667,1,"2020/08/11, 06:53:42",True,"2020/08/11, 06:53:42",6576.0,547452.0,0,"All of the Elastic APM agents, with the exception of the RUM JavaScript agent, have a  verify_server_cert  configuration variable.",0.0,0.0,1.0,0.0
Elastic APM,63351255,63339667,1,"2020/08/11, 06:53:42",True,"2020/08/11, 06:53:42",6576.0,547452.0,0,You can set this to  false  to disable server TLS certificate verification.,0.0,0.0,1.0,0.0
Elastic APM,63353158,63339667,1,"2020/08/11, 10:06:22",False,"2020/08/11, 10:06:22",32819.0,433570.0,0,I am using docker-compose,0.0,0.0,1.0,0.0
Elastic APM,63353158,63339667,1,"2020/08/11, 10:06:22",False,"2020/08/11, 10:06:22",32819.0,433570.0,0,and the following option doesn't work.,0.0,0.0,1.0,0.0
Elastic APM,63353158,63339667,1,"2020/08/11, 10:06:22",False,"2020/08/11, 10:06:22",32819.0,433570.0,0,I think its a bug for apm..,0.0,0.0,1.0,0.0
Elastic APM,63353158,63339667,1,"2020/08/11, 10:06:22",False,"2020/08/11, 10:06:22",32819.0,433570.0,0,When I use same option in apm-server.yml it works fine.,0.0,0.184,0.816,0.2023
Elastic APM,63934347,63931550,0,"2020/09/17, 11:40:33",True,"2020/09/17, 11:40:33",3264.0,1783306.0,1,One thing you can use is the  Filter API  for this.,0.0,0.0,1.0,0.0
Elastic APM,63934347,63931550,0,"2020/09/17, 11:40:33",True,"2020/09/17, 11:40:33",3264.0,1783306.0,1,With that you have access to all transactions and spans before they are sent to the APM Server.,0.0,0.0,1.0,0.0
Elastic APM,63934347,63931550,0,"2020/09/17, 11:40:33",True,"2020/09/17, 11:40:33",3264.0,1783306.0,1,"You can't run through all the spans on a given transaction, so you need some tweaking - for this I use a  Dictionary  in my sample.",0.0,0.0,1.0,0.0
Elastic APM,63934347,63931550,0,"2020/09/17, 11:40:33",True,"2020/09/17, 11:40:33",3264.0,1783306.0,1,Couple of thing here:,0.0,0.0,1.0,0.0
Elastic APM,65484213,65441133,0,"2020/12/28, 23:19:29",True,"2020/12/28, 23:19:29",109.0,1175351.0,0,Looks like it can be done using drop_event processor in api-server.yml.,0.0,0.2,0.8,0.3612
Elastic APM,65484213,65441133,0,"2020/12/28, 23:19:29",True,"2020/12/28, 23:19:29",109.0,1175351.0,0,and in code set custom context:,0.0,0.0,1.0,0.0
Elastic APM,66062278,66026533,0,"2021/02/05, 13:15:44",True,"2021/02/05, 13:15:44",59.0,8278891.0,0,Commenting out  'SERVER_URL': '127.0.0.1:8200'  solved the problem.,0.276,0.214,0.51,-0.1531
Elastic APM,66089252,66088061,1,"2021/02/07, 16:52:15",True,"2021/02/07, 16:52:15",3264.0,1783306.0,1,I want to know if there is a way to create a transaction (or span) using a traceparent that is being sent from another system not using a HTTP protocol.,0.0,0.129,0.871,0.34
Elastic APM,66089252,66088061,1,"2021/02/07, 16:52:15",True,"2021/02/07, 16:52:15",3264.0,1783306.0,1,"Yes, this is possible and there is an API for it.",0.0,0.213,0.787,0.4019
Elastic APM,66089252,66088061,1,"2021/02/07, 16:52:15",True,"2021/02/07, 16:52:15",3264.0,1783306.0,1,This part of the documentation  explains it.,0.0,0.0,1.0,0.0
Elastic APM,66089252,66088061,1,"2021/02/07, 16:52:15",True,"2021/02/07, 16:52:15",3264.0,1783306.0,1,So you'll need to do this when you start your transaction - I imagine in your scenario this will be when you read a message from RabbitMQ.,0.0,0.0,1.0,0.0
Elastic APM,66089252,66088061,1,"2021/02/07, 16:52:15",True,"2021/02/07, 16:52:15",3264.0,1783306.0,1,"When you  start the transaction  there is an optional parameter called  distributedTracingData  - if you pass it, then the transaction will reuse the traceid which you passed through RabbitMQ, and this way the new transaction will be part of the whole trace.",0.0,0.0,1.0,0.0
Elastic APM,66089252,66088061,1,"2021/02/07, 16:52:15",True,"2021/02/07, 16:52:15",3264.0,1783306.0,1,"If you don't pass this parameter, a new traceid will be generated and a new trace will be started.",0.0,0.0,1.0,0.0
Elastic APM,66089252,66088061,1,"2021/02/07, 16:52:15",True,"2021/02/07, 16:52:15",3264.0,1783306.0,1,Another comment that may help: you pass the trace id into the method where you start the transaction and each span will inherit this trace id within a transaction - so you control this on the transaction level and accordingly you don't pass it into a span.,0.0,0.059,0.941,0.4019
Elastic APM,66089252,66088061,1,"2021/02/07, 16:52:15",True,"2021/02/07, 16:52:15",3264.0,1783306.0,1,Here is a small code snippet on how this would look:,0.0,0.0,1.0,0.0
Elastic APM,66094811,66088061,0,"2021/02/08, 03:19:53",False,"2021/02/08, 03:19:53",417.0,2803435.0,1,"@gregkalapos, again thank you for the information.",0.0,0.294,0.706,0.3612
Elastic APM,66094811,66088061,0,"2021/02/08, 03:19:53",False,"2021/02/08, 03:19:53",417.0,2803435.0,1,I checked how to acquire the neccessary trace information as in  node.js agent documentation  and when I debugged noticed that it was the trace id.,0.0,0.0,1.0,0.0
Elastic APM,66094811,66088061,0,"2021/02/08, 03:19:53",False,"2021/02/08, 03:19:53",417.0,2803435.0,1,Next in the C# consumer end I placed a code snippet as mentioned in the  .Net agent  and gave it a run.,0.0,0.0,1.0,0.0
Elastic APM,66094811,66088061,0,"2021/02/08, 03:19:53",False,"2021/02/08, 03:19:53",417.0,2803435.0,1,Kibana displayed the transactions from two different services in a single trace as I hoped it would.,0.0,0.157,0.843,0.3818
Elastic APM,66221229,66219545,0,"2021/02/16, 10:51:24",False,"2021/02/16, 10:51:24",417.0,2803435.0,0,Initially asked this questions because Visual Studio did not show the source as expected in the editor.,0.0,0.0,1.0,0.0
Elastic APM,66221229,66219545,0,"2021/02/16, 10:51:24",False,"2021/02/16, 10:51:24",417.0,2803435.0,0,So what I did was clicked Build after right clicking on the .sln (solution) then noticed that necessary version of .Net SDK was not there and version of MSBuild related to it.,0.0,0.0,1.0,0.0
Elastic APM,66221229,66219545,0,"2021/02/16, 10:51:24",False,"2021/02/16, 10:51:24",417.0,2803435.0,0,Once I updated Visual Studio the sources were visible.,0.0,0.0,1.0,0.0
Elastic APM,66221229,66219545,0,"2021/02/16, 10:51:24",False,"2021/02/16, 10:51:24",417.0,2803435.0,0,Hope this would help if someone faced a similar situation.,0.0,0.444,0.556,0.6808
Elastic APM,66324541,66316731,1,"2021/02/23, 00:45:53",False,"2021/02/23, 00:45:53",633.0,13659075.0,1,Using labels should be the best way to add custom details to the transaction/span but you can also use the  addCustomContext()  method:,0.0,0.11,0.89,0.3818
Elastic APM,66324541,66316731,1,"2021/02/23, 00:45:53",False,"2021/02/23, 00:45:53",633.0,13659075.0,1,https://www.elastic.co/guide/en/apm/agent/java/current/public-api.html#api-transaction-add-custom-context,0.0,0.0,1.0,0.0
Elastic APM,66782709,66781290,0,"2021/03/24, 16:01:20",False,"2021/03/24, 16:01:20",11.0,13628343.0,1,That error indicates the agent can't connect to apm-server.,0.252,0.0,0.748,-0.4019
Elastic APM,66782709,66781290,0,"2021/03/24, 16:01:20",False,"2021/03/24, 16:01:20",11.0,13628343.0,1,SERVER_URL  should be  ELASTIC_APM_SERVER_URL  in the apm-agent-container env.,0.0,0.0,1.0,0.0
Elastic APM,66798562,66781290,1,"2021/03/25, 13:27:56",False,"2021/03/25, 13:27:56",1.0,15469419.0,0,"Thanks for the reply, I'm able to connect apm-server with the agent, but in kibana dashboard, I'm getting &quot; No data has been received from agents yet&quot; .",0.094,0.066,0.84,-0.2144
Elastic APM,66798562,66781290,1,"2021/03/25, 13:27:56",False,"2021/03/25, 13:27:56",1.0,15469419.0,0,My application is running fine,0.0,0.31,0.69,0.2023
Elastic APM,66869872,66868273,2,"2021/03/30, 14:09:32",True,"2021/03/30, 15:56:11",163369.0,4604579.0,1,Good job so far.,0.0,0.492,0.508,0.4404
Elastic APM,66869872,66868273,2,"2021/03/30, 14:09:32",True,"2021/03/30, 15:56:11",163369.0,4604579.0,1,"Your pipeline is almost good, however, the grok pattern needs some fixing and you have some orphan curly braces.",0.0,0.127,0.873,0.3832
Elastic APM,66869872,66868273,2,"2021/03/30, 14:09:32",True,"2021/03/30, 15:56:11",163369.0,4604579.0,1,Here is a working example:,0.0,0.0,1.0,0.0
Elastic APM,66869872,66868273,2,"2021/03/30, 14:09:32",True,"2021/03/30, 15:56:11",163369.0,4604579.0,1,Response:,0.0,0.0,1.0,0.0
Elastic APM,66869872,66868273,2,"2021/03/30, 14:09:32",True,"2021/03/30, 15:56:11",163369.0,4604579.0,1,Just note that the exact date is missing so the @timestamp field resolve to January 1st this year.,0.106,0.125,0.769,0.1027
Elastic APM,67065273,67057172,1,"2021/04/12, 23:27:40",True,"2021/04/13, 00:52:22",633.0,13659075.0,0,Try to specify the  config_file  using the following notation:,0.0,0.0,1.0,0.0
Elastic APM,67065273,67057172,1,"2021/04/12, 23:27:40",True,"2021/04/13, 00:52:22",633.0,13659075.0,0,-Delastic.apm.config_file=elasticapm.properties,0.0,0.0,1.0,0.0
Elastic APM,67065273,67057172,1,"2021/04/12, 23:27:40",True,"2021/04/13, 00:52:22",633.0,13659075.0,0,The attacher can create the log file depending on the settings configured during startup.,0.0,0.139,0.861,0.2732
Elastic APM,67065273,67057172,1,"2021/04/12, 23:27:40",True,"2021/04/13, 00:52:22",633.0,13659075.0,0,See the [1] current code for a better understanding.,0.0,0.293,0.707,0.4404
Elastic APM,67065273,67057172,1,"2021/04/12, 23:27:40",True,"2021/04/13, 00:52:22",633.0,13659075.0,0,[1]  https://github.com/elastic/apm-agent-java/blob/0465d479430172c3e745afd2ef5b62a3da6b60aa/apm-agent-attach-cli/src/main/java/co/elastic/apm/attach/AgentAttacher.java#L79,0.0,0.0,1.0,0.0
Elastic APM,61196475,60947830,0,"2020/04/13, 23:48:14",False,"2020/04/13, 23:48:14",64.0,12032134.0,0,"Do you mean that you need new ""Transaction type""?",0.0,0.0,1.0,0.0
Elastic APM,61196475,60947830,0,"2020/04/13, 23:48:14",False,"2020/04/13, 23:48:14",64.0,12032134.0,0,"If yes, so you should set  type  annotation parameter.",0.0,0.252,0.748,0.4019
Elastic APM,61196475,60947830,0,"2020/04/13, 23:48:14",False,"2020/04/13, 23:48:14",64.0,12032134.0,0,But @CaptureTranscation annotation work  in case :,0.0,0.0,1.0,0.0
Elastic APM,64793314,55457083,0,"2020/11/11, 21:58:43",True,"2020/11/11, 21:58:43",2674.0,3019008.0,0,"I worked with the Elastic APM team, who had just rolled out this package:  https://www.npmjs.com/package/elastic-apm-node",0.0,0.0,1.0,0.0
Elastic APM,64793314,55457083,0,"2020/11/11, 21:58:43",True,"2020/11/11, 21:58:43",2674.0,3019008.0,0,"The directions are pretty self-explanatory, works like a charm.",0.0,0.627,0.373,0.8126
Elastic APM,52714271,52696696,1,"2018/10/09, 09:00:41",False,"2018/10/09, 09:00:41",48155.0,2989261.0,0,Hard to tell without debugging but since some connections are getting dropped when you add more load + concurrency it's likely that you need more replicas on your  Kubernetes deployments  and possibly adjusts the  Resources  on your container pod specs.,0.031,0.0,0.969,-0.0516
Elastic APM,52714271,52696696,1,"2018/10/09, 09:00:41",False,"2018/10/09, 09:00:41",48155.0,2989261.0,0,If this turns out to be the case you can also configure an  HPA  (Horizontal Pod Autoscaler) to handle your load.,0.0,0.0,1.0,0.0
Elastic APM,56835665,56832275,0,"2019/07/01, 15:24:24",False,"2019/07/01, 15:24:24",9394.0,573153.0,0,Are you not building a custom Dockerfile and you could just add it there (using wget or curl probably)?,0.0,0.0,1.0,0.0
Elastic APM,56835665,56832275,0,"2019/07/01, 15:24:24",False,"2019/07/01, 15:24:24",9394.0,573153.0,0,"If you really want a build dependency,  https://search.maven.org/artifact/co.elastic.apm/elastic-apm-agent/1.7.0/jar  should be what you want.",0.0,0.242,0.758,0.2928
Elastic APM,56835665,56832275,0,"2019/07/01, 15:24:24",False,"2019/07/01, 15:24:24",9394.0,573153.0,0,"PS: IMO it's a feature that this is only a runtime dependency and you can just add, remove, change it independently of your application; unless you want to do some custom instrumentation.",0.0,0.043,0.957,0.0772
Elastic APM,65706105,65703890,6,"2021/01/13, 18:38:19",False,"2021/01/13, 18:38:19",633.0,13659075.0,0,If the objective is to attach labels to a transaction over multiple spans then using the  public APIs from the Elastic APM for Java  is a better choice instead of instrumenting the JVM with ByteBuddy.,0.0,0.083,0.917,0.4404
Elastic APM,65706105,65703890,6,"2021/01/13, 18:38:19",False,"2021/01/13, 18:38:19",633.0,13659075.0,0,You will have much more freedom to do what you want to do without relying on a hacking.,0.0,0.279,0.721,0.6997
Elastic APM,65706105,65703890,6,"2021/01/13, 18:38:19",False,"2021/01/13, 18:38:19",633.0,13659075.0,0,"FYI, the Elastic APM agent for Java already instrument the JVM with additional bytecode so what you are doing may get even more confusing because of this.",0.074,0.085,0.841,0.0875
Elastic APM,65706105,65703890,6,"2021/01/13, 18:38:19",False,"2021/01/13, 18:38:19",633.0,13659075.0,0,"Alternatively, you can also use the  OpenTracing Bridge  to set labels in a transaction.",0.0,0.0,1.0,0.0
Elastic APM,65712813,65703890,0,"2021/01/14, 05:02:19",False,"2021/01/14, 05:02:19",49309.0,1082681.0,0,Disclaimer: This answer is a stub for now.,0.0,0.0,1.0,0.0
Elastic APM,65712813,65703890,0,"2021/01/14, 05:02:19",False,"2021/01/14, 05:02:19",49309.0,1082681.0,0,You should first try to explore a more canonical way of doing things like Ricardo suggested.,0.0,0.152,0.848,0.3612
Elastic APM,65712813,65703890,0,"2021/01/14, 05:02:19",False,"2021/01/14, 05:02:19",49309.0,1082681.0,0,"If for some reason that does not work, then we could explore ways to instrument your agent class - not so much because I think it is a good idea but because it is technically interesting.",0.0,0.151,0.849,0.6705
Elastic APM,65712813,65703890,0,"2021/01/14, 05:02:19",False,"2021/01/14, 05:02:19",49309.0,1082681.0,0,"Basically, we would have to find out if maybe the class you want to instrument was already loaded before your ByteBuddy agent gets active.",0.0,0.154,0.846,0.4588
Elastic APM,65712813,65703890,0,"2021/01/14, 05:02:19",False,"2021/01/14, 05:02:19",49309.0,1082681.0,0,Then you would have to use class retransformation rather than redefinition.,0.0,0.0,1.0,0.0
Elastic APM,65712813,65703890,0,"2021/01/14, 05:02:19",False,"2021/01/14, 05:02:19",49309.0,1082681.0,0,You would have to make sure the advice you apply can do its job without the need to change the class structure with regard to method signatures and fields.,0.0,0.076,0.924,0.3182
Elastic APM,65712813,65703890,0,"2021/01/14, 05:02:19",False,"2021/01/14, 05:02:19",49309.0,1082681.0,0,"You would also need to make sure that the advice and ByteBuddy are visible to the other agent's classloader, e.g.",0.0,0.108,0.892,0.3182
Elastic APM,65712813,65703890,0,"2021/01/14, 05:02:19",False,"2021/01/14, 05:02:19",49309.0,1082681.0,0,by putting both on the boot class path.,0.0,0.0,1.0,0.0
Elastic APM,65712813,65703890,0,"2021/01/14, 05:02:19",False,"2021/01/14, 05:02:19",49309.0,1082681.0,0,But let's not get ahead of ourselves.,0.0,0.0,1.0,0.0
Elastic APM,65712813,65703890,0,"2021/01/14, 05:02:19",False,"2021/01/14, 05:02:19",49309.0,1082681.0,0,"Explore Ricardo's ideas first, please.",0.0,0.365,0.635,0.3182
Elastic APM,65498418,65484826,1,"2020/12/29, 22:34:40",True,"2020/12/29, 22:34:40",156911.0,4668.0,1,Based on what I've seen it looks like there isn't a &quot;right&quot; way to do this with the stock  nuxt  command line application.,0.0,0.106,0.894,0.3612
Elastic APM,65498418,65484826,1,"2020/12/29, 22:34:40",True,"2020/12/29, 22:34:40",156911.0,4668.0,1,"The problem seems to be that while  nuxt.config.js  is the first time a user has a chance to add some javascript, that the  nuxt  command line application bootstraps the Node's HTTP frameworks before this config file is  required .",0.07,0.052,0.879,-0.1779
Elastic APM,65498418,65484826,1,"2020/12/29, 22:34:40",True,"2020/12/29, 22:34:40",156911.0,4668.0,1,This means the elastic agent (or any APM agent) doesn't have a chance to hook into the modules.,0.098,0.0,0.902,-0.1877
Elastic APM,65498418,65484826,1,"2020/12/29, 22:34:40",True,"2020/12/29, 22:34:40",156911.0,4668.0,1,The  current recommendations  from the Nuxt team appears to be,0.0,0.0,1.0,0.0
Elastic APM,65498418,65484826,1,"2020/12/29, 22:34:40",True,"2020/12/29, 22:34:40",156911.0,4668.0,1,Invoke  nuxt  manually via  -r,0.0,0.0,1.0,0.0
Elastic APM,65498418,65484826,1,"2020/12/29, 22:34:40",True,"2020/12/29, 22:34:40",156911.0,4668.0,1,Skip  nuxt  and  use NuxtJS programmatically  as a middleware in your framework of choice,0.0,0.0,1.0,0.0
Elastic APM,65978053,65484826,0,"2021/01/31, 12:30:39",False,"2021/01/31, 12:36:15",31.0,5761761.0,1,Based on Alan Storm answer (from Nuxt team) I made it work but with a little modification:,0.0,0.0,1.0,0.0
Elastic APM,65384228,65380989,4,"2020/12/20, 21:58:43",True,"2020/12/20, 22:08:14",58.0,14860242.0,1,"actually there are many reasons why your app not starting depending on how you setup and configured your ELK stack , but for me I did the following and it's working fine :",0.0,0.071,0.929,0.296
Elastic APM,65384228,65380989,4,"2020/12/20, 21:58:43",True,"2020/12/20, 22:08:14",58.0,14860242.0,1,create image from this Dockerfile:,0.0,0.344,0.656,0.2732
Elastic APM,65384228,65380989,4,"2020/12/20, 21:58:43",True,"2020/12/20, 22:08:14",58.0,14860242.0,1,run the created image :,0.0,0.4,0.6,0.25
Elastic APM,59233925,59228679,1,"2019/12/08, 11:07:43",True,"2019/12/08, 11:07:43",37971.0,1237575.0,0,The exception comes from the constructur of the  Kernel32  class which is a class of the Maven coordinate  net.java.dev.jna:jna-platform  which itself depends on  net.java.dev.jna:jna .,0.0,0.0,1.0,0.0
Elastic APM,59233925,59228679,1,"2019/12/08, 11:07:43",True,"2019/12/08, 11:07:43",37971.0,1237575.0,0,It seems to me like you have to incompatible versions of those dependencies on the class path.,0.0,0.135,0.865,0.3612
Elastic APM,59233925,59228679,1,"2019/12/08, 11:07:43",True,"2019/12/08, 11:07:43",37971.0,1237575.0,0,I assume that you use version 4 of JNA core and version 5 of JNA platform.,0.0,0.0,1.0,0.0
Elastic APM,59233925,59228679,1,"2019/12/08, 11:07:43",True,"2019/12/08, 11:07:43",37971.0,1237575.0,0,Upgrade the first or downgrade the latter and the error should disappear.,0.315,0.0,0.685,-0.5574
Elastic APM,58917376,58916834,9,"2019/11/18, 16:59:13",True,"2020/03/24, 22:19:43",18074.0,4728685.0,1,"Well, as an option, you can use something like that",0.0,0.365,0.635,0.5574
Elastic APM,56067795,56065263,1,"2019/05/10, 00:25:17",False,"2019/05/10, 00:25:17",2737.0,7330758.0,0,It seems that you are using the oss distribution of elasticsearch but the defaut version of apm.,0.0,0.0,1.0,0.0
Elastic APM,56067795,56065263,1,"2019/05/10, 00:25:17",False,"2019/05/10, 00:25:17",2737.0,7330758.0,0,upgrade the elasticsearch cluster to the default disto or use this oss apm docker image: docker.elastic.co/apm/apm-server-oss:7.0.1,0.0,0.0,1.0,0.0
Elastic APM,57365645,55962977,3,"2019/08/05, 23:23:09",True,"2019/08/05, 23:23:09",5287.0,780798.0,2,OpenTracing   is a set of standard APIs that consistently model and describe the behavior of distributed systems ),0.0,0.0,1.0,0.0
Elastic APM,57365645,55962977,3,"2019/08/05, 23:23:09",True,"2019/08/05, 23:23:09",5287.0,780798.0,2,"OpenTracing did not describe how to collect, report, store or represent the data of interrelated traces and spans.",0.0,0.0,1.0,0.0
Elastic APM,57365645,55962977,3,"2019/08/05, 23:23:09",True,"2019/08/05, 23:23:09",5287.0,780798.0,2,It is implementation details (such as  jaeger  or  wavefront ).,0.0,0.0,1.0,0.0
Elastic APM,57365645,55962977,3,"2019/08/05, 23:23:09",True,"2019/08/05, 23:23:09",5287.0,780798.0,2,jaeger-client-csharp is very jaeger-specific.,0.0,0.0,1.0,0.0
Elastic APM,57365645,55962977,3,"2019/08/05, 23:23:09",True,"2019/08/05, 23:23:09",5287.0,780798.0,2,"But there is one exception, called  zipkin  which in turns is not fully OpenTracing compliant, even it has similar terms.",0.0,0.0,1.0,0.0
Elastic APM,57365645,55962977,3,"2019/08/05, 23:23:09",True,"2019/08/05, 23:23:09",5287.0,780798.0,2,"If you are OK with  opentracing-contrib/csharp-netcore  (hope you are using this library) then if you want to achieve ""no code change"" (in target microservice) in order to configure tracing subsystem, you should use some plug-in model.",0.056,0.107,0.837,0.2577
Elastic APM,57365645,55962977,3,"2019/08/05, 23:23:09",True,"2019/08/05, 23:23:09",5287.0,780798.0,2,"Good news that aspnetcore has concept of  hosted startup assemblies , which allow you to configure tracing system.",0.0,0.242,0.758,0.5859
Elastic APM,57365645,55962977,3,"2019/08/05, 23:23:09",True,"2019/08/05, 23:23:09",5287.0,780798.0,2,"So, you can have some library called  JaegerStartup  where you will implement IHostedStartup like follows:",0.0,0.152,0.848,0.3612
Elastic APM,57365645,55962977,3,"2019/08/05, 23:23:09",True,"2019/08/05, 23:23:09",5287.0,780798.0,2,"When you decide to switch the tracing system - you need to create another library, which can be loaded automatically, and target microservice code will not be touched.",0.0,0.075,0.925,0.2732
Elastic APM,67181477,67173030,2,"2021/04/20, 18:01:19",False,"2021/04/20, 18:01:19",633.0,13659075.0,0,The best way to troubleshoot what is going on is to check if the events from Heartbeat are being collected.,0.0,0.25,0.75,0.7184
Elastic APM,67181477,67173030,2,"2021/04/20, 18:01:19",False,"2021/04/20, 18:01:19",633.0,13659075.0,0,"The Uptime application only displays events from Heartbeat, and therefore — this is the Beat that you need to check.",0.0,0.0,1.0,0.0
Elastic APM,67181477,67173030,2,"2021/04/20, 18:01:19",False,"2021/04/20, 18:01:19",633.0,13659075.0,0,"First, check the connectivity of Heartbeat and the configured output:",0.0,0.0,1.0,0.0
Elastic APM,67181477,67173030,2,"2021/04/20, 18:01:19",False,"2021/04/20, 18:01:19",633.0,13659075.0,0,"Secondly, check if the events are being generated.",0.0,0.0,1.0,0.0
Elastic APM,67181477,67173030,2,"2021/04/20, 18:01:19",False,"2021/04/20, 18:01:19",633.0,13659075.0,0,You can check this by commenting out your existing output (Likely Elasticsearc/Elastic Cloud) and enabling either the  Console  output or the  File  output.,0.0,0.0,1.0,0.0
Elastic APM,67181477,67173030,2,"2021/04/20, 18:01:19",False,"2021/04/20, 18:01:19",633.0,13659075.0,0,Then start your Metricbeat and check if events are being generated.,0.0,0.0,1.0,0.0
Elastic APM,67181477,67173030,2,"2021/04/20, 18:01:19",False,"2021/04/20, 18:01:19",633.0,13659075.0,0,"If they are, then it might be something with the backend side of things; maybe Elasticsearch is rejecting the documents sent and refusing to index them.",0.192,0.0,0.808,-0.6908
Elastic APM,67181477,67173030,2,"2021/04/20, 18:01:19",False,"2021/04/20, 18:01:19",633.0,13659075.0,0,"Apropos, Elastic is implementing a native  Jenkins  plugin that allows you to observe your CI pipeline using OpenTelemetry compatible backends such as  Elastic APM .",0.0,0.0,1.0,0.0
Elastic APM,67181477,67173030,2,"2021/04/20, 18:01:19",False,"2021/04/20, 18:01:19",633.0,13659075.0,0,You can learn more about this plugin  here .,0.0,0.0,1.0,0.0
Elastic APM,65005103,64923213,0,"2020/11/25, 14:52:51",True,"2020/11/25, 14:52:51",176.0,14018385.0,0,"The only way to get configuration is to check apm-server.yml in your instance, but if you want to check your agent configuration you can use Agent Configuration API, for more information check  https://www.elastic.co/guide/en/apm/server/current/agent-configuration-api.html .",0.0,0.043,0.957,0.1154
Elastic APM,64854473,64854472,0,"2020/11/16, 10:09:07",False,"2020/11/16, 10:09:07",31.0,1973221.0,0,I can by pass it when I comment entityframework-&gt;Interceptor node in web.config file,0.0,0.0,1.0,0.0
Elastic APM,64854473,64854472,0,"2020/11/16, 10:09:07",False,"2020/11/16, 10:09:07",31.0,1973221.0,0,And I can continue after uncomment it,0.0,0.0,1.0,0.0
Elastic APM,63598396,63576369,1,"2020/08/26, 16:09:20",False,"2020/08/26, 16:09:20",15.0,12391397.0,1,About the possibility of using some kind of Proxy between your gke cluster and elastic apm.,0.0,0.0,1.0,0.0
Elastic APM,63598396,63576369,1,"2020/08/26, 16:09:20",False,"2020/08/26, 16:09:20",15.0,12391397.0,1,"You can check the following link [1], to see if it can fit your necessities.",0.0,0.152,0.848,0.3612
Elastic APM,63598396,63576369,1,"2020/08/26, 16:09:20",False,"2020/08/26, 16:09:20",15.0,12391397.0,1,[1]  https://cloud.google.com/vpc/docs/special-configurations#proxyvm,0.0,0.0,1.0,0.0
Elastic APM,63480663,63480314,1,"2020/08/19, 08:40:43",True,"2020/08/19, 08:40:43",6576.0,547452.0,1,Since you didn't mention it above: did you instrument a Go application?,0.0,0.0,1.0,0.0
Elastic APM,63480663,63480314,1,"2020/08/19, 08:40:43",True,"2020/08/19, 08:40:43",6576.0,547452.0,1,The Elastic APM Go &quot;Agent&quot; is a package which you use to instrument your application source code.,0.0,0.0,1.0,0.0
Elastic APM,63480663,63480314,1,"2020/08/19, 08:40:43",True,"2020/08/19, 08:40:43",6576.0,547452.0,1,"It is not an independent process, but runs within your application.",0.0,0.0,1.0,0.0
Elastic APM,63480663,63480314,1,"2020/08/19, 08:40:43",True,"2020/08/19, 08:40:43",6576.0,547452.0,1,"So, first (if you haven't already) instrument your application.",0.0,0.0,1.0,0.0
Elastic APM,63480663,63480314,1,"2020/08/19, 08:40:43",True,"2020/08/19, 08:40:43",6576.0,547452.0,1,See  https://www.elastic.co/guide/en/apm/agent/go/current/getting-started.html#instrumenting-source,0.0,0.0,1.0,0.0
Elastic APM,63480663,63480314,1,"2020/08/19, 08:40:43",True,"2020/08/19, 08:40:43",6576.0,547452.0,1,"Here's an example web server using  Echo , and the  apmechov4  instrumentation module:",0.0,0.0,1.0,0.0
Elastic APM,63480663,63480314,1,"2020/08/19, 08:40:43",True,"2020/08/19, 08:40:43",6576.0,547452.0,1,"If you run that and send some requests to  http://localhost:8080/hello/world , you should soon see requests in the APM app in Kibana.",0.0,0.0,1.0,0.0
Elastic APM,63480663,63480314,1,"2020/08/19, 08:40:43",True,"2020/08/19, 08:40:43",6576.0,547452.0,1,"If you still don't see anything in Kibana, you can follow  https://www.elastic.co/guide/en/apm/agent/go/current/troubleshooting.html#agent-logging  to enable logging.",0.0,0.0,1.0,0.0
Elastic APM,63480663,63480314,1,"2020/08/19, 08:40:43",True,"2020/08/19, 08:40:43",6576.0,547452.0,1,Here's what you can expect to see if the agent is able to successfully send data to the server:,0.0,0.151,0.849,0.4939
Elastic APM,63480663,63480314,1,"2020/08/19, 08:40:43",True,"2020/08/19, 08:40:43",6576.0,547452.0,1,"If on the other hand the server is inaccessible, you would see something like this:",0.0,0.305,0.695,0.6908
Elastic APM,62871884,62865170,0,"2020/07/13, 11:20:58",True,"2020/07/13, 11:20:58",31.0,2259926.0,0,"Sorry my bad, the server urls weren't correctly passed to docker.",0.348,0.0,0.652,-0.5859
Elastic APM,60327076,60326859,0,"2020/02/20, 21:16:01",False,"2020/02/20, 21:16:01",1428.0,2924546.0,0,"This is by design in Django, and it is intentionally designed in this way.",0.0,0.0,1.0,0.0
Elastic APM,60327076,60326859,0,"2020/02/20, 21:16:01",False,"2020/02/20, 21:16:01",1428.0,2924546.0,0,this is a parametrized way.,0.0,0.0,1.0,0.0
Elastic APM,60327076,60326859,0,"2020/02/20, 21:16:01",False,"2020/02/20, 21:16:01",1428.0,2924546.0,0,suppose someone has a column name with spaces like  test column name  then think what would happen.,0.0,0.143,0.857,0.3612
Elastic APM,60327076,60326859,0,"2020/02/20, 21:16:01",False,"2020/02/20, 21:16:01",1428.0,2924546.0,0,"it will lead to some unwanted errors, so don't change the underlying logic of the framework.",0.235,0.0,0.765,-0.5106
Elastic APM,60327397,60326859,0,"2020/02/20, 21:40:06",True,"2020/02/20, 21:40:06",1.0,7133255.0,0,"Thanks @BjarniRagnarsson, The upper case letters was making this behavior of framework as @Sanjay mentioned.",0.0,0.172,0.828,0.4404
Elastic APM,60327397,60326859,0,"2020/02/20, 21:40:06",True,"2020/02/20, 21:40:06",1.0,7133255.0,0,Solution:,0.0,1.0,0.0,0.3182
Elastic APM,59466931,58615241,0,"2019/12/24, 11:52:49",False,"2019/12/24, 11:52:49",2686.0,1360278.0,0,"For a high-level overview type of information, have a look at  Elastic Stack Monitoring .",0.0,0.0,1.0,0.0
Elastic APM,59466931,58615241,0,"2019/12/24, 11:52:49",False,"2019/12/24, 11:52:49",2686.0,1360278.0,0,"If you want to look at any monitoring in more detail, have a look at the  monitoring APIs themselves .",0.0,0.071,0.929,0.0772
Elastic APM,59466931,58615241,0,"2019/12/24, 11:52:49",False,"2019/12/24, 11:52:49",2686.0,1360278.0,0,"If you want to log this sort of information, you should set thresholds  for your Elasticsearch slow log .",0.0,0.071,0.929,0.0772
Elastic APM,59466931,58615241,0,"2019/12/24, 11:52:49",False,"2019/12/24, 11:52:49",2686.0,1360278.0,0,"If you want to index and then view data from the slow log,  you can always use Filebeat to ingest that slow log data back into Elasticsearch .",0.0,0.048,0.952,0.0772
Elastic APM,54616102,54559465,0,"2019/02/10, 13:55:02",False,"2019/02/10, 13:55:02",8336.0,1773866.0,1,You're calling one method from another.,0.0,0.0,1.0,0.0
Elastic APM,54616102,54559465,0,"2019/02/10, 13:55:02",False,"2019/02/10, 13:55:02",8336.0,1773866.0,1,Spring is creating a proxy around your method.,0.0,0.268,0.732,0.296
Elastic APM,54616102,54559465,0,"2019/02/10, 13:55:02",False,"2019/02/10, 13:55:02",8336.0,1773866.0,1,If you call one method from another from the same class then you're not going through the proxy.,0.0,0.0,1.0,0.0
Elastic APM,54616102,54559465,0,"2019/02/10, 13:55:02",False,"2019/02/10, 13:55:02",8336.0,1773866.0,1,Extract the method annotated with new span to a separate class and it will work fine.,0.0,0.114,0.886,0.2023
Jaeger,51790666,51785812,2,"2018/08/10, 19:32:28",True,"2019/07/15, 20:44:47",373.0,10206966.0,14,"After searching a solution for some time, I found a docker-compose.yml file which had the Jaeger Query,Agent,collector and Elasticsearch configurations.",0.0,0.126,0.874,0.3182
Jaeger,51790666,51785812,2,"2018/08/10, 19:32:28",True,"2019/07/15, 20:44:47",373.0,10206966.0,14,docker-compose.yml,0.0,0.0,1.0,0.0
Jaeger,51790666,51785812,2,"2018/08/10, 19:32:28",True,"2019/07/15, 20:44:47",373.0,10206966.0,14,"The docker-compose.yml file installs the elasticsearch, Jaeger collector,query and agent.",0.0,0.0,1.0,0.0
Jaeger,51790666,51785812,2,"2018/08/10, 19:32:28",True,"2019/07/15, 20:44:47",373.0,10206966.0,14,"Install docker and docker compose first
 https://docs.docker.com/compose/install/#install-compose",0.0,0.0,1.0,0.0
Jaeger,51790666,51785812,2,"2018/08/10, 19:32:28",True,"2019/07/15, 20:44:47",373.0,10206966.0,14,"Then, execute these commands in order",0.0,0.0,1.0,0.0
Jaeger,51790666,51785812,2,"2018/08/10, 19:32:28",True,"2019/07/15, 20:44:47",373.0,10206966.0,14,"start all the docker containers - Jaeger agent,collector,query and elasticsearch.",0.0,0.0,1.0,0.0
Jaeger,51790666,51785812,2,"2018/08/10, 19:32:28",True,"2019/07/15, 20:44:47",373.0,10206966.0,14,sudo docker start container-id,0.0,0.0,1.0,0.0
Jaeger,51790666,51785812,2,"2018/08/10, 19:32:28",True,"2019/07/15, 20:44:47",373.0,10206966.0,14,access -   http://localhost:16686/,0.0,0.0,1.0,0.0
Jaeger,61147810,51785812,0,"2020/04/10, 23:00:13",False,"2020/04/10, 23:00:13",71.0,9731647.0,0,"If Jaeger needs to be set up in Kubernetes cluster as a helm chart, one can use this:  https://github.com/jaegertracing/helm-charts/tree/master/charts/jaeger 
It can delploy either Elasticsearch or Cassandara as a storage backend.",0.0,0.0,1.0,0.0
Jaeger,61147810,51785812,0,"2020/04/10, 23:00:13",False,"2020/04/10, 23:00:13",71.0,9731647.0,0,Which is just a matter of right value being passed in to the chart:,0.0,0.241,0.759,0.3612
Jaeger,61147810,51785812,0,"2020/04/10, 23:00:13",False,"2020/04/10, 23:00:13",71.0,9731647.0,0,"This section shows the helm command as an example:
 https://github.com/jaegertracing/helm-charts/tree/master/charts/jaeger#installing-the-chart-using-a-new-elasticsearch-cluster",0.0,0.0,1.0,0.0
Jaeger,62418210,51785812,0,"2020/06/17, 00:57:19",False,"2020/06/17, 00:57:19",488.0,5789008.0,1,If you would like to deploy the Jaeger with Elasticsearch and Kibana to quickly validate and check the stack e.g.,0.0,0.217,0.783,0.6124
Jaeger,62418210,51785812,0,"2020/06/17, 00:57:19",False,"2020/06/17, 00:57:19",488.0,5789008.0,1,"in kind or Minikube, the following snippet may help you.",0.0,0.433,0.567,0.7269
Jaeger,63530756,51785812,0,"2020/08/22, 01:23:06",False,"2020/09/02, 13:12:23",30911.0,2000548.0,0,"For people who are using OpenTelemetry, Jaeger, and Elasticsearch, here is the way.",0.0,0.0,1.0,0.0
Jaeger,63530756,51785812,0,"2020/08/22, 01:23:06",False,"2020/09/02, 13:12:23",30911.0,2000548.0,0,Note the image being used are  jaegertracing/jaeger-opentelemetry-collector  and  jaegertracing/jaeger-opentelemetry-agent .,0.0,0.0,1.0,0.0
Jaeger,63530756,51785812,0,"2020/08/22, 01:23:06",False,"2020/09/02, 13:12:23",30911.0,2000548.0,0,Then just need,0.0,0.0,1.0,0.0
Jaeger,63530756,51785812,0,"2020/08/22, 01:23:06",False,"2020/09/02, 13:12:23",30911.0,2000548.0,0,Reference:  https://github.com/jaegertracing/jaeger/blob/master/crossdock/jaeger-opentelemetry-docker-compose.yml,0.0,0.0,1.0,0.0
Jaeger,64755733,51785812,0,"2020/11/09, 18:47:27",False,"2020/11/09, 18:47:27",328.0,1221718.0,1,"As I mentioned in my comment on the OP's first answer above, I was getting an error when running the docker-compose exactly as given:",0.114,0.0,0.886,-0.4019
Jaeger,64755733,51785812,0,"2020/11/09, 18:47:27",False,"2020/11/09, 18:47:27",328.0,1221718.0,1,Error: unknown flag: --collector.host-port,0.474,0.0,0.526,-0.4019
Jaeger,64755733,51785812,0,"2020/11/09, 18:47:27",False,"2020/11/09, 18:47:27",328.0,1221718.0,1,I think this CLI flag has been deprecated by the Jaeger folks since that answer was written.,0.0,0.0,1.0,0.0
Jaeger,64755733,51785812,0,"2020/11/09, 18:47:27",False,"2020/11/09, 18:47:27",328.0,1221718.0,1,So I poked around in the jaeger-agent documentation a bit:,0.0,0.0,1.0,0.0
Jaeger,64755733,51785812,0,"2020/11/09, 18:47:27",False,"2020/11/09, 18:47:27",328.0,1221718.0,1,And I got this to work with a couple of small modifications:,0.0,0.0,1.0,0.0
Jaeger,64755733,51785812,0,"2020/11/09, 18:47:27",False,"2020/11/09, 18:47:27",328.0,1221718.0,1,The updated docker-compose.yaml:,0.0,0.0,1.0,0.0
Jaeger,50985064,50855480,0,"2018/06/22, 12:38:46",True,"2018/06/22, 12:38:46",231.0,4158442.0,7,https://github.com/opentracing-contrib/java-spring-cloud  project automatically sends standard logging to the active span.,0.0,0.231,0.769,0.4019
Jaeger,50985064,50855480,0,"2018/06/22, 12:38:46",True,"2018/06/22, 12:38:46",231.0,4158442.0,7,Just add the following dependency to your pom.xml,0.0,0.0,1.0,0.0
Jaeger,50985064,50855480,0,"2018/06/22, 12:38:46",True,"2018/06/22, 12:38:46",231.0,4158442.0,7,Or use this  https://github.com/opentracing-contrib/java-spring-cloud/tree/master/instrument-starters/opentracing-spring-cloud-core  starter if you want only logging integration.,0.0,0.115,0.885,0.0772
Jaeger,56222158,50855480,2,"2019/05/20, 16:39:29",False,"2019/05/20, 16:39:29",61.0,9483992.0,0,Then I use  opentracing-spring-jaeger-cloud-starter,0.0,0.0,1.0,0.0
Jaeger,56222158,50855480,2,"2019/05/20, 16:39:29",False,"2019/05/20, 16:39:29",61.0,9483992.0,0,"I got just one line in console with current trace and span
i.j.internal.reporters.LoggingReporter   : Span reported: f1a264bbe2c7eae9:f1a264bbe2c7eae9:0:1 - my_method",0.0,0.0,1.0,0.0
Jaeger,56222158,50855480,2,"2019/05/20, 16:39:29",False,"2019/05/20, 16:39:29",61.0,9483992.0,0,Then I use  spring-cloud-starter-sleuth,0.0,0.0,1.0,0.0
Jaeger,56222158,50855480,2,"2019/05/20, 16:39:29",False,"2019/05/20, 16:39:29",61.0,9483992.0,0,"I got the Trace and Spans like [my-service,90e1114e35c897d6,90e1114e35c897d6,false] in each line and it's helpfull for filebeat in ELK",0.0,0.135,0.865,0.3612
Jaeger,56222158,50855480,2,"2019/05/20, 16:39:29",False,"2019/05/20, 16:39:29",61.0,9483992.0,0,How could I get the same log in console using opentracing-spring-jaeger-cloud-starter ?,0.0,0.0,1.0,0.0
Jaeger,56222158,50855480,2,"2019/05/20, 16:39:29",False,"2019/05/20, 16:39:29",61.0,9483992.0,0,my opentracing config,0.0,0.0,1.0,0.0
Jaeger,58798104,50855480,0,"2019/11/11, 10:48:18",False,"2019/11/11, 10:48:18",11.0,10465104.0,1,Here is what I did to make jdbc related logs from Logback (Slf4j) write into Jaeger server:,0.0,0.0,1.0,0.0
Jaeger,58798104,50855480,0,"2019/11/11, 10:48:18",False,"2019/11/11, 10:48:18",11.0,10465104.0,1,Beginning with Logback config (logback-spring.xml):,0.0,0.0,1.0,0.0
Jaeger,58798104,50855480,0,"2019/11/11, 10:48:18",False,"2019/11/11, 10:48:18",11.0,10465104.0,1,Here is my appender:,0.0,0.0,1.0,0.0
Jaeger,62995670,57268147,0,"2020/07/20, 15:44:52",False,"2020/07/20, 15:44:52",537.0,2841947.0,0,I was facing similar issue.,0.0,0.0,1.0,0.0
Jaeger,62995670,57268147,0,"2020/07/20, 15:44:52",False,"2020/07/20, 15:44:52",537.0,2841947.0,0,ConnectionInfo was getting traced but not the SQL statements.,0.0,0.0,1.0,0.0
Jaeger,62995670,57268147,0,"2020/07/20, 15:44:52",False,"2020/07/20, 15:44:52",537.0,2841947.0,0,"In my case, I had to enable traceWithActiveSpanOnly=true.",0.0,0.0,1.0,0.0
Jaeger,62995670,57268147,0,"2020/07/20, 15:44:52",False,"2020/07/20, 15:44:52",537.0,2841947.0,0,"For e.g, : jdbc:tracing:h2:mem:test?traceWithActiveSpanOnly=true",0.0,0.0,1.0,0.0
Jaeger,62995670,57268147,0,"2020/07/20, 15:44:52",False,"2020/07/20, 15:44:52",537.0,2841947.0,0,After that the statements started getting traced.,0.0,0.0,1.0,0.0
Jaeger,62995670,57268147,0,"2020/07/20, 15:44:52",False,"2020/07/20, 15:44:52",537.0,2841947.0,0,Check the documentation of opentracing java-jdbc module here,0.0,0.0,1.0,0.0
Jaeger,53464346,53459759,2,"2018/11/25, 05:17:34",False,"2018/11/26, 04:04:55",2451.0,1432067.0,4,"Based on my experience and reading online, I found this interesting line in Istio  mixer faq",0.0,0.182,0.818,0.481
Jaeger,53464346,53459759,2,"2018/11/25, 05:17:34",False,"2018/11/26, 04:04:55",2451.0,1432067.0,4,"Mixer trace generation is controlled by command-line flags: trace_zipkin_url, trace_jaeger_url, and trace_log_spans.",0.0,0.0,1.0,0.0
Jaeger,53464346,53459759,2,"2018/11/25, 05:17:34",False,"2018/11/26, 04:04:55",2451.0,1432067.0,4,"If any of those flag values are set, trace data will be written directly to those locations.",0.0,0.144,0.856,0.4019
Jaeger,53464346,53459759,2,"2018/11/25, 05:17:34",False,"2018/11/26, 04:04:55",2451.0,1432067.0,4,"If no tracing options are provided, Mixer will not generate any application-level trace information.",0.145,0.0,0.855,-0.296
Jaeger,53464346,53459759,2,"2018/11/25, 05:17:34",False,"2018/11/26, 04:04:55",2451.0,1432067.0,4,"Also, if you go deep into mixer  helm chart , you will find traces of Zipkin and Jaeger signifying that it’s mixer that is passing trace info to Jaeger.",0.0,0.0,1.0,0.0
Jaeger,53464346,53459759,2,"2018/11/25, 05:17:34",False,"2018/11/26, 04:04:55",2451.0,1432067.0,4,I also got confused which reading this line in one of the articles,0.173,0.0,0.827,-0.3182
Jaeger,53464346,53459759,2,"2018/11/25, 05:17:34",False,"2018/11/26, 04:04:55",2451.0,1432067.0,4,Istio injects a sidecar proxy (Envoy) in the pod in which your application container is running.,0.0,0.0,1.0,0.0
Jaeger,53464346,53459759,2,"2018/11/25, 05:17:34",False,"2018/11/26, 04:04:55",2451.0,1432067.0,4,This sidecar proxy transparently intercepts (iptables magic) all network traffic going in and out of your application.,0.0,0.0,1.0,0.0
Jaeger,53464346,53459759,2,"2018/11/25, 05:17:34",False,"2018/11/26, 04:04:55",2451.0,1432067.0,4,"Because of this interception, the sidecar proxy is in a unique position to automatically trace all network requests (HTTP/1.1, HTTP/2.0 &amp; gRPC).",0.0,0.0,1.0,0.0
Jaeger,53464346,53459759,2,"2018/11/25, 05:17:34",False,"2018/11/26, 04:04:55",2451.0,1432067.0,4,"On Istio mixer documentation, The Envoy sidecar logically calls Mixer before each request to perform precondition checks, and after each request to report telemetry.",0.0,0.0,1.0,0.0
Jaeger,53464346,53459759,2,"2018/11/25, 05:17:34",False,"2018/11/26, 04:04:55",2451.0,1432067.0,4,The sidecar has local caching such that a large percentage of precondition checks can be performed from cache.,0.0,0.0,1.0,0.0
Jaeger,53464346,53459759,2,"2018/11/25, 05:17:34",False,"2018/11/26, 04:04:55",2451.0,1432067.0,4,"Additionally, the sidecar buffers outgoing telemetry such that it only calls Mixer infrequently.",0.0,0.155,0.845,0.296
Jaeger,53464346,53459759,2,"2018/11/25, 05:17:34",False,"2018/11/26, 04:04:55",2451.0,1432067.0,4,Update:  You can enable tracing to understand what happens to a request in Istio and also the role of mixer and envoy.,0.0,0.0,1.0,0.0
Jaeger,53464346,53459759,2,"2018/11/25, 05:17:34",False,"2018/11/26, 04:04:55",2451.0,1432067.0,4,Read more information  here,0.0,0.0,1.0,0.0
Jaeger,50230156,50173643,1,"2018/05/08, 12:19:17",True,"2018/05/08, 12:19:17",193.0,9725840.0,13,"I found the solution to my problem, in case anybody is facing similar issues.",0.169,0.144,0.688,-0.1027
Jaeger,50230156,50173643,1,"2018/05/08, 12:19:17",True,"2018/05/08, 12:19:17",193.0,9725840.0,13,"I was missing the environment variable  JAEGER_SAMPLER_MANAGER_HOST_PORT , which is necessary if the (default) remote controlled sampler is used for tracing.",0.109,0.0,0.891,-0.296
Jaeger,50230156,50173643,1,"2018/05/08, 12:19:17",True,"2018/05/08, 12:19:17",193.0,9725840.0,13,This is the working docker-compose file:,0.0,0.0,1.0,0.0
Jaeger,66317822,50173643,0,"2021/02/22, 16:41:31",False,"2021/02/23, 08:30:03",89.0,668438.0,0,For anyone finding themselves on this page looking at a simialr issue for jaeger opentracing in .net core below is a working docker compose snippet and working code in Startup.cs.,0.0,0.0,1.0,0.0
Jaeger,66317822,50173643,0,"2021/02/22, 16:41:31",False,"2021/02/23, 08:30:03",89.0,668438.0,0,The piece I was missing was getting jaeger to read the environment variables from docker-compose as by default it was trying to send the traces over udp on localhost.,0.075,0.0,0.925,-0.296
Jaeger,66317822,50173643,0,"2021/02/22, 16:41:31",False,"2021/02/23, 08:30:03",89.0,668438.0,0,Add following nuget packages to your api's.,0.0,0.0,1.0,0.0
Jaeger,66317822,50173643,0,"2021/02/22, 16:41:31",False,"2021/02/23, 08:30:03",89.0,668438.0,0,"Then inside your Startup.cs Configure method, you will need to configure jaeger and opentracing as below.",0.0,0.0,1.0,0.0
Jaeger,62348886,62216558,0,"2020/06/12, 19:51:43",False,"2020/06/12, 19:51:43",19192.0,4275342.0,2,The jaeger-agent service should look like,0.0,0.333,0.667,0.3612
Jaeger,62348886,62216558,0,"2020/06/12, 19:51:43",False,"2020/06/12, 19:51:43",19192.0,4275342.0,2,"Don't use IP, use FQDN.",0.0,0.0,1.0,0.0
Jaeger,62348886,62216558,0,"2020/06/12, 19:51:43",False,"2020/06/12, 19:51:43",19192.0,4275342.0,2,"First, try to hardcode value for  jaegerHost",0.0,0.286,0.714,0.34
Jaeger,62348886,62216558,0,"2020/06/12, 19:51:43",False,"2020/06/12, 19:51:43",19192.0,4275342.0,2,"where  jaeger-agent  - service name,  jaeger  - namespace of service",0.0,0.0,1.0,0.0
Jaeger,62348886,62216558,0,"2020/06/12, 19:51:43",False,"2020/06/12, 19:51:43",19192.0,4275342.0,2,"Also, you should create  jaeger-collector  service",0.0,0.296,0.704,0.2732
Jaeger,52894567,52886127,2,"2018/10/19, 17:34:17",True,"2018/10/31, 13:44:01",13313.0,524946.0,2,Are you closing the tracer and the scope?,0.0,0.0,1.0,0.0
Jaeger,52894567,52886127,2,"2018/10/19, 17:34:17",True,"2018/10/31, 13:44:01",13313.0,524946.0,2,"If you are using a version before 0.32.0, you should manually call  tracer.close()  before your process terminates, otherwise the spans in the buffer might not get dispatched.",0.0,0.0,1.0,0.0
Jaeger,52894567,52886127,2,"2018/10/19, 17:34:17",True,"2018/10/31, 13:44:01",13313.0,524946.0,2,"As for the scope, it's common to wrap it in a try-with-resources statement:",0.0,0.0,1.0,0.0
Jaeger,52894567,52886127,2,"2018/10/19, 17:34:17",True,"2018/10/31, 13:44:01",13313.0,524946.0,2,You might also want to check the OpenTracing tutorial at  https://github.com/yurishkuro/opentracing-tutorial  or the Katacoda-based version at  https://www.katacoda.com/courses/opentracing,0.0,0.075,0.925,0.0772
Jaeger,52894567,52886127,2,"2018/10/19, 17:34:17",True,"2018/10/31, 13:44:01",13313.0,524946.0,2,-- EDIT,0.0,0.0,1.0,0.0
Jaeger,52894567,52886127,2,"2018/10/19, 17:34:17",True,"2018/10/31, 13:44:01",13313.0,524946.0,2,and is deployed on a different hostname and port,0.0,0.0,1.0,0.0
Jaeger,52894567,52886127,2,"2018/10/19, 17:34:17",True,"2018/10/31, 13:44:01",13313.0,524946.0,2,Then you do need to tell the tracer where to send the traces.,0.0,0.0,1.0,0.0
Jaeger,52894567,52886127,2,"2018/10/19, 17:34:17",True,"2018/10/31, 13:44:01",13313.0,524946.0,2,"Either export the  JAEGER_ENDPOINT  environment variable, pointing to a collector endpoint, or set  JAEGER_AGENT_HOST / JAEGER_AGENT_PORT , with the location of the agent.",0.0,0.0,1.0,0.0
Jaeger,52894567,52886127,2,"2018/10/19, 17:34:17",True,"2018/10/31, 13:44:01",13313.0,524946.0,2,You can check the available environment variables for your client on the following URL:  https://www.jaegertracing.io/docs/1.7/client-features/,0.0,0.0,1.0,0.0
Jaeger,47756263,47706011,7,"2017/12/11, 17:35:05",True,"2017/12/11, 17:35:05",91.0,629338.0,2,You need to add some more properties to your config options.,0.0,0.0,1.0,0.0
Jaeger,47756263,47706011,7,"2017/12/11, 17:35:05",True,"2017/12/11, 17:35:05",91.0,629338.0,2,For reporter deployed on localhost and local sampler strategy :,0.0,0.0,1.0,0.0
Jaeger,47756263,47706011,7,"2017/12/11, 17:35:05",True,"2017/12/11, 17:35:05",91.0,629338.0,2,Replace  localhost  by server or route name to target another host for Jeager runtime.,0.0,0.0,1.0,0.0
Jaeger,64818358,63407052,0,"2020/11/13, 11:22:51",False,"2020/11/13, 11:22:51",51.0,8394088.0,0,This link ( https://objectpartners.com/2019/04/25/distributed-tracing-with-apache-kafka-and-jaeger/ ) provides the details of how to enable jaeger traces.,0.0,0.0,1.0,0.0
Jaeger,64818358,63407052,0,"2020/11/13, 11:22:51",False,"2020/11/13, 11:22:51",51.0,8394088.0,0,The simplest way to enable jaeger to spring-boot application is add the dependency and the required properties.,0.0,0.0,1.0,0.0
Jaeger,64818358,63407052,0,"2020/11/13, 11:22:51",False,"2020/11/13, 11:22:51",51.0,8394088.0,0,Dependency:,0.0,0.0,1.0,0.0
Jaeger,64818358,63407052,0,"2020/11/13, 11:22:51",False,"2020/11/13, 11:22:51",51.0,8394088.0,0,Example Properties,0.0,0.0,1.0,0.0
Jaeger,65029618,63407052,0,"2020/11/26, 23:49:03",True,"2020/11/26, 23:49:03",36.0,4216591.0,1,Answering to your question about dependencies it is explained here in Dependencies section ( https://github.com/opentracing-contrib/java-spring-jaeger ):,0.167,0.0,0.833,-0.4215
Jaeger,65029618,63407052,0,"2020/11/26, 23:49:03",True,"2020/11/26, 23:49:03",36.0,4216591.0,1,"The opentracing-spring-jaeger-web-starter starter is convenience starter that includes both opentracing-spring-jaeger-starter and opentracing-spring-web-starter This means that by including it, simple web Spring Boot microservices include all the necessary dependencies to instrument Web requests / responses and send traces to Jaeger.",0.0,0.0,1.0,0.0
Jaeger,65029618,63407052,0,"2020/11/26, 23:49:03",True,"2020/11/26, 23:49:03",36.0,4216591.0,1,"The opentracing-spring-jaeger-cloud-starter starter is convenience starter that includes both opentracing-spring-jaeger-starter and opentracing-spring-cloud-starter This means that by including it, all parts of the Spring Cloud stack supported by Opentracing will be instrumented",0.0,0.071,0.929,0.3182
Jaeger,65029618,63407052,0,"2020/11/26, 23:49:03",True,"2020/11/26, 23:49:03",36.0,4216591.0,1,And by the way:,0.0,0.0,1.0,0.0
Jaeger,65029618,63407052,0,"2020/11/26, 23:49:03",True,"2020/11/26, 23:49:03",36.0,4216591.0,1,same as:,0.0,0.0,1.0,0.0
Jaeger,61154134,61154096,3,"2020/04/11, 11:43:10",False,"2020/04/11, 11:43:10",27576.0,1839482.0,2,You can use  Jaeger Operator  to deploy Jaeger on kubernetes.The Jaeger Operator is an implementation of a Kubernetes Operator.,0.0,0.0,1.0,0.0
Jaeger,61154134,61154096,3,"2020/04/11, 11:43:10",False,"2020/04/11, 11:43:10",27576.0,1839482.0,2,Operators are pieces of software that ease the operational complexity of running another piece of software.,0.0,0.143,0.857,0.3612
Jaeger,61154134,61154096,3,"2020/04/11, 11:43:10",False,"2020/04/11, 11:43:10",27576.0,1839482.0,2,"More technically, Operators are a method of packaging, deploying, and managing a Kubernetes application",0.0,0.0,1.0,0.0
Jaeger,61154180,61154096,0,"2020/04/11, 11:47:26",True,"2020/04/11, 22:39:56",382.0,7847042.0,1,Follow this link for steps to deploy JAEGER on kubernetes .,0.0,0.0,1.0,0.0
Jaeger,61154180,61154096,0,"2020/04/11, 11:47:26",True,"2020/04/11, 22:39:56",382.0,7847042.0,1,https://www.upnxtblog.com/index.php/2018/07/16/kubernetes-tutorial-distributed-tracing-with-jaeger/,0.0,0.0,1.0,0.0
Jaeger,61154180,61154096,0,"2020/04/11, 11:47:26",True,"2020/04/11, 22:39:56",382.0,7847042.0,1,make following changes in application.properties,0.0,0.0,1.0,0.0
Jaeger,61160646,61154096,0,"2020/04/11, 20:07:08",False,"2020/04/11, 20:07:08",181.0,12530105.0,1,You can use this link for a better understanding -  https://www.upnxtblog.com/index.php/2018/07/16/kubernetes-tutorial-distributed-tracing-with-jaeger/,0.0,0.266,0.734,0.4404
Jaeger,61152144,61149872,7,"2020/04/11, 07:27:43",False,"2020/04/11, 10:29:27",27576.0,1839482.0,1,You add  opentracing-spring-jaeger-starter   library  into the project which simply contains the code needed to provide a Jaeger implementation of the OpenTracing's  io.opentracing.Tracer  interface.,0.0,0.0,1.0,0.0
Jaeger,61152144,61149872,7,"2020/04/11, 07:27:43",False,"2020/04/11, 10:29:27",27576.0,1839482.0,1,Since you have the jaeger deployed in Kubernetes and exposed it via a loadbalancer service you can use the loadbalancer IP and port to connect to it from outside the Kubernetes cluster.,0.042,0.0,0.958,-0.0772
Jaeger,61154289,61149872,1,"2020/04/11, 11:56:40",False,"2021/04/10, 03:12:05",382.0,7847042.0,1,"So the solution that works for me is -
I have made the following changes in my application.properties file of application",0.0,0.125,0.875,0.3774
Jaeger,62832644,61149872,3,"2020/07/10, 13:45:36",False,"2020/07/10, 13:45:36",9.0,8694436.0,0,You should config:,0.0,0.0,1.0,0.0
Jaeger,59260272,58409078,0,"2019/12/10, 06:32:34",False,"2019/12/10, 06:32:34",471.0,2673284.0,0,"This is why tracing is a useful tool, it often reveals issues like this that you wouldn't suspect otherwise.",0.0,0.327,0.673,0.7421
Jaeger,59260272,58409078,0,"2019/12/10, 06:32:34",False,"2019/12/10, 06:32:34",471.0,2673284.0,0,"If your application is using async framework, these gaps may indicate execution waiting on available threads.",0.0,0.0,1.0,0.0
Jaeger,59260272,58409078,0,"2019/12/10, 06:32:34",False,"2019/12/10, 06:32:34",471.0,2673284.0,0,Or your application may be CPU throttled during and between the spans.,0.0,0.0,1.0,0.0
Jaeger,59260272,58409078,0,"2019/12/10, 06:32:34",False,"2019/12/10, 06:32:34",471.0,2673284.0,0,"You cannot really explain the gaps from the trace itself, but you surely do have them.",0.0,0.204,0.796,0.5927
Jaeger,59260272,58409078,0,"2019/12/10, 06:32:34",False,"2019/12/10, 06:32:34",471.0,2673284.0,0,Time to whip out the profiler.,0.0,0.0,1.0,0.0
Jaeger,55138790,53453160,1,"2019/03/13, 11:48:49",True,"2019/03/13, 11:48:49",1678.0,2104921.0,1,"Turns out that  Feign  clients are  currently not supported  or to be precise, the spring startes do not configure the Feign clients accordingly.",0.082,0.0,0.918,-0.2411
Jaeger,55138790,53453160,1,"2019/03/13, 11:48:49",True,"2019/03/13, 11:48:49",1678.0,2104921.0,1,"If you want to use Jaeger with your Feign clients, you have to provide an integration of your own.",0.0,0.067,0.933,0.0772
Jaeger,55138790,53453160,1,"2019/03/13, 11:48:49",True,"2019/03/13, 11:48:49",1678.0,2104921.0,1,"In my experience so far, the jaeger community is a lesser supportive one, thus you have to gain such knowledge on your own, which in my opinion is a big downside and you should probably consider, using an alternative.",0.048,0.135,0.817,0.5574
Jaeger,63367857,53453160,0,"2020/08/12, 02:50:32",False,"2020/08/12, 02:50:32",1.0,8149668.0,0,"In some cases it might be necessary to explicitly expose the  Feign Client  in the Spring configuration, in order to get the traceId propagated.",0.065,0.0,0.935,-0.1531
Jaeger,63367857,53453160,0,"2020/08/12, 02:50:32",False,"2020/08/12, 02:50:32",1.0,8149668.0,0,This can be done easily by adding the following into one of your configuration classes,0.0,0.146,0.854,0.34
Jaeger,53343467,53322900,1,"2018/11/16, 20:27:10",True,"2018/11/16, 20:27:10",463.0,1749786.0,1,The best way to move forward in order to use Jaegar is NOT TO USE JAEGAR CLIENT!,0.0,0.219,0.781,0.6696
Jaeger,53343467,53322900,1,"2018/11/16, 20:27:10",True,"2018/11/16, 20:27:10",463.0,1749786.0,1,Jaegar has the ability to collect Zipkin spans:,0.0,0.247,0.753,0.3182
Jaeger,53343467,53322900,1,"2018/11/16, 20:27:10",True,"2018/11/16, 20:27:10",463.0,1749786.0,1,https://www.jaegertracing.io/docs/1.8/getting-started/#migrating-from-zipkin,0.0,0.0,1.0,0.0
Jaeger,53343467,53322900,1,"2018/11/16, 20:27:10",True,"2018/11/16, 20:27:10",463.0,1749786.0,1,You should take advantage of this and use the below Sleuth+Zipkin dependency and exclude Jaegar agent jars in your spring boot app.,0.079,0.084,0.837,0.0258
Jaeger,53343467,53322900,1,"2018/11/16, 20:27:10",True,"2018/11/16, 20:27:10",463.0,1749786.0,1,The above will send Zipkin spans to  http://localhost:9411  by default.,0.0,0.0,1.0,0.0
Jaeger,53343467,53322900,1,"2018/11/16, 20:27:10",True,"2018/11/16, 20:27:10",463.0,1749786.0,1,You can override this in your Spring Boot app to point to your Jaegar server easily by overriding the zipkin base URL.,0.0,0.103,0.897,0.34
Jaeger,53343467,53322900,1,"2018/11/16, 20:27:10",True,"2018/11/16, 20:27:10",463.0,1749786.0,1,spring.zipkin.base-url= http://your-jaegar-server:9411,0.0,0.0,1.0,0.0
Jaeger,53343467,53322900,1,"2018/11/16, 20:27:10",True,"2018/11/16, 20:27:10",463.0,1749786.0,1,Sleuth will do all the heavy lifting and the default logging will log the span and traceIds.,0.0,0.0,1.0,0.0
Jaeger,53343467,53322900,1,"2018/11/16, 20:27:10",True,"2018/11/16, 20:27:10",463.0,1749786.0,1,"In the log4j2.xml file, all you have to mention is",0.0,0.0,1.0,0.0
Jaeger,53343467,53322900,1,"2018/11/16, 20:27:10",True,"2018/11/16, 20:27:10",463.0,1749786.0,1,[%X],0.0,0.0,1.0,0.0
Jaeger,53343467,53322900,1,"2018/11/16, 20:27:10",True,"2018/11/16, 20:27:10",463.0,1749786.0,1,You can find the sample code here:,0.0,0.0,1.0,0.0
Jaeger,53343467,53322900,1,"2018/11/16, 20:27:10",True,"2018/11/16, 20:27:10",463.0,1749786.0,1,https://github.com/anoophp777/spring-webflux-jaegar-log4j2,0.0,0.0,1.0,0.0
Jaeger,52111563,52038025,1,"2018/08/31, 11:36:35",True,"2018/08/31, 11:36:35",284.0,4828509.0,3,Some web frameworks return empty string if a non-existent header is queried.,0.153,0.0,0.847,-0.2023
Jaeger,52111563,52038025,1,"2018/08/31, 11:36:35",True,"2018/08/31, 11:36:35",284.0,4828509.0,3,I have seen this in Spring Boot and KoaJS.,0.0,0.0,1.0,0.0
Jaeger,52111563,52038025,1,"2018/08/31, 11:36:35",True,"2018/08/31, 11:36:35",284.0,4828509.0,3,"If any of the tracing headers is not sent by Istio, this header logic causes us to send empty string for those non-existent headers which breaks tracing.",0.065,0.0,0.935,-0.2023
Jaeger,52111563,52038025,1,"2018/08/31, 11:36:35",True,"2018/08/31, 11:36:35",284.0,4828509.0,3,My suggestion is after getting the values for headers filter out the ones with empty string as their values and propogate the remaining ones.,0.064,0.191,0.745,0.5574
Jaeger,51929991,51838896,0,"2018/08/20, 14:39:54",False,"2018/08/20, 14:39:54",116.0,7510189.0,0,"Also, do you get an Error Message?",0.31,0.0,0.69,-0.4019
Jaeger,51929991,51838896,0,"2018/08/20, 14:39:54",False,"2018/08/20, 14:39:54",116.0,7510189.0,0,"If so, please post it.",0.0,0.393,0.607,0.3804
Jaeger,51720029,51719575,0,"2018/08/07, 09:18:44",False,"2018/08/07, 09:34:22",8280.0,1057291.0,3,BINGO!,0.0,0.0,1.0,0.0
Jaeger,51720029,51719575,0,"2018/08/07, 09:18:44",False,"2018/08/07, 09:34:22",8280.0,1057291.0,3,We need to setup the ReporterConfigurations as below.,0.0,0.0,1.0,0.0
Jaeger,51720029,51719575,0,"2018/08/07, 09:18:44",False,"2018/08/07, 09:34:22",8280.0,1057291.0,3,previously my ones were default ones that's why it always connected to local.,0.0,0.0,1.0,0.0
Jaeger,51720029,51719575,0,"2018/08/07, 09:18:44",False,"2018/08/07, 09:34:22",8280.0,1057291.0,3,"Even better, you can create the Configuration from Environment as below providing the environment variables as below",0.0,0.25,0.75,0.6124
Jaeger,51720029,51719575,0,"2018/08/07, 09:18:44",False,"2018/08/07, 09:34:22",8280.0,1057291.0,3,You can provide this when you run the docker container,0.0,0.0,1.0,0.0
Jaeger,51720029,51719575,0,"2018/08/07, 09:18:44",False,"2018/08/07, 09:34:22",8280.0,1057291.0,3,"-e JAVA_OPTS=""",0.0,0.0,1.0,0.0
Jaeger,51720029,51719575,0,"2018/08/07, 09:18:44",False,"2018/08/07, 09:34:22",8280.0,1057291.0,3,""" ....",0.0,0.0,1.0,0.0
Jaeger,59170296,51719575,2,"2019/12/04, 08:43:41",False,"2019/12/04, 13:31:17",11.0,12478106.0,1,Step 1 : First we need to configure remote host address and port.,0.0,0.0,1.0,0.0
Jaeger,59170296,51719575,2,"2019/12/04, 08:43:41",False,"2019/12/04, 13:31:17",11.0,12478106.0,1,"Step 2 : configure sender configuration and pass the remote host and port in                           withAgentHost, withAgentPort.",0.0,0.0,1.0,0.0
Jaeger,59170296,51719575,2,"2019/12/04, 08:43:41",False,"2019/12/04, 13:31:17",11.0,12478106.0,1,"SenderConfiguration senderConfig = Configuration.SenderConfiguration.fromEnv()
                    .withAgentHost(JAEGER_HOST)
                    .withAgentPort(JAEGER_PORT);",0.0,0.0,1.0,0.0
Jaeger,59170296,51719575,2,"2019/12/04, 08:43:41",False,"2019/12/04, 13:31:17",11.0,12478106.0,1,Step 3 : Pass sender configuration in reporter configuration,0.0,0.0,1.0,0.0
Jaeger,59170296,51719575,2,"2019/12/04, 08:43:41",False,"2019/12/04, 13:31:17",11.0,12478106.0,1,"Configuration.ReporterConfiguration reporterConfig = Configuration.ReporterConfiguration.fromEnv()
                .withLogSpans(true)
                .withSender(senderConfig);",0.0,0.0,1.0,0.0
Jaeger,52813394,48423910,0,"2018/10/15, 12:16:23",False,"2018/10/15, 12:16:23",13313.0,524946.0,0,This seems a bit old and it's a bit hard to tell what's wrong.,0.31,0.0,0.69,-0.5423
Jaeger,52813394,48423910,0,"2018/10/15, 12:16:23",False,"2018/10/15, 12:16:23",13313.0,524946.0,0,"My first guess would be the sampling strategy, as Jaeger samples one trace in one thousand, but looks like you did set it.",0.0,0.129,0.871,0.5023
Jaeger,52813394,48423910,0,"2018/10/15, 12:16:23",False,"2018/10/15, 12:16:23",13313.0,524946.0,0,"
JAEGER_SAMPLER_TYPE=const
JAEGER_SAMPLER_PARAM=1",0.0,0.0,1.0,0.0
Jaeger,52813394,48423910,0,"2018/10/15, 12:16:23",False,"2018/10/15, 12:16:23",13313.0,524946.0,0,I would recommend you start by using a simple  Configuration.fromEnv().getTracer()  to get your tracer.,0.0,0.185,0.815,0.3612
Jaeger,52813394,48423910,0,"2018/10/15, 12:16:23",False,"2018/10/15, 12:16:23",13313.0,524946.0,0,"Then, control it via env vars, probably setting  JAEGER_REPORTER_LOG_SPANS  to  true .",0.0,0.219,0.781,0.4215
Jaeger,52813394,48423910,0,"2018/10/15, 12:16:23",False,"2018/10/15, 12:16:23",13313.0,524946.0,0,"With this option, you should be able to see in the logs whenever Jaeger emits a span.",0.0,0.0,1.0,0.0
Jaeger,52813394,48423910,0,"2018/10/15, 12:16:23",False,"2018/10/15, 12:16:23",13313.0,524946.0,0,"You can also set the  --log-level=debug  option to the agent and collector (or all-in-one, in your case) to see when these components receive a span from a client.",0.0,0.0,1.0,0.0
Jaeger,64399197,64391505,1,"2020/10/17, 08:06:03",False,"2020/10/17, 08:06:03",4190.0,1733929.0,1,You can see that jaeger-query configuration includes: SPAN_STORAGE_TYPE: &quot;kafka&quot;,0.0,0.0,1.0,0.0
Jaeger,64399197,64391505,1,"2020/10/17, 08:06:03",False,"2020/10/17, 08:06:03",4190.0,1733929.0,1,"The error indicates that a kafka client used by jaeger-query to store spans in Kafka cannot in fact reach Kafka, and therefore the jaeger storage factory fails to initialize.",0.208,0.0,0.792,-0.6782
Jaeger,64399197,64391505,1,"2020/10/17, 08:06:03",False,"2020/10/17, 08:06:03",4190.0,1733929.0,1,This can be either because Kafka failed to start (did you check)?,0.231,0.0,0.769,-0.5106
Jaeger,64399197,64391505,1,"2020/10/17, 08:06:03",False,"2020/10/17, 08:06:03",4190.0,1733929.0,1,Or a misconfig of the network in your docker.,0.0,0.0,1.0,0.0
Jaeger,64572391,64391505,0,"2020/10/28, 13:57:45",True,"2020/10/28, 13:57:45",1101.0,1141160.0,1,I was missing a lot of information.,0.355,0.0,0.645,-0.296
Jaeger,64572391,64391505,0,"2020/10/28, 13:57:45",True,"2020/10/28, 13:57:45",1101.0,1141160.0,1,I managed to get it working:,0.0,0.0,1.0,0.0
Jaeger,64090692,64018706,9,"2020/09/27, 19:21:51",False,"2020/09/28, 12:43:21",3656.0,3503019.0,0,Update :,0.0,0.0,1.0,0.0
Jaeger,64090692,64018706,9,"2020/09/27, 19:21:51",False,"2020/09/28, 12:43:21",3656.0,3503019.0,0,I got the same exception ( Tracer bean is not configured!.. ),0.0,0.0,1.0,0.0
Jaeger,64090692,64018706,9,"2020/09/27, 19:21:51",False,"2020/09/28, 12:43:21",3656.0,3503019.0,0,when I use your version of spring cloud jaeger dependencies.,0.0,0.0,1.0,0.0
Jaeger,64090692,64018706,9,"2020/09/27, 19:21:51",False,"2020/09/28, 12:43:21",3656.0,3503019.0,0,This is irrespective of the  RxJava  package.,0.0,0.0,1.0,0.0
Jaeger,64090692,64018706,9,"2020/09/27, 19:21:51",False,"2020/09/28, 12:43:21",3656.0,3503019.0,0,I think you can directly use  opentracing-spring-jaeger-cloud-starter  which is a combination of  opentracing-spring-cloud-starter  and  opentracing-spring-jaeger-starter .,0.0,0.0,1.0,0.0
Jaeger,64090692,64018706,9,"2020/09/27, 19:21:51",False,"2020/09/28, 12:43:21",3656.0,3503019.0,0,Read  this details  for java spring jaeger.,0.0,0.0,1.0,0.0
Jaeger,64090692,64018706,9,"2020/09/27, 19:21:51",False,"2020/09/28, 12:43:21",3656.0,3503019.0,0,"The opentracing-spring-jaeger-cloud-starter starter is convenience
starter that includes both opentracing-spring-jaeger-starter and
opentracing-spring-cloud-starter This means that by including it, all
parts of the Spring Cloud stack supported by Opentracing will be
instrumented",0.0,0.071,0.929,0.3182
Jaeger,64090692,64018706,9,"2020/09/27, 19:21:51",False,"2020/09/28, 12:43:21",3656.0,3503019.0,0,Note :,0.0,0.0,1.0,0.0
Jaeger,64090692,64018706,9,"2020/09/27, 19:21:51",False,"2020/09/28, 12:43:21",3656.0,3503019.0,0,Maybe RxJava tracing won't work without registering the tracer using the decorators provided by  opentracing-contrib .,0.0,0.0,1.0,0.0
Jaeger,64090692,64018706,9,"2020/09/27, 19:21:51",False,"2020/09/28, 12:43:21",3656.0,3503019.0,0,Please see the working app  here .,0.0,0.315,0.685,0.3182
Jaeger,64090692,64018706,9,"2020/09/27, 19:21:51",False,"2020/09/28, 12:43:21",3656.0,3503019.0,0,I have followed  this spring guide  for Reactive Restful webservice and  jaeger  worked with below  pom.xml  without any  Tracer bean  -,0.0,0.122,0.878,0.3612
Jaeger,63455905,63443921,4,"2020/08/17, 20:36:56",True,"2020/08/17, 20:36:56",111.0,2035350.0,0,"Thing is, you should use opentelemetry collector if you use opentelemetry exporter.",0.0,0.0,1.0,0.0
Jaeger,63455905,63443921,4,"2020/08/17, 20:36:56",True,"2020/08/17, 20:36:56",111.0,2035350.0,0,Pls see schema in attachment,0.0,0.538,0.462,0.3612
Jaeger,63455905,63443921,4,"2020/08/17, 20:36:56",True,"2020/08/17, 20:36:56",111.0,2035350.0,0,"Also I created a gist, which will help you to setup
pls see",0.0,0.429,0.571,0.6124
Jaeger,63455905,63443921,4,"2020/08/17, 20:36:56",True,"2020/08/17, 20:36:56",111.0,2035350.0,0,https://gist.github.com/AndrewGrachov/11a18bc7268e43f1a36960d630a0838f,0.0,0.0,1.0,0.0
Jaeger,63455905,63443921,4,"2020/08/17, 20:36:56",True,"2020/08/17, 20:36:56",111.0,2035350.0,0,"(just tune the values, export to jaeger-all-in-one instead of separate + cassandra, etc)",0.0,0.197,0.803,0.4019
Jaeger,59260037,59153293,1,"2019/12/10, 06:02:36",True,"2019/12/10, 14:41:51",471.0,2673284.0,4,From the official FAQ ( https://www.jaegertracing.io/docs/latest/faq/#do-i-need-to-run-jaeger-agent ):,0.359,0.0,0.641,-0.4215
Jaeger,59260037,59153293,1,"2019/12/10, 06:02:36",True,"2019/12/10, 14:41:51",471.0,2673284.0,4,jaeger-agent  is not always necessary.,0.0,0.0,1.0,0.0
Jaeger,59260037,59153293,1,"2019/12/10, 06:02:36",True,"2019/12/10, 14:41:51",471.0,2673284.0,4,Jaeger client libraries can be configured to export trace data directly to  jaeger-collector .,0.0,0.0,1.0,0.0
Jaeger,59260037,59153293,1,"2019/12/10, 06:02:36",True,"2019/12/10, 14:41:51",471.0,2673284.0,4,"However, the following are the reasons why running  jaeger-agent  is recommended:",0.0,0.153,0.847,0.2023
Jaeger,58511794,58511640,0,"2019/10/22, 23:25:32",True,"2019/10/22, 23:39:29",34894.0,3288890.0,0,If you check the your  base image  it from scratch.,0.0,0.0,1.0,0.0
Jaeger,58511794,58511640,0,"2019/10/22, 23:25:32",True,"2019/10/22, 23:39:29",34894.0,3288890.0,0,"So there is no  Bash, ash  as the image is from scratch so it will only cotnain  hotrod-linux .",0.127,0.0,0.873,-0.3535
Jaeger,58511794,58511640,0,"2019/10/22, 23:25:32",True,"2019/10/22, 23:39:29",34894.0,3288890.0,0,"To get sh or bash in such cases you need to use multi-stage Dockerfile, you can use the base image in Dockerfile and then copy the binaries from the base image in multi-stage in Dockerfile.",0.0,0.0,1.0,0.0
Jaeger,58511794,58511640,0,"2019/10/22, 23:25:32",True,"2019/10/22, 23:39:29",34894.0,3288890.0,0,Here you go,0.0,0.0,1.0,0.0
Jaeger,58511794,58511640,0,"2019/10/22, 23:25:32",True,"2019/10/22, 23:39:29",34894.0,3288890.0,0,"so now you can build and test and you will able to run command inside container using docker exec, here is the example",0.0,0.0,1.0,0.0
Jaeger,58511799,58511640,0,"2019/10/22, 23:25:50",False,"2019/10/22, 23:25:50",76.0,12259556.0,2,https://github.com/jaegertracing/jaeger/blob/master/examples/hotrod/Dockerfile,0.0,0.0,1.0,0.0
Jaeger,58511799,58511640,0,"2019/10/22, 23:25:50",False,"2019/10/22, 23:25:50",76.0,12259556.0,2,As I can see hotrod image was built from scratch image.,0.0,0.0,1.0,0.0
Jaeger,58511799,58511640,0,"2019/10/22, 23:25:50",False,"2019/10/22, 23:25:50",76.0,12259556.0,2,And from the docker hub:,0.0,0.0,1.0,0.0
Jaeger,58511799,58511640,0,"2019/10/22, 23:25:50",False,"2019/10/22, 23:25:50",76.0,12259556.0,2,"""an explicitly empty image, especially for building images ""FROM
  scratch""...",0.167,0.0,0.833,-0.2023
Jaeger,58511799,58511640,0,"2019/10/22, 23:25:50",False,"2019/10/22, 23:25:50",76.0,12259556.0,2,"""This image is most useful in the context of building base images
  (such as debian and busybox) or super minimal images (that contain
  only a single binary and whatever it requires, such as hello-world).""",0.0,0.186,0.814,0.796
Jaeger,58511799,58511640,0,"2019/10/22, 23:25:50",False,"2019/10/22, 23:25:50",76.0,12259556.0,2,https://hub.docker.com/_/scratch,0.0,0.0,1.0,0.0
Jaeger,58511799,58511640,0,"2019/10/22, 23:25:50",False,"2019/10/22, 23:25:50",76.0,12259556.0,2,"So, I think there is not bash inside this image",0.0,0.0,1.0,0.0
Jaeger,57986473,57976158,0,"2019/09/18, 09:36:52",True,"2019/09/18, 09:36:52",31.0,10489752.0,1,"'reporting_host' must be not 'localhost', but 'jaeger', just as service in docker-compose.yml called.",0.0,0.0,1.0,0.0
Jaeger,57986473,57976158,0,"2019/09/18, 09:36:52",True,"2019/09/18, 09:36:52",31.0,10489752.0,1,"'reporting_host' =&gt; 'jaeger',",0.0,0.0,1.0,0.0
Jaeger,57986473,57976158,0,"2019/09/18, 09:36:52",True,"2019/09/18, 09:36:52",31.0,10489752.0,1,"Also, I needed to add  $tracer-&gt;flush();  after all, it closes all the entities and does sending via UDP behind the scenes.",0.0,0.0,1.0,0.0
Jaeger,56927362,56847558,1,"2019/07/08, 04:26:18",True,"2019/07/08, 04:26:18",1084.0,2915603.0,1,"So, answering my own question.",0.0,0.0,1.0,0.0
Jaeger,56927362,56847558,1,"2019/07/08, 04:26:18",True,"2019/07/08, 04:26:18",1084.0,2915603.0,1,Jaeger does not support cross system spans.,0.273,0.0,0.727,-0.3089
Jaeger,56927362,56847558,1,"2019/07/08, 04:26:18",True,"2019/07/08, 04:26:18",1084.0,2915603.0,1,Every sub-system is responsible for its own span in the whole system.,0.0,0.173,0.827,0.3182
Jaeger,56927362,56847558,1,"2019/07/08, 04:26:18",True,"2019/07/08, 04:26:18",1084.0,2915603.0,1,"For reference, check this answer  https://github.com/opentracing/specification/issues/143",0.0,0.0,1.0,0.0
Jaeger,56581518,55256948,2,"2019/06/13, 16:23:02",True,"2019/06/13, 16:23:02",2678.0,3349358.0,5,"If anyone else would like to set up Jaeger in spring project, here's what I did:",0.0,0.152,0.848,0.3612
Jaeger,56581518,55256948,2,"2019/06/13, 16:23:02",True,"2019/06/13, 16:23:02",2678.0,3349358.0,5,Add dependencies to pom:,0.0,0.0,1.0,0.0
Jaeger,56581518,55256948,2,"2019/06/13, 16:23:02",True,"2019/06/13, 16:23:02",2678.0,3349358.0,5,Set up you web.xml to register new tracing filter  tracingFilter  to intercept REST API:,0.0,0.0,1.0,0.0
Jaeger,56581518,55256948,2,"2019/06/13, 16:23:02",True,"2019/06/13, 16:23:02",2678.0,3349358.0,5,Register jaeger tracer in spring mvc:,0.0,0.0,1.0,0.0
Jaeger,56581518,55256948,2,"2019/06/13, 16:23:02",True,"2019/06/13, 16:23:02",2678.0,3349358.0,5,Set up the  tracingFilter  bean we described in web.xml:,0.0,0.0,1.0,0.0
Jaeger,56581518,55256948,2,"2019/06/13, 16:23:02",True,"2019/06/13, 16:23:02",2678.0,3349358.0,5,Finally define jaeger tracer spring configuration:,0.0,0.0,1.0,0.0
Jaeger,63082785,55256948,2,"2020/07/25, 02:41:06",False,"2020/07/25, 02:41:06",176.0,1446358.0,0,"I have got following gradle dependencies working,",0.0,0.0,1.0,0.0
Jaeger,63082785,55256948,2,"2020/07/25, 02:41:06",False,"2020/07/25, 02:41:06",176.0,1446358.0,0,"Following tracer bean configuration,",0.0,0.0,1.0,0.0
Jaeger,63082785,55256948,2,"2020/07/25, 02:41:06",False,"2020/07/25, 02:41:06",176.0,1446358.0,0,And then the spans can be recorded as,0.0,0.0,1.0,0.0
Jaeger,63082785,55256948,2,"2020/07/25, 02:41:06",False,"2020/07/25, 02:41:06",176.0,1446358.0,0,Here is the working example you can refer  https://github.com/krushnatkhawale/jaeger-with-spring-boot-web-app,0.0,0.0,1.0,0.0
Jaeger,53757860,53754605,0,"2018/12/13, 10:35:24",False,"2018/12/13, 10:35:24",39.0,10384577.0,1,So did you install direct  or created a yaml from the templates ?,0.0,0.167,0.833,0.25
Jaeger,53757860,53754605,0,"2018/12/13, 10:35:24",False,"2018/12/13, 10:35:24",39.0,10384577.0,1,"I would run the command you used to install but with template function and then add the options for jaeger,Kiali and grafana.",0.0,0.0,1.0,0.0
Jaeger,53761233,53754605,3,"2018/12/13, 13:50:31",True,"2018/12/13, 13:50:31",2167.0,9485673.0,6,Here is  howto : from official repository.,0.0,0.0,1.0,0.0
Jaeger,53761233,53754605,3,"2018/12/13, 13:50:31",True,"2018/12/13, 13:50:31",2167.0,9485673.0,6,you need to update  values.yaml .,0.0,0.0,1.0,0.0
Jaeger,53761233,53754605,3,"2018/12/13, 13:50:31",True,"2018/12/13, 13:50:31",2167.0,9485673.0,6,"and turn on  grafana,  kiali and jaeger.",0.0,0.0,1.0,0.0
Jaeger,53761233,53754605,3,"2018/12/13, 13:50:31",True,"2018/12/13, 13:50:31",2167.0,9485673.0,6,For example with kiali change:,0.0,0.0,1.0,0.0
Jaeger,53761233,53754605,3,"2018/12/13, 13:50:31",True,"2018/12/13, 13:50:31",2167.0,9485673.0,6,to,0.0,0.0,1.0,0.0
Jaeger,53761233,53754605,3,"2018/12/13, 13:50:31",True,"2018/12/13, 13:50:31",2167.0,9485673.0,6,than rebuild the Helm dependencies:,0.0,0.0,1.0,0.0
Jaeger,53761233,53754605,3,"2018/12/13, 13:50:31",True,"2018/12/13, 13:50:31",2167.0,9485673.0,6,than upgrade your istio inside kubernetes:,0.0,0.0,1.0,0.0
Jaeger,53761233,53754605,3,"2018/12/13, 13:50:31",True,"2018/12/13, 13:50:31",2167.0,9485673.0,6,"that's it, hope it was helpful",0.0,0.588,0.412,0.6908
Jaeger,53705114,53654993,0,"2018/12/10, 13:51:09",False,"2018/12/10, 13:51:09",63.0,326167.0,0,Scalabilty is dependent on sampling frequency and volumes.,0.0,0.0,1.0,0.0
Jaeger,53705114,53654993,0,"2018/12/10, 13:51:09",False,"2018/12/10, 13:51:09",63.0,326167.0,0,The agent supports adaptive sampling which is a feedback loop from the collector to your instrumented app.,0.0,0.143,0.857,0.3612
Jaeger,53705114,53654993,0,"2018/12/10, 13:51:09",False,"2018/12/10, 13:51:09",63.0,326167.0,0,You can statically define this up front in your instrumentation but you lose the adaptive features.,0.191,0.0,0.809,-0.5499
Jaeger,53706156,53654993,0,"2018/12/10, 14:56:58",False,"2018/12/10, 14:56:58",21.0,10770815.0,1,It is possible to bypass agent all together and send metrics directly to collector.,0.0,0.0,1.0,0.0
Jaeger,53706156,53654993,0,"2018/12/10, 14:56:58",False,"2018/12/10, 14:56:58",21.0,10770815.0,1,Just define variable JAEGER_ENDPOINT in your app running environment.,0.0,0.0,1.0,0.0
Jaeger,53706156,53654993,0,"2018/12/10, 14:56:58",False,"2018/12/10, 14:56:58",21.0,10770815.0,1,This behaviour is documented but buried down in the Jager git repo:,0.0,0.0,1.0,0.0
Jaeger,53706156,53654993,0,"2018/12/10, 14:56:58",False,"2018/12/10, 14:56:58",21.0,10770815.0,1,https://github.com/jaegertracing/jaeger-client-java/blob/master/jaeger-core/README.md,0.0,0.0,1.0,0.0
Jaeger,54325898,53654993,0,"2019/01/23, 13:09:57",False,"2019/01/23, 13:09:57",13313.0,524946.0,0,Would a single agent colocated with a single collector be possible in a Jaeger deployment?,0.0,0.0,1.0,0.0
Jaeger,54325898,53654993,0,"2019/01/23, 13:09:57",False,"2019/01/23, 13:09:57",13313.0,524946.0,0,"It's possible, and that's how the  ""all-in-one""  image works.",0.0,0.0,1.0,0.0
Jaeger,54325898,53654993,0,"2019/01/23, 13:09:57",False,"2019/01/23, 13:09:57",13313.0,524946.0,0,Would it be advisable?,0.0,0.0,1.0,0.0
Jaeger,54325898,53654993,0,"2019/01/23, 13:09:57",False,"2019/01/23, 13:09:57",13313.0,524946.0,0,Depends on your architecture.,0.0,0.0,1.0,0.0
Jaeger,54325898,53654993,0,"2019/01/23, 13:09:57",False,"2019/01/23, 13:09:57",13313.0,524946.0,0,"If you don't expect your Jaeger infra to grow, using the all-in-one is easier from the maintenance perspective.",0.0,0.141,0.859,0.4215
Jaeger,54325898,53654993,0,"2019/01/23, 13:09:57",False,"2019/01/23, 13:09:57",13313.0,524946.0,0,"If you need your Jaeger infra to be highly available, then you probably want to place your agents closer to your instrumented applications than to your collector and scale the collectors separately.",0.0,0.04,0.96,0.0772
Jaeger,54325898,53654993,0,"2019/01/23, 13:09:57",False,"2019/01/23, 13:09:57",13313.0,524946.0,0,More about the Jaeger Agent is discussed in the following blog posts:,0.0,0.0,1.0,0.0
Jaeger,54325898,53654993,0,"2019/01/23, 13:09:57",False,"2019/01/23, 13:09:57",13313.0,524946.0,0,"Running Jaeger Agent on bare metal 
 Deployment strategies for the Jaeger Agent",0.0,0.0,1.0,0.0
Jaeger,54325898,53654993,0,"2019/01/23, 13:09:57",False,"2019/01/23, 13:09:57",13313.0,524946.0,0,Is it possible to skip the agent altogether and submit spans directly to the collector over HTTP?,0.0,0.0,1.0,0.0
Jaeger,54325898,53654993,0,"2019/01/23, 13:09:57",False,"2019/01/23, 13:09:57",13313.0,524946.0,0,"For some clients (Java, NodeJS and C#), yes.",0.0,0.278,0.722,0.4019
Jaeger,54325898,53654993,0,"2019/01/23, 13:09:57",False,"2019/01/23, 13:09:57",13313.0,524946.0,0,Look for the  JAEGER_ENDPOINT  option .,0.0,0.0,1.0,0.0
Jaeger,53409452,53172758,0,"2018/11/21, 11:57:59",False,"2018/11/21, 11:57:59",13313.0,524946.0,0,"You can turn on the debugging in the client by setting the option  JAEGER_REPORTER_LOG_SPANS  to true (or use the related option in the  ReporterConfiguration , as it seems that's how you are using it).",0.0,0.08,0.92,0.4215
Jaeger,53409452,53172758,0,"2018/11/21, 11:57:59",False,"2018/11/21, 11:57:59",13313.0,524946.0,0,https://www.jaegertracing.io/docs/1.8/client-features/,0.0,0.0,1.0,0.0
Jaeger,53409452,53172758,0,"2018/11/21, 11:57:59",False,"2018/11/21, 11:57:59",13313.0,524946.0,0,"Once you confirm the traces are being generated and sent to the agent, set the log-level in the agent to  debug :",0.0,0.0,1.0,0.0
Jaeger,53409452,53172758,0,"2018/11/21, 11:57:59",False,"2018/11/21, 11:57:59",13313.0,524946.0,0,"
docker run -p... jaegertracing/jaeger-agent:1.8 --log-level=debug",0.0,0.0,1.0,0.0
Jaeger,53409452,53172758,0,"2018/11/21, 11:57:59",False,"2018/11/21, 11:57:59",13313.0,524946.0,0,"If you don't see anything in the logs there indicating that the agent received a span (or span batch), then you might need to configure your client with the Agent's address ( JAEGER_AGENT_HOST  and  JAEGER_AGENT_PORT , or related options in the  Configuration  object).",0.0,0.0,1.0,0.0
Jaeger,53409452,53172758,0,"2018/11/21, 11:57:59",False,"2018/11/21, 11:57:59",13313.0,524946.0,0,"You mentioned that you are deploying in Azure AKS, so, I guess that the agent isn't available at  localhost , which is the default location where the client sends the spans.",0.0,0.0,1.0,0.0
Jaeger,53409452,53172758,0,"2018/11/21, 11:57:59",False,"2018/11/21, 11:57:59",13313.0,524946.0,0,"Typically, the agent would be deployed as a sidecar in such a scenario:",0.0,0.0,1.0,0.0
Jaeger,53409452,53172758,0,"2018/11/21, 11:57:59",False,"2018/11/21, 11:57:59",13313.0,524946.0,0,https://github.com/jaegertracing/jaeger-kubernetes#deploying-the-agent-as-sidecar,0.0,0.0,1.0,0.0
Jaeger,53341599,51929412,0,"2018/11/16, 18:11:14",False,"2020/12/19, 21:55:48",463.0,1749786.0,2,The best way to move forward in order to use Jaegar is NOT TO USE JAEGAR CLIENT!,0.0,0.219,0.781,0.6696
Jaeger,53341599,51929412,0,"2018/11/16, 18:11:14",False,"2020/12/19, 21:55:48",463.0,1749786.0,2,Jaegar has the ability to collect Zipkin spans.,0.0,0.247,0.753,0.3182
Jaeger,53341599,51929412,0,"2018/11/16, 18:11:14",False,"2020/12/19, 21:55:48",463.0,1749786.0,2,https://www.jaegertracing.io/docs/1.8/getting-started/#migrating-from-zipkin,0.0,0.0,1.0,0.0
Jaeger,53341599,51929412,0,"2018/11/16, 18:11:14",False,"2020/12/19, 21:55:48",463.0,1749786.0,2,You should take advantage of this and use the below Sleuth+Zipkin dependency and exclude Jaegar agent jars in your spring boot app.,0.079,0.084,0.837,0.0258
Jaeger,53341599,51929412,0,"2018/11/16, 18:11:14",False,"2020/12/19, 21:55:48",463.0,1749786.0,2,The above will send Zipkin spans to http://localhost:9411 by default.,0.0,0.0,1.0,0.0
Jaeger,53341599,51929412,0,"2018/11/16, 18:11:14",False,"2020/12/19, 21:55:48",463.0,1749786.0,2,You can override this in your Spring Boot app to point to your Jaegar server easily by overriding the zipkin base URL.,0.0,0.103,0.897,0.34
Jaeger,53341599,51929412,0,"2018/11/16, 18:11:14",False,"2020/12/19, 21:55:48",463.0,1749786.0,2,Sleuth will do all the heavy lifting and the default logging will log the span and traceIds.,0.0,0.0,1.0,0.0
Jaeger,53341599,51929412,0,"2018/11/16, 18:11:14",False,"2020/12/19, 21:55:48",463.0,1749786.0,2,"In the  log4j2.xml  file, all you have to mention is",0.0,0.0,1.0,0.0
Jaeger,53341599,51929412,0,"2018/11/16, 18:11:14",False,"2020/12/19, 21:55:48",463.0,1749786.0,2,I'll be uploading a working example of this approach into my GitHub and sharing the link.,0.0,0.167,0.833,0.4215
Jaeger,53341599,51929412,0,"2018/11/16, 18:11:14",False,"2020/12/19, 21:55:48",463.0,1749786.0,2,EDIT 1:,0.0,0.0,1.0,0.0
Jaeger,53341599,51929412,0,"2018/11/16, 18:11:14",False,"2020/12/19, 21:55:48",463.0,1749786.0,2,You can find the sample code here:,0.0,0.0,1.0,0.0
Jaeger,53341599,51929412,0,"2018/11/16, 18:11:14",False,"2020/12/19, 21:55:48",463.0,1749786.0,2,https://github.com/anoophp777/spring-webflux-jaegar-log4j2,0.0,0.0,1.0,0.0
Jaeger,59675721,50179555,0,"2020/01/10, 06:59:31",True,"2020/01/10, 06:59:31",842.0,6907909.0,1,The Jaeger helm chart is now available  here .,0.0,0.0,1.0,0.0
Jaeger,59675721,50179555,0,"2020/01/10, 06:59:31",True,"2020/01/10, 06:59:31",842.0,6907909.0,1,You need to add the helm repo first using the following:,0.0,0.0,1.0,0.0
Jaeger,59675721,50179555,0,"2020/01/10, 06:59:31",True,"2020/01/10, 06:59:31",842.0,6907909.0,1,This can be installed with:,0.0,0.0,1.0,0.0
Jaeger,66917361,66916739,0,"2021/04/02, 12:39:37",True,"2021/04/02, 12:39:37",8336.0,1773866.0,1,"Yes you can, and I have shown that numerous times during my presentations ( https://toomuchcoding.com/talks ) and we describe it extensively in the documentation ( https://docs.spring.io/spring-cloud-sleuth/docs/current/reference/html/ ).",0.0,0.109,0.891,0.4019
Jaeger,66917361,66916739,0,"2021/04/02, 12:39:37",True,"2021/04/02, 12:39:37",8336.0,1773866.0,1,Sleuth will set up your logging pattern which you can then parse and visualize using the ELK stack.,0.0,0.0,1.0,0.0
Jaeger,66917361,66916739,0,"2021/04/02, 12:39:37",True,"2021/04/02, 12:39:37",8336.0,1773866.0,1,Sleuth takes care of tracing context propagation and can send the spans to a span storage (e.g.,0.0,0.176,0.824,0.4939
Jaeger,66917361,66916739,0,"2021/04/02, 12:39:37",True,"2021/04/02, 12:39:37",8336.0,1773866.0,1,Zipkin or Jaeger).,0.0,0.0,1.0,0.0
Jaeger,66917361,66916739,0,"2021/04/02, 12:39:37",True,"2021/04/02, 12:39:37",8336.0,1773866.0,1,Sleuth does take care of updating the MDC for you.,0.0,0.262,0.738,0.4939
Jaeger,66917361,66916739,0,"2021/04/02, 12:39:37",True,"2021/04/02, 12:39:37",8336.0,1773866.0,1,Please always read the documentation and the project page before filing a question,0.0,0.173,0.827,0.3182
Jaeger,66584227,66527792,0,"2021/03/11, 16:09:56",True,"2021/03/11, 16:09:56",2573.0,2519395.0,2,The solution for this one is to simply increase the memory size in the  istio-config.yaml  file.,0.0,0.247,0.753,0.5574
Jaeger,66584227,66527792,0,"2021/03/11, 16:09:56",True,"2021/03/11, 16:09:56",2573.0,2519395.0,2,"in my case, I'm updating the PVC and it looks like it's already filled with data and decreasing it wasn't an option for istio, so I increased it in the config file instead:",0.0,0.149,0.851,0.6418
Jaeger,66314862,66299365,0,"2021/02/22, 13:29:44",True,"2021/02/22, 13:29:44",4113.0,12201084.0,1,Based on following example from  jaeger docs :,0.0,0.0,1.0,0.0
Jaeger,66314862,66299365,0,"2021/02/22, 13:29:44",True,"2021/02/22, 13:29:44",4113.0,12201084.0,1,and on example  cli falgs :,0.0,0.0,1.0,0.0
Jaeger,66314862,66299365,0,"2021/02/22, 13:29:44",True,"2021/02/22, 13:29:44",4113.0,12201084.0,1,I infere that you should be able to do the following:,0.0,0.0,1.0,0.0
Jaeger,66314862,66299365,0,"2021/02/22, 13:29:44",True,"2021/02/22, 13:29:44",4113.0,12201084.0,1,Notice that I split the cli options with the dot and added it as a nested fields in yaml.,0.0,0.0,1.0,0.0
Jaeger,66314862,66299365,0,"2021/02/22, 13:29:44",True,"2021/02/22, 13:29:44",4113.0,12201084.0,1,Do the same to other parameters by analogy.,0.0,0.0,1.0,0.0
Jaeger,65163350,65138941,0,"2020/12/06, 02:12:58",False,"2020/12/06, 02:12:58",166.0,13922022.0,0,"See the answer on this  jaeger issue , you will need to query elastic search or the source where the data is stored.",0.0,0.0,1.0,0.0
Jaeger,65163350,65138941,0,"2020/12/06, 02:12:58",False,"2020/12/06, 02:12:58",166.0,13922022.0,0,"Alternatively, you should raise an issue on  jaeger-ui  detailing your case.",0.0,0.0,1.0,0.0
Jaeger,65299006,65138941,0,"2020/12/15, 04:01:30",False,"2020/12/15, 04:01:30",471.0,2673284.0,0,"When you open an individual trace in the Jaeger UI, there is a View dropdown in the top right corner.",0.0,0.091,0.909,0.2023
Jaeger,65299006,65138941,0,"2020/12/15, 04:01:30",False,"2020/12/15, 04:01:30",471.0,2673284.0,0,One of the options is to view/download the given trace as a JSON file.,0.0,0.0,1.0,0.0
Jaeger,65299006,65138941,0,"2020/12/15, 04:01:30",False,"2020/12/15, 04:01:30",471.0,2673284.0,0,"You can also programmatically query the Jaeger query service via JSON/Protobuf API, but those endpoints will not result in a data format that you can load back into the UI.",0.0,0.0,1.0,0.0
Jaeger,65299006,65138941,0,"2020/12/15, 04:01:30",False,"2020/12/15, 04:01:30",471.0,2673284.0,0,https://www.jaegertracing.io/docs/latest/apis/#trace-retrieval-apis,0.0,0.0,1.0,0.0
Jaeger,64970836,64938228,0,"2020/11/23, 16:58:26",True,"2020/11/23, 16:58:26",328.0,1221718.0,1,"OK, I figured out the issue here which may be obvious to those with more expertise.",0.0,0.173,0.827,0.4466
Jaeger,64970836,64938228,0,"2020/11/23, 16:58:26",True,"2020/11/23, 16:58:26",328.0,1221718.0,1,The guide I linked to above that describes how to make an Ingress spec for gRPC  is specific to NGINX.,0.0,0.0,1.0,0.0
Jaeger,64970836,64938228,0,"2020/11/23, 16:58:26",True,"2020/11/23, 16:58:26",328.0,1221718.0,1,"Meanwhile, I am using K3S which came out of the box with Traefik as the Ingress Controller.",0.0,0.0,1.0,0.0
Jaeger,64970836,64938228,0,"2020/11/23, 16:58:26",True,"2020/11/23, 16:58:26",328.0,1221718.0,1,"Therefore, the annotations I used in my Ingress spec had no affect:",0.18,0.0,0.82,-0.296
Jaeger,64970836,64938228,0,"2020/11/23, 16:58:26",True,"2020/11/23, 16:58:26",328.0,1221718.0,1,So I found  another Stack Overflow post discussing Traefik and gRPC  and modified my original Ingress spec above a bit to include the annotations mentioned there:,0.0,0.091,0.909,0.3182
Jaeger,64970836,64938228,0,"2020/11/23, 16:58:26",True,"2020/11/23, 16:58:26",328.0,1221718.0,1,These are the changes I made:,0.0,0.0,1.0,0.0
Jaeger,64970836,64938228,0,"2020/11/23, 16:58:26",True,"2020/11/23, 16:58:26",328.0,1221718.0,1,Hopefully this helps someone else running into this same confusion.,0.152,0.366,0.483,0.4767
Jaeger,64814852,64755896,0,"2020/11/13, 05:00:24",True,"2020/11/13, 05:00:24",121.0,3792242.0,1,"It's not clear in the documentation, but I managed to get it working by providing the  SPAN_STORAGE_TYPE  and the respective connection details to allow the jaeger components to talk to the storage running outside of the all-in-one container.",0.038,0.145,0.816,0.666
Jaeger,64814852,64755896,0,"2020/11/13, 05:00:24",True,"2020/11/13, 05:00:24",121.0,3792242.0,1,"For instance, I'm running elasticsearch on my Mac, so I used the following command to run all-in-one:",0.0,0.0,1.0,0.0
Jaeger,64532290,64484169,0,"2020/10/26, 08:17:55",True,"2020/10/26, 08:17:55",5749.0,11977760.0,0,According to istio  documentation,0.0,0.0,1.0,0.0
Jaeger,64532290,64484169,0,"2020/10/26, 08:17:55",True,"2020/10/26, 08:17:55",5749.0,11977760.0,0,"To see trace data, you must send requests to your service.",0.0,0.0,1.0,0.0
Jaeger,64532290,64484169,0,"2020/10/26, 08:17:55",True,"2020/10/26, 08:17:55",5749.0,11977760.0,0,The number of requests depends on Istio’s sampling rate.,0.0,0.14,0.86,0.0772
Jaeger,64532290,64484169,0,"2020/10/26, 08:17:55",True,"2020/10/26, 08:17:55",5749.0,11977760.0,0,You set this rate when you install Istio.,0.0,0.0,1.0,0.0
Jaeger,64532290,64484169,0,"2020/10/26, 08:17:55",True,"2020/10/26, 08:17:55",5749.0,11977760.0,0,The default sampling rate is 1%.,0.0,0.0,1.0,0.0
Jaeger,64532290,64484169,0,"2020/10/26, 08:17:55",True,"2020/10/26, 08:17:55",5749.0,11977760.0,0,You need to send at least 100 requests before the first trace is visible.,0.0,0.0,1.0,0.0
Jaeger,64532290,64484169,0,"2020/10/26, 08:17:55",True,"2020/10/26, 08:17:55",5749.0,11977760.0,0,Could you try to send at least 100 requests and check if it works?,0.0,0.0,1.0,0.0
Jaeger,64532290,64484169,0,"2020/10/26, 08:17:55",True,"2020/10/26, 08:17:55",5749.0,11977760.0,0,If you wan't to change the default sampling rate then there is istio  documentation  about that.,0.0,0.0,1.0,0.0
Jaeger,64532290,64484169,0,"2020/10/26, 08:17:55",True,"2020/10/26, 08:17:55",5749.0,11977760.0,0,Customizing Trace sampling,0.0,0.0,1.0,0.0
Jaeger,64532290,64484169,0,"2020/10/26, 08:17:55",True,"2020/10/26, 08:17:55",5749.0,11977760.0,0,The sampling rate option can be used to control what percentage of requests get reported to your tracing system.,0.0,0.0,1.0,0.0
Jaeger,64532290,64484169,0,"2020/10/26, 08:17:55",True,"2020/10/26, 08:17:55",5749.0,11977760.0,0,This should be configured depending upon your traffic in the mesh and the amount of tracing data you want to collect.,0.0,0.061,0.939,0.0772
Jaeger,64532290,64484169,0,"2020/10/26, 08:17:55",True,"2020/10/26, 08:17:55",5749.0,11977760.0,0,The default rate is 1%.,0.0,0.0,1.0,0.0
Jaeger,64532290,64484169,0,"2020/10/26, 08:17:55",True,"2020/10/26, 08:17:55",5749.0,11977760.0,0,"To modify the default random sampling to 50, add the following option to your tracing.yaml file.",0.0,0.0,1.0,0.0
Jaeger,64532290,64484169,0,"2020/10/26, 08:17:55",True,"2020/10/26, 08:17:55",5749.0,11977760.0,0,The sampling rate should be in the range of 0.0 to 100.0 with a precision of 0.01.,0.0,0.0,1.0,0.0
Jaeger,64532290,64484169,0,"2020/10/26, 08:17:55",True,"2020/10/26, 08:17:55",5749.0,11977760.0,0,"For example, to trace 5 requests out of every 10000, use 0.05 as the value here.",0.0,0.146,0.854,0.34
Jaeger,64039073,64036098,0,"2020/09/24, 06:22:45",True,"2020/09/24, 06:22:45",328.0,1221718.0,1,After digging around in the OpenTracing C# .NET Core source ( https://github.com/opentracing-contrib/csharp-netcore ) I figured out how to override the top level Span.OperationName.,0.0,0.087,0.913,0.2023
Jaeger,64039073,64036098,0,"2020/09/24, 06:22:45",True,"2020/09/24, 06:22:45",328.0,1221718.0,1,I had to update my  Startup.ConfigureServices()  call to  services.AddOpenTracing()  to the following:,0.0,0.0,1.0,0.0
Jaeger,63116714,62992614,0,"2020/07/27, 16:44:44",False,"2020/07/27, 16:44:44",43.0,7017126.0,0,I have resolved it after configuring port as 14250 as JaegerGrpcSpanExporter internally uses grpc port which has been configured to 14250 for jaeger-collector,0.0,0.075,0.925,0.1779
Jaeger,62966022,62948787,1,"2020/07/18, 10:50:12",False,"2020/07/18, 10:50:12",669.0,9291851.0,1,"I was studying Opentracing and Jeager and I've used this tutorial to get familiar with the basic possibilities:
 https://github.com/yurishkuro/opentracing-tutorial/tree/master/java",0.0,0.0,1.0,0.0
Jaeger,62966022,62948787,1,"2020/07/18, 10:50:12",False,"2020/07/18, 10:50:12",669.0,9291851.0,1,"If you take a look in the case 1 (Hello World), it explains how to &quot; Annotate the Trace with Tags and Logs &quot;.",0.0,0.0,1.0,0.0
Jaeger,62966022,62948787,1,"2020/07/18, 10:50:12",False,"2020/07/18, 10:50:12",669.0,9291851.0,1,"That would answer your questions 1, 2 and 3, as with that you can add all the info that you would like within spans and logs.",0.0,0.094,0.906,0.3612
Jaeger,62966022,62948787,1,"2020/07/18, 10:50:12",False,"2020/07/18, 10:50:12",669.0,9291851.0,1,"Here is a snippet from the repository (but I'd recommend checking there, as it has a more detailed explanation):",0.0,0.135,0.865,0.3612
Jaeger,62966022,62948787,1,"2020/07/18, 10:50:12",False,"2020/07/18, 10:50:12",669.0,9291851.0,1,"In this case  helloTo  is a variable containing a name, to whom the app will say hello.",0.0,0.0,1.0,0.0
Jaeger,62966022,62948787,1,"2020/07/18, 10:50:12",False,"2020/07/18, 10:50:12",669.0,9291851.0,1,It would create a span tag called hello-to with the value that is coming from the execution.,0.0,0.243,0.757,0.5423
Jaeger,62966022,62948787,1,"2020/07/18, 10:50:12",False,"2020/07/18, 10:50:12",669.0,9291851.0,1,"Below we have an example for the logs case, where the whole  helloStr  message is added to the logs:",0.0,0.0,1.0,0.0
Jaeger,62966022,62948787,1,"2020/07/18, 10:50:12",False,"2020/07/18, 10:50:12",669.0,9291851.0,1,"Regarding the last question, that would be easier, you can use the Jaeger UI to search for the trace you would like, there is a field for that on the top left corner:",0.0,0.197,0.803,0.7269
Jaeger,63082657,62948787,0,"2020/07/25, 02:23:11",False,"2020/07/25, 02:23:11",176.0,1446358.0,1,There you go.,0.0,0.0,1.0,0.0
Jaeger,63082657,62948787,0,"2020/07/25, 02:23:11",False,"2020/07/25, 02:23:11",176.0,1446358.0,1,There are various overloaded methods as follows,0.0,0.0,1.0,0.0
Jaeger,63082657,62948787,0,"2020/07/25, 02:23:11",False,"2020/07/25, 02:23:11",176.0,1446358.0,1,I want to add some fields to span tags so that it's easy to search in JaegerUI.,0.0,0.242,0.758,0.5367
Jaeger,63082657,62948787,0,"2020/07/25, 02:23:11",False,"2020/07/25, 02:23:11",176.0,1446358.0,1,"Jaeger API provides  log  method to log multiple fields that needs to be added to a map, the method signature is as follows,",0.0,0.0,1.0,0.0
Jaeger,63082657,62948787,0,"2020/07/25, 02:23:11",False,"2020/07/25, 02:23:11",176.0,1446358.0,1,"Span log(Map&lt;String, ?&gt; fields);",0.0,0.0,1.0,0.0
Jaeger,63082657,62948787,0,"2020/07/25, 02:23:11",False,"2020/07/25, 02:23:11",176.0,1446358.0,1,eg:,0.0,0.0,1.0,0.0
Jaeger,63082657,62948787,0,"2020/07/25, 02:23:11",False,"2020/07/25, 02:23:11",176.0,1446358.0,1,"spanId and traceId are stored in JaegerSpanContext class, which can be obtained from context() method of Span class.",0.0,0.0,1.0,0.0
Jaeger,63082657,62948787,0,"2020/07/25, 02:23:11",False,"2020/07/25, 02:23:11",176.0,1446358.0,1,There is a search box in the navigation bar of Jaeger UI where you can search traces by trace ID.,0.0,0.0,1.0,0.0
Jaeger,64036080,62420728,0,"2020/09/23, 23:55:37",False,"2020/09/23, 23:55:37",1048.0,1112106.0,1,"What you did is for http 1.x, and it doesn't work for http2/grpc.",0.0,0.0,1.0,0.0
Jaeger,64036080,62420728,0,"2020/09/23, 23:55:37",False,"2020/09/23, 23:55:37",1048.0,1112106.0,1,Please dive into grpc impl in springboot doc.,0.0,0.247,0.753,0.3182
Jaeger,62300680,62273414,0,"2020/06/10, 12:52:33",True,"2020/06/10, 12:52:33",155.0,1222237.0,0,Thanks Yuri.,0.0,0.744,0.256,0.4404
Jaeger,62300680,62273414,0,"2020/06/10, 12:52:33",True,"2020/06/10, 12:52:33",155.0,1222237.0,0,Yes it was a clock issue.,0.0,0.403,0.597,0.4019
Jaeger,62300680,62273414,0,"2020/06/10, 12:52:33",True,"2020/06/10, 12:52:33",155.0,1222237.0,0,"Although the host machine (VM) updated its clock on every unpause, docker for windows did not.",0.0,0.0,1.0,0.0
Jaeger,62300680,62273414,0,"2020/06/10, 12:52:33",True,"2020/06/10, 12:52:33",155.0,1222237.0,0,The timezones were correct for all containers but the times were all out by the exact same amount.,0.0,0.0,1.0,0.0
Jaeger,62300680,62273414,0,"2020/06/10, 12:52:33",True,"2020/06/10, 12:52:33",155.0,1222237.0,0,This must be the docker internal clock that seems to only get updated once on launch and not at the launch of every new container.,0.0,0.0,1.0,0.0
Jaeger,62300680,62273414,0,"2020/06/10, 12:52:33",True,"2020/06/10, 12:52:33",155.0,1222237.0,0,"Although all container clocks were out by the same amount, the windows host machine was correct.",0.0,0.0,1.0,0.0
Jaeger,62300680,62273414,0,"2020/06/10, 12:52:33",True,"2020/06/10, 12:52:33",155.0,1222237.0,0,The messages were arriving but the times/dates were outside the time frame window the UI was displaying.,0.0,0.0,1.0,0.0
Jaeger,62300680,62273414,0,"2020/06/10, 12:52:33",True,"2020/06/10, 12:52:33",155.0,1222237.0,0,If I set a custom date range I'm sure they would appear.,0.0,0.204,0.796,0.3182
Jaeger,62300680,62273414,0,"2020/06/10, 12:52:33",True,"2020/06/10, 12:52:33",155.0,1222237.0,0,The containers must have been out by a few days with the continual stopping and starting and not by just a few hours.,0.074,0.0,0.926,-0.1531
Jaeger,66277887,62273414,0,"2021/02/19, 14:49:58",False,"2021/02/19, 14:49:58",169.0,3301685.0,0,Time drift in docker is a known issue on Mac and Windows OS.,0.0,0.0,1.0,0.0
Jaeger,66277887,62273414,0,"2021/02/19, 14:49:58",False,"2021/02/19, 14:49:58",169.0,3301685.0,0,Check the date/time in a docker container using this (apologies in advance),0.0,0.0,1.0,0.0
Jaeger,66277887,62273414,0,"2021/02/19, 14:49:58",False,"2021/02/19, 14:49:58",169.0,3301685.0,0,or calculate the drift...,0.0,0.0,1.0,0.0
Jaeger,66277887,62273414,0,"2021/02/19, 14:49:58",False,"2021/02/19, 14:49:58",169.0,3301685.0,0,Unfortunately there doesnt seem to be a better solution than occasionally restarting the container.,0.136,0.295,0.568,0.4215
Jaeger,61677933,61565012,0,"2020/05/08, 14:18:10",False,"2020/05/08, 14:18:10",167.0,11335868.0,0,The problem is that your Jaeger collector is not accessible from outside docker network host as you specified in your docker command.,0.114,0.0,0.886,-0.4019
Jaeger,61677933,61565012,0,"2020/05/08, 14:18:10",False,"2020/05/08, 14:18:10",167.0,11335868.0,0,This would only work if your spring boot application is deployed on the host network too.,0.0,0.0,1.0,0.0
Jaeger,61677933,61565012,0,"2020/05/08, 14:18:10",False,"2020/05/08, 14:18:10",167.0,11335868.0,0,Try to run Jaeger as follows:,0.0,0.0,1.0,0.0
Jaeger,61677933,61565012,0,"2020/05/08, 14:18:10",False,"2020/05/08, 14:18:10",167.0,11335868.0,0,It should trigger.,0.0,0.0,1.0,0.0
Jaeger,61171646,61163242,0,"2020/04/12, 15:42:14",False,"2020/04/14, 21:51:42",79.0,9827874.0,0,Have you tried looking at the logs being generated by your pods?,0.0,0.0,1.0,0.0
Jaeger,61171646,61163242,0,"2020/04/12, 15:42:14",False,"2020/04/14, 21:51:42",79.0,9827874.0,0,In my case I got the following,0.0,0.0,1.0,0.0
Jaeger,61171646,61163242,0,"2020/04/12, 15:42:14",False,"2020/04/14, 21:51:42",79.0,9827874.0,0,"ERROR Failed to flush spans in reporter: error sending spans over UDP:
  Error: getaddrinfo ENOTFOUND  http://jaeger-agent , packet size: 984,
  bytes sent: undefined",0.403,0.0,0.597,-0.9029
Jaeger,61171646,61163242,0,"2020/04/12, 15:42:14",False,"2020/04/14, 21:51:42",79.0,9827874.0,0,Changing it to jaeger-agent worked for me.,0.0,0.0,1.0,0.0
Jaeger,61171646,61163242,0,"2020/04/12, 15:42:14",False,"2020/04/14, 21:51:42",79.0,9827874.0,0,Also if it helps I have declared this under my jaeger image in docker-compose.yml:,0.0,0.178,0.822,0.3818
Jaeger,60500953,60489344,0,"2020/03/03, 08:01:57",True,"2020/03/03, 08:01:57",5749.0,11977760.0,2,The answer here is to install istio with  --set values.global.tracer.zipkin.address  as provided in  istio documentation,0.0,0.0,1.0,0.0
Jaeger,60500953,60489344,0,"2020/03/03, 08:01:57",True,"2020/03/03, 08:01:57",5749.0,11977760.0,2,And,0.0,0.0,1.0,0.0
Jaeger,60500953,60489344,0,"2020/03/03, 08:01:57",True,"2020/03/03, 08:01:57",5749.0,11977760.0,2,"Use the original TracingService  setting: service: ""zipkin.istio-system:9411""  as Donato Szilagyi confirmed in comments.",0.0,0.161,0.839,0.3182
Jaeger,60500953,60489344,0,"2020/03/03, 08:01:57",True,"2020/03/03, 08:01:57",5749.0,11977760.0,2,Great!,0.0,1.0,0.0,0.6588
Jaeger,60500953,60489344,0,"2020/03/03, 08:01:57",True,"2020/03/03, 08:01:57",5749.0,11977760.0,2,It works.,0.0,0.0,1.0,0.0
Jaeger,60500953,60489344,0,"2020/03/03, 08:01:57",True,"2020/03/03, 08:01:57",5749.0,11977760.0,2,"And this time I used the original TracingService setting: service: ""zipkin.istio-system:9411"" – Donato Szilagy",0.0,0.173,0.827,0.3182
Jaeger,59678839,59655255,6,"2020/01/10, 11:30:18",False,"2020/01/10, 11:30:18",23024.0,9773937.0,0,"Not sure, but it seems you miss setting the TLS for Cassandra Storage in Azure Cosmos DB.",0.184,0.0,0.816,-0.3359
Jaeger,59678839,59655255,6,"2020/01/10, 11:30:18",False,"2020/01/10, 11:30:18",23024.0,9773937.0,0,"When you use the Cassandra client to connect the Cassandra Storage in Azure Cosmos DB, it will give out the time out error, but if you enable the SSL, the connection works well.",0.052,0.075,0.873,0.2023
Jaeger,59678839,59655255,6,"2020/01/10, 11:30:18",False,"2020/01/10, 11:30:18",23024.0,9773937.0,0,So I think you can try to enable the TLS for Cassandra in your values.yaml following the steps in the Github which provided.,0.0,0.0,1.0,0.0
Jaeger,59578042,59577629,0,"2020/01/03, 13:44:16",False,"2020/01/03, 13:53:57",,,1,A colleague of mine provided the answer...,0.0,0.0,1.0,0.0
Jaeger,59578042,59577629,0,"2020/01/03, 13:44:16",False,"2020/01/03, 13:53:57",,,1,"It was hidden in the Makefile, which hadn't worked for me as I don't use Golang (and it had been more complex than just installing Golang and running it, but I digress...).",0.0,0.0,1.0,0.0
Jaeger,59578042,59577629,0,"2020/01/03, 13:44:16",False,"2020/01/03, 13:53:57",,,1,The following .sh will do the trick.,0.167,0.0,0.833,-0.0516
Jaeger,59578042,59577629,0,"2020/01/03, 13:44:16",False,"2020/01/03, 13:53:57",,,1,"This assumes the query.proto file is a subdirectory from the same location as the script below, under model/proto/api_v2/ (as it appears in the main Jaeger repo).",0.0,0.0,1.0,0.0
Jaeger,59578042,59577629,0,"2020/01/03, 13:44:16",False,"2020/01/03, 13:53:57",,,1,"This will definitely generate the needed Python file, but it will still be missing dependencies.",0.159,0.105,0.737,-0.2382
Jaeger,59380813,59303550,0,"2019/12/17, 21:23:20",False,"2019/12/17, 21:23:20",471.0,2673284.0,3,"Jaeger clients implement so-called  head-based sampling , where a sampling decision is made at the root of the call tree and propagated down the tree along with the trace context.",0.0,0.0,1.0,0.0
Jaeger,59380813,59303550,0,"2019/12/17, 21:23:20",False,"2019/12/17, 21:23:20",471.0,2673284.0,3,"This is done to guarantee consistent sampling of all spans of a given trace (or none of them), because we don't want to make the coin flip at every node and end up with partial/broken traces.",0.034,0.055,0.911,0.1969
Jaeger,59380813,59303550,0,"2019/12/17, 21:23:20",False,"2019/12/17, 21:23:20",471.0,2673284.0,3,Implementing on-error sampling in the head-based sampling system is not really possible.,0.0,0.0,1.0,0.0
Jaeger,59380813,59303550,0,"2019/12/17, 21:23:20",False,"2019/12/17, 21:23:20",471.0,2673284.0,3,"Imaging that your service is calling service A, which returns successfully, and then service B, which returns an error.",0.118,0.14,0.742,0.128
Jaeger,59380813,59303550,0,"2019/12/17, 21:23:20",False,"2019/12/17, 21:23:20",471.0,2673284.0,3,Let's assume the root of the trace was not sampled (because otherwise you'd catch the error normally).,0.144,0.0,0.856,-0.4019
Jaeger,59380813,59303550,0,"2019/12/17, 21:23:20",False,"2019/12/17, 21:23:20",471.0,2673284.0,3,"That means by the time you know of an error from B, the whole sub-tree at A has been already executed and all spans discarded because of the earlier decision not to sample.",0.145,0.0,0.855,-0.6249
Jaeger,59380813,59303550,0,"2019/12/17, 21:23:20",False,"2019/12/17, 21:23:20",471.0,2673284.0,3,The sub-tree at B has also finished executing.,0.0,0.0,1.0,0.0
Jaeger,59380813,59303550,0,"2019/12/17, 21:23:20",False,"2019/12/17, 21:23:20",471.0,2673284.0,3,The only thing you can sample at this point is the spans in the current service.,0.0,0.0,1.0,0.0
Jaeger,59380813,59303550,0,"2019/12/17, 21:23:20",False,"2019/12/17, 21:23:20",471.0,2673284.0,3,You could also implement a reverse propagation of the sampling decision via response to your caller.,0.0,0.0,1.0,0.0
Jaeger,59380813,59303550,0,"2019/12/17, 21:23:20",False,"2019/12/17, 21:23:20",471.0,2673284.0,3,"So in the best case you could end up with a sub-branch of the whole trace sampled, and possible future branches if the trace continues from above (e.g.",0.0,0.147,0.853,0.6666
Jaeger,59380813,59303550,0,"2019/12/17, 21:23:20",False,"2019/12/17, 21:23:20",471.0,2673284.0,3,via retries).,0.0,0.0,1.0,0.0
Jaeger,59380813,59303550,0,"2019/12/17, 21:23:20",False,"2019/12/17, 21:23:20",471.0,2673284.0,3,"But you can never capture the full trace, and sometimes the reason B failed was because A (successfully) returned some data that caused the error later.",0.267,0.0,0.733,-0.8402
Jaeger,59380813,59303550,0,"2019/12/17, 21:23:20",False,"2019/12/17, 21:23:20",471.0,2673284.0,3,"Note that reverse propagation is not supported by the OpenTracing or OpenTelemetry today, but it has been discussed in the last meetings of the W3C Trace Context working group.",0.05,0.0,0.95,-0.1232
Jaeger,59380813,59303550,0,"2019/12/17, 21:23:20",False,"2019/12/17, 21:23:20",471.0,2673284.0,3,"The alternative way to implement sampling is with  tail-based sampling , a technique employed by some of the commercial vendors today, such as Lightstep, DataDog.",0.0,0.0,1.0,0.0
Jaeger,59380813,59303550,0,"2019/12/17, 21:23:20",False,"2019/12/17, 21:23:20",471.0,2673284.0,3,It is also on the roadmap for Jaeger (we're working on it right now at Uber).,0.0,0.0,1.0,0.0
Jaeger,59380813,59303550,0,"2019/12/17, 21:23:20",False,"2019/12/17, 21:23:20",471.0,2673284.0,3,"With tail-based sampling 100% of spans are captured from the application, but only stored in memory in a collection tier, until the full trace is gathered and a sampling decision is made.",0.0,0.0,1.0,0.0
Jaeger,59380813,59303550,0,"2019/12/17, 21:23:20",False,"2019/12/17, 21:23:20",471.0,2673284.0,3,"The decision making code has a lot more information now, including errors, unusual latencies, etc.",0.156,0.0,0.844,-0.34
Jaeger,59380813,59303550,0,"2019/12/17, 21:23:20",False,"2019/12/17, 21:23:20",471.0,2673284.0,3,"If we decide to sample the trace, only then it goes to disk storage, otherwise we evict it from memory, so that we only need to keep spans in memory for a few seconds on average.",0.0,0.0,1.0,0.0
Jaeger,59380813,59303550,0,"2019/12/17, 21:23:20",False,"2019/12/17, 21:23:20",471.0,2673284.0,3,Tail-based sampling imposes heavier performance penalty on the traced applications because 100% of traffic needs to be profiled by tracing instrumentation.,0.188,0.0,0.812,-0.5267
Jaeger,59380813,59303550,0,"2019/12/17, 21:23:20",False,"2019/12/17, 21:23:20",471.0,2673284.0,3,"You can read more about head-based and tail-based sampling either in Chapter 3 of my book ( https://www.shkuro.com/books/2019-mastering-distributed-tracing/ ) or in the awesome paper  ""So, you want to trace your distributed system?",0.0,0.167,0.833,0.6597
Jaeger,59380813,59303550,0,"2019/12/17, 21:23:20",False,"2019/12/17, 21:23:20",471.0,2673284.0,3,"Key design insights from years of practical experience""  by Raja R. Sambasivan, Rodrigo Fonseca, Ilari Shafer, Gregory R. Ganger ( http://www.pdl.cmu.edu/PDL-FTP/SelfStar/CMU-PDL-14-102.pdf ).",0.0,0.0,1.0,0.0
Jaeger,59047238,59044026,1,"2019/11/26, 11:05:04",False,"2019/11/26, 11:05:04",13313.0,524946.0,1,"You can bind it to metrics and logging frameworks, but you don't have to.",0.0,0.0,1.0,0.0
Jaeger,59047238,59044026,1,"2019/11/26, 11:05:04",False,"2019/11/26, 11:05:04",13313.0,524946.0,1,"You can simply just call  cfg.NewTracer() , like in this example:",0.0,0.217,0.783,0.3612
Jaeger,59047238,59044026,1,"2019/11/26, 11:05:04",False,"2019/11/26, 11:05:04",13313.0,524946.0,1,Source:  https://github.com/jaegertracing/jaeger-client-go/blob/3585cc566102e0ea2225177423e3fcc3d2e5fd7a/config/example_test.go#L88-L105,0.0,0.0,1.0,0.0
Jaeger,59047238,59044026,1,"2019/11/26, 11:05:04",False,"2019/11/26, 11:05:04",13313.0,524946.0,1,Check the Jaeger Go Client readme for more information on the metrics/logging integration:  https://github.com/jaegertracing/jaeger-client-go,0.0,0.0,1.0,0.0
Jaeger,59260094,59044026,0,"2019/12/10, 06:10:40",True,"2019/12/10, 06:10:40",471.0,2673284.0,2,Jaeger clients are designed to have a minimum set of dependencies.,0.0,0.0,1.0,0.0
Jaeger,59260094,59044026,0,"2019/12/10, 06:10:40",True,"2019/12/10, 06:10:40",471.0,2673284.0,2,We don't know if your application is using Prometheus metrics or Zap logger.,0.0,0.0,1.0,0.0
Jaeger,59260094,59044026,0,"2019/12/10, 06:10:40",True,"2019/12/10, 06:10:40",471.0,2673284.0,2,This is why  jaeger-client-go  (as well as many other Jaeger clients in other languages) provide two lightweight interfaces for a Logger and MetricsFactory that can be implemented for a specific logs/metrics backend that your application is using.,0.0,0.058,0.942,0.2732
Jaeger,59260094,59044026,0,"2019/12/10, 06:10:40",True,"2019/12/10, 06:10:40",471.0,2673284.0,2,"Of course, the bindings for Prometheus and Zap are already implemented in the  jaeger-lib  and can be included optionally.",0.0,0.0,1.0,0.0
Jaeger,59507206,58947001,1,"2019/12/28, 02:23:31",True,"2019/12/28, 02:23:31",721.0,1563297.0,1,It looks like you have different versions of opentracing.,0.0,0.238,0.762,0.3612
Jaeger,59507206,58947001,1,"2019/12/28, 02:23:31",True,"2019/12/28, 02:23:31",721.0,1563297.0,1,"The spring-starter-jaeger version 2.x upgrade the version of opentracing, so you might have introduced this breaking changes when you upgraded the dependency version.",0.0,0.0,1.0,0.0
Jaeger,64808184,58763972,1,"2020/11/12, 18:51:10",False,"2020/11/12, 18:51:10",163.0,823789.0,0,"Unfortunatelly, the reporter interface is used to report FINISHED spans, it is invoked on JaegerSpan.finish.",0.0,0.0,1.0,0.0
Jaeger,64808184,58763972,1,"2020/11/12, 18:51:10",False,"2020/11/12, 18:51:10",163.0,823789.0,0,I presume this is why it does not appear in logs.,0.0,0.0,1.0,0.0
Jaeger,58919712,58642294,0,"2019/11/18, 19:08:56",False,"2019/11/18, 19:08:56",721.0,1563297.0,0,"If you are using spring boot with auto configuration, the logs printed using log4j will be instrumented and sent automatically in the span.",0.0,0.0,1.0,0.0
Jaeger,59260222,58642294,0,"2019/12/10, 06:26:49",False,"2019/12/10, 06:26:49",471.0,2673284.0,0,"In Go this is not very straightforward, and largely depends on the logging library you use and the interface it provides.",0.0,0.0,1.0,0.0
Jaeger,59260222,58642294,0,"2019/12/10, 06:26:49",False,"2019/12/10, 06:26:49",471.0,2673284.0,0,One example is implemented in the  HotROD demo  in the Jaeger repository and it is described in the  blog post accompanying the demo .,0.0,0.0,1.0,0.0
Jaeger,59260222,58642294,0,"2019/12/10, 06:26:49",False,"2019/12/10, 06:26:49",471.0,2673284.0,0,It uses  go.uber.org/zap  logging library underneath and allows to write log statements accepting the Context argument:,0.131,0.136,0.733,0.0258
Jaeger,59260222,58642294,0,"2019/12/10, 06:26:49",False,"2019/12/10, 06:26:49",471.0,2673284.0,0,"Behind the scenes  logger.For(ctx)  captures the current tracing spans and adds all log statements to that span, in addition to sending then to  stdout  as usual.",0.0,0.0,1.0,0.0
Jaeger,59260222,58642294,0,"2019/12/10, 06:26:49",False,"2019/12/10, 06:26:49",471.0,2673284.0,0,"To my knowledge,  go.uber.org/zap  does not yet support this mode of logging natively, and therefore requires a wrapper.",0.124,0.0,0.876,-0.3089
Jaeger,59260222,58642294,0,"2019/12/10, 06:26:49",False,"2019/12/10, 06:26:49",471.0,2673284.0,0,"If you use another logging library, than on the high level this is what needs to happen:",0.0,0.0,1.0,0.0
Jaeger,58696749,58504344,2,"2019/11/04, 17:45:56",False,"2019/11/04, 17:45:56",721.0,1563297.0,1,spring-cloud-openfeign  since is from the spring-cloud family should be instrumented automatically once you add  opentracing-spring-jaeger-cloud-starter  the  as stated  here .,0.0,0.0,1.0,0.0
Jaeger,58696749,58504344,2,"2019/11/04, 17:45:56",False,"2019/11/04, 17:45:56",721.0,1563297.0,1,"But sometimes (depending on how you create your feign client bean) you need to explicitly expose the bean to the spring context, so that the autoconfiguration can instrument your Feign Client.",0.057,0.079,0.864,0.1901
Jaeger,58696749,58504344,2,"2019/11/04, 17:45:56",False,"2019/11/04, 17:45:56",721.0,1563297.0,1,Something like this:,0.0,0.556,0.444,0.3612
Jaeger,58696749,58504344,2,"2019/11/04, 17:45:56",False,"2019/11/04, 17:45:56",721.0,1563297.0,1,it is kotlin but you can adapt.,0.0,0.0,1.0,0.0
Jaeger,58440891,58406367,0,"2019/10/18, 00:21:43",True,"2019/10/18, 00:21:43",105.0,8754016.0,1,This was due to the helidon dependency.,0.0,0.0,1.0,0.0
Jaeger,58440891,58406367,0,"2019/10/18, 00:21:43",True,"2019/10/18, 00:21:43",105.0,8754016.0,1,https://helidon.io/docs/latest/#/guides/03_quickstart-mp,0.0,0.0,1.0,0.0
Jaeger,58440891,58406367,0,"2019/10/18, 00:21:43",True,"2019/10/18, 00:21:43",105.0,8754016.0,1,Also I had to upgrade  opentracing-api  version to  0.33.0,0.0,0.0,1.0,0.0
Jaeger,59260378,58134618,0,"2019/12/10, 06:43:54",True,"2019/12/10, 06:43:54",471.0,2673284.0,1,"If you are already using Istio in the deployment, then enabling tracing in it will provide more complete picture of request processing, such as accounting for the time spent in the network between the proxies.",0.0,0.0,1.0,0.0
Jaeger,59260378,58134618,0,"2019/12/10, 06:43:54",True,"2019/12/10, 06:43:54",471.0,2673284.0,1,"You also don't need to have full tracing instrumentation in your services as long as they pass through certain headers, then Istio can still provide a pretty accurate picture of the traces (but you cannot capture any business specific data in the traces).",0.0,0.117,0.883,0.6486
Jaeger,59260378,58134618,0,"2019/12/10, 06:43:54",True,"2019/12/10, 06:43:54",471.0,2673284.0,1,"Traces generated by Istio will have standardized span names that you can use to reason about the SLAs across the whole infrastructure, whereas explicit tracing instrumentation inside the services can often use different naming schemes, especially when services are written in different languages and using different frameworks.",0.0,0.0,1.0,0.0
Jaeger,59260378,58134618,0,"2019/12/10, 06:43:54",True,"2019/12/10, 06:43:54",471.0,2673284.0,1,"For the best of both worlds, I would recommend adding instrumentation inside the services for full fidelity, and also enabling tracing in Istio to capture full picture of request execution (and all network latencies).",0.0,0.178,0.822,0.7717
Jaeger,58058871,57870219,0,"2019/09/23, 11:55:31",False,"2019/09/23, 11:55:31",13313.0,524946.0,0,You most likely have a mismatch with your OpenTracing libraries.,0.0,0.0,1.0,0.0
Jaeger,58058871,57870219,0,"2019/09/23, 11:55:31",False,"2019/09/23, 11:55:31",13313.0,524946.0,0,It looks like your Servlet Filter integration (io.opentracing.contrib.web.servlet.filter.TracingFilter) is making use of a method that doesn't exist.,0.0,0.143,0.857,0.3612
Jaeger,57855614,57854200,3,"2019/09/09, 17:02:34",True,"2019/09/09, 17:02:34",3892.0,3603660.0,2,You can use opentracing  java-jdbc  extension  it will works in Quarkus (I didn't test the native mode).,0.0,0.0,1.0,0.0
Jaeger,57855614,57854200,3,"2019/09/09, 17:02:34",True,"2019/09/09, 17:02:34",3892.0,3603660.0,2,You need to use the version 0.0.12 as the latest one is based on Opentracing 0.33 but Quarkus use the version 0.31.,0.0,0.0,1.0,0.0
Jaeger,57855614,57854200,3,"2019/09/09, 17:02:34",True,"2019/09/09, 17:02:34",3892.0,3603660.0,2,Add the dependency to your pom.xml:,0.0,0.0,1.0,0.0
Jaeger,57855614,57854200,3,"2019/09/09, 17:02:34",True,"2019/09/09, 17:02:34",3892.0,3603660.0,2,"Update your application.properties to use the opentracing-jdbc driver, the following are for a Postgres database:",0.0,0.0,1.0,0.0
Jaeger,57855614,57854200,3,"2019/09/09, 17:02:34",True,"2019/09/09, 17:02:34",3892.0,3603660.0,2,You will then saw the SQL queries in Jaeger as spans.,0.0,0.0,1.0,0.0
Jaeger,57679113,57608146,1,"2019/08/27, 19:45:11",False,"2019/08/27, 19:45:11",55.0,11863447.0,1,What I eventually did is create a JaegerTraces and annotated with Bean,0.0,0.189,0.811,0.2732
Jaeger,66138678,57608146,0,"2021/02/10, 16:19:25",True,"2021/02/10, 16:19:25",12820.0,5277820.0,0,"Apache Camel doesn't provide an implementation of  OpenTracing , so you have to add also an implementation to your dependencies.",0.0,0.0,1.0,0.0
Jaeger,66138678,57608146,0,"2021/02/10, 16:19:25",True,"2021/02/10, 16:19:25",12820.0,5277820.0,0,For example  Jaeger .,0.0,0.0,1.0,0.0
Jaeger,66138678,57608146,0,"2021/02/10, 16:19:25",True,"2021/02/10, 16:19:25",12820.0,5277820.0,0,Maven POM:,0.0,0.0,1.0,0.0
Jaeger,66138678,57608146,0,"2021/02/10, 16:19:25",True,"2021/02/10, 16:19:25",12820.0,5277820.0,0,"Also you have to enable OpenTracing for Apache Camel on your Spring Boot application class, see  Spring Boot :",0.0,0.0,1.0,0.0
Jaeger,66138678,57608146,0,"2021/02/10, 16:19:25",True,"2021/02/10, 16:19:25",12820.0,5277820.0,0,"If you are using Spring Boot then you can add the  camel-opentracing-starter  dependency, and turn on OpenTracing by annotating the main class with  @CamelOpenTracing .",0.0,0.0,1.0,0.0
Jaeger,66138678,57608146,0,"2021/02/10, 16:19:25",True,"2021/02/10, 16:19:25",12820.0,5277820.0,0,"The Tracer will be implicitly obtained from the camel context’s Registry, or the ServiceLoader, unless a Tracer bean has been defined by the application.",0.0,0.0,1.0,0.0
Jaeger,66138678,57608146,0,"2021/02/10, 16:19:25",True,"2021/02/10, 16:19:25",12820.0,5277820.0,0,Spring Boot application class:,0.0,0.0,1.0,0.0
Jaeger,55247113,55239593,1,"2019/03/19, 19:47:45",False,"2019/03/19, 19:47:45",76.0,1496147.0,1,"Could you try using a more recent version of Jaeger:  https://www.jaegertracing.io/docs/latest/getting-started/#all-in-one  - actually 1.11 is now out, so could try that.",0.0,0.0,1.0,0.0
Jaeger,55255528,55239593,0,"2019/03/20, 09:29:23",True,"2019/03/20, 09:29:23",48481.0,2504224.0,3,The problem is that you are using  RestTemplate template = new RestTemplate();  to get an instance of the  RestTemplate  to make a REST call.,0.114,0.0,0.886,-0.4019
Jaeger,55255528,55239593,0,"2019/03/20, 09:29:23",True,"2019/03/20, 09:29:23",48481.0,2504224.0,3,Doing that means that Opentracing cannot instrument the call to add necessary HTTP headers.,0.0,0.0,1.0,0.0
Jaeger,55255528,55239593,0,"2019/03/20, 09:29:23",True,"2019/03/20, 09:29:23",48481.0,2504224.0,3,Please consider using  @Autowired RestTemplate restTemplate,0.0,0.315,0.685,0.3182
Jaeger,55225031,55216969,2,"2019/03/18, 17:41:35",True,"2019/03/18, 17:41:35",48481.0,2504224.0,0,"While doing  mvnDebug quarkus:dev  (without  jvm.args ) and placing a breakpoint  here , I see that you all your params are being passed except  quarkus.jaeger.sampler.parameter  which is wrong.",0.119,0.0,0.881,-0.4767
Jaeger,55225031,55216969,2,"2019/03/18, 17:41:35",True,"2019/03/18, 17:41:35",48481.0,2504224.0,0,It should be  quarkus.jaeger.sampler.param,0.0,0.0,1.0,0.0
Jaeger,53949856,53914858,5,"2018/12/27, 21:21:46",True,"2019/01/01, 03:38:13",6500.0,9521610.0,1,When you are accessing the service from the pod in the  same namespace  you can use just the service name.,0.0,0.0,1.0,0.0
Jaeger,53949856,53914858,5,"2018/12/27, 21:21:46",True,"2019/01/01, 03:38:13",6500.0,9521610.0,1,Example:,0.0,0.0,1.0,0.0
Jaeger,53949856,53914858,5,"2018/12/27, 21:21:46",True,"2019/01/01, 03:38:13",6500.0,9521610.0,1,If you are accessing the service from the pod in the  different namespace  you should also specify the namespace.,0.0,0.0,1.0,0.0
Jaeger,53949856,53914858,5,"2018/12/27, 21:21:46",True,"2019/01/01, 03:38:13",6500.0,9521610.0,1,Example:,0.0,0.0,1.0,0.0
Jaeger,53949856,53914858,5,"2018/12/27, 21:21:46",True,"2019/01/01, 03:38:13",6500.0,9521610.0,1,"To check in what namespace the service is located, use the following command:",0.0,0.0,1.0,0.0
Jaeger,53949856,53914858,5,"2018/12/27, 21:21:46",True,"2019/01/01, 03:38:13",6500.0,9521610.0,1,Note : Changing ConfigMap does not apply it to deployment instantly.,0.0,0.0,1.0,0.0
Jaeger,53949856,53914858,5,"2018/12/27, 21:21:46",True,"2019/01/01, 03:38:13",6500.0,9521610.0,1,"Usually, you need to restart all pods in the deployment to apply new ConfigMap values.",0.0,0.162,0.838,0.4019
Jaeger,53949856,53914858,5,"2018/12/27, 21:21:46",True,"2019/01/01, 03:38:13",6500.0,9521610.0,1,"There is no rolling-restart functionality at the moment, but you can use the following command as a workaround: 
 (replace deployment name and pod name with the real ones)",0.058,0.0,0.942,-0.1531
Jaeger,54367583,53709029,0,"2019/01/25, 16:46:35",False,"2019/01/25, 16:46:35",13313.0,524946.0,0,"I don't think it's possible at the moment and you should definitely ask for this feature in the mailing list, Gitter or GitHub issue.",0.0,0.109,0.891,0.4019
Jaeger,54367583,53709029,0,"2019/01/25, 16:46:35",False,"2019/01/25, 16:46:35",13313.0,524946.0,0,"The current assumption is that a clear TChannel connection can be made between the agent and collector(s), all being part of the same trusted network.",0.0,0.206,0.794,0.6908
Jaeger,54367583,53709029,0,"2019/01/25, 16:46:35",False,"2019/01/25, 16:46:35",13313.0,524946.0,0,"If you are using the Java, Node or C# client, my recommendation in your situation is to have your Jaeger Client to talk directly to the collector.",0.0,0.0,1.0,0.0
Jaeger,54367583,53709029,0,"2019/01/25, 16:46:35",False,"2019/01/25, 16:46:35",13313.0,524946.0,0,Look for the env var  JAEGER_ENDPOINT  in the  Client Features  documentation page.,0.0,0.0,1.0,0.0
Jaeger,53537859,53518480,0,"2018/11/29, 13:19:37",True,"2018/11/29, 13:19:37",321.0,4125383.0,2,"The problem was, that the Report instance used a NoopSender -- thus ignoring the connection settings.",0.293,0.0,0.707,-0.6597
Jaeger,53537859,53518480,0,"2018/11/29, 13:19:37",True,"2018/11/29, 13:19:37",321.0,4125383.0,2,Using,0.0,0.0,1.0,0.0
Jaeger,53537859,53518480,0,"2018/11/29, 13:19:37",True,"2018/11/29, 13:19:37",321.0,4125383.0,2,in your POM will provide an appropriate Sender to the SenderFactory used by Jaeger's SenderResolver::resolve method.,0.0,0.0,1.0,0.0
Jaeger,53537859,53518480,0,"2018/11/29, 13:19:37",True,"2018/11/29, 13:19:37",321.0,4125383.0,2,This solved my problem.,0.397,0.309,0.294,-0.1531
Jaeger,52813216,52764397,2,"2018/10/15, 12:05:59",False,"2018/10/15, 12:05:59",13313.0,524946.0,0,The Jaeger tracer (the part that runs along with your application) will send spans via UDP to an agent running on localhost by default.,0.0,0.0,1.0,0.0
Jaeger,52813216,52764397,2,"2018/10/15, 12:05:59",False,"2018/10/15, 12:05:59",13313.0,524946.0,0,"If your agent is somewhere else, set the  JAEGER_AGENT_HOST / JAEGER_AGENT_PORT  env vars accordingly.",0.0,0.0,1.0,0.0
Jaeger,52813216,52764397,2,"2018/10/15, 12:05:59",False,"2018/10/15, 12:05:59",13313.0,524946.0,0,"If you don't want an agent running on localhost and want to access a Jaeger Collector directly via HTTP, then set the  JAEGER_ENDPOINT  env var.",0.1,0.0,0.9,-0.1139
Jaeger,52813216,52764397,2,"2018/10/15, 12:05:59",False,"2018/10/15, 12:05:59",13313.0,524946.0,0,"More info about these env vars can be found in the  documentation  or here:
 https://github.com/jaegertracing/jaeger-client-java/tree/master/jaeger-core#configuration-via-environment",0.0,0.0,1.0,0.0
Jaeger,52721877,52628054,0,"2018/10/09, 16:09:20",True,"2018/10/09, 16:09:20",13313.0,524946.0,0,"No,  it cannot , but it wouldn't hurt to open an issue there with this suggestion.",0.088,0.201,0.712,0.4703
Jaeger,57364908,52522049,0,"2019/08/05, 22:22:29",False,"2019/12/18, 12:08:53",5287.0,780798.0,1,There is several components which works together and can fully satisfy your requirement.,0.0,0.215,0.785,0.5095
Jaeger,57364908,52522049,0,"2019/08/05, 22:22:29",False,"2019/12/18, 12:08:53",5287.0,780798.0,1,"Common  opentracing library , consisted of abstract layer for span, tracer, injectors and extractors, etc.",0.0,0.0,1.0,0.0
Jaeger,57364908,52522049,0,"2019/08/05, 22:22:29",False,"2019/12/18, 12:08:53",5287.0,780798.0,1,Official  jaeger-client-csharp .,0.0,0.0,1.0,0.0
Jaeger,57364908,52522049,0,"2019/08/05, 22:22:29",False,"2019/12/18, 12:08:53",5287.0,780798.0,1,"Full list of clients can be found  here , which implement  opentracing abstraction layer  mentioned earlier.",0.0,0.0,1.0,0.0
Jaeger,57364908,52522049,0,"2019/08/05, 22:22:29",False,"2019/12/18, 12:08:53",5287.0,780798.0,1,"The final piece is the  OpenTracing API for .NET , which is glue between  opentracing library  and  DiagnosticSource  concept in dotnet.",0.0,0.0,1.0,0.0
Jaeger,57364908,52522049,0,"2019/08/05, 22:22:29",False,"2019/12/18, 12:08:53",5287.0,780798.0,1,"Actually, the final library has  sample  which uses jaeger csharp implementation of ITracer and configure it as default GlobalTracer.",0.0,0.0,1.0,0.0
Jaeger,57364908,52522049,0,"2019/08/05, 22:22:29",False,"2019/12/18, 12:08:53",5287.0,780798.0,1,"At the rest in your Startup.cs, you will end up with something like from that sample (services is IServiceCollection):",0.0,0.122,0.878,0.3612
Jaeger,51977735,51878525,0,"2018/08/23, 05:56:58",True,"2018/08/23, 05:56:58",4269.0,1047335.0,1,I resolved this.,0.0,0.63,0.37,0.1779
Jaeger,51977735,51878525,0,"2018/08/23, 05:56:58",True,"2018/08/23, 05:56:58",4269.0,1047335.0,1,It related to the sample rate.,0.0,0.0,1.0,0.0
Jaeger,51977735,51878525,0,"2018/08/23, 05:56:58",True,"2018/08/23, 05:56:58",4269.0,1047335.0,1,"After I configured the  JAEGER_SAMPLER_TYPE  and  JAEGER_SAMPLER_PARAM , I can see the data.",0.0,0.0,1.0,0.0
Jaeger,51792533,51720184,0,"2018/08/10, 21:53:47",False,"2018/08/10, 22:38:32",373.0,10206966.0,3,"In server 2 , Install jaeger",0.0,0.0,1.0,0.0
Jaeger,51792533,51720184,0,"2018/08/10, 21:53:47",False,"2018/08/10, 22:38:32",373.0,10206966.0,3,"In server 1, set these environment variables.",0.0,0.0,1.0,0.0
Jaeger,51792533,51720184,0,"2018/08/10, 21:53:47",False,"2018/08/10, 22:38:32",373.0,10206966.0,3,"Change the tracer registration application code as below in server 1, so that it will get the configurations from the environment variables.",0.0,0.0,1.0,0.0
Jaeger,51792533,51720184,0,"2018/08/10, 21:53:47",False,"2018/08/10, 22:38:32",373.0,10206966.0,3,Hope this works!,0.0,0.615,0.385,0.4926
Jaeger,51792533,51720184,0,"2018/08/10, 21:53:47",False,"2018/08/10, 22:38:32",373.0,10206966.0,3,Check this link for integrating elasticsearch as the persistence storage backend so that the traces will not remove once the Jaeger instance is stopped.,0.076,0.0,0.924,-0.2263
Jaeger,51792533,51720184,0,"2018/08/10, 21:53:47",False,"2018/08/10, 22:38:32",373.0,10206966.0,3,How to configure Jaeger with elasticsearch?,0.0,0.0,1.0,0.0
Jaeger,49311159,49179249,0,"2018/03/16, 01:55:29",True,"2018/03/16, 01:55:29",1116.0,1953109.0,2,I finally figured this out after trying out different combinations.,0.0,0.0,1.0,0.0
Jaeger,49311159,49179249,0,"2018/03/16, 01:55:29",True,"2018/03/16, 01:55:29",1116.0,1953109.0,2,This is happening because Jaeger agent is not receiving any UDP packets from my application.,0.0,0.0,1.0,0.0
Jaeger,49311159,49179249,0,"2018/03/16, 01:55:29",True,"2018/03/16, 01:55:29",1116.0,1953109.0,2,"You need to tell the tracer where to send UDP packets, which in this case is  docker-machine ip  
I added:",0.0,0.0,1.0,0.0
Jaeger,49311159,49179249,0,"2018/03/16, 01:55:29",True,"2018/03/16, 01:55:29",1116.0,1953109.0,2,and then I was able to see my services in Jaeger UI.,0.0,0.0,1.0,0.0
Jaeger,48432561,48095718,0,"2018/01/25, 00:12:09",False,"2018/01/25, 00:12:09",471.0,2673284.0,1,Service graph data must be generated in Jaeger.,0.0,0.0,1.0,0.0
Jaeger,48432561,48095718,0,"2018/01/25, 00:12:09",False,"2018/01/25, 00:12:09",471.0,2673284.0,1,Currently it's possible with via a Spark job here:  https://github.com/jaegertracing/spark-dependencies,0.0,0.192,0.808,0.2263
Jaeger,48432603,46561079,0,"2018/01/25, 00:16:25",True,"2019/12/10, 06:06:47",471.0,2673284.0,2,"The Downloads page ( https://www.jaegertracing.io/download/ ) lists both the Docker images and the raw binaries built for various platforms (Linux, macOS, windows).",0.0,0.0,1.0,0.0
Jaeger,48432603,46561079,0,"2018/01/25, 00:16:25",True,"2019/12/10, 06:06:47",471.0,2673284.0,2,You can also build binaries from source.,0.0,0.0,1.0,0.0
Jaeger,66277779,46561079,0,"2021/02/19, 14:43:21",False,"2021/02/19, 14:43:21",169.0,3301685.0,0,"Just to add to Yuris answer, you can also download the source from github -  Github - Jaeger  This is useful for diagnosing issues, or just getting a better understanding of how it all works.",0.0,0.162,0.838,0.7003
Jaeger,66277779,46561079,0,"2021/02/19, 14:43:21",False,"2021/02/19, 14:43:21",169.0,3301685.0,0,I have run both the released apps and custom versions on both windows and linux servers without issues.,0.0,0.0,1.0,0.0
Jaeger,66277779,46561079,0,"2021/02/19, 14:43:21",False,"2021/02/19, 14:43:21",169.0,3301685.0,0,For windows I would recommend running as a service using Nssm.,0.0,0.238,0.762,0.3612
Jaeger,66277779,46561079,0,"2021/02/19, 14:43:21",False,"2021/02/19, 14:43:21",169.0,3301685.0,0,Nssm details,0.0,0.0,1.0,0.0
Jaeger,66277685,66236610,0,"2021/02/19, 14:35:11",False,"2021/02/19, 14:35:11",169.0,3301685.0,0,Elastic search works fine for this.,0.0,0.265,0.735,0.2023
Jaeger,66277685,66236610,0,"2021/02/19, 14:35:11",False,"2021/02/19, 14:35:11",169.0,3301685.0,0,And Kibana allows you to build nice aggregated views of the traffic.,0.0,0.203,0.797,0.4215
Jaeger,66277685,66236610,0,"2021/02/19, 14:35:11",False,"2021/02/19, 14:35:11",169.0,3301685.0,0,A recommendation from my experience is to use the  --es.tags-as-fields.dot-replacement  option and specify a character.,0.0,0.0,1.0,0.0
Jaeger,66277685,66236610,0,"2021/02/19, 14:35:11",False,"2021/02/19, 14:35:11",169.0,3301685.0,0,This flattens the data structure.,0.0,0.0,1.0,0.0
Jaeger,66277685,66236610,0,"2021/02/19, 14:35:11",False,"2021/02/19, 14:35:11",169.0,3301685.0,0,Its very useful because ElasticSearch/Kibana struggle with the tags data as an array.,0.139,0.194,0.667,0.2247
Jaeger,65888740,65490691,0,"2021/01/25, 18:32:56",True,"2021/01/25, 18:32:56",1869.0,3511252.0,1,I had the same problem.,0.474,0.0,0.526,-0.4019
Jaeger,65888740,65490691,0,"2021/01/25, 18:32:56",True,"2021/01/25, 18:32:56",1869.0,3511252.0,1,Found this page that explains how to configure Thrift sender:  https://github.com/jaegertracing/jaeger-client-csharp/blob/master/src/Senders/Jaeger.Senders.Thrift/README.md,0.0,0.0,1.0,0.0
Jaeger,65888740,65490691,0,"2021/01/25, 18:32:56",True,"2021/01/25, 18:32:56",1869.0,3511252.0,1,The C# tutorial does not mention it though ...,0.0,0.0,1.0,0.0
Jaeger,65888740,65490691,0,"2021/01/25, 18:32:56",True,"2021/01/25, 18:32:56",1869.0,3511252.0,1,And here is my InitTracer().,0.0,0.0,1.0,0.0
Jaeger,65888740,65490691,0,"2021/01/25, 18:32:56",True,"2021/01/25, 18:32:56",1869.0,3511252.0,1,Works fine with Jaeger launched from binary:,0.0,0.398,0.602,0.3182
Jaeger,65207511,65149873,0,"2020/12/08, 23:57:48",False,"2020/12/08, 23:57:48",357.0,3703933.0,1,I solved it by using this library instead  https://github.com/opentracing-contrib/java-spring-cloud,0.0,0.231,0.769,0.2732
Jaeger,65207511,65149873,0,"2020/12/08, 23:57:48",False,"2020/12/08, 23:57:48",357.0,3703933.0,1,It seem to have an option to enable or disable different instrumentation feature.,0.0,0.0,1.0,0.0
Jaeger,65207511,65149873,0,"2020/12/08, 23:57:48",False,"2020/12/08, 23:57:48",357.0,3703933.0,1,Read about  opentracing.spring.cloud.async.enabled  for more info.,0.0,0.0,1.0,0.0
Jaeger,65059275,65059109,1,"2020/11/29, 12:48:21",True,"2020/11/29, 12:48:21",1058.0,1609014.0,0,"Looks like your DaemonSet misses the  hostNetwork  property, to be able to listen on the node IP.",0.098,0.129,0.773,0.1531
Jaeger,65059275,65059109,1,"2020/11/29, 12:48:21",True,"2020/11/29, 12:48:21",1058.0,1609014.0,0,You can check that article for further info:  https://medium.com/@masroor.hasan/tracing-infrastructure-with-jaeger-on-kubernetes-6800132a677,0.0,0.0,1.0,0.0
Jaeger,65299081,65056880,0,"2020/12/15, 04:11:35",False,"2020/12/15, 04:11:35",471.0,2673284.0,0,You have two options:,0.0,0.0,1.0,0.0
Jaeger,65299081,65056880,0,"2020/12/15, 04:11:35",False,"2020/12/15, 04:11:35",471.0,2673284.0,0,"For (2), you can pass the environment variable to you applications:",0.0,0.0,1.0,0.0
Jaeger,65299081,65056880,0,"2020/12/15, 04:11:35",False,"2020/12/15, 04:11:35",471.0,2673284.0,0,Additional references:,0.0,0.0,1.0,0.0
Jaeger,63654371,63634257,3,"2020/08/30, 08:47:00",True,"2020/08/30, 08:47:00",366.0,10202496.0,1,"By default, OpenTracing doesn't log automatically into span logs, only important messages that Jaeger feels it needs to be logged and is needed for tracing would be there :).",0.0,0.06,0.94,0.2023
Jaeger,63654371,63634257,3,"2020/08/30, 08:47:00",True,"2020/08/30, 08:47:00",366.0,10202496.0,1,"The idea is to separate responsibilities between Tracing and Log management,  Check this GitHub discussion .",0.0,0.0,1.0,0.0
Jaeger,63654371,63634257,3,"2020/08/30, 08:47:00",True,"2020/08/30, 08:47:00",366.0,10202496.0,1,An alternative would be to use centralized log management and print traceId &amp; spanId into your logs for troubleshooting and correlating logs and tracing.,0.0,0.069,0.931,0.1779
Jaeger,63686699,63634257,0,"2020/09/01, 14:23:48",False,"2020/09/01, 14:23:48",363.0,977959.0,0,"As iabughosh said, the main focus on jaeger is traceability, monitoring and performance, not for logging.",0.0,0.0,1.0,0.0
Jaeger,63686699,63634257,0,"2020/09/01, 14:23:48",False,"2020/09/01, 14:23:48",363.0,977959.0,0,"Anyway, i found that using the @traced Bean injection you can insert a tag into the current span, that will be printed on Jaeger UI.",0.0,0.0,1.0,0.0
Jaeger,63686699,63634257,0,"2020/09/01, 14:23:48",False,"2020/09/01, 14:23:48",363.0,977959.0,0,this example will added the first 4 lines of an excpetion to the Tags seccion.,0.0,0.0,1.0,0.0
Jaeger,63686699,63634257,0,"2020/09/01, 14:23:48",False,"2020/09/01, 14:23:48",363.0,977959.0,0,(I use it on my global ExceptionHandler to add more info about the error):,0.0,0.0,1.0,0.0
Jaeger,63686699,63634257,0,"2020/09/01, 14:23:48",False,"2020/09/01, 14:23:48",363.0,977959.0,0,},0.0,0.0,0.0,0.0
Jaeger,63686699,63634257,0,"2020/09/01, 14:23:48",False,"2020/09/01, 14:23:48",363.0,977959.0,0,and you will see the little stacktrace at JaegerUI.,0.0,0.0,1.0,0.0
Jaeger,63686699,63634257,0,"2020/09/01, 14:23:48",False,"2020/09/01, 14:23:48",363.0,977959.0,0,Hope helps,0.0,1.0,0.0,0.6705
Jaeger,55568365,55220796,0,"2019/04/08, 10:37:58",False,"2019/04/08, 10:37:58",76.0,1496147.0,0,Did you install the operator on openshift using the instructions listed:  https://github.com/jaegertracing/jaeger-operator#openshift  ?,0.0,0.0,1.0,0.0
Jaeger,55568365,55220796,0,"2019/04/08, 10:37:58",False,"2019/04/08, 10:37:58",76.0,1496147.0,0,"Did the operator start up ok, if you not were there errors in the log?",0.0,0.246,0.754,0.5
Jaeger,55568365,55220796,0,"2019/04/08, 10:37:58",False,"2019/04/08, 10:37:58",76.0,1496147.0,0,"The  Creating a new Jaeger Instance  section starts with a link to some examples, including  simple-prod.yaml , which creates a Jaeger instance that uses an Elasticsearch cluster at the specified URL.",0.0,0.147,0.853,0.5106
Jaeger,55568365,55220796,0,"2019/04/08, 10:37:58",False,"2019/04/08, 10:37:58",76.0,1496147.0,0,You simply run:,0.0,0.0,1.0,0.0
Jaeger,67077099,67003774,0,"2021/04/13, 17:44:16",False,"2021/04/13, 17:44:16",193.0,3774803.0,0,It doesn't work in golang grpc client.,0.0,0.0,1.0,0.0
Jaeger,67077099,67003774,0,"2021/04/13, 17:44:16",False,"2021/04/13, 17:44:16",193.0,3774803.0,0,I used openTelemetry  load balancing  Another option - use kubernetes to balance requests to backends.,0.0,0.0,1.0,0.0
Jaeger,66621994,66597254,0,"2021/03/14, 09:14:12",False,"2021/03/14, 09:14:12",13.0,15381526.0,1,I realized that I had got into a completely wrong direction.,0.326,0.0,0.674,-0.5256
Jaeger,66621994,66597254,0,"2021/03/14, 09:14:12",False,"2021/03/14, 09:14:12",13.0,15381526.0,1,"I thought that I have to access the backend storage to get the trace data, which actually make the problem much more complex.",0.119,0.0,0.881,-0.4019
Jaeger,66621994,66597254,0,"2021/03/14, 09:14:12",False,"2021/03/14, 09:14:12",13.0,15381526.0,1,I got the answer from github discussion and here is the address  https://github.com/jaegertracing/jaeger/discussions/2876#discussioncomment-477176,0.0,0.0,1.0,0.0
Jaeger,65650224,65595635,0,"2021/01/10, 06:52:21",False,"2021/01/10, 06:52:21",31.0,3289465.0,0,Can you paste the Collector config file?,0.0,0.0,1.0,0.0
Jaeger,65650224,65595635,0,"2021/01/10, 06:52:21",False,"2021/01/10, 06:52:21",31.0,3289465.0,0,It seems you are using the gRPC protocol and it's not supported on the system where the collector is running.,0.094,0.0,0.906,-0.2411
Jaeger,65650224,65595635,0,"2021/01/10, 06:52:21",False,"2021/01/10, 06:52:21",31.0,3289465.0,0,https://github.com/open-telemetry/opentelemetry-collector/blob/master/exporter/otlpexporter/README.md,0.0,0.0,1.0,0.0
Jaeger,65910568,65595635,1,"2021/01/27, 01:06:48",True,"2021/01/27, 01:06:48",483.0,4895267.0,0,gRPC port isn't enabled in your jaeger instance.,0.0,0.0,1.0,0.0
Jaeger,65910568,65595635,1,"2021/01/27, 01:06:48",True,"2021/01/27, 01:06:48",483.0,4895267.0,0,You can try a docker-compose file like this,0.0,0.294,0.706,0.3612
Jaeger,65910568,65595635,1,"2021/01/27, 01:06:48",True,"2021/01/27, 01:06:48",483.0,4895267.0,0,And you can connect to it without problems,0.0,0.244,0.756,0.3089
Jaeger,65853125,65202244,0,"2021/01/22, 23:41:07",False,"2021/01/22, 23:41:07",483.0,4895267.0,0,Remove your dependencies and use the following one that will include also the instrumentation you need,0.0,0.0,1.0,0.0
Jaeger,65041185,65040519,0,"2020/11/27, 19:06:58",True,"2020/11/27, 19:06:58",328.0,1221718.0,0,I figured it out... the Jaeger Operator doesn't create a  Service  exposing the metrics endpoints.,0.124,0.124,0.752,0.0
Jaeger,65041185,65040519,0,"2020/11/27, 19:06:58",True,"2020/11/27, 19:06:58",328.0,1221718.0,0,These endpoints are just exposed via the pods for the Collector and Query components.,0.091,0.0,0.909,-0.0772
Jaeger,65041185,65040519,0,"2020/11/27, 19:06:58",True,"2020/11/27, 19:06:58",328.0,1221718.0,0,An example from the Collector pod spec:,0.0,0.0,1.0,0.0
Jaeger,65041185,65040519,0,"2020/11/27, 19:06:58",True,"2020/11/27, 19:06:58",328.0,1221718.0,0,Note the  admin-http  port there.,0.0,0.0,1.0,0.0
Jaeger,65041185,65040519,0,"2020/11/27, 19:06:58",True,"2020/11/27, 19:06:58",328.0,1221718.0,0,"So to get the Prometheus Operator to scrape these metrics, I created a  PodMonitor  which covers both the Collector and Query components because both of them have the  labels/app: jaeger  and  admin-http  ports defined:",0.0,0.061,0.939,0.25
Jaeger,64592134,64486524,0,"2020/10/29, 15:30:59",False,"2020/10/29, 15:30:59",51.0,8394088.0,0,referring to the documentation provided in below link helped to resolve the issue.,0.0,0.178,0.822,0.3818
Jaeger,64592134,64486524,0,"2020/10/29, 15:30:59",False,"2020/10/29, 15:30:59",51.0,8394088.0,0,java.lang.IllegalStateException: This should not happen as headers() should only be called while a record is processed,0.0,0.0,1.0,0.0
Jaeger,64481993,64465875,0,"2020/10/22, 15:14:22",True,"2020/10/22, 16:39:57",2287.0,261708.0,1,I got it working as mentioned below,0.0,0.0,1.0,0.0
Jaeger,64481993,64465875,0,"2020/10/22, 15:14:22",True,"2020/10/22, 16:39:57",2287.0,261708.0,1,I would assume that it may not be the right way of exposing the services.,0.139,0.0,0.861,-0.2732
Jaeger,64481993,64465875,0,"2020/10/22, 15:14:22",True,"2020/10/22, 16:39:57",2287.0,261708.0,1,Instead,0.0,0.0,1.0,0.0
Jaeger,64356244,64342820,0,"2020/10/14, 18:06:36",True,"2020/10/14, 18:06:36",39034.0,785745.0,0,This is the simplest working example that I was able to find.,0.0,0.0,1.0,0.0
Jaeger,64356244,64342820,0,"2020/10/14, 18:06:36",True,"2020/10/14, 18:06:36",39034.0,785745.0,0,Here is a more realistic example that builds the tracer from a configuration.,0.0,0.0,1.0,0.0
Jaeger,64536471,64266056,0,"2020/10/26, 13:43:36",False,"2020/10/26, 13:43:36",1157.0,1700378.0,0,"GitLab Helm charts support tracing, and you can configure it with:",0.0,0.213,0.787,0.4019
Jaeger,64536471,64266056,0,"2020/10/26, 13:43:36",False,"2020/10/26, 13:43:36",1157.0,1700378.0,0,For more details refer : https://docs.gitlab.com/charts/charts/globals.html#tracing,0.0,0.0,1.0,0.0
Jaeger,64209717,64201190,0,"2020/10/05, 16:34:52",False,"2020/10/05, 16:34:52",2576.0,12014434.0,1,According to  istio  documentation:,0.0,0.0,1.0,0.0
Jaeger,64209717,64201190,0,"2020/10/05, 16:34:52",False,"2020/10/05, 16:34:52",2576.0,12014434.0,1,"Consult the   Jaeger documentation   to
get started.",0.0,0.0,1.0,0.0
Jaeger,64209717,64201190,0,"2020/10/05, 16:34:52",False,"2020/10/05, 16:34:52",2576.0,12014434.0,1,"No special changes are needed for Jaeger to work with
Istio.",0.158,0.194,0.647,0.128
Jaeger,64209717,64201190,0,"2020/10/05, 16:34:52",False,"2020/10/05, 16:34:52",2576.0,12014434.0,1,"Once Jaeger is installed, you will need to point Istio proxies to send
traces to the deployment.",0.0,0.0,1.0,0.0
Jaeger,64209717,64201190,0,"2020/10/05, 16:34:52",False,"2020/10/05, 16:34:52",2576.0,12014434.0,1,"This can be configured with   --set values.global.tracer.zipkin.address=&lt;jaeger-collector-address&gt;:9411 
at installation time.",0.0,0.0,1.0,0.0
Jaeger,64209717,64201190,0,"2020/10/05, 16:34:52",False,"2020/10/05, 16:34:52",2576.0,12014434.0,1,"See the
 ProxyConfig.Tracing 
for advanced configuration such as TLS settings.",0.0,0.182,0.818,0.25
Jaeger,64209717,64201190,0,"2020/10/05, 16:34:52",False,"2020/10/05, 16:34:52",2576.0,12014434.0,1,Istio documentation states to use jaeger collector address in  global.tracer.zipkin.address .,0.0,0.0,1.0,0.0
Jaeger,64209717,64201190,0,"2020/10/05, 16:34:52",False,"2020/10/05, 16:34:52",2576.0,12014434.0,1,"As for the Jaeger agent host, according to  Jaeger  Operator documentation:",0.0,0.0,1.0,0.0
Jaeger,64209717,64201190,0,"2020/10/05, 16:34:52",False,"2020/10/05, 16:34:52",2576.0,12014434.0,1,"&lt;9&gt; By default, the operator assumes that agents are deployed as
sidecars within the target pods.",0.0,0.0,1.0,0.0
Jaeger,64209717,64201190,0,"2020/10/05, 16:34:52",False,"2020/10/05, 16:34:52",2576.0,12014434.0,1,"Specifying the strategy as
“DaemonSet” changes that and makes the operator deploy the agent as
DaemonSet.",0.0,0.0,1.0,0.0
Jaeger,64209717,64201190,0,"2020/10/05, 16:34:52",False,"2020/10/05, 16:34:52",2576.0,12014434.0,1,"Note that your tracer client will probably have to override
the “JAEGER_AGENT_HOST” environment variable to use the node’s IP.",0.0,0.0,1.0,0.0
Jaeger,64209717,64201190,0,"2020/10/05, 16:34:52",False,"2020/10/05, 16:34:52",2576.0,12014434.0,1,Your tracer client will then most likely need to be told where the agent is located.,0.0,0.0,1.0,0.0
Jaeger,64209717,64201190,0,"2020/10/05, 16:34:52",False,"2020/10/05, 16:34:52",2576.0,12014434.0,1,"This is usually done by setting the environment variable   JAEGER_AGENT_HOST   to the value of the Kubernetes node’s IP, for example:",0.0,0.112,0.888,0.34
Jaeger,63837122,63835165,0,"2020/09/10, 23:16:48",True,"2020/09/10, 23:16:48",147.0,1729409.0,0,Solved by adding dependency in pom file on jaeger-thrift.,0.0,0.208,0.792,0.2732
Jaeger,64240007,63745859,0,"2020/10/07, 11:18:29",True,"2020/10/07, 11:18:29",391.0,6452043.0,1,I enabled instrumentation on the services using those two dependencies:,0.0,0.0,1.0,0.0
Jaeger,64240007,63745859,0,"2020/10/07, 11:18:29",True,"2020/10/07, 11:18:29",391.0,6452043.0,1,"And, I used jaeger-client to configure the tracer using environment variables:",0.0,0.0,1.0,0.0
Jaeger,64240007,63745859,0,"2020/10/07, 11:18:29",True,"2020/10/07, 11:18:29",391.0,6452043.0,1,Getting a Tracer Instance:,0.0,0.0,1.0,0.0
Jaeger,64240007,63745859,0,"2020/10/07, 11:18:29",True,"2020/10/07, 11:18:29",391.0,6452043.0,1,"Finally, in the dropwizard application, you have to register the tracer like so",0.0,0.172,0.828,0.3612
Jaeger,63583340,63581052,1,"2020/08/25, 19:34:35",False,"2020/08/25, 20:14:13",336.0,8095979.0,2,You need to keep double quotes as it is.,0.0,0.0,1.0,0.0
Jaeger,63583340,63581052,1,"2020/08/25, 19:34:35",False,"2020/08/25, 20:14:13",336.0,8095979.0,2,An issue has been identified similar to this [1] and has been fixed recently.,0.0,0.0,1.0,0.0
Jaeger,63583340,63581052,1,"2020/08/25, 19:34:35",False,"2020/08/25, 20:14:13",336.0,8095979.0,2,Can you try to get the latest WUM updated API Manager 3.1.0 and try enabling Jaeger open tracing?,0.0,0.0,1.0,0.0
Jaeger,63583340,63581052,1,"2020/08/25, 19:34:35",False,"2020/08/25, 20:14:13",336.0,8095979.0,2,"Alternatively, this issue will not occur when using &quot;localhost&quot; as the hostname.",0.0,0.0,1.0,0.0
Jaeger,63583340,63581052,1,"2020/08/25, 19:34:35",False,"2020/08/25, 20:14:13",336.0,8095979.0,2,[1]  https://github.com/wso2/product-apim/issues/7940,0.0,0.0,1.0,0.0
Jaeger,63583870,63581052,0,"2020/08/25, 20:08:00",False,"2020/12/03, 10:54:28",2466.0,3176125.0,0,Run Jager using the docker image as follows.,0.0,0.0,1.0,0.0
Jaeger,63583870,63581052,0,"2020/08/25, 20:08:00",False,"2020/12/03, 10:54:28",2466.0,3176125.0,0,Then add the following config to the deployment.toml.,0.0,0.0,1.0,0.0
Jaeger,63583870,63581052,0,"2020/08/25, 20:08:00",False,"2020/12/03, 10:54:28",2466.0,3176125.0,0,Side Note: For zipkin you can use the following.,0.0,0.0,1.0,0.0
Jaeger,63470131,63221267,0,"2020/08/18, 16:51:58",False,"2020/08/18, 16:51:58",213.0,7933630.0,0,"I'm not sure if what you're looking for exists today per se, but you could accomplish this with OpenTelemetry by writing traces through the LogReporter then using a serverless function to read the cloudwatch stream and send it to Jaeger (or to an OpenTelemetry Collector that sends them to Jaeger).",0.028,0.071,0.901,0.4971
Jaeger,63470131,63221267,0,"2020/08/18, 16:51:58",False,"2020/08/18, 16:51:58",213.0,7933630.0,0,You could also write a custom plugin for the OpenTelemetry Collector that read a cloudwatch stream into OTLP then exported it to any endpoint supported by the collector.,0.0,0.084,0.916,0.3182
Jaeger,63218533,63218451,2,"2020/08/02, 19:38:53",False,"2020/08/02, 19:38:53",721.0,1563297.0,0,You are missing the configuration of Jaeger address.,0.239,0.0,0.761,-0.296
Jaeger,63218533,63218451,2,"2020/08/02, 19:38:53",False,"2020/08/02, 19:38:53",721.0,1563297.0,0,"Since you did not provided it, it is trying to connect to the default one, which is TCP protocol,  127.0.0.1  and port 5778.",0.0,0.0,1.0,0.0
Jaeger,63218533,63218451,2,"2020/08/02, 19:38:53",False,"2020/08/02, 19:38:53",721.0,1563297.0,0,Check for details the configuration section  here .,0.0,0.0,1.0,0.0
Jaeger,63198835,63198673,0,"2020/07/31, 23:41:17",True,"2020/07/31, 23:41:17",7166.0,3838328.0,0,You just need to make use of  tags.value  instead of  value  in your match query.,0.0,0.146,0.854,0.34
Jaeger,63198835,63198673,0,"2020/07/31, 23:41:17",True,"2020/07/31, 23:41:17",7166.0,3838328.0,0,Below query should help:,0.0,0.474,0.526,0.4019
Jaeger,62910785,62830150,0,"2020/07/15, 11:34:09",False,"2020/07/15, 11:34:09",184.0,8575474.0,0,"if it throws error then it is unable to reach host, this method is costly I agree but this is the only viable solution I find",0.11,0.207,0.683,0.4019
Jaeger,62141984,62112814,0,"2020/06/02, 01:29:32",False,"2020/06/02, 01:34:32",113.0,4573609.0,0,so I give it a try and my answer to the question above are:,0.0,0.0,1.0,0.0
Jaeger,62141984,62112814,0,"2020/06/02, 01:29:32",False,"2020/06/02, 01:34:32",113.0,4573609.0,0,"Q1) yes it is possible (congrats to the jaeger team, package pretty easy to grasp with a good documentation)",0.0,0.455,0.545,0.8934
Jaeger,62141984,62112814,0,"2020/06/02, 01:29:32",False,"2020/06/02, 01:34:32",113.0,4573609.0,0,"Q2) I did struggle a bit with this one and thanks to  https://github.com/CHOMNANP/jaeger-js-text-map-demo  I implemented a solution by adding a ""textCarrier"" with a ref.",0.102,0.231,0.667,0.4404
Jaeger,62141984,62112814,0,"2020/06/02, 01:29:32",False,"2020/06/02, 01:34:32",113.0,4573609.0,0,"to the span context formatted as ""FORMAT_TEXT_MAP"" to the message Component 1 was publishing towards Component 2.",0.0,0.0,1.0,0.0
Jaeger,62141984,62112814,0,"2020/06/02, 01:29:32",False,"2020/06/02, 01:34:32",113.0,4573609.0,0,Code snipper in C1 on the first API invocation,0.0,0.0,1.0,0.0
Jaeger,62141984,62112814,0,"2020/06/02, 01:29:32",False,"2020/06/02, 01:34:32",113.0,4573609.0,0,followed by this part when sending the msg on redis:,0.0,0.0,1.0,0.0
Jaeger,62141984,62112814,0,"2020/06/02, 01:29:32",False,"2020/06/02, 01:34:32",113.0,4573609.0,0,The getTextCarrierBySpanObject function is coming from  https://github.com/CHOMNANP/jaeger-js-text-map-demo,0.0,0.0,1.0,0.0
Jaeger,62141984,62112814,0,"2020/06/02, 01:29:32",False,"2020/06/02, 01:34:32",113.0,4573609.0,0,Code snippet in C2 receiving the msg from redis,0.0,0.0,1.0,0.0
Jaeger,62141984,62112814,0,"2020/06/02, 01:29:32",False,"2020/06/02, 01:34:32",113.0,4573609.0,0,I tested with version 1.13,0.0,0.0,1.0,0.0
Jaeger,62141984,62112814,0,"2020/06/02, 01:29:32",False,"2020/06/02, 01:34:32",113.0,4573609.0,0,"So all in all a pretty convincing prototyping exercise with Jaeger, will most probably pilot it now on a real project before trying it in production.",0.0,0.211,0.789,0.7096
Jaeger,61666591,61619122,0,"2020/05/07, 22:54:39",True,"2020/05/07, 22:54:39",13.0,7069613.0,0,According to  https://helm.sh/docs/chart_template_guide/control_structures/  a string is converted to a boolean of True.,0.0,0.237,0.763,0.4215
Jaeger,61666591,61619122,0,"2020/05/07, 22:54:39",True,"2020/05/07, 22:54:39",13.0,7069613.0,0,So even a string of false would get evaluated as a Boolean of True by Helm.,0.0,0.177,0.823,0.4215
Jaeger,61666591,61619122,0,"2020/05/07, 22:54:39",True,"2020/05/07, 22:54:39",13.0,7069613.0,0,"I was using Spinnaker which handles all overrides as a string unless the ""Raw Overrides"" box is checked.",0.0,0.0,1.0,0.0
Jaeger,61666591,61619122,0,"2020/05/07, 22:54:39",True,"2020/05/07, 22:54:39",13.0,7069613.0,0,If that box is checked than it converts the string to primitives where applicable.,0.0,0.0,1.0,0.0
Jaeger,61666591,61619122,0,"2020/05/07, 22:54:39",True,"2020/05/07, 22:54:39",13.0,7069613.0,0,"My issue was that even though I was overriding with a value of false, Spinnaker would pass that as a string to Helm which would then evaluate that as True.",0.0,0.172,0.828,0.6369
Jaeger,61666591,61619122,0,"2020/05/07, 22:54:39",True,"2020/05/07, 22:54:39",13.0,7069613.0,0,"The solution was to check the ""Raw Overrides"" box in Spinnaker.",0.0,0.187,0.813,0.3182
Jaeger,65852979,61431244,0,"2021/01/22, 23:26:39",False,"2021/01/22, 23:26:39",483.0,4895267.0,1,You can set a tag to Span creating a new custom Span,0.0,0.196,0.804,0.296
Jaeger,65852979,61431244,0,"2021/01/22, 23:26:39",False,"2021/01/22, 23:26:39",483.0,4895267.0,1,or retrieving the current active Span,0.0,0.351,0.649,0.4019
Jaeger,61163147,61073781,0,"2020/04/11, 23:15:30",False,"2020/04/11, 23:15:30",67.0,1466001.0,0,Here is a working example:  https://github.com/jobinesh/cloud-native-applications/tree/master/helidon-example-mp-jaeger .,0.0,0.0,1.0,0.0
Jaeger,61163147,61073781,0,"2020/04/11, 23:15:30",False,"2020/04/11, 23:15:30",67.0,1466001.0,0,See if that helps you.,0.0,0.394,0.606,0.3818
Jaeger,61163147,61073781,0,"2020/04/11, 23:15:30",False,"2020/04/11, 23:15:30",67.0,1466001.0,0,"If you are interested, see the details captured here:  https://www.jobinesh.com/2020/04/tracing-api-calls-in-your-helidon.html",0.0,0.231,0.769,0.4019
Jaeger,60796655,60793517,1,"2020/03/22, 08:47:00",False,"2020/03/22, 08:47:00",27576.0,1839482.0,0,You need to enable open tracing in nginx ingress controller.,0.0,0.0,1.0,0.0
Jaeger,60796655,60793517,1,"2020/03/22, 08:47:00",False,"2020/03/22, 08:47:00",27576.0,1839482.0,0,To enable the instrumentation we must enable OpenTracing in the configuration ConfigMap:,0.0,0.0,1.0,0.0
Jaeger,60796655,60793517,1,"2020/03/22, 08:47:00",False,"2020/03/22, 08:47:00",27576.0,1839482.0,0,"To enable or disable instrumentation for a single Ingress, use the enable-opentracing annotation:",0.0,0.0,1.0,0.0
Jaeger,60796655,60793517,1,"2020/03/22, 08:47:00",False,"2020/03/22, 08:47:00",27576.0,1839482.0,0,You must also set the host to use when uploading traces:,0.0,0.0,1.0,0.0
Jaeger,60796655,60793517,1,"2020/03/22, 08:47:00",False,"2020/03/22, 08:47:00",27576.0,1839482.0,0,https://kubernetes.github.io/ingress-nginx/user-guide/third-party-addons/opentracing/,0.0,0.0,1.0,0.0
Jaeger,62828373,60655072,0,"2020/07/10, 09:20:22",False,"2020/07/10, 09:20:22",81.0,12550162.0,0,Are you connecting it to only elasticsearch or stack like ELK/EFK?.,0.0,0.2,0.8,0.3612
Jaeger,62828373,60655072,0,"2020/07/10, 09:20:22",False,"2020/07/10, 09:20:22",81.0,12550162.0,0,"I had tried but we cannot configure ELK in jeager-all-one.exe alone in windows without docker.You can do it by running Jeager-collector, Jeager agent and Jeager query individually by mentioning configurations related to ELK.",0.075,0.0,0.925,-0.3612
Jaeger,62828373,60655072,0,"2020/07/10, 09:20:22",False,"2020/07/10, 09:20:22",81.0,12550162.0,0,In Jeager collector and Jeager query you need to set up variables  SPAN_STORAGE_TYPES  and  ES_SERVER_URLS .,0.0,0.0,1.0,0.0
Jaeger,61860648,60506609,0,"2020/05/18, 03:34:12",True,"2020/05/18, 03:34:12",829.0,9428538.0,2,To start a jaeger container:,0.0,0.0,1.0,0.0
Jaeger,61860648,60506609,0,"2020/05/18, 03:34:12",True,"2020/05/18, 03:34:12",829.0,9428538.0,2,Then you should by able to access to the Jaeger UI at  http://localhost:16686,0.0,0.0,1.0,0.0
Jaeger,61860648,60506609,0,"2020/05/18, 03:34:12",True,"2020/05/18, 03:34:12",829.0,9428538.0,2,"Once you've have a Jaeger up and running, you need to configure a Jaeger exporter to forward spans to Jaeger.",0.0,0.0,1.0,0.0
Jaeger,61860648,60506609,0,"2020/05/18, 03:34:12",True,"2020/05/18, 03:34:12",829.0,9428538.0,2,This will depends of the language used.,0.0,0.0,1.0,0.0
Jaeger,61860648,60506609,0,"2020/05/18, 03:34:12",True,"2020/05/18, 03:34:12",829.0,9428538.0,2,Here  is the straightforward documentation to do so in python.,0.0,0.0,1.0,0.0
Jaeger,59790526,59782209,0,"2020/01/17, 17:42:43",False,"2020/01/17, 17:42:43",2105.0,2461073.0,0,"Not out of the box, you have to plug a behaviour into NSB that uses open telemetry
 https://github.com/open-telemetry/opentelemetry-dotnet 
You will have to write custom code.",0.0,0.0,1.0,0.0
Jaeger,59790526,59782209,0,"2020/01/17, 17:42:43",False,"2020/01/17, 17:42:43",2105.0,2461073.0,0,"Plus you can do push metrics as well as shown in our app insights, Prometheus and other samples.",0.0,0.11,0.89,0.2732
Jaeger,59790526,59782209,0,"2020/01/17, 17:42:43",False,"2020/01/17, 17:42:43",2105.0,2461073.0,0,Let's continue the conversation in our support channels?,0.0,0.278,0.722,0.4019
Jaeger,66675598,59782209,0,"2021/03/17, 16:55:15",False,"2021/03/17, 17:02:16",970.0,5253437.0,0,Not sure if you are still looking for a solution for this.,0.148,0.173,0.679,0.0869
Jaeger,66675598,59782209,0,"2021/03/17, 16:55:15",False,"2021/03/17, 17:02:16",970.0,5253437.0,0,You should be able to do this currently using the  NServiceBus.Extensions.Diagnostics.OpenTelemetry  package from  nuget .,0.0,0.0,1.0,0.0
Jaeger,66675598,59782209,0,"2021/03/17, 16:55:15",False,"2021/03/17, 17:02:16",970.0,5253437.0,0,This is built by Jimmy Bogard and instruments NServiceBus with the required support for  Open Telemetry .,0.0,0.153,0.847,0.4019
Jaeger,66675598,59782209,0,"2021/03/17, 16:55:15",False,"2021/03/17, 17:02:16",970.0,5253437.0,0,The source for this is available  here .,0.0,0.0,1.0,0.0
Jaeger,66675598,59782209,0,"2021/03/17, 16:55:15",False,"2021/03/17, 17:02:16",970.0,5253437.0,0,You can connect this to any backend of your choice that supports  Open Telemetry  including but not limited to  Jaeger  and  Zipkin .,0.0,0.158,0.842,0.4116
Jaeger,66675598,59782209,0,"2021/03/17, 16:55:15",False,"2021/03/17, 17:02:16",970.0,5253437.0,0,"Additionally, here is an  example  that shows this in action.",0.0,0.0,1.0,0.0
Jaeger,59628491,59561262,0,"2020/01/07, 14:31:30",False,"2020/01/07, 14:31:30",13.0,4977370.0,0,Got it!,0.0,0.0,1.0,0.0
Jaeger,59628491,59561262,0,"2020/01/07, 14:31:30",False,"2020/01/07, 14:31:30",13.0,4977370.0,0,We need to enable sampling strategy to reach the collector endpoint.,0.0,0.099,0.901,0.0258
Jaeger,59391314,59372759,0,"2019/12/18, 13:44:56",True,"2019/12/18, 13:44:56",425.0,9112151.0,0,Found out how.,0.0,0.0,1.0,0.0
Jaeger,59391314,59372759,0,"2019/12/18, 13:44:56",True,"2019/12/18, 13:44:56",425.0,9112151.0,0,I just added one single line of code into tracing.py of django_opentracing lib:,0.0,0.0,1.0,0.0
Jaeger,59391314,59372759,0,"2019/12/18, 13:44:56",True,"2019/12/18, 13:44:56",425.0,9112151.0,0,And the result:,0.0,0.0,1.0,0.0
Jaeger,59076610,59076369,1,"2019/11/27, 20:38:47",False,"2019/11/27, 20:38:47",12299.0,369759.0,0,I see..,0.0,0.0,1.0,0.0
Jaeger,59076610,59076369,1,"2019/11/27, 20:38:47",False,"2019/11/27, 20:38:47",12299.0,369759.0,0,I thought  @Traced  will be somehow propagated to my db-services/repositories.,0.0,0.0,1.0,0.0
Jaeger,59076610,59076369,1,"2019/11/27, 20:38:47",False,"2019/11/27, 20:38:47",12299.0,369759.0,0,"No, I have to put it explicitly:",0.306,0.0,0.694,-0.296
Jaeger,59076610,59076369,1,"2019/11/27, 20:38:47",False,"2019/11/27, 20:38:47",12299.0,369759.0,0,That fixes the issue.,0.0,0.0,1.0,0.0
Jaeger,58745678,58704937,0,"2019/11/07, 11:30:57",False,"2019/11/07, 12:06:45",2049.0,11207414.0,0,According to the documentation  Remotely Accessing Telemetry Addons .,0.0,0.0,1.0,0.0
Jaeger,58745678,58704937,0,"2019/11/07, 11:30:57",False,"2019/11/07, 12:06:45",2049.0,11207414.0,0,There are different ways how to acces telemetry.,0.0,0.0,1.0,0.0
Jaeger,58745678,58704937,0,"2019/11/07, 11:30:57",False,"2019/11/07, 12:06:45",2049.0,11207414.0,0,The Recommended way is to create Secure acces using https instead of http.,0.0,0.387,0.613,0.6486
Jaeger,58745678,58704937,0,"2019/11/07, 11:30:57",False,"2019/11/07, 12:06:45",2049.0,11207414.0,0,Note for both methods:,0.0,0.0,1.0,0.0
Jaeger,58745678,58704937,0,"2019/11/07, 11:30:57",False,"2019/11/07, 12:06:45",2049.0,11207414.0,0,This option covers securing the transport layer only.,0.0,0.247,0.753,0.3182
Jaeger,58745678,58704937,0,"2019/11/07, 11:30:57",False,"2019/11/07, 12:06:45",2049.0,11207414.0,0,You should also configure the telemetry addons to require authentication when exposing them externally.,0.139,0.0,0.861,-0.2732
Jaeger,58745678,58704937,0,"2019/11/07, 11:30:57",False,"2019/11/07, 12:06:45",2049.0,11207414.0,0,Please note that jaeger itself doesn't support authentication methods  github  and workaround using Apache httpd server  here .,0.115,0.118,0.767,0.0108
Jaeger,58745678,58704937,0,"2019/11/07, 11:30:57",False,"2019/11/07, 12:06:45",2049.0,11207414.0,0,With your recruitments you can use Gateways (SDS)  with self-signed certificates :,0.0,0.0,1.0,0.0
Jaeger,58745678,58704937,0,"2019/11/07, 11:30:57",False,"2019/11/07, 12:06:45",2049.0,11207414.0,0,a .),0.0,0.0,1.0,0.0
Jaeger,58745678,58704937,0,"2019/11/07, 11:30:57",False,"2019/11/07, 12:06:45",2049.0,11207414.0,0,Make sure your that during istio instalation youe have enabled SDS at ingress gateway  --set gateways.istio-ingressgateway.sds.enabled=true  and  --set tracing.enabled=true  for tacing purposes.,0.0,0.099,0.901,0.3182
Jaeger,58745678,58704937,0,"2019/11/07, 11:30:57",False,"2019/11/07, 12:06:45",2049.0,11207414.0,0,b .),0.0,0.0,1.0,0.0
Jaeger,58745678,58704937,0,"2019/11/07, 11:30:57",False,"2019/11/07, 12:06:45",2049.0,11207414.0,0,Create self signed certificates for testing purposes you can use this  example and repository .,0.0,0.139,0.861,0.2732
Jaeger,58745678,58704937,0,"2019/11/07, 11:30:57",False,"2019/11/07, 12:06:45",2049.0,11207414.0,0,c .),0.0,0.0,1.0,0.0
Jaeger,58745678,58704937,0,"2019/11/07, 11:30:57",False,"2019/11/07, 12:06:45",2049.0,11207414.0,0,Please follow  Generate client and server certificates and keys   and  Configure a TLS ingress gateway using SDS .,0.0,0.133,0.867,0.3182
Jaeger,58745678,58704937,0,"2019/11/07, 11:30:57",False,"2019/11/07, 12:06:45",2049.0,11207414.0,0,Create Virtualservice and Gateway:,0.0,0.412,0.588,0.2732
Jaeger,58745678,58704937,0,"2019/11/07, 11:30:57",False,"2019/11/07, 12:06:45",2049.0,11207414.0,0,Hope this help,0.0,0.848,0.152,0.6808
Jaeger,58561231,58514716,0,"2019/10/25, 18:12:58",False,"2019/10/25, 18:12:58",721.0,1563297.0,1,The prometheus-es-exporter provides a way to create metrics using queries.,0.0,0.208,0.792,0.2732
Jaeger,58561231,58514716,0,"2019/10/25, 18:12:58",False,"2019/10/25, 18:12:58",721.0,1563297.0,1,For further details you can check  prometheus-es-exporter#query-metrics,0.0,0.0,1.0,0.0
Jaeger,58138322,58011545,0,"2019/09/27, 19:26:25",False,"2019/09/27, 19:26:25",1.0,4734979.0,0,Great question and a very popular one too.,0.0,0.59,0.41,0.8016
Jaeger,58138322,58011545,0,"2019/09/27, 19:26:25",False,"2019/09/27, 19:26:25",1.0,4734979.0,0,"In short, yes, code changes are required.",0.0,0.31,0.69,0.4019
Jaeger,58138322,58011545,0,"2019/09/27, 19:26:25",False,"2019/09/27, 19:26:25",1.0,4734979.0,0,Not just in one service but in all the services that a request will go through.,0.0,0.0,1.0,0.0
Jaeger,58138322,58011545,0,"2019/09/27, 19:26:25",False,"2019/09/27, 19:26:25",1.0,4734979.0,0,You need to instrument all services to get continuous traces that will be able to tell you the story of a request as it travels through the system.,0.0,0.0,1.0,0.0
Jaeger,57474156,57473539,0,"2019/08/13, 11:45:41",False,"2019/08/13, 11:45:41",1642.0,2255344.0,1,"I believe you can reuse Elasticsearch for multiple purposes - each would use a different set of indices, so separation is good.",0.0,0.149,0.851,0.4877
Jaeger,57474156,57473539,0,"2019/08/13, 11:45:41",False,"2019/08/13, 11:45:41",1642.0,2255344.0,1,from:  https://www.jaegertracing.io/docs/1.11/deployment/  :,0.0,0.0,1.0,0.0
Jaeger,57474156,57473539,0,"2019/08/13, 11:45:41",False,"2019/08/13, 11:45:41",1642.0,2255344.0,1,Collectors require a persistent storage backend.,0.0,0.0,1.0,0.0
Jaeger,57474156,57473539,0,"2019/08/13, 11:45:41",False,"2019/08/13, 11:45:41",1642.0,2255344.0,1,Cassandra and Elasticsearch are the primary supported storage backends,0.0,0.223,0.777,0.3182
Jaeger,57474156,57473539,0,"2019/08/13, 11:45:41",False,"2019/08/13, 11:45:41",1642.0,2255344.0,1,"Tying the networking all together, a docker-compose example:
 How to configure Jaeger with elasticsearch?",0.0,0.0,1.0,0.0
Jaeger,57476852,57473539,0,"2019/08/13, 14:30:22",False,"2019/08/13, 14:30:22",4433.0,243104.0,1,"While this isn't exactly what you asked, it sounds like what you're trying to achieve is seeing tracing for your JMS calls in Jaegar.",0.0,0.098,0.902,0.3612
Jaeger,57476852,57473539,0,"2019/08/13, 14:30:22",False,"2019/08/13, 14:30:22",4433.0,243104.0,1,"If that is the case, you could use an OpenTracing tracing solution for JMS or ActiveMQ to report tracing data directly to Jaegar.",0.0,0.095,0.905,0.3182
Jaeger,57476852,57473539,0,"2019/08/13, 14:30:22",False,"2019/08/13, 14:30:22",4433.0,243104.0,1,Here's one potential solution I found with a quick google.,0.0,0.247,0.753,0.3182
Jaeger,57476852,57473539,0,"2019/08/13, 14:30:22",False,"2019/08/13, 14:30:22",4433.0,243104.0,1,There may be others.,0.0,0.0,1.0,0.0
Jaeger,57476852,57473539,0,"2019/08/13, 14:30:22",False,"2019/08/13, 14:30:22",4433.0,243104.0,1,https://github.com/opentracing-contrib/java-jms,0.0,0.0,1.0,0.0
Jaeger,57420811,57419866,0,"2019/08/09, 00:26:00",False,"2019/08/09, 00:26:00",121.0,5799778.0,0,"Maybe you should check whether the application services which you set up in a hurry are both in the same azure resource group as the VM running the Jaeger all-in-one instance, otherwise the second application might not be able to communicate with the Jaeger instance at all.",0.0,0.0,1.0,0.0
Jaeger,57295501,57278976,0,"2019/07/31, 19:41:53",False,"2019/07/31, 19:41:53",13313.0,524946.0,0,"It doesn't really matter in which language your individual microservices are written, you should see them all in the same trace.",0.061,0.0,0.939,-0.0749
Jaeger,57295501,57278976,0,"2019/07/31, 19:41:53",False,"2019/07/31, 19:41:53",13313.0,524946.0,0,"Given that you are seeing three traces instead of one trace with three spans, it appears that the context propagation isn't working.",0.0,0.0,1.0,0.0
Jaeger,57295501,57278976,0,"2019/07/31, 19:41:53",False,"2019/07/31, 19:41:53",13313.0,524946.0,0,"Check your HTTP client in your nodejs services, they should perform the  ""inject"" operation .",0.0,0.0,1.0,0.0
Jaeger,57295501,57278976,0,"2019/07/31, 19:41:53",False,"2019/07/31, 19:41:53",13313.0,524946.0,0,"Your service ""B"" and ""C"" should then perform the ""extract"" operation.",0.0,0.0,1.0,0.0
Jaeger,57295501,57278976,0,"2019/07/31, 19:41:53",False,"2019/07/31, 19:41:53",13313.0,524946.0,0,"If you haven't yet, check  Yuri Shkuro's OpenTracing Tutorial .",0.0,0.0,1.0,0.0
Jaeger,57295501,57278976,0,"2019/07/31, 19:41:53",False,"2019/07/31, 19:41:53",13313.0,524946.0,0,"The lesson 3 is about the context propagation, including the inject and extract operations.",0.0,0.0,1.0,0.0
Jaeger,57295501,57278976,0,"2019/07/31, 19:41:53",False,"2019/07/31, 19:41:53",13313.0,524946.0,0,"I'm not quite sure how it works in the NodeJS world, but in Java, it should be sufficient to have the  opentracing-contrib/java-web-servlet-filter  instrumentation library in your classpath, as it would register the necessary pieces in the right hooks and make the trace context available for each incoming HTTP request.",0.032,0.0,0.968,-0.1505
Jaeger,57039417,57038416,0,"2019/07/15, 15:06:42",False,"2019/07/15, 15:06:42",4887.0,6700019.0,0,It seems that PyInstaller can't resolve  jaeger_client  import.,0.238,0.0,0.762,-0.2924
Jaeger,57039417,57038416,0,"2019/07/15, 15:06:42",False,"2019/07/15, 15:06:42",4887.0,6700019.0,0,So an easy way is to just edit your spec file and add the whole  jaeger_client  library as a  Tree  class:,0.0,0.143,0.857,0.4902
Jaeger,57039417,57038416,0,"2019/07/15, 15:06:42",False,"2019/07/15, 15:06:42",4887.0,6700019.0,0,And generate your executable with  pyinstaller script.spec .,0.0,0.0,1.0,0.0
Jaeger,55241560,55236000,1,"2019/03/19, 14:53:31",False,"2019/03/19, 14:53:31",9399.0,502575.0,0,You can create a  NodePort  service using the  app: jaeger  selector to expose the UI outside the cluster.,0.086,0.112,0.802,0.128
Jaeger,55487339,55236000,0,"2019/04/03, 07:54:28",False,"2019/04/03, 07:54:28",44552.0,308174.0,0,"kubectl port-forward  command default is expose to  localhost  network only, try to add  --address 0.0.0.0",0.103,0.0,0.897,-0.1531
Jaeger,55487339,55236000,0,"2019/04/03, 07:54:28",False,"2019/04/03, 07:54:28",44552.0,308174.0,0,see  kubectl command reference,0.0,0.0,1.0,0.0
Jaeger,55490575,55236000,0,"2019/04/03, 11:33:16",False,"2019/04/03, 11:33:16",5244.0,5564578.0,1,There are several ways of doing this.,0.0,0.0,1.0,0.0
Jaeger,55490575,55236000,0,"2019/04/03, 11:33:16",False,"2019/04/03, 11:33:16",5244.0,5564578.0,1,The  port-forward  works fine on Google Cloud Shell.,0.0,0.205,0.795,0.2023
Jaeger,55490575,55236000,0,"2019/04/03, 11:33:16",False,"2019/04/03, 11:33:16",5244.0,5564578.0,1,"If you are using GKE, then I strongly recommend using Cloud Shell, and  port-forward  as it is the easiest way.",0.0,0.316,0.684,0.7506
Jaeger,55490575,55236000,0,"2019/04/03, 11:33:16",False,"2019/04/03, 11:33:16",5244.0,5564578.0,1,"On other clouds, I don't know.",0.0,0.0,1.0,0.0
Jaeger,55490575,55236000,0,"2019/04/03, 11:33:16",False,"2019/04/03, 11:33:16",5244.0,5564578.0,1,What is suggesting Stefan would work.,0.0,0.0,1.0,0.0
Jaeger,55490575,55236000,0,"2019/04/03, 11:33:16",False,"2019/04/03, 11:33:16",5244.0,5564578.0,1,"You can edit the jaeger service with  kubectl edit svc jaeger-query , then change the type of the service from  ClusterIP  to  NodePort .",0.0,0.0,1.0,0.0
Jaeger,55490575,55236000,0,"2019/04/03, 11:33:16",False,"2019/04/03, 11:33:16",5244.0,5564578.0,1,"Finally, you can access the service with  NODE_IP:PORT  (any node).",0.0,0.0,1.0,0.0
Jaeger,55490575,55236000,0,"2019/04/03, 11:33:16",False,"2019/04/03, 11:33:16",5244.0,5564578.0,1,"If you do  kubectl get svc , you will see the new port assigned to the service.",0.0,0.0,1.0,0.0
Jaeger,55490575,55236000,0,"2019/04/03, 11:33:16",False,"2019/04/03, 11:33:16",5244.0,5564578.0,1,Note: You might need to open a firewall rule for that port.,0.0,0.0,1.0,0.0
Jaeger,55490575,55236000,0,"2019/04/03, 11:33:16",False,"2019/04/03, 11:33:16",5244.0,5564578.0,1,"You can also make the service type  LoadBalancer , if you have a control plane to set up an external IP address.",0.0,0.0,1.0,0.0
Jaeger,55490575,55236000,0,"2019/04/03, 11:33:16",False,"2019/04/03, 11:33:16",5244.0,5564578.0,1,"This would be a more expensive solution, but you would have a dedicated external IP address for your service.",0.0,0.278,0.722,0.6993
Jaeger,55490575,55236000,0,"2019/04/03, 11:33:16",False,"2019/04/03, 11:33:16",5244.0,5564578.0,1,"There are more ways, but I would say these are the appropriate ones.",0.0,0.0,1.0,0.0
Jaeger,55277542,55222810,0,"2019/03/21, 11:44:21",True,"2019/03/21, 11:44:21",654.0,955379.0,0,This issue looks more to have to do with Java it self then either Opentracing and Jaeger.,0.0,0.0,1.0,0.0
Jaeger,55277542,55222810,0,"2019/03/21, 11:44:21",True,"2019/03/21, 11:44:21",654.0,955379.0,0,as  ex.getStackTrace()  is more of the problem.,0.331,0.0,0.669,-0.4522
Jaeger,55277542,55222810,0,"2019/03/21, 11:44:21",True,"2019/03/21, 11:44:21",654.0,955379.0,0,As it should be more like,0.0,0.358,0.642,0.4201
Jaeger,55277542,55222810,0,"2019/03/21, 11:44:21",True,"2019/03/21, 11:44:21",654.0,955379.0,0,Problem solved.,0.562,0.437,0.0,-0.1531
Jaeger,54867389,54675467,0,"2019/02/25, 15:32:18",False,"2019/02/25, 15:32:18",13313.0,524946.0,0,Setting a baggage item is  not  the same as setting an HTTP header.,0.0,0.0,1.0,0.0
Jaeger,54867389,54675467,0,"2019/02/25, 15:32:18",False,"2019/02/25, 15:32:18",13313.0,524946.0,0,You should use your HTTP client (not shown in your example) to set the HTTP header.,0.0,0.0,1.0,0.0
Jaeger,54867389,54675467,0,"2019/02/25, 15:32:18",False,"2019/02/25, 15:32:18",13313.0,524946.0,0,"Baggage items might or not be available as individual HTTP headers: it's a detail implementation of the underlying tracer, such as Jaeger's.",0.0,0.0,1.0,0.0
Jaeger,54367285,54365010,0,"2019/01/25, 16:30:16",False,"2019/01/25, 16:30:16",13313.0,524946.0,0,"Each ""dot"" would be a new child node in the YAML file, like:",0.0,0.185,0.815,0.3612
Jaeger,54367285,54365010,0,"2019/01/25, 16:30:16",False,"2019/01/25, 16:30:16",13313.0,524946.0,0,"Make sure to run the process with the env var SPAN_STORAGE_TYPE set to elasticsearch, like:",0.0,0.27,0.73,0.5859
Jaeger,54367285,54365010,0,"2019/01/25, 16:30:16",False,"2019/01/25, 16:30:16",13313.0,524946.0,0,(as seen on  https://github.com/jaegertracing/jaeger/issues/1299 ),0.0,0.0,1.0,0.0
Jaeger,54325706,53885456,1,"2019/01/23, 13:00:46",False,"2019/01/23, 13:00:46",13313.0,524946.0,1,"So as per this config, does it mean that I'm sampling each and every trace or just the few traces randomly?",0.0,0.0,1.0,0.0
Jaeger,54325706,53885456,1,"2019/01/23, 13:00:46",False,"2019/01/23, 13:00:46",13313.0,524946.0,1,Using the sampler type as  const  with  1  as the value means that you are sampling everything.,0.0,0.138,0.862,0.34
Jaeger,54325706,53885456,1,"2019/01/23, 13:00:46",False,"2019/01/23, 13:00:46",13313.0,524946.0,1,"Mysteriously, when I'm passing random inputs to create spans for my microservices, the spans are getting generated only after 4 to 5 minutes.",0.0,0.095,0.905,0.2732
Jaeger,54325706,53885456,1,"2019/01/23, 13:00:46",False,"2019/01/23, 13:00:46",13313.0,524946.0,1,I would like to understand this configuration spec more but not able to.,0.0,0.137,0.863,0.1901
Jaeger,54325706,53885456,1,"2019/01/23, 13:00:46",False,"2019/01/23, 13:00:46",13313.0,524946.0,1,There are several things that might be happening.,0.0,0.0,1.0,0.0
Jaeger,54325706,53885456,1,"2019/01/23, 13:00:46",False,"2019/01/23, 13:00:46",13313.0,524946.0,1,"You might not be closing spans, for instance.",0.0,0.0,1.0,0.0
Jaeger,54325706,53885456,1,"2019/01/23, 13:00:46",False,"2019/01/23, 13:00:46",13313.0,524946.0,1,I recommend reading the following two blog posts to try to understand what might be happening:,0.0,0.152,0.848,0.3612
Jaeger,54325706,53885456,1,"2019/01/23, 13:00:46",False,"2019/01/23, 13:00:46",13313.0,524946.0,1,Help!,0.0,1.0,0.0,0.4574
Jaeger,54325706,53885456,1,"2019/01/23, 13:00:46",False,"2019/01/23, 13:00:46",13313.0,524946.0,1,Something is wrong with my Jaeger installation!,0.361,0.0,0.639,-0.5255
Jaeger,54325706,53885456,1,"2019/01/23, 13:00:46",False,"2019/01/23, 13:00:46",13313.0,524946.0,1,The life of a span,0.0,0.0,1.0,0.0
Jaeger,52329845,52145774,0,"2018/09/14, 13:15:30",True,"2018/09/14, 13:15:30",360.0,1796107.0,0,"Your best chance is to get the data from whatever storage the Jaeger collector is using (Cassandra, Elastic.)",0.0,0.279,0.721,0.7351
Jaeger,52329845,52145774,0,"2018/09/14, 13:15:30",True,"2018/09/14, 13:15:30",360.0,1796107.0,0,( https://www.jaegertracing.io/docs/1.6/deployment/  ),0.0,0.0,1.0,0.0
Jaeger,52329845,52145774,0,"2018/09/14, 13:15:30",True,"2018/09/14, 13:15:30",360.0,1796107.0,0,My suggestion is to store in Elastic and use Kibana to accomplish what you need.,0.0,0.167,0.833,0.4215
Jaeger,65919685,52145774,0,"2021/01/27, 14:59:44",False,"2021/01/27, 14:59:44",1869.0,3511252.0,0,Old topic but the current version of Jaeger Query UI is a single page app and has an underlying API that allows the same queries capabilities as the UI.,0.0,0.0,1.0,0.0
Jaeger,52167852,52104427,6,"2018/09/04, 16:48:04",False,"2018/09/04, 16:48:04",8336.0,1773866.0,-1,You're using  Camden  release train with boot  2.0  and Sleuth  2.0 .,0.0,0.0,1.0,0.0
Jaeger,52167852,52104427,6,"2018/09/04, 16:48:04",False,"2018/09/04, 16:48:04",8336.0,1773866.0,-1,That's completely incompatible.,0.0,0.0,1.0,0.0
Jaeger,52167852,52104427,6,"2018/09/04, 16:48:04",False,"2018/09/04, 16:48:04",8336.0,1773866.0,-1,"Please generate a project from start.spring.io from scratch, please don't put any versions manually for spring cloud projects, and please try again.",0.0,0.277,0.723,0.7096
Jaeger,52167852,52104427,6,"2018/09/04, 16:48:04",False,"2018/09/04, 16:48:04",8336.0,1773866.0,-1,Try using  Finchley  release train instead of  Camden,0.0,0.0,1.0,0.0
Jaeger,52722089,50906213,0,"2018/10/09, 16:19:28",True,"2018/10/09, 16:19:28",13313.0,524946.0,1,"The problem is that if I kill the Docker running the jaeger collector- systemctl stop docker and later restart docker and jaegertracing/all-in-one, the services are no longer up at  http://localhost:16686/api/services",0.321,0.0,0.679,-0.8957
Jaeger,52722089,50906213,0,"2018/10/09, 16:19:28",True,"2018/10/09, 16:19:28",13313.0,524946.0,1,That's because you are using the in-memory storage.,0.0,0.0,1.0,0.0
Jaeger,52722089,50906213,0,"2018/10/09, 16:19:28",True,"2018/10/09, 16:19:28",13313.0,524946.0,1,"If you stop and start the container, the storage is reset, so, you'll effectively lose your data.",0.231,0.142,0.627,-0.2466
Jaeger,52722089,50906213,0,"2018/10/09, 16:19:28",True,"2018/10/09, 16:19:28",13313.0,524946.0,1,"For production purposes, you should use a backing storage like Cassandra or Elasticsearch.",0.0,0.265,0.735,0.3818
Jaeger,52722089,50906213,0,"2018/10/09, 16:19:28",True,"2018/10/09, 16:19:28",13313.0,524946.0,1,Does the Jaeger collector needs to be running before starting the Jaeger clients?,0.0,0.0,1.0,0.0
Jaeger,52722089,50906213,0,"2018/10/09, 16:19:28",True,"2018/10/09, 16:19:28",13313.0,524946.0,1,"No, but spans reported by clients when the collector isn't available might get dropped.",0.11,0.0,0.89,-0.1531
Jaeger,52722089,50906213,0,"2018/10/09, 16:19:28",True,"2018/10/09, 16:19:28",13313.0,524946.0,1,"Note that clients will send spans to the agent by default, and will not contact the collector directly.",0.0,0.0,1.0,0.0
Jaeger,52722089,50906213,0,"2018/10/09, 16:19:28",True,"2018/10/09, 16:19:28",13313.0,524946.0,1,"So, if the agent isn't available, spans might get dropped as well.",0.0,0.16,0.84,0.2732
Jaeger,52722089,50906213,0,"2018/10/09, 16:19:28",True,"2018/10/09, 16:19:28",13313.0,524946.0,1,how can I flush the memory used by Jaeger OpenTracing so that my host doesn't run out of memory?,0.0,0.0,1.0,0.0
Jaeger,52722089,50906213,0,"2018/10/09, 16:19:28",True,"2018/10/09, 16:19:28",13313.0,524946.0,1,Use the configuration option  --memory.max-traces .,0.0,0.0,1.0,0.0
Jaeger,52722089,50906213,0,"2018/10/09, 16:19:28",True,"2018/10/09, 16:19:28",13313.0,524946.0,1,"With this option, older traces will get overwritten by new ones once this limit is reached.",0.0,0.085,0.915,0.1027
Jaeger,52813470,50118186,1,"2018/10/15, 12:20:43",True,"2018/10/15, 12:20:43",13313.0,524946.0,1,But when I wrap the Spring-Boot application inside a Docker container with the following docker-compose file and start the Jaeger client again I can't see any traces,0.0,0.0,1.0,0.0
Jaeger,52813470,50118186,1,"2018/10/15, 12:20:43",True,"2018/10/15, 12:20:43",13313.0,524946.0,1,"That's because the Jaeger client will, by default, send the spans via UDP to an agent at  localhost .",0.0,0.0,1.0,0.0
Jaeger,52813470,50118186,1,"2018/10/15, 12:20:43",True,"2018/10/15, 12:20:43",13313.0,524946.0,1,"When your application is running in a Docker container, your  localhost  there is the container itself, so that the spans are lost.",0.103,0.0,0.897,-0.3182
Jaeger,52813470,50118186,1,"2018/10/15, 12:20:43",True,"2018/10/15, 12:20:43",13313.0,524946.0,1,"As you are linking the Jaeger container with your application, you may want to get it solved by exporting the env var  JAEGER_AGENT_HOST  to  jaeger .",0.0,0.129,0.871,0.34
Jaeger,50339003,49909075,0,"2018/05/15, 00:05:40",True,"2018/05/15, 00:05:40",611.0,118116.0,1,Turns out I don't need neither Zipkin nor Jaeger to have my traces on Stackdriver.,0.0,0.0,1.0,0.0
Jaeger,50339003,49909075,0,"2018/05/15, 00:05:40",True,"2018/05/15, 00:05:40",611.0,118116.0,1,All that is needed is a deployment of  zipkin-collector  and a service to point to it and all my traces are now reporting as expected on GCP Stackdriver.,0.0,0.0,1.0,0.0
Jaeger,49626058,49624555,0,"2018/04/03, 12:00:37",False,"2018/04/04, 08:31:53",81.0,2819181.0,0,"Not jaeger, able to send traces to zipkin server, using zipkin-simple.",0.0,0.0,1.0,0.0
Jaeger,49626058,49624555,0,"2018/04/03, 12:00:37",False,"2018/04/04, 08:31:53",81.0,2819181.0,0,Related code is in repository  https://github.com/debmalya/calculator,0.0,0.0,1.0,0.0
Jaeger,49637708,49624555,0,"2018/04/03, 22:24:22",False,"2018/04/03, 22:24:22",21.0,9593079.0,2,node-jaeger-client currently doesn't run in the browser.,0.0,0.0,1.0,0.0
Jaeger,49637708,49624555,0,"2018/04/03, 22:24:22",False,"2018/04/03, 22:24:22",21.0,9593079.0,2,There is ongoing  work  to make jaeger-client browser friendly.,0.0,0.286,0.714,0.4939
Jaeger,49637708,49624555,0,"2018/04/03, 22:24:22",False,"2018/04/03, 22:24:22",21.0,9593079.0,2,This issue:  readFileSync is not a function  contains relevant information to why you're seeing the error message.,0.153,0.0,0.847,-0.4019
Jaeger,49637708,49624555,0,"2018/04/03, 22:24:22",False,"2018/04/03, 22:24:22",21.0,9593079.0,2,"Essentially, you're trying to run jaeger-client (a nodejs library) using react-scripts which doesn't contain the modules that jaeger-client needs.",0.0,0.0,1.0,0.0
Jaeger,44337567,43766832,0,"2017/06/03, 00:18:56",True,"2017/06/03, 00:18:56",471.0,2673284.0,0,There are two issues here.,0.0,0.0,1.0,0.0
Jaeger,44337567,43766832,0,"2017/06/03, 00:18:56",True,"2017/06/03, 00:18:56",471.0,2673284.0,0,One is that your code sets the port for Jaeger client to 5775.,0.0,0.0,1.0,0.0
Jaeger,44337567,43766832,0,"2017/06/03, 00:18:56",True,"2017/06/03, 00:18:56",471.0,2673284.0,0,"This port expects a different data model than what Node.js client sends, you can remove the  agentHost  and  agentPort  parameters and rely on defaults.",0.0,0.0,1.0,0.0
Jaeger,44337567,43766832,0,"2017/06/03, 00:18:56",True,"2017/06/03, 00:18:56",471.0,2673284.0,0,The second issue is that you're running the Docker image without exposing the required UDP port.,0.0,0.108,0.892,0.2057
Jaeger,44337567,43766832,0,"2017/06/03, 00:18:56",True,"2017/06/03, 00:18:56",471.0,2673284.0,0,"The correct command is shown in the  documentation , as of today it should be this (one long line):",0.0,0.0,1.0,0.0
Jaeger,54372644,54371809,0,"2019/01/25, 22:56:14",False,"2019/01/25, 22:56:14",324.0,5600810.0,0,Liberty does not have Open Tracing Tracer implementation for Jaeger yet.,0.0,0.254,0.746,0.5267
Jaeger,54372644,54371809,0,"2019/01/25, 22:56:14",False,"2019/01/25, 22:56:14",324.0,5600810.0,0,We have a sample Tracer implementation for Zipkin.,0.0,0.0,1.0,0.0
Jaeger,54372644,54371809,0,"2019/01/25, 22:56:14",False,"2019/01/25, 22:56:14",324.0,5600810.0,0,You can find it at  https://github.com/WASdev/sample.opentracing.zipkintracer .,0.0,0.0,1.0,0.0
Jaeger,54372644,54371809,0,"2019/01/25, 22:56:14",False,"2019/01/25, 22:56:14",324.0,5600810.0,0,Jaegar claims it is backward compatible with Zipkin by accepting spans in Zipkin formats over HTTP.,0.0,0.148,0.852,0.3818
Jaeger,54372644,54371809,0,"2019/01/25, 22:56:14",False,"2019/01/25, 22:56:14",324.0,5600810.0,0,Feel free to open a RFE at  https://developer.ibm.com/wasdev/help/submit-rfe/,0.0,0.355,0.645,0.5106
Jaeger,52894461,52881039,0,"2018/10/19, 17:29:25",False,"2018/10/19, 17:29:25",13313.0,524946.0,0,This is most likely caused by the static assets  not  being included in the binary.,0.0,0.108,0.892,0.1779
Jaeger,52894461,52881039,0,"2018/10/19, 17:29:25",False,"2018/10/19, 17:29:25",13313.0,524946.0,0,You can try that out by running the binary you compiled.,0.0,0.0,1.0,0.0
Jaeger,52894461,52881039,0,"2018/10/19, 17:29:25",False,"2018/10/19, 17:29:25",13313.0,524946.0,0,"Instead of compiling on your own, a better approach would be to get the official binaries from the releases page and build your Docker container using that.",0.0,0.104,0.896,0.4404
Jaeger,52894461,52881039,0,"2018/10/19, 17:29:25",False,"2018/10/19, 17:29:25",13313.0,524946.0,0,https://github.com/jaegertracing/jaeger/releases/latest,0.0,0.0,1.0,0.0
Jaeger,56227367,56156809,0,"2019/05/20, 22:55:44",True,"2019/05/20, 22:55:44",386.0,6692626.0,9,"This is a limitation in the Serilog logger factory implementation; in particular, Serilog currently ignores added providers and assumes that Serilog Sinks will replace them instead.",0.158,0.0,0.842,-0.5106
Jaeger,56227367,56156809,0,"2019/05/20, 22:55:44",True,"2019/05/20, 22:55:44",386.0,6692626.0,9,"So, the solutions is implementaion a simple  WriteTo.OpenTracing()  method to connect Serilog directly to  OpenTracing",0.0,0.132,0.868,0.2449
Jaeger,62140570,54597464,0,"2020/06/01, 23:33:01",False,"2020/06/01, 23:33:01",1111.0,898472.0,0,I had this problem while using gunicorn with gevent as the worker class.,0.197,0.0,0.803,-0.4019
Jaeger,62140570,54597464,0,"2020/06/01, 23:33:01",False,"2020/06/01, 23:33:01",1111.0,898472.0,0,To resolve and get cloud traces working the solution was to monkey patch grpc like so,0.0,0.363,0.637,0.7506
Jaeger,62140570,54597464,0,"2020/06/01, 23:33:01",False,"2020/06/01, 23:33:01",1111.0,898472.0,0,See  https://github.com/grpc/grpc/issues/4629#issuecomment-376962677,0.0,0.0,1.0,0.0
Jaeger,32155765,32155133,4,"2015/08/22, 14:48:32",True,"2016/11/03, 17:57:34",,,13,For your exact question create a character class,0.0,0.259,0.741,0.2732
Jaeger,32155765,32155133,4,"2015/08/22, 14:48:32",True,"2016/11/03, 17:57:34",,,13,And then you can just add * on the end to get 0 or unlimited number of them or alternatively 1 or an unlimited number with +,0.0,0.11,0.89,0.1531
Jaeger,32155765,32155133,4,"2015/08/22, 14:48:32",True,"2016/11/03, 17:57:34",,,13,or,0.0,0.0,1.0,0.0
Jaeger,32155765,32155133,4,"2015/08/22, 14:48:32",True,"2016/11/03, 17:57:34",,,13,"Also there is this below, found at  https://regex101.com/  under the library tab when searching for json",0.0,0.0,1.0,0.0
Jaeger,32155765,32155133,4,"2015/08/22, 14:48:32",True,"2016/11/03, 17:57:34",,,13,"This should match any valid json, you can also test it at the website above",0.0,0.0,1.0,0.0
Jaeger,32155765,32155133,4,"2015/08/22, 14:48:32",True,"2016/11/03, 17:57:34",,,13,EDIT:,0.0,0.0,1.0,0.0
Jaeger,32155765,32155133,4,"2015/08/22, 14:48:32",True,"2016/11/03, 17:57:34",,,13,Link to the regex,0.0,0.0,1.0,0.0
Jaeger,56941159,56863669,0,"2019/07/08, 22:06:55",False,"2019/07/08, 22:06:55",504.0,9187876.0,2,"No, there is no out-of-the-box possibility to change the HTTP header name.",0.306,0.0,0.694,-0.5267
Jaeger,56941159,56863669,0,"2019/07/08, 22:06:55",False,"2019/07/08, 22:06:55",504.0,9187876.0,2,"However, you can enable B3 header propagation with  opentracing.jaeger.enable-b3-propagation=true .",0.0,0.0,1.0,0.0
Jaeger,56941159,56863669,0,"2019/07/08, 22:06:55",False,"2019/07/08, 22:06:55",504.0,9187876.0,2,"To configure Traefik to send the trace data as B3 headers, see  https://github.com/containous/traefik/blob/master/docs/content/observability/tracing/jaeger.md#propagation .",0.0,0.0,1.0,0.0
Jaeger,56941159,56863669,0,"2019/07/08, 22:06:55",False,"2019/07/08, 22:06:55",504.0,9187876.0,2,traceContextHeaderName  should also be configured as  X-B3-TraceId  then.,0.0,0.0,1.0,0.0
Jaeger,44370244,44166525,0,"2017/06/05, 16:35:47",True,"2017/06/05, 16:35:47",8336.0,1773866.0,2,There's an ongoing discussion over here -  https://github.com/spring-cloud/spring-cloud-sleuth/issues/599  .,0.0,0.0,1.0,0.0
Jaeger,44370244,44166525,0,"2017/06/05, 16:35:47",True,"2017/06/05, 16:35:47",8336.0,1773866.0,2,In general we don't explicitly use the OpenTracing API but we are Zipkin compatible in terms of header propagation.,0.0,0.0,1.0,0.0
Jaeger,44370244,44166525,0,"2017/06/05, 16:35:47",True,"2017/06/05, 16:35:47",8336.0,1773866.0,2,You can also manipulate the header names as you wish so if any sort of library you're using requires other header names for span / trace etc.,0.0,0.097,0.903,0.4019
Jaeger,44370244,44166525,0,"2017/06/05, 16:35:47",True,"2017/06/05, 16:35:47",8336.0,1773866.0,2,then you can set it yourself as you want to.,0.0,0.126,0.874,0.0772
Jaeger,53370207,44166525,0,"2018/11/19, 09:38:53",False,"2018/11/19, 09:38:53",463.0,1749786.0,14,Spring Sleuth is now OpenTracing compatible.,0.0,0.0,1.0,0.0
Jaeger,53370207,44166525,0,"2018/11/19, 09:38:53",False,"2018/11/19, 09:38:53",463.0,1749786.0,14,All you have to do is use OpenTracing Jars in your class path.,0.0,0.0,1.0,0.0
Jaeger,53370207,44166525,0,"2018/11/19, 09:38:53",False,"2018/11/19, 09:38:53",463.0,1749786.0,14,You can then use Sleuth-Zipkin to send instrumentation data to Jaeger's Zipkin collector.,0.0,0.0,1.0,0.0
Jaeger,53370207,44166525,0,"2018/11/19, 09:38:53",False,"2018/11/19, 09:38:53",463.0,1749786.0,14,This way you achieve everything you want with minimal configuration.,0.0,0.126,0.874,0.0772
Jaeger,53370207,44166525,0,"2018/11/19, 09:38:53",False,"2018/11/19, 09:38:53",463.0,1749786.0,14,You can use my sample program as an example here:,0.0,0.0,1.0,0.0
Jaeger,53370207,44166525,0,"2018/11/19, 09:38:53",False,"2018/11/19, 09:38:53",463.0,1749786.0,14,https://github.com/anoophp777/spring-webflux-jaegar-log4j2,0.0,0.0,1.0,0.0
Jaeger,23347674,23266043,1,"2014/04/28, 20:38:39",True,"2014/04/28, 20:38:39",832.0,23854.0,4,It looks like this is not currently possible with Cordova 3.4 when attempting to read a video file out of the application assets.,0.0,0.174,0.826,0.4939
Jaeger,23347674,23266043,1,"2014/04/28, 20:38:39",True,"2014/04/28, 20:38:39",832.0,23854.0,4,See  https://issues.apache.org/jira/browse/CB-6079,0.0,0.0,1.0,0.0
Jaeger,23347674,23266043,1,"2014/04/28, 20:38:39",True,"2014/04/28, 20:38:39",832.0,23854.0,4,"It is possible to read the file if its copied to a directory outside the application assets, or the file is stored remotely.",0.0,0.075,0.925,0.1779
Jaeger,23347674,23266043,1,"2014/04/28, 20:38:39",True,"2014/04/28, 20:38:39",832.0,23854.0,4,But not in the app assets folder any longer.,0.0,0.204,0.796,0.2617
Jaeger,23347674,23266043,1,"2014/04/28, 20:38:39",True,"2014/04/28, 20:38:39",832.0,23854.0,4,I have a similar issue - my application has a welcome screen with a short video explaining the application (~300k) which I cannot play out of the APK itself.,0.078,0.115,0.807,0.2415
Jaeger,17730606,16491312,0,"2013/07/18, 20:49:47",True,"2013/07/18, 20:49:47",3963.0,2161256.0,4,"jquery.limitkeypress.js has some issue with ie, I recommend you to use a more powerful library.",0.0,0.337,0.663,0.6801
Jaeger,17730606,16491312,0,"2013/07/18, 20:49:47",True,"2013/07/18, 20:49:47",3963.0,2161256.0,4,http://github.com/RobinHerbots/jquery.inputmask,0.0,0.0,1.0,0.0
Jaeger,17730606,16491312,0,"2013/07/18, 20:49:47",True,"2013/07/18, 20:49:47",3963.0,2161256.0,4,With this library you can use something like this:,0.0,0.238,0.762,0.3612
Jaeger,17730606,16491312,0,"2013/07/18, 20:49:47",True,"2013/07/18, 20:49:47",3963.0,2161256.0,4,It works perfectly on ie.,0.0,0.512,0.488,0.6369
Jaeger,17730606,16491312,0,"2013/07/18, 20:49:47",True,"2013/07/18, 20:49:47",3963.0,2161256.0,4,:),0.0,1.0,0.0,0.4588
Jaeger,35784198,16491312,0,"2016/03/04, 00:18:17",False,"2016/03/04, 16:45:12",11.0,6015643.0,1,Sorry i have not updated that plugin in a few years but...,0.126,0.0,0.874,-0.0772
Jaeger,35784198,16491312,0,"2016/03/04, 00:18:17",False,"2016/03/04, 16:45:12",11.0,6015643.0,1,jquery.limitkeypress  now works with IE9+ there was an issue with how the selection was determined.,0.0,0.146,0.854,0.34
Jaeger,35784198,16491312,0,"2016/03/04, 00:18:17",False,"2016/03/04, 16:45:12",11.0,6015643.0,1,IE11 killed support for their document.selection but they kept the document.setSelectionRange which i was using to test what browser was being used...,0.117,0.078,0.805,-0.2263
Jaeger,35784198,16491312,0,"2016/03/04, 00:18:17",False,"2016/03/04, 16:45:12",11.0,6015643.0,1,IE9 enabled document.selectionStart and document.selectionEnd so i now check directly what browser version of IE peoples are using...,0.0,0.0,1.0,0.0
Jaeger,35784198,16491312,0,"2016/03/04, 00:18:17",False,"2016/03/04, 16:45:12",11.0,6015643.0,1,I added this to check for IE version:,0.0,0.0,1.0,0.0
Jaeger,35784198,16491312,0,"2016/03/04, 00:18:17",False,"2016/03/04, 16:45:12",11.0,6015643.0,1,So my selection functions now look like this:,0.0,0.263,0.737,0.3612
Jaeger,63582546,63507749,1,"2020/08/25, 18:46:03",False,"2020/08/25, 18:46:03",61.0,14137910.0,3,After few days of digging I've figured it out.,0.0,0.0,1.0,0.0
Jaeger,63582546,63507749,1,"2020/08/25, 18:46:03",False,"2020/08/25, 18:46:03",61.0,14137910.0,3,Problem is in the format of the  x-request-id  header that nginx ingress controller uses.,0.172,0.0,0.828,-0.4019
Jaeger,63582546,63507749,1,"2020/08/25, 18:46:03",False,"2020/08/25, 18:46:03",61.0,14137910.0,3,Envoy proxy expects it to be an UUID (e.g.,0.0,0.0,1.0,0.0
Jaeger,63582546,63507749,1,"2020/08/25, 18:46:03",False,"2020/08/25, 18:46:03",61.0,14137910.0,3,x-request-id: 3e21578f-cd04-9246-aa50-67188d790051 ) but ingrex controller passes it as a non-formatted random string ( x-request-id: 60e82827a270070cfbda38c6f30f478a ).,0.0,0.0,1.0,0.0
Jaeger,63582546,63507749,1,"2020/08/25, 18:46:03",False,"2020/08/25, 18:46:03",61.0,14137910.0,3,When I pass properly formatted x-request-id header in the request to ingress controller its getting passed down to envoy proxy and request is getting sampled as expected.,0.0,0.0,1.0,0.0
Jaeger,63582546,63507749,1,"2020/08/25, 18:46:03",False,"2020/08/25, 18:46:03",61.0,14137910.0,3,"I also tried to remove
x-request-id header from the request from ingress controller to ServiceA with a simple EnvoyFilter.",0.0,0.0,1.0,0.0
Jaeger,63582546,63507749,1,"2020/08/25, 18:46:03",False,"2020/08/25, 18:46:03",61.0,14137910.0,3,And it also works as expected.,0.0,0.0,1.0,0.0
Jaeger,63582546,63507749,1,"2020/08/25, 18:46:03",False,"2020/08/25, 18:46:03",61.0,14137910.0,3,Envoy proxy generates a new x-request-id and request is getting traced.,0.0,0.0,1.0,0.0
Jaeger,63486777,63485673,2,"2020/08/19, 15:18:24",True,"2020/08/19, 15:18:24",91.0,1548178.0,4,The demo you tried is using older configuration and opencensus which should be replaced with otlp receiver.,0.0,0.0,1.0,0.0
Jaeger,63486777,63485673,2,"2020/08/19, 15:18:24",True,"2020/08/19, 15:18:24",91.0,1548178.0,4,"Having said that this is a working example
 https://github.com/open-telemetry/opentelemetry-js/tree/master/examples/collector-exporter-node/docker 
So I'm copying the files from there:",0.0,0.0,1.0,0.0
Jaeger,63486777,63485673,2,"2020/08/19, 15:18:24",True,"2020/08/19, 15:18:24",91.0,1548178.0,4,docker-compose.yaml,0.0,0.0,1.0,0.0
Jaeger,63486777,63485673,2,"2020/08/19, 15:18:24",True,"2020/08/19, 15:18:24",91.0,1548178.0,4,collector-config.yaml,0.0,0.0,1.0,0.0
Jaeger,63486777,63485673,2,"2020/08/19, 15:18:24",True,"2020/08/19, 15:18:24",91.0,1548178.0,4,prometheus.yaml,0.0,0.0,1.0,0.0
Jaeger,63486777,63485673,2,"2020/08/19, 15:18:24",True,"2020/08/19, 15:18:24",91.0,1548178.0,4,This should work fine with opentelemetry-js ver.,0.0,0.231,0.769,0.2023
Jaeger,63486777,63485673,2,"2020/08/19, 15:18:24",True,"2020/08/19, 15:18:24",91.0,1548178.0,4,0.10.2,0.0,0.0,1.0,0.0
Jaeger,63486777,63485673,2,"2020/08/19, 15:18:24",True,"2020/08/19, 15:18:24",91.0,1548178.0,4,Default port for traces is 55680 and for metrics 55681,0.0,0.0,1.0,0.0
Jaeger,63486777,63485673,2,"2020/08/19, 15:18:24",True,"2020/08/19, 15:18:24",91.0,1548178.0,4,"The link I posted previously - you will always find there the latest up to date working example:
 https://github.com/open-telemetry/opentelemetry-js/tree/master/examples/collector-exporter-node 
And for web example you can use the same docker and see all working examples here:
 https://github.com/open-telemetry/opentelemetry-js/tree/master/examples/tracer-web/",0.0,0.0,1.0,0.0
Jaeger,63489195,63485673,2,"2020/08/19, 17:27:00",False,"2020/08/19, 18:54:58",30911.0,2000548.0,3,Thank you sooo much for @BObecny's help!,0.0,0.523,0.477,0.6696
Jaeger,63489195,63485673,2,"2020/08/19, 17:27:00",False,"2020/08/19, 18:54:58",30911.0,2000548.0,3,This is a complement of @BObecny's answer.,0.0,0.0,1.0,0.0
Jaeger,63489195,63485673,2,"2020/08/19, 17:27:00",False,"2020/08/19, 18:54:58",30911.0,2000548.0,3,Since I am more interested in integrating with Jaeger.,0.0,0.3,0.7,0.4576
Jaeger,63489195,63485673,2,"2020/08/19, 17:27:00",False,"2020/08/19, 18:54:58",30911.0,2000548.0,3,"So here is the config to set up with all Jaeger, Zipkin, Prometheus.",0.0,0.0,1.0,0.0
Jaeger,63489195,63485673,2,"2020/08/19, 17:27:00",False,"2020/08/19, 18:54:58",30911.0,2000548.0,3,And now it works on both front end and back end.,0.0,0.0,1.0,0.0
Jaeger,63489195,63485673,2,"2020/08/19, 17:27:00",False,"2020/08/19, 18:54:58",30911.0,2000548.0,3,First both front end and back end use same exporter code:,0.0,0.0,1.0,0.0
Jaeger,63489195,63485673,2,"2020/08/19, 17:27:00",False,"2020/08/19, 18:54:58",30911.0,2000548.0,3,docker-compose.yaml,0.0,0.0,1.0,0.0
Jaeger,63489195,63485673,2,"2020/08/19, 17:27:00",False,"2020/08/19, 18:54:58",30911.0,2000548.0,3,collector-config.yaml,0.0,0.0,1.0,0.0
Jaeger,63489195,63485673,2,"2020/08/19, 17:27:00",False,"2020/08/19, 18:54:58",30911.0,2000548.0,3,prometheus.yaml,0.0,0.0,1.0,0.0
Jaeger,59684931,59679605,3,"2020/01/10, 17:51:33",False,"2020/01/10, 17:51:33",519.0,12405999.0,3,"Per the log file, there are more than 10,000 started threads.",0.0,0.0,1.0,0.0
Jaeger,59684931,59679605,3,"2020/01/10, 17:51:33",False,"2020/01/10, 17:51:33",519.0,12405999.0,3,That's  a lot  even if we don't look at the less that 2 CPUs/cores reserved for the container (limits.cpu = request.cpu = 1600 millicores).,0.0,0.0,1.0,0.0
Jaeger,59684931,59679605,3,"2020/01/10, 17:51:33",False,"2020/01/10, 17:51:33",519.0,12405999.0,3,"Each thread, and its stack, is allocated in memory separate from the heap.",0.0,0.0,1.0,0.0
Jaeger,59684931,59679605,3,"2020/01/10, 17:51:33",False,"2020/01/10, 17:51:33",519.0,12405999.0,3,It is quite possible that the large number of started threads is the cause for the OOM problem.,0.135,0.065,0.8,-0.34
Jaeger,59684931,59679605,3,"2020/01/10, 17:51:33",False,"2020/01/10, 17:51:33",519.0,12405999.0,3,"The JVM is started with the Native Memory Tracking related options ( -XX:NativeMemoryTracking=detail, -XX:+UnlockDiagnosticVMOptions, -XX:+PrintNMTStatistics)  that could help to see the memory usage, including what's consumed by those threads.",0.0,0.091,0.909,0.4019
Jaeger,59684931,59679605,3,"2020/01/10, 17:51:33",False,"2020/01/10, 17:51:33",519.0,12405999.0,3,This doc  could be a starting point for Java 11.,0.0,0.0,1.0,0.0
Jaeger,59684931,59679605,3,"2020/01/10, 17:51:33",False,"2020/01/10, 17:51:33",519.0,12405999.0,3,"In any case, it would be highly recommended to  not  have that many threads started.",0.0,0.13,0.87,0.2716
Jaeger,59684931,59679605,3,"2020/01/10, 17:51:33",False,"2020/01/10, 17:51:33",519.0,12405999.0,3,E.g.,0.0,0.0,1.0,0.0
Jaeger,59684931,59679605,3,"2020/01/10, 17:51:33",False,"2020/01/10, 17:51:33",519.0,12405999.0,3,"use a pool, start and stop them when not needed anymore...",0.196,0.0,0.804,-0.296
Jaeger,59717806,59679605,0,"2020/01/13, 15:39:55",False,"2020/01/13, 15:39:55",2389.0,12524159.0,2,There are two reasons a container is OOM Killed: Container Quota and System Quota.,0.273,0.0,0.727,-0.6705
Jaeger,59717806,59679605,0,"2020/01/13, 15:39:55",False,"2020/01/13, 15:39:55",2389.0,12524159.0,2,OOM Killer  only  triggers with memory related issues.,0.381,0.0,0.619,-0.6486
Jaeger,59717806,59679605,0,"2020/01/13, 15:39:55",False,"2020/01/13, 15:39:55",2389.0,12524159.0,2,"If your system is far from being out of memory, there is probably a limit in your container.",0.0,0.0,1.0,0.0
Jaeger,59717806,59679605,0,"2020/01/13, 15:39:55",False,"2020/01/13, 15:39:55",2389.0,12524159.0,2,"To your process inside the pod, the pod resource limit is like the whole system being OOM.",0.0,0.135,0.865,0.3612
Jaeger,59717806,59679605,0,"2020/01/13, 15:39:55",False,"2020/01/13, 15:39:55",2389.0,12524159.0,2,"Also, it's worth checking the Resource Requests because by default they are not set.",0.0,0.128,0.872,0.2263
Jaeger,59717806,59679605,0,"2020/01/13, 15:39:55",False,"2020/01/13, 15:39:55",2389.0,12524159.0,2,Requests must be less than or equal to container limits.,0.0,0.0,1.0,0.0
Jaeger,59717806,59679605,0,"2020/01/13, 15:39:55",False,"2020/01/13, 15:39:55",2389.0,12524159.0,2,That means that containers could be overcommitted on nodes and killed by  OOMK if multiple containers are using more memory than their respective requests at the same time.,0.135,0.084,0.781,-0.4019
Jaeger,60649575,59679605,0,"2020/03/12, 09:43:50",True,"2020/03/12, 09:43:50",2227.0,1004374.0,1,In my case the issue was with debugger component that is located in CMD line of Docker file,0.0,0.0,1.0,0.0
Jaeger,60649575,59679605,0,"2020/03/12, 09:43:50",True,"2020/03/12, 09:43:50",2227.0,1004374.0,1,After removal application stopped leaking.,0.322,0.0,0.678,-0.2263
Jaeger,60649575,59679605,0,"2020/03/12, 09:43:50",True,"2020/03/12, 09:43:50",2227.0,1004374.0,1,But disappeared only native memory leak.,0.577,0.0,0.423,-0.6652
Jaeger,60649575,59679605,0,"2020/03/12, 09:43:50",True,"2020/03/12, 09:43:50",2227.0,1004374.0,1,As later investigated there also was heap memory leak induced by jaegger tracer component (luckily here we have much more tools).,0.107,0.0,0.893,-0.34
Jaeger,60649575,59679605,0,"2020/03/12, 09:43:50",True,"2020/03/12, 09:43:50",2227.0,1004374.0,1,After its removal application became stable.,0.0,0.306,0.694,0.296
Jaeger,60649575,59679605,0,"2020/03/12, 09:43:50",True,"2020/03/12, 09:43:50",2227.0,1004374.0,1,I don't know if those components were leaky by itself or with combination with other components but fact is that now it is stable.,0.0,0.113,0.887,0.4215
Jaeger,58318625,58249869,0,"2019/10/10, 11:31:45",False,"2019/10/10, 11:31:45",6049.0,3156333.0,0,"Istio have this feature called  Distributed Tracing , which enables users to track requests in mesh that is distributed across multiple services.",0.0,0.0,1.0,0.0
Jaeger,58318625,58249869,0,"2019/10/10, 11:31:45",False,"2019/10/10, 11:31:45",6049.0,3156333.0,0,"This can be used to visualize request latency, serialization and parallelism.",0.0,0.0,1.0,0.0
Jaeger,58318625,58249869,0,"2019/10/10, 11:31:45",False,"2019/10/10, 11:31:45",6049.0,3156333.0,0,For this to work Istio uses  Envoy Proxy - Tracing  feature.,0.0,0.0,1.0,0.0
Jaeger,58318625,58249869,0,"2019/10/10, 11:31:45",False,"2019/10/10, 11:31:45",6049.0,3156333.0,0,You can deploy  Bookinfo Application  and see how  Trace context propagation  works.,0.0,0.0,1.0,0.0
Jaeger,58414249,58249869,0,"2019/10/16, 16:14:47",False,"2019/10/16, 16:14:47",334.0,4734545.0,0,"If you have the same issue explained in this ticket, you need to wait for the next release of micronaut or use the workaround mentioned by micronaut guys there.",0.0,0.0,1.0,0.0
Jaeger,58414249,58249869,0,"2019/10/16, 16:14:47",False,"2019/10/16, 16:14:47",334.0,4734545.0,0,https://github.com/micronaut-projects/micronaut-core/issues/2209,0.0,0.0,1.0,0.0
Jaeger,58209862,58209785,0,"2019/10/03, 00:55:41",False,"2019/10/03, 00:55:41",966.0,324449.0,0,"latest  is just a tag like any other -- you will want  docker image inspect , which will give you information about the other tags on your image.",0.0,0.137,0.863,0.4215
Jaeger,58209862,58209785,0,"2019/10/03, 00:55:41",False,"2019/10/03, 00:55:41",966.0,324449.0,0,"In the case of  jaegertracing/jaeger-agent:latest , it doesn't look this image has any other tags, so it's probable that this image is tracking something like the master branch of a source control repository, i.e., it doesn't correspond to a published version at all.",0.0,0.06,0.94,0.3612
Jaeger,58244607,58209785,0,"2019/10/05, 04:10:08",False,"2019/10/05, 04:10:08",12605.0,11934042.0,1,"As @max-gasner mentioned, it's common for  latest  to be tracking the  master  branch of a git repository.",0.0,0.0,1.0,0.0
Jaeger,58244607,58209785,0,"2019/10/05, 04:10:08",False,"2019/10/05, 04:10:08",12605.0,11934042.0,1,This allows the engineers to quickly build and test images before they are released and version tagged.,0.0,0.0,1.0,0.0
Jaeger,58244607,58209785,0,"2019/10/05, 04:10:08",False,"2019/10/05, 04:10:08",12605.0,11934042.0,1,This is one of the reasons why it's not recommended to ever use  latest  tags for anything critical where you need reproducibility.,0.163,0.0,0.837,-0.4389
Jaeger,58244607,58209785,0,"2019/10/05, 04:10:08",False,"2019/10/05, 04:10:08",12605.0,11934042.0,1,"jaegertracing/jaeger-agent:latest  doesn't have any other tags so the only way to determine which ""version"" of  latest  you are using is to look at the digest.",0.0,0.0,1.0,0.0
Jaeger,58244607,58209785,0,"2019/10/05, 04:10:08",False,"2019/10/05, 04:10:08",12605.0,11934042.0,1,This uniquely identifies the image build.,0.0,0.0,1.0,0.0
Jaeger,58244607,58209785,0,"2019/10/05, 04:10:08",False,"2019/10/05, 04:10:08",12605.0,11934042.0,1,Tags actually resolve to digests.,0.0,0.394,0.606,0.3818
Jaeger,58244607,58209785,0,"2019/10/05, 04:10:08",False,"2019/10/05, 04:10:08",12605.0,11934042.0,1,"So when a new image is built with the  latest  tag, that tag will then resolve to the digest of the new image.",0.0,0.11,0.89,0.3818
Jaeger,58244607,58209785,0,"2019/10/05, 04:10:08",False,"2019/10/05, 04:10:08",12605.0,11934042.0,1,DockerHub only shows the short version.,0.0,0.0,1.0,0.0
Jaeger,58244607,58209785,0,"2019/10/05, 04:10:08",False,"2019/10/05, 04:10:08",12605.0,11934042.0,1,You can inspect the full digest like this:,0.0,0.263,0.737,0.3612
Jaeger,52908915,52875462,1,"2018/10/20, 21:37:17",True,"2018/10/20, 21:37:17",46.0,4618624.0,2,There are similar ideas in this zipkin/brave repo by @jeqo.,0.0,0.0,1.0,0.0
Jaeger,52908915,52875462,1,"2018/10/20, 21:37:17",True,"2018/10/20, 21:37:17",46.0,4618624.0,2,https://github.com/jeqo/brave/tree/kafka-streams-processor/instrumentation/kafka-streams,0.0,0.0,1.0,0.0
Jaeger,52908915,52875462,1,"2018/10/20, 21:37:17",True,"2018/10/20, 21:37:17",46.0,4618624.0,2,There also seems to be something available in opentracing-contrib repo but it seems to only at trace producer/consumer level.,0.0,0.0,1.0,0.0
Jaeger,52908915,52875462,1,"2018/10/20, 21:37:17",True,"2018/10/20, 21:37:17",46.0,4618624.0,2,https://github.com/opentracing-contrib/java-kafka-client/tree/master/opentracing-kafka-streams,0.0,0.0,1.0,0.0
Jaeger,64052707,64050154,0,"2020/09/24, 21:55:46",False,"2020/09/24, 22:01:08",2441.0,10020419.0,4,"As, you can see there are few missing components - There are few pods missing istio-citadel, istio-pilot, istio-policy, istio-sidecar, istio-telemetry, istio-tracing etc.",0.188,0.0,0.812,-0.5267
Jaeger,64052707,64050154,0,"2020/09/24, 21:55:46",False,"2020/09/24, 22:01:08",2441.0,10020419.0,4,These components were available in 1.4.2.,0.0,0.0,1.0,0.0
Jaeger,64052707,64050154,0,"2020/09/24, 21:55:46",False,"2020/09/24, 22:01:08",2441.0,10020419.0,4,These components where merged with version 1.5 into one service named  istiod .,0.0,0.0,1.0,0.0
Jaeger,64052707,64050154,0,"2020/09/24, 21:55:46",False,"2020/09/24, 22:01:08",2441.0,10020419.0,4,See:  https://istio.io/latest/blog/2020/istiod/,0.0,0.0,1.0,0.0
Jaeger,64052707,64050154,0,"2020/09/24, 21:55:46",False,"2020/09/24, 22:01:08",2441.0,10020419.0,4,"In 1.4.2 installation I could see grafana, jaeger, kiali, prometheus, zipkin dashboards.",0.0,0.0,1.0,0.0
Jaeger,64052707,64050154,0,"2020/09/24, 21:55:46",False,"2020/09/24, 22:01:08",2441.0,10020419.0,4,But these are now missing.,0.412,0.0,0.588,-0.4215
Jaeger,64052707,64050154,0,"2020/09/24, 21:55:46",False,"2020/09/24, 22:01:08",2441.0,10020419.0,4,These AddonComponents must be installed manually and are not part of  istioctl  since version 1.7.,0.0,0.0,1.0,0.0
Jaeger,64052707,64050154,0,"2020/09/24, 21:55:46",False,"2020/09/24, 22:01:08",2441.0,10020419.0,4,See:  https://istio.io/latest/blog/2020/addon-rework/,0.0,0.0,1.0,0.0
Jaeger,64052707,64050154,0,"2020/09/24, 21:55:46",False,"2020/09/24, 22:01:08",2441.0,10020419.0,4,So your installation is not broken.,0.0,0.338,0.662,0.3724
Jaeger,64052707,64050154,0,"2020/09/24, 21:55:46",False,"2020/09/24, 22:01:08",2441.0,10020419.0,4,It's just a lot has changed since 1.4.,0.0,0.0,1.0,0.0
Jaeger,64052707,64050154,0,"2020/09/24, 21:55:46",False,"2020/09/24, 22:01:08",2441.0,10020419.0,4,I would suggest to go through the release announcements to read about all changes:  https://istio.io/latest/news/releases/,0.0,0.0,1.0,0.0
Jaeger,62861784,62859856,0,"2020/07/12, 16:39:14",True,"2020/07/12, 16:39:14",654.0,955379.0,8,Iv finally found the solution.,0.0,0.365,0.635,0.3182
Jaeger,62861784,62859856,0,"2020/07/12, 16:39:14",True,"2020/07/12, 16:39:14",654.0,955379.0,8,It seemed to have to do with how the reporter is started up.,0.0,0.0,1.0,0.0
Jaeger,62861784,62859856,0,"2020/07/12, 16:39:14",True,"2020/07/12, 16:39:14",654.0,955379.0,8,"Anyhow, I changed my tracer class to this.",0.0,0.0,1.0,0.0
Jaeger,62861784,62859856,0,"2020/07/12, 16:39:14",True,"2020/07/12, 16:39:14",654.0,955379.0,8,I know there are several inactive variables here right now.,0.0,0.0,1.0,0.0
Jaeger,62861784,62859856,0,"2020/07/12, 16:39:14",True,"2020/07/12, 16:39:14",654.0,955379.0,8,Will see if they still can be of use some how.,0.0,0.0,1.0,0.0
Jaeger,62861784,62859856,0,"2020/07/12, 16:39:14",True,"2020/07/12, 16:39:14",654.0,955379.0,8,But none is needed right now to get it rolling.,0.0,0.0,1.0,0.0
Jaeger,62861784,62859856,0,"2020/07/12, 16:39:14",True,"2020/07/12, 16:39:14",654.0,955379.0,8,Hope this might help someone else trying to get the .NET Core working properly together with a remote Jeagertracing server.,0.0,0.248,0.752,0.6808
Jaeger,60706055,60636937,0,"2020/03/16, 14:29:14",False,"2020/03/16, 14:29:14",596.0,5089120.0,1,I use  io.opentracing.contrib:opentracing-spring-rabbitmq-starter:3.0.0 .,0.0,0.0,1.0,0.0
Jaeger,60706055,60636937,0,"2020/03/16, 14:29:14",False,"2020/03/16, 14:29:14",596.0,5089120.0,1,You can find some documentation  here .,0.0,0.0,1.0,0.0
Jaeger,60706055,60636937,0,"2020/03/16, 14:29:14",False,"2020/03/16, 14:29:14",596.0,5089120.0,1,With this mechanism I achieved exactly what you asked for.,0.0,0.0,1.0,0.0
Jaeger,60706055,60636937,0,"2020/03/16, 14:29:14",False,"2020/03/16, 14:29:14",596.0,5089120.0,1,I found 2 important things to make sure:,0.0,0.506,0.494,0.4767
Jaeger,60706055,60636937,0,"2020/03/16, 14:29:14",False,"2020/03/16, 14:29:14",596.0,5089120.0,1,Hopefully you find this helpful.,0.0,0.665,0.335,0.714
Jaeger,57119911,57119864,0,"2019/07/20, 00:47:14",True,"2019/07/20, 00:47:14",1067.0,1795610.0,1,Ugh.,1.0,0.0,0.0,-0.4215
Jaeger,57119911,57119864,0,"2019/07/20, 00:47:14",True,"2019/07/20, 00:47:14",1067.0,1795610.0,1,I am an idiot.,0.623,0.0,0.377,-0.5106
Jaeger,57119911,57119864,0,"2019/07/20, 00:47:14",True,"2019/07/20, 00:47:14",1067.0,1795610.0,1,Here is what was going wrong for anyone else who might be stuck something like this:,0.248,0.121,0.631,-0.3818
Jaeger,57119911,57119864,0,"2019/07/20, 00:47:14",True,"2019/07/20, 00:47:14",1067.0,1795610.0,1,"The frontend application  is  receiving a header, I was just looking in the wrong place.",0.205,0.0,0.795,-0.4767
Jaeger,57119911,57119864,0,"2019/07/20, 00:47:14",True,"2019/07/20, 00:47:14",1067.0,1795610.0,1,The request comes from the load balancer to the node frontend microservice which sends its response to the browser.,0.0,0.0,1.0,0.0
Jaeger,57119911,57119864,0,"2019/07/20, 00:47:14",True,"2019/07/20, 00:47:14",1067.0,1795610.0,1,"I was checking the browser for the header, but the node frontend microservice was not forwarding this header to the browser.",0.0,0.0,1.0,0.0
Jaeger,56143481,56054438,0,"2019/05/15, 10:07:09",True,"2019/05/15, 10:07:09",1520.0,824434.0,3,If anyone is interested; I ended up solving this by creating some publish and consume MassTransit middleware to do the trace propagation via trace injection and extraction respectively.,0.0,0.297,0.703,0.8271
Jaeger,56143481,56054438,0,"2019/05/15, 10:07:09",True,"2019/05/15, 10:07:09",1520.0,824434.0,3,I've put the solution up on GitHub -  https://github.com/yesmarket/MassTransit.OpenTracing,0.0,0.247,0.753,0.3182
Jaeger,56143481,56054438,0,"2019/05/15, 10:07:09",True,"2019/05/15, 10:07:09",1520.0,824434.0,3,Still interested to hear if there's a better way of doing this...,0.0,0.384,0.616,0.6808
Jaeger,55858211,55506381,1,"2019/04/26, 01:01:46",True,"2019/04/26, 01:01:46",1448.0,5788941.0,2,"In istio 1.1, the default sampling rate is 1%, so you need to send at least 100 requests before the first trace is visible.",0.0,0.0,1.0,0.0
Jaeger,55858211,55506381,1,"2019/04/26, 01:01:46",True,"2019/04/26, 01:01:46",1448.0,5788941.0,2,This can be configured through the  pilot.traceSampling  option.,0.0,0.0,1.0,0.0
Jaeger,54325585,54325234,2,"2019/01/23, 12:54:00",True,"2019/01/23, 12:54:00",13313.0,524946.0,2,OpenTracing is the API that  your  code will interact with directly.,0.0,0.0,1.0,0.0
Jaeger,54325585,54325234,2,"2019/01/23, 12:54:00",True,"2019/01/23, 12:54:00",13313.0,524946.0,2,"Basically, your application would be ""instrumented"" using the OpenTracing API and a concrete tracer (like Jaeger or Brave/Zipkin) would capture the data and send it somewhere.",0.0,0.0,1.0,0.0
Jaeger,54325585,54325234,2,"2019/01/23, 12:54:00",True,"2019/01/23, 12:54:00",13313.0,524946.0,2,This allows your application to use a neutral API throughout your code so that you can change from one provider to another without having to change your entire code base.,0.0,0.0,1.0,0.0
Jaeger,54325585,54325234,2,"2019/01/23, 12:54:00",True,"2019/01/23, 12:54:00",13313.0,524946.0,2,"Another way to think about it is that OpenTracing is like SLF4J in the Java logging world, whereas Jaeger and Zipkin are the concrete implementations, such as Log4j in the Java logging world.",0.0,0.072,0.928,0.3612
Jaeger,59524696,48794097,0,"2019/12/30, 03:11:09",False,"2019/12/30, 03:11:09",2172.0,3514300.0,0,Trace context propagation might be missing.,0.306,0.0,0.694,-0.296
Jaeger,59524696,48794097,0,"2019/12/30, 03:11:09",False,"2019/12/30, 03:11:09",2172.0,3514300.0,0,https://istio.io/docs/tasks/observability/distributed-tracing/overview/#trace-context-propagation,0.0,0.0,1.0,0.0
Jaeger,25716102,25715304,4,"2014/09/08, 03:49:31",False,"2014/09/08, 03:49:31",194.0,1478294.0,0,"I cannot test your code, but the only thing i can think off, is that the order of execution is wrong.",0.187,0.0,0.813,-0.631
Jaeger,25716102,25715304,4,"2014/09/08, 03:49:31",False,"2014/09/08, 03:49:31",194.0,1478294.0,0,"You first make a string, then you make it Base64, then you encrypt it.",0.0,0.0,1.0,0.0
Jaeger,25716102,25715304,4,"2014/09/08, 03:49:31",False,"2014/09/08, 03:49:31",194.0,1478294.0,0,Now you undo the Base64 and afterwards you decrypt the encoded string.,0.0,0.0,1.0,0.0
Jaeger,25716102,25715304,4,"2014/09/08, 03:49:31",False,"2014/09/08, 03:49:31",194.0,1478294.0,0,These last two must be swapped.,0.0,0.0,1.0,0.0
Jaeger,25756628,25715304,1,"2014/09/10, 05:56:20",True,"2014/09/10, 08:14:52",22907.0,2714852.0,7,"A reminder that  security is unusually treacherous territory , and if there's a way to call on other well-tested code even more of your toplevel task than just what Go's OpenPGP package is handling for you, consider it.",0.0,0.066,0.934,0.34
Jaeger,25756628,25715304,1,"2014/09/10, 05:56:20",True,"2014/09/10, 08:14:52",22907.0,2714852.0,7,It's good that at least low-level details are outsourced to  openpgp  because they're nasty and so so easy to get wrong.,0.218,0.228,0.554,0.0745
Jaeger,25756628,25715304,1,"2014/09/10, 05:56:20",True,"2014/09/10, 08:14:52",22907.0,2714852.0,7,"But tiny mistakes at any level can make crypto features worse than useless; if there's a way to write less security-critical code, that's one of the best things anyone can do for security.",0.236,0.189,0.574,-0.296
Jaeger,25756628,25715304,1,"2014/09/10, 05:56:20",True,"2014/09/10, 08:14:52",22907.0,2714852.0,7,"On the specific question: you have to  Close()  the writer to get everything flushed out (a trait OpenPGP's writer shares with, say,  compress/gzip 's).",0.0,0.087,0.913,0.296
Jaeger,25756628,25715304,1,"2014/09/10, 05:56:20",True,"2014/09/10, 08:14:52",22907.0,2714852.0,7,"Unrelated changes: the way you're printing things is a better fit  log.Println , which just lets you pass a bunch of values you want printed with spaces in between (like, say, Python  print ), rather than needing format specifiers like  ""%s""  or  ""%d"" .",0.0,0.254,0.746,0.872
Jaeger,25756628,25715304,1,"2014/09/10, 05:56:20",True,"2014/09/10, 08:14:52",22907.0,2714852.0,7,"(The ""EXTRA"" in your initial output is what Go's  Printf  emits when you pass more things than you had format specifiers for.)",0.0,0.0,1.0,0.0
Jaeger,25756628,25715304,1,"2014/09/10, 05:56:20",True,"2014/09/10, 08:14:52",22907.0,2714852.0,7,"It's also best practice to check errors (I dropped  if err != nil s where I saw a need, but inelegantly and without much thought, and I may not have gotten all the calls) and to run  go fmt  on your code.",0.042,0.071,0.887,0.2942
Jaeger,25756628,25715304,1,"2014/09/10, 05:56:20",True,"2014/09/10, 08:14:52",22907.0,2714852.0,7,"Again,  I can't testify to the seaworthiness of this code or anything like that.",0.0,0.172,0.828,0.3612
Jaeger,25756628,25715304,1,"2014/09/10, 05:56:20",True,"2014/09/10, 08:14:52",22907.0,2714852.0,7,But now it round-trips all the text.,0.0,0.0,1.0,0.0
Jaeger,25756628,25715304,1,"2014/09/10, 05:56:20",True,"2014/09/10, 08:14:52",22907.0,2714852.0,7,I wound up with:,0.0,0.0,1.0,0.0
Jaeger,62563034,62560345,1,"2020/06/24, 22:52:11",True,"2020/06/24, 23:07:58",3381.0,4108803.0,1,"You could try with  StartSpanFromContext , inside your gRPC handlers:",0.0,0.0,1.0,0.0
Jaeger,62563034,62560345,1,"2020/06/24, 22:52:11",True,"2020/06/24, 23:07:58",3381.0,4108803.0,1,As the documentation of  otgrpc.OpenTracingServerInterceptor  says:,0.0,0.0,1.0,0.0
Jaeger,62563034,62560345,1,"2020/06/24, 22:52:11",True,"2020/06/24, 23:07:58",3381.0,4108803.0,1,"[...] the server Span will be embedded in the context.Context for the
application-specific gRPC handler(s) to access.",0.0,0.0,1.0,0.0
Jaeger,62563034,62560345,1,"2020/06/24, 22:52:11",True,"2020/06/24, 23:07:58",3381.0,4108803.0,1,If we look at the function implementation:,0.0,0.0,1.0,0.0
Jaeger,62563034,62560345,1,"2020/06/24, 22:52:11",True,"2020/06/24, 23:07:58",3381.0,4108803.0,1,"
 Edit : Given the above, you probably can omit this code:",0.0,0.0,1.0,0.0
Jaeger,61468536,60753860,0,"2020/04/28, 00:11:21",True,"2020/04/28, 00:11:21",184.0,8575474.0,0,The issue was related to  yaml  file parsing,0.0,0.0,1.0,0.0
Jaeger,57877033,57588551,0,"2019/09/10, 22:21:23",False,"2019/09/10, 22:21:23",1040.0,1688998.0,0,"Jaeger has a UI to look at your data, but no tools to create statistics.",0.16,0.152,0.688,-0.0387
Jaeger,57877033,57588551,0,"2019/09/10, 22:21:23",False,"2019/09/10, 22:21:23",1040.0,1688998.0,0,However all your data is being stored in a DB of your choice.,0.0,0.0,1.0,0.0
Jaeger,57877033,57588551,0,"2019/09/10, 22:21:23",False,"2019/09/10, 22:21:23",1040.0,1688998.0,0,Storing it in e.g.,0.0,0.0,1.0,0.0
Jaeger,57877033,57588551,0,"2019/09/10, 22:21:23",False,"2019/09/10, 22:21:23",1040.0,1688998.0,0,Elasticsearch gives you a powerful query language to look at the data as well as many other tools that integrate with it.,0.0,0.205,0.795,0.5994
Jaeger,56349205,56135086,5,"2019/05/28, 22:52:10",False,"2019/05/28, 22:52:10",838.0,10587554.0,0,"Correct me, if I'm wrong.",0.437,0.0,0.563,-0.4767
Jaeger,56349205,56135086,5,"2019/05/28, 22:52:10",False,"2019/05/28, 22:52:10",838.0,10587554.0,0,"If you mean how to find the trace-id on the server side, you can try to access the OpenTracing span by  get_active_span .",0.0,0.0,1.0,0.0
Jaeger,56349205,56135086,5,"2019/05/28, 22:52:10",False,"2019/05/28, 22:52:10",838.0,10587554.0,0,"The trace-id, I suppose, should be one of the tags in it.",0.0,0.0,1.0,0.0
Jaeger,56584198,56135086,0,"2019/06/13, 18:46:22",True,"2019/06/13, 18:46:22",5906.0,474819.0,1,I had missed a key piece of documentation.,0.306,0.0,0.694,-0.296
Jaeger,56584198,56135086,0,"2019/06/13, 18:46:22",True,"2019/06/13, 18:46:22",5906.0,474819.0,1,"In order to get a trace ID, you have to create a span on the client side.",0.0,0.13,0.87,0.2732
Jaeger,56584198,56135086,0,"2019/06/13, 18:46:22",True,"2019/06/13, 18:46:22",5906.0,474819.0,1,This span will have the trace ID that can be used to examine data in the Jaeger UI.,0.0,0.0,1.0,0.0
Jaeger,56584198,56135086,0,"2019/06/13, 18:46:22",True,"2019/06/13, 18:46:22",5906.0,474819.0,1,The span has to be added into the GRPC messages via an  ActiveSpanSource  instance.,0.0,0.0,1.0,0.0
Jaeger,56584198,56135086,0,"2019/06/13, 18:46:22",True,"2019/06/13, 18:46:22",5906.0,474819.0,1,"Of course, you could switch the ordering of the  with  statements so that the span is created after the GRPC channel.",0.0,0.091,0.909,0.25
Jaeger,56584198,56135086,0,"2019/06/13, 18:46:22",True,"2019/06/13, 18:46:22",5906.0,474819.0,1,That part doesn't make any difference.,0.0,0.0,1.0,0.0
Jaeger,53131562,53131007,1,"2018/11/03, 15:00:17",True,"2018/11/03, 15:00:17",17024.0,858913.0,0,"You assumption is correct, the elements are there, but not exactly where you think they are.",0.0,0.0,1.0,0.0
Jaeger,53131562,53131007,1,"2018/11/03, 15:00:17",True,"2018/11/03, 15:00:17",17024.0,858913.0,0,To easily check if an element is part of the response html and not being loaded by javascript I normally recommend using a  browser plugin to disable javascript .,0.0,0.17,0.83,0.5994
Jaeger,53131562,53131007,1,"2018/11/03, 15:00:17",True,"2018/11/03, 15:00:17",17024.0,858913.0,0,"If you want the images, they are still part of the html response, you can get them with:",0.0,0.071,0.929,0.0772
Jaeger,53131562,53131007,1,"2018/11/03, 15:00:17",True,"2018/11/03, 15:00:17",17024.0,858913.0,0,the main image appears separately:,0.0,0.0,1.0,0.0
Jaeger,53131562,53131007,1,"2018/11/03, 15:00:17",True,"2018/11/03, 15:00:17",17024.0,858913.0,0,Hope that helps you.,0.0,0.733,0.267,0.6705
Jaeger,48593245,48592875,1,"2018/02/03, 04:04:40",False,"2018/02/03, 04:14:38",31.0,7234392.0,1,You haven't initialized the variables for the next few iterations.,0.0,0.0,1.0,0.0
Jaeger,48593245,48592875,1,"2018/02/03, 04:04:40",False,"2018/02/03, 04:14:38",31.0,7234392.0,1,You need to reinitialize the variables used for while loop's condition check outside their respective while loops.,0.0,0.149,0.851,0.4215
Jaeger,48593245,48592875,1,"2018/02/03, 04:04:40",False,"2018/02/03, 04:14:38",31.0,7234392.0,1,i.e,0.0,0.0,1.0,0.0
Jaeger,48593245,48592875,1,"2018/02/03, 04:04:40",False,"2018/02/03, 04:14:38",31.0,7234392.0,1,Similarly do it for the while loops which use variables  c  &amp;  d .,0.0,0.0,1.0,0.0
Jaeger,48594819,48592875,0,"2018/02/03, 08:59:49",False,"2018/02/03, 08:59:49",15431.0,3992939.0,0,"Daniel's answer is correct  : the 
structure of the  while  loop should be:",0.0,0.0,1.0,0.0
Jaeger,36821033,36819104,1,"2016/04/24, 11:48:50",True,"2016/04/24, 12:14:06",2648.0,6230407.0,0,"It may not be possible to input text with conditional formatting, but you can change the font color.",0.0,0.0,1.0,0.0
Jaeger,36821033,36819104,1,"2016/04/24, 11:48:50",True,"2016/04/24, 12:14:06",2648.0,6230407.0,0,"A solution could be to put the word ""LATE in the specified cell(s) beforehand and set the font-color equal to the background-color, which makes the word invisable.",0.0,0.084,0.916,0.3182
Jaeger,36821033,36819104,1,"2016/04/24, 11:48:50",True,"2016/04/24, 12:14:06",2648.0,6230407.0,0,"When the condition (formula) evaluates true, the new format will change the font-color and the word LATE appears.",0.0,0.141,0.859,0.4215
Jaeger,36821033,36819104,1,"2016/04/24, 11:48:50",True,"2016/04/24, 12:14:06",2648.0,6230407.0,0,No VBA requiered.,0.524,0.0,0.476,-0.296
Jaeger,36821033,36819104,1,"2016/04/24, 11:48:50",True,"2016/04/24, 12:14:06",2648.0,6230407.0,0,On the other hand: wouldn't a simple if-formula be better?,0.0,0.466,0.534,0.7269
Jaeger,36821033,36819104,1,"2016/04/24, 11:48:50",True,"2016/04/24, 12:14:06",2648.0,6230407.0,0,Something like:,0.0,0.714,0.286,0.3612
Jaeger,36821033,36819104,1,"2016/04/24, 11:48:50",True,"2016/04/24, 12:14:06",2648.0,6230407.0,0,If you wish you can then change the background with conditional formating,0.0,0.197,0.803,0.4019
Jaeger,60859448,60846142,1,"2020/03/26, 02:44:07",False,"2020/03/26, 02:44:07",50169.0,594589.0,0,There's really not many options for other than starting a span in each function you'd like to instrument:,0.0,0.135,0.865,0.3612
Jaeger,60859448,60846142,1,"2020/03/26, 02:44:07",False,"2020/03/26, 02:44:07",50169.0,594589.0,0,"If your functions have a common call signature, or you can coalesce your function into a common call signature, you can write a wrapper.",0.0,0.0,1.0,0.0
Jaeger,60859448,60846142,1,"2020/03/26, 02:44:07",False,"2020/03/26, 02:44:07",50169.0,594589.0,0,"Examples of this can be seen in http  ""middleware"" .",0.0,0.0,1.0,0.0
Jaeger,60859448,60846142,1,"2020/03/26, 02:44:07",False,"2020/03/26, 02:44:07",50169.0,594589.0,0,"Consider the http.Handler, you could write a  decorator  for your functions that handles the span lifecycle:",0.0,0.0,1.0,0.0
Jaeger,60859448,60846142,1,"2020/03/26, 02:44:07",False,"2020/03/26, 02:44:07",50169.0,594589.0,0,A similar pattern could be applied by  embedding  structs.,0.0,0.0,1.0,0.0
Jaeger,58197357,58148102,0,"2019/10/02, 10:55:46",True,"2019/10/02, 10:55:46",109.0,318536.0,2,Is it possible to set these when working on OpenShift?,0.0,0.0,1.0,0.0
Jaeger,58197357,58148102,0,"2019/10/02, 10:55:46",True,"2019/10/02, 10:55:46",109.0,318536.0,2,"Yes, you can configure it for the Che master of your installation.",0.0,0.197,0.803,0.4019
Jaeger,58197357,58148102,0,"2019/10/02, 10:55:46",True,"2019/10/02, 10:55:46",109.0,318536.0,2,OpenShift is the Saas Che offering,0.0,0.0,1.0,0.0
Jaeger,58197357,58148102,0,"2019/10/02, 10:55:46",True,"2019/10/02, 10:55:46",109.0,318536.0,2,As a user of che.openshift.io you can't leverage from tracing capabilities of Che at this moment.,0.0,0.0,1.0,0.0
Jaeger,57297860,57292517,0,"2019/07/31, 22:41:03",False,"2019/07/31, 22:41:03",48155.0,2989261.0,2,This 'appears' to be related to the switch from AWS CNI to weave.,0.0,0.0,1.0,0.0
Jaeger,57297860,57292517,0,"2019/07/31, 22:41:03",False,"2019/07/31, 22:41:03",48155.0,2989261.0,2,"CNI uses the IP range of your VPC while weave uses its own address range (for pods), so there may be remaining iptables rules from AWS CNI, for example.",0.0,0.0,1.0,0.0
Jaeger,57297860,57292517,0,"2019/07/31, 22:41:03",False,"2019/07/31, 22:41:03",48155.0,2989261.0,2,"Internal error occurred: failed calling admission webhook ""pilot.validation.istio.io"": Post  https://istio-galley.istio-system.svc:443/admitpilot?timeout=30s : Address is not allowed",0.333,0.0,0.667,-0.7184
Jaeger,57297860,57292517,0,"2019/07/31, 22:41:03",False,"2019/07/31, 22:41:03",48155.0,2989261.0,2,"The message above implies that whatever address  istio-galley.istio-system.svc  resolves to, internally in your K8s cluster, is not a valid IP address.",0.0,0.082,0.918,0.1779
Jaeger,57297860,57292517,0,"2019/07/31, 22:41:03",False,"2019/07/31, 22:41:03",48155.0,2989261.0,2,So I would also try to see what that resolves to.,0.0,0.159,0.841,0.1779
Jaeger,57297860,57292517,0,"2019/07/31, 22:41:03",False,"2019/07/31, 22:41:03",48155.0,2989261.0,2,(It may be related to  coreDNS ).,0.0,0.0,1.0,0.0
Jaeger,57297860,57292517,0,"2019/07/31, 22:41:03",False,"2019/07/31, 22:41:03",48155.0,2989261.0,2,You can also try the following  these steps ;,0.0,0.0,1.0,0.0
Jaeger,57297860,57292517,0,"2019/07/31, 22:41:03",False,"2019/07/31, 22:41:03",48155.0,2989261.0,2,"Basically, (quoted)",0.0,0.0,1.0,0.0
Jaeger,57297860,57292517,0,"2019/07/31, 22:41:03",False,"2019/07/31, 22:41:03",48155.0,2989261.0,2,"Furthermore, you can try reinstalling everything from the beginning using weave.",0.0,0.0,1.0,0.0
Jaeger,57297860,57292517,0,"2019/07/31, 22:41:03",False,"2019/07/31, 22:41:03",48155.0,2989261.0,2,Hope it helps!,0.0,0.853,0.147,0.6996
Jaeger,54199894,54084447,2,"2019/01/15, 15:32:19",True,"2019/01/15, 15:32:19",294930.0,208809.0,2,OpenTracing is a framework for Distributed Tracing.,0.0,0.0,1.0,0.0
Jaeger,54199894,54084447,2,"2019/01/15, 15:32:19",True,"2019/01/15, 15:32:19",294930.0,208809.0,2,"As such, it is more about performance monitoring and observability than logging (what NLog is about).",0.0,0.0,1.0,0.0
Jaeger,54199894,54084447,2,"2019/01/15, 15:32:19",True,"2019/01/15, 15:32:19",294930.0,208809.0,2,OpenTracing allows you to manually instrument your code to generate traces with relevant spans containing information about code execution in your app.,0.0,0.0,1.0,0.0
Jaeger,54199894,54084447,2,"2019/01/15, 15:32:19",True,"2019/01/15, 15:32:19",294930.0,208809.0,2,"This includes annotating spans with errors and arbitrary keys and values, which you  could  use instead of logging.",0.114,0.128,0.758,0.0772
Jaeger,54199894,54084447,2,"2019/01/15, 15:32:19",True,"2019/01/15, 15:32:19",294930.0,208809.0,2,"However, that's not the same as dedicated structured logging.",0.0,0.273,0.727,0.4588
Jaeger,54028704,54021635,1,"2019/01/03, 21:35:20",False,"2019/01/03, 21:35:20",113.0,9236736.0,1,Here's an article/guide on how to work with the limit-ranger and its default values [1],0.0,0.162,0.838,0.4019
Jaeger,54028704,54021635,1,"2019/01/03, 21:35:20",False,"2019/01/03, 21:35:20",113.0,9236736.0,1,[1] https://medium.com/@betz.mark/understanding-resource-limits-in-kubernetes-cpu-time-9eff74d3161b,0.0,0.0,1.0,0.0
Jaeger,54200196,53428114,0,"2019/01/15, 15:48:26",True,"2019/01/15, 15:48:26",294930.0,208809.0,1,"The  span.kind=server  tag denotes an entry span, e.g.",0.0,0.0,1.0,0.0
Jaeger,54200196,53428114,0,"2019/01/15, 15:48:26",True,"2019/01/15, 15:48:26",294930.0,208809.0,1,a span created in the local code in response to an external request.,0.0,0.154,0.846,0.25
Jaeger,54200196,53428114,0,"2019/01/15, 15:48:26",True,"2019/01/15, 15:48:26",294930.0,208809.0,1,"Likewise,  span.kind=client  denotes an exit span, e.g.",0.0,0.0,1.0,0.0
Jaeger,54200196,53428114,0,"2019/01/15, 15:48:26",True,"2019/01/15, 15:48:26",294930.0,208809.0,1,a call made from the local code to another server.,0.0,0.0,1.0,0.0
Jaeger,54200196,53428114,0,"2019/01/15, 15:48:26",True,"2019/01/15, 15:48:26",294930.0,208809.0,1,"In your example, the span generated for Foo is a  span.kind=server  and the span recording the call to Buzz is a  span.kind=client .",0.0,0.0,1.0,0.0
Jaeger,51745456,51723902,2,"2018/08/08, 14:22:23",False,"2018/08/08, 14:22:23",4058.0,9928809.0,1,Kubernetes provides quite a big variety of Networking and Load Balancing features from the box.,0.0,0.0,1.0,0.0
Jaeger,51745456,51723902,2,"2018/08/08, 14:22:23",False,"2018/08/08, 14:22:23",4058.0,9928809.0,1,"However, the idea to simplify and extend the functionality  of  Istio sidecars  is a good choice as they are used for automatic injection into the Pods in order to proxy the traffic between internal Kubernetes services.",0.0,0.122,0.878,0.5574
Jaeger,51745456,51723902,2,"2018/08/08, 14:22:23",False,"2018/08/08, 14:22:23",4058.0,9928809.0,1,You can implement  sidecars  manually or automatically.,0.0,0.0,1.0,0.0
Jaeger,51745456,51723902,2,"2018/08/08, 14:22:23",False,"2018/08/08, 14:22:23",4058.0,9928809.0,1,"If you choose the manual way, make sure to add the appropriate parameter under Pod's annotation field:",0.0,0.126,0.874,0.3182
Jaeger,51745456,51723902,2,"2018/08/08, 14:22:23",False,"2018/08/08, 14:22:23",4058.0,9928809.0,1,"Automatic  sidecar  injection requires  Mutating Webhook admission controller , available since Kubernetes version 1.9 released, therefore  sidecars  can be integrated for Pod's creation process as well.",0.0,0.154,0.846,0.4939
Jaeger,51745456,51723902,2,"2018/08/08, 14:22:23",False,"2018/08/08, 14:22:23",4058.0,9928809.0,1,Get yourself familiar with this  Article  to shed light on using different monitoring and traffic management tools in Istio.,0.0,0.0,1.0,0.0
Jaeger,52545681,51723902,0,"2018/09/28, 00:56:57",True,"2018/09/28, 00:56:57",363.0,3463514.0,1,Yes - it is possible to use external services with istio.,0.0,0.231,0.769,0.4019
Jaeger,52545681,51723902,0,"2018/09/28, 00:56:57",True,"2018/09/28, 00:56:57",363.0,3463514.0,1,"You can disable grafana and prometheus just by setting proper flags in values.yaml of istio helm chart (grafana.enabled=false, etc).",0.0,0.0,1.0,0.0
Jaeger,52545681,51723902,0,"2018/09/28, 00:56:57",True,"2018/09/28, 00:56:57",363.0,3463514.0,1,"You can check  kyma-project  project to see how istio is integrated with prometheus-operator, grafana deployment with custom dashboards, and custom jaeger deployment.",0.0,0.0,1.0,0.0
Jaeger,52545681,51723902,0,"2018/09/28, 00:56:57",True,"2018/09/28, 00:56:57",363.0,3463514.0,1,From your list only certmanager is missing.,0.268,0.0,0.732,-0.296
Jaeger,66249819,66213790,0,"2021/02/17, 22:55:27",False,"2021/02/17, 22:55:27",2140.0,3697695.0,1,I am not sure why Istio doesn't automatically trace your calls to external APIs.,0.141,0.0,0.859,-0.2411
Jaeger,66249819,66213790,0,"2021/02/17, 22:55:27",False,"2021/02/17, 22:55:27",2140.0,3697695.0,1,"Perhaps it requires an egress gateway to be used, I'm not sure.",0.151,0.0,0.849,-0.2411
Jaeger,66249819,66213790,0,"2021/02/17, 22:55:27",False,"2021/02/17, 22:55:27",2140.0,3697695.0,1,"Note also that Istio creates traces for http(s) traffic, not TCP.",0.0,0.174,0.826,0.2732
Jaeger,66249819,66213790,0,"2021/02/17, 22:55:27",False,"2021/02/17, 22:55:27",2140.0,3697695.0,1,"However, this is something you can still do programmatically.",0.0,0.0,1.0,0.0
Jaeger,66249819,66213790,0,"2021/02/17, 22:55:27",False,"2021/02/17, 22:55:27",2140.0,3697695.0,1,You can use any of the  Jaeger client libraries  to augment&quot;the traces already created by Envoy by appending your own spans.,0.0,0.091,0.909,0.25
Jaeger,66249819,66213790,0,"2021/02/17, 22:55:27",False,"2021/02/17, 22:55:27",2140.0,3697695.0,1,"To do so, you need first to extract the trace context from the HTTP headers of the incoming request (assuming that your external API calls are consecutive to an incoming request), and then create a new span as child of that previous span context.",0.0,0.048,0.952,0.2732
Jaeger,66249819,66213790,0,"2021/02/17, 22:55:27",False,"2021/02/17, 22:55:27",2140.0,3697695.0,1,A good idea would be to use  OpenTracing semantic conventions  when you tag your new span.,0.0,0.172,0.828,0.4404
Jaeger,66249819,66213790,0,"2021/02/17, 22:55:27",False,"2021/02/17, 22:55:27",2140.0,3697695.0,1,Tools like Kiali will be able to leverage some information if it follows this convention.,0.0,0.152,0.848,0.3612
Jaeger,66249819,66213790,0,"2021/02/17, 22:55:27",False,"2021/02/17, 22:55:27",2140.0,3697695.0,1,I've found this blog post that explains how to do it with the nodejs jaeger client:  https://rhonabwy.com/2019/01/06/adding-tracing-with-jaeger-to-an-express-application/,0.0,0.0,1.0,0.0
Jaeger,65485312,65377607,0,"2020/12/29, 01:18:49",False,"2020/12/29, 01:18:49",166.0,12249096.0,0,"Yes, the OpenCensus collector should be injected with the Linkerd proxy because the proxies themselves send the span info using mTLS.",0.0,0.119,0.881,0.4019
Jaeger,65485312,65377607,0,"2020/12/29, 01:18:49",False,"2020/12/29, 01:18:49",166.0,12249096.0,0,"With mTLS, the sending (client) and receiving (server) sides of the request must present certificates to each other in to  verify  that identities to each other in a way that validates that the identity was issued by the same trusted source.",0.0,0.126,0.874,0.6705
Jaeger,65485312,65377607,0,"2020/12/29, 01:18:49",False,"2020/12/29, 01:18:49",166.0,12249096.0,0,The Linkerd service mesh is made up of the control plane and the data plane.,0.0,0.0,1.0,0.0
Jaeger,65485312,65377607,0,"2020/12/29, 01:18:49",False,"2020/12/29, 01:18:49",166.0,12249096.0,0,The control plane is a set of services that run within the cluster to implement the features of the service mesh.,0.0,0.0,1.0,0.0
Jaeger,65485312,65377607,0,"2020/12/29, 01:18:49",False,"2020/12/29, 01:18:49",166.0,12249096.0,0,Mutual TLS (mTLS) is one of those features and is implemented by the  linkerd-identity  component of the control plane.,0.0,0.0,1.0,0.0
Jaeger,65485312,65377607,0,"2020/12/29, 01:18:49",False,"2020/12/29, 01:18:49",166.0,12249096.0,0,"The data plane is comprised of any number of the Linkerd proxies which are injected into the services in the application, like the OpenCensus collector.",0.0,0.142,0.858,0.4215
Jaeger,65485312,65377607,0,"2020/12/29, 01:18:49",False,"2020/12/29, 01:18:49",166.0,12249096.0,0,"Whenever a proxy is started within a pod, it sends a certificate signing request to the  linkerd-identity  component and receives a certificate in return.",0.0,0.0,1.0,0.0
Jaeger,65485312,65377607,0,"2020/12/29, 01:18:49",False,"2020/12/29, 01:18:49",166.0,12249096.0,0,"So, when the Linkerd proxies in the control plane send the spans to the collector, they authenticate themselves with those certificates, which must be verified by the proxy injected into the OpenCensus collector Pod.",0.0,0.0,1.0,0.0
Jaeger,65485312,65377607,0,"2020/12/29, 01:18:49",False,"2020/12/29, 01:18:49",166.0,12249096.0,0,"This ensures that all traffic, even distributed traces, are sent securely within the cluster.",0.0,0.156,0.844,0.34
Jaeger,65485312,65377607,0,"2020/12/29, 01:18:49",False,"2020/12/29, 01:18:49",166.0,12249096.0,0,"In your case, you should suffix the service account with the namespace.",0.0,0.0,1.0,0.0
Jaeger,65485312,65377607,0,"2020/12/29, 01:18:49",False,"2020/12/29, 01:18:49",166.0,12249096.0,0,"By default, Linkerd will use the Pod namespace, so if the service account doesn't exist in the Pod namespace, then the configuration will be invalid.",0.0,0.0,1.0,0.0
Jaeger,65485312,65377607,0,"2020/12/29, 01:18:49",False,"2020/12/29, 01:18:49",166.0,12249096.0,0,"The  logic  has a function that checks for a namespace in the annotation name and appends it, if it exists:",0.0,0.0,1.0,0.0
Jaeger,65485312,65377607,0,"2020/12/29, 01:18:49",False,"2020/12/29, 01:18:49",166.0,12249096.0,0,"So, this one is correct:",0.0,0.0,1.0,0.0
Jaeger,64175114,64174906,1,"2020/10/02, 19:29:05",False,"2020/10/02, 19:29:05",1.0,540250.0,0,you are using System.Configuration namespace which causes ambiguity.,0.0,0.0,1.0,0.0
Jaeger,64175114,64174906,1,"2020/10/02, 19:29:05",False,"2020/10/02, 19:29:05",1.0,540250.0,0,i would suggest remove the using System.Configuration.,0.0,0.0,1.0,0.0
Jaeger,64175114,64174906,1,"2020/10/02, 19:29:05",False,"2020/10/02, 19:29:05",1.0,540250.0,0,And try specifying fully qualified name for the Configuration.,0.0,0.0,1.0,0.0
Jaeger,64175114,64174906,1,"2020/10/02, 19:29:05",False,"2020/10/02, 19:29:05",1.0,540250.0,0,visual studio would suggest possible candidates (press Ctrl .,0.0,0.0,1.0,0.0
Jaeger,64175114,64174906,1,"2020/10/02, 19:29:05",False,"2020/10/02, 19:29:05",1.0,540250.0,0,on the Class name you want to qualify) provided you have added all required references in project already.,0.0,0.071,0.929,0.0772
Jaeger,64175147,64174906,4,"2020/10/02, 19:31:24",False,"2020/10/02, 19:31:24",85555.0,880990.0,0,If you have a,0.0,0.0,1.0,0.0
Jaeger,64175147,64174906,4,"2020/10/02, 19:31:24",False,"2020/10/02, 19:31:24",85555.0,880990.0,0,then the C# compiler gets confused with  Configuration  and thinks it refers to the namespace  System.Configuration .,0.133,0.0,0.867,-0.3182
Jaeger,64175147,64174906,4,"2020/10/02, 19:31:24",False,"2020/10/02, 19:31:24",85555.0,880990.0,0,You can solve it by using the explicit namespace  Jaeger :,0.0,0.167,0.833,0.2023
Jaeger,63843385,63843312,1,"2020/09/11, 11:21:58",True,"2020/09/11, 11:21:58",1911.0,11506391.0,2,There is no possibility of doing it in the Dockerfile if you want to keep two separate image.,0.113,0.067,0.821,-0.2263
Jaeger,63843385,63843312,1,"2020/09/11, 11:21:58",True,"2020/09/11, 11:21:58",1911.0,11506391.0,2,How should you know in advance the name/id of the container you're going to link ?,0.0,0.0,1.0,0.0
Jaeger,63843385,63843312,1,"2020/09/11, 11:21:58",True,"2020/09/11, 11:21:58",1911.0,11506391.0,2,Below are two solutions :,0.0,0.362,0.638,0.1779
Jaeger,63843675,63843312,0,"2020/09/11, 11:43:16",False,"2020/09/11, 11:43:16",206.0,9641548.0,0,"I recommend you using  netwoking , by creating:",0.0,0.54,0.46,0.5719
Jaeger,63843675,63843312,0,"2020/09/11, 11:43:16",False,"2020/09/11, 11:43:16",206.0,9641548.0,0,"and then run with --network=&quot;network&quot;
using docker-compose with  network  and link to each other
example:",0.0,0.0,1.0,0.0
Jaeger,60463466,60462137,6,"2020/02/29, 10:52:57",False,"2020/03/01, 14:12:35",35494.0,7582247.0,1,-I  is used for  include  paths.,0.0,0.0,1.0,0.0
Jaeger,60463466,60462137,6,"2020/02/29, 10:52:57",False,"2020/03/01, 14:12:35",35494.0,7582247.0,1,Use  -L  for library paths:,0.0,0.0,1.0,0.0
Jaeger,60463466,60462137,6,"2020/02/29, 10:52:57",False,"2020/03/01, 14:12:35",35494.0,7582247.0,1,It also looks like you've linked with a shared library  libyaml-cppd.so  - not the static library  libyaml-cpp.a .,0.0,0.274,0.726,0.5994
Jaeger,60463466,60462137,6,"2020/02/29, 10:52:57",False,"2020/03/01, 14:12:35",35494.0,7582247.0,1,I don't recognize the  d  in  libyaml-cppd.so  though.,0.0,0.0,1.0,0.0
Jaeger,60463466,60462137,6,"2020/02/29, 10:52:57",False,"2020/03/01, 14:12:35",35494.0,7582247.0,1,I'd check if that's really the library you built.,0.0,0.0,1.0,0.0
Jaeger,60463466,60462137,6,"2020/02/29, 10:52:57",False,"2020/03/01, 14:12:35",35494.0,7582247.0,1,"libyaml-cpp  will be built as a static library by default ( libyaml-cpp.a ) and on a 64 bit machine, it will probably default to being installed in  /usr/local/lib64 .",0.0,0.0,1.0,0.0
Jaeger,60463466,60462137,6,"2020/02/29, 10:52:57",False,"2020/03/01, 14:12:35",35494.0,7582247.0,1,You are only allowed to do very limited things in  namespace std .,0.166,0.0,0.834,-0.2944
Jaeger,60463466,60462137,6,"2020/02/29, 10:52:57",False,"2020/03/01, 14:12:35",35494.0,7582247.0,1,Adding new functions/classes are not allowed (unless as template specializations including user defined types) - so remove  namespace std { ... }  around your program.,0.0,0.0,1.0,0.0
Jaeger,60463466,60462137,6,"2020/02/29, 10:52:57",False,"2020/03/01, 14:12:35",35494.0,7582247.0,1,Also.,0.0,0.0,1.0,0.0
Jaeger,60463466,60462137,6,"2020/02/29, 10:52:57",False,"2020/03/01, 14:12:35",35494.0,7582247.0,1,the  main  function should be in the global namespace.,0.0,0.0,1.0,0.0
Jaeger,60463466,60462137,6,"2020/02/29, 10:52:57",False,"2020/03/01, 14:12:35",35494.0,7582247.0,1,The reason it's not found by the linker is because you put it in a namespace ( std ).,0.0,0.0,1.0,0.0
Jaeger,60463466,60462137,6,"2020/02/29, 10:52:57",False,"2020/03/01, 14:12:35",35494.0,7582247.0,1,UPDATE : The issue is resolved follow this link exaclty  https://github.com/jaegertracing/jaeger-client-cpp/issues/162#issuecomment-565892473  (use thrift version 0.11 or 0.11+),0.0,0.102,0.898,0.1779
Jaeger,59285133,59276096,0,"2019/12/11, 13:47:46",False,"2019/12/11, 13:47:46",2576.0,12014434.0,2,Fisrt of all according to istio  documentation  Prometheus is used as default observation operator in istio mesh by default:,0.0,0.0,1.0,0.0
Jaeger,59285133,59276096,0,"2019/12/11, 13:47:46",False,"2019/12/11, 13:47:46",2576.0,12014434.0,2,The  default Istio metrics  are defined by a set of configuration artifacts that ship with Istio and are exported to  Prometheus  by default.,0.0,0.0,1.0,0.0
Jaeger,59285133,59276096,0,"2019/12/11, 13:47:46",False,"2019/12/11, 13:47:46",2576.0,12014434.0,2,"Operators are free to modify the shape and content of these metrics, as well as to change their collection mechanism, to meet their individual monitoring needs.",0.0,0.184,0.816,0.6597
Jaeger,59285133,59276096,0,"2019/12/11, 13:47:46",False,"2019/12/11, 13:47:46",2576.0,12014434.0,2,So by having istio injected prometheus operator You end up with two Prometheus operators in Your istio mesh.,0.0,0.0,1.0,0.0
Jaeger,59285133,59276096,0,"2019/12/11, 13:47:46",False,"2019/12/11, 13:47:46",2576.0,12014434.0,2,"Secondly, when you enforce Mutual TLS in Your istio mesh every connection has to be secure ( TLS ).",0.0,0.124,0.876,0.34
Jaeger,59285133,59276096,0,"2019/12/11, 13:47:46",False,"2019/12/11, 13:47:46",2576.0,12014434.0,2,And as You mentioned it works when there is no istio injection.,0.167,0.0,0.833,-0.296
Jaeger,59285133,59276096,0,"2019/12/11, 13:47:46",False,"2019/12/11, 13:47:46",2576.0,12014434.0,2,So the most likely cause is that the readiness probe fails because it is using  HTTP  protocol which is insecure (plain text) and this is one of the reason why You would get  503  error.,0.201,0.048,0.751,-0.743
Jaeger,59285133,59276096,0,"2019/12/11, 13:47:46",False,"2019/12/11, 13:47:46",2576.0,12014434.0,2,"If you really need prometheus operator within istio mesh, this could be fixed by creating  DestinationRule  to  Disable  tls mode just for the readiness probe.",0.0,0.154,0.846,0.4939
Jaeger,59285133,59276096,0,"2019/12/11, 13:47:46",False,"2019/12/11, 13:47:46",2576.0,12014434.0,2,Example:,0.0,0.0,1.0,0.0
Jaeger,59285133,59276096,0,"2019/12/11, 13:47:46",False,"2019/12/11, 13:47:46",2576.0,12014434.0,2,Note: Make sure to modify it so that it matches Your namespaces and hosts.,0.0,0.15,0.85,0.3182
Jaeger,59285133,59276096,0,"2019/12/11, 13:47:46",False,"2019/12/11, 13:47:46",2576.0,12014434.0,2,Also there could be some other prometheus collisions within mesh.,0.189,0.0,0.811,-0.2732
Jaeger,59285133,59276096,0,"2019/12/11, 13:47:46",False,"2019/12/11, 13:47:46",2576.0,12014434.0,2,The other solution would be not to have prometheus istio injected in the first place.,0.0,0.141,0.859,0.3182
Jaeger,59285133,59276096,0,"2019/12/11, 13:47:46",False,"2019/12/11, 13:47:46",2576.0,12014434.0,2,You can disable istio injection in prometheus namespace by using the following commands:,0.0,0.0,1.0,0.0
Jaeger,59144234,58999288,0,"2019/12/02, 19:50:35",True,"2019/12/02, 19:50:35",3020.0,1264920.0,0,I was able to publish some spans so I could see them on  http://localhost:16686,0.0,0.0,1.0,0.0
Jaeger,59144234,58999288,0,"2019/12/02, 19:50:35",True,"2019/12/02, 19:50:35",3020.0,1264920.0,0,This is the updated main function:,0.0,0.0,1.0,0.0
Jaeger,57068778,57068664,0,"2019/07/17, 08:00:18",False,"2019/07/17, 08:00:18",1034326.0,6309.0,1,go run  compiles and runs the named main Go package.,0.0,0.0,1.0,0.0
Jaeger,57068778,57068664,0,"2019/07/17, 08:00:18",False,"2019/07/17, 08:00:18",1034326.0,6309.0,1,"Only  go build  or  go install  would compile the packages named by the import paths, along with their dependencies,",0.0,0.0,1.0,0.0
Jaeger,54185032,54134885,0,"2019/01/14, 18:04:36",True,"2019/01/14, 18:14:37",147.0,2560123.0,1,I guess if you set sampler to 0 in the configuration then no traces will be captured.,0.136,0.0,0.864,-0.296
Jaeger,54185032,54134885,0,"2019/01/14, 18:04:36",True,"2019/01/14, 18:14:37",147.0,2560123.0,1,https://github.com/jaegertracing/jaeger-client-java#testing,0.0,0.0,1.0,0.0
Jaeger,54185032,54134885,0,"2019/01/14, 18:04:36",True,"2019/01/14, 18:14:37",147.0,2560123.0,1,But it's specific to Jaeger.,0.0,0.0,1.0,0.0
Jaeger,54185032,54134885,0,"2019/01/14, 18:04:36",True,"2019/01/14, 18:14:37",147.0,2560123.0,1,Otherwise you can use NoopTracer like  Tracer tracer = NoopTracerFactory.create();   Maven,0.0,0.217,0.783,0.3612
Jaeger,66493806,66487682,1,"2021/03/05, 15:54:27",False,"2021/03/05, 15:54:27",1.0,15337079.0,0,You can set the service name in the code as follows:,0.0,0.0,1.0,0.0
Jaeger,66519661,66487682,0,"2021/03/07, 19:57:21",True,"2021/03/07, 19:57:21",343.0,2500390.0,0,"Based on some of the hints provided by Opentelemetry Java community, created two providers which indeed creates two services.",0.0,0.194,0.806,0.4767
Jaeger,66519661,66487682,0,"2021/03/07, 19:57:21",True,"2021/03/07, 19:57:21",343.0,2500390.0,0,"Reusing same exporter, so that multiple connections to the backend can be avoided.",0.167,0.0,0.833,-0.34
Jaeger,66519661,66487682,0,"2021/03/07, 19:57:21",True,"2021/03/07, 19:57:21",343.0,2500390.0,0,I will learn more about  reusing exporter to create two or more provides in the same application in coming days.,0.0,0.104,0.896,0.2732
Jaeger,66519661,66487682,0,"2021/03/07, 19:57:21",True,"2021/03/07, 19:57:21",343.0,2500390.0,0,"Thank you to amazing Opentelemetry java community - @John Watson (jkwatson), Bogdan Drutu, Rupinder Singh, Anuraag Agrawal.",0.0,0.31,0.69,0.743
Jaeger,66147291,66141019,0,"2021/02/11, 02:50:17",False,"2021/02/11, 02:50:17",1693.0,971735.0,0,Based on this:,0.0,0.0,1.0,0.0
Jaeger,66147291,66141019,0,"2021/02/11, 02:50:17",False,"2021/02/11, 02:50:17",1693.0,971735.0,0,The problem is each service is capturing trace without issue but I can't see service to service communication and architectural design.,0.089,0.0,0.911,-0.2144
Jaeger,66147291,66141019,0,"2021/02/11, 02:50:17",False,"2021/02/11, 02:50:17",1693.0,971735.0,0,it seems that the tracing information is not propagated across services.,0.0,0.0,1.0,0.0
Jaeger,66147291,66141019,0,"2021/02/11, 02:50:17",False,"2021/02/11, 02:50:17",1693.0,971735.0,0,You can check this by looking into the HTTP headers and check the  traceId .,0.0,0.0,1.0,0.0
Jaeger,66147291,66141019,0,"2021/02/11, 02:50:17",False,"2021/02/11, 02:50:17",1693.0,971735.0,0,In order to make this work the  traceId  should be the same across the requests.,0.0,0.0,1.0,0.0
Jaeger,66147291,66141019,0,"2021/02/11, 02:50:17",False,"2021/02/11, 02:50:17",1693.0,971735.0,0,You should see the same  traceId  in the logs too.,0.0,0.0,1.0,0.0
Jaeger,66147291,66141019,0,"2021/02/11, 02:50:17",False,"2021/02/11, 02:50:17",1693.0,971735.0,0,The documentation gives you some pointers how to troubleshoot this:,0.0,0.167,0.833,0.2023
Jaeger,66147115,66048959,2,"2021/02/11, 02:26:16",False,"2021/02/11, 02:26:16",4433.0,243104.0,0,"I'm not 100% sure what the problem is you're experiencing, but here's some things to consider.",0.192,0.0,0.808,-0.325
Jaeger,66147115,66048959,2,"2021/02/11, 02:26:16",False,"2021/02/11, 02:26:16",4433.0,243104.0,0,"According to  this post on the Traefik forums , that message you're seeing is  debug  level because it's not something you should be worried about.",0.087,0.0,0.913,-0.296
Jaeger,66147115,66048959,2,"2021/02/11, 02:26:16",False,"2021/02/11, 02:26:16",4433.0,243104.0,0,"It's just logging that no trace context was found, so a new one will be created.",0.128,0.116,0.756,-0.0516
Jaeger,66147115,66048959,2,"2021/02/11, 02:26:16",False,"2021/02/11, 02:26:16",4433.0,243104.0,0,"That second part is not in the message, but apparently that's what happens.",0.0,0.0,1.0,0.0
Jaeger,66147115,66048959,2,"2021/02/11, 02:26:16",False,"2021/02/11, 02:26:16",4433.0,243104.0,0,You should check to see if you're getting data appearing in Jaeger.,0.0,0.0,1.0,0.0
Jaeger,66147115,66048959,2,"2021/02/11, 02:26:16",False,"2021/02/11, 02:26:16",4433.0,243104.0,0,"If you are, that message is probably nothing to worry.",0.0,0.211,0.789,0.3412
Jaeger,66147115,66048959,2,"2021/02/11, 02:26:16",False,"2021/02/11, 02:26:16",4433.0,243104.0,0,"If you are getting data in Jaeger, but it's not connected, that will be because Traefik can only only work with trace context that is already in inbound requests, but it can't add trace context to outbound requests.",0.0,0.0,1.0,0.0
Jaeger,66147115,66048959,2,"2021/02/11, 02:26:16",False,"2021/02/11, 02:26:16",4433.0,243104.0,0,"Within your application, you'll need to implement trace propagation so that your outbound requests include the trace context that was received as part of the incoming request.",0.0,0.0,1.0,0.0
Jaeger,66147115,66048959,2,"2021/02/11, 02:26:16",False,"2021/02/11, 02:26:16",4433.0,243104.0,0,"Without doing that, every request will be sent without trace context and will start a new trace when it is received at the next Traefik ingress point.",0.0,0.0,1.0,0.0
Jaeger,66151385,66048959,0,"2021/02/11, 10:45:37",True,"2021/02/11, 10:45:37",19808.0,376483.0,0,The problem actually was with the  traceContextHeaderName .,0.31,0.0,0.69,-0.4019
Jaeger,66151385,66048959,0,"2021/02/11, 10:45:37",True,"2021/02/11, 10:45:37",19808.0,376483.0,0,Sadly I can not tell exactly what the problem was as the  git diff  only shows that nothing changed around traefik and jaeger at the point where I fixed it.,0.175,0.0,0.825,-0.6705
Jaeger,66151385,66048959,0,"2021/02/11, 10:45:37",True,"2021/02/11, 10:45:37",19808.0,376483.0,0,I assume config got &quot;stuck&quot; somehow.,0.0,0.0,1.0,0.0
Jaeger,66151385,66048959,0,"2021/02/11, 10:45:37",True,"2021/02/11, 10:45:37",19808.0,376483.0,0,"I tracked down the  related lines in source , but as I am no Go-Dev, I can only guess if there's a bug.",0.141,0.0,0.859,-0.4215
Jaeger,66151385,66048959,0,"2021/02/11, 10:45:37",True,"2021/02/11, 10:45:37",19808.0,376483.0,0,"What I did was to switch back to  uber-trace-id , which magically &quot;fixed&quot; it.",0.0,0.0,1.0,0.0
Jaeger,66151385,66048959,0,"2021/02/11, 10:45:37",True,"2021/02/11, 10:45:37",19808.0,376483.0,0,"After I ran some traces and connected another service (node, npm  jaeger-client  with  process.env.TRACER_STATE_HEADER_NAME  set to an equal value), I switched back to  traefik-trace-id  and things worked.",0.0,0.0,1.0,0.0
Jaeger,65298980,65219729,1,"2020/12/15, 03:56:21",True,"2020/12/15, 03:56:21",471.0,2673284.0,1,"To my knowledge, the design of  ForkJoinPool.commonPool()  makes it impossible to actually replace that pool programmatically with an instrumented version.",0.0,0.0,1.0,0.0
Jaeger,65298980,65219729,1,"2020/12/15, 03:56:21",True,"2020/12/15, 03:56:21",471.0,2673284.0,1,So the only workaround is to do it via bytecode manipulation.,0.18,0.0,0.82,-0.296
Jaeger,65298980,65219729,1,"2020/12/15, 03:56:21",True,"2020/12/15, 03:56:21",471.0,2673284.0,1,"The  OpenTelemetry Java Automatic Instrumentation  libraries perform a lot of magic to be able to take care of correctly propagating context through async/concurrency primitives, you may want to give them a try.",0.0,0.138,0.862,0.5423
Jaeger,64891826,64878686,1,"2020/11/18, 13:05:30",False,"2020/11/18, 13:05:30",557.0,615104.0,2,"Tracing is enabled by default for JAX-RS endpoints only, not for reactive routes at the moment.",0.0,0.0,1.0,0.0
Jaeger,64891826,64878686,1,"2020/11/18, 13:05:30",False,"2020/11/18, 13:05:30",557.0,615104.0,2,You can activate tracing by annotating your route with  @org.eclipse.microprofile.opentracing.Traced .,0.0,0.0,1.0,0.0
Jaeger,66257794,64878686,0,"2021/02/18, 12:11:01",True,"2021/02/25, 01:42:56",1332.0,1687162.0,1,"Yes, adding @Traced enable to activate tracing on reactive routes.",0.0,0.231,0.769,0.4019
Jaeger,66257794,64878686,0,"2021/02/18, 12:11:01",True,"2021/02/25, 01:42:56",1332.0,1687162.0,1,"Unfortunately, using both JAX-RS reactive and reactive routes bugs the tracing on event-loop threads used by JAX-RS reactive endpoint when they get executed.",0.098,0.0,0.902,-0.34
Jaeger,66257794,64878686,0,"2021/02/18, 12:11:01",True,"2021/02/25, 01:42:56",1332.0,1687162.0,1,"I only started Quarkus 2 days ago so i don't really the reason of this behavior (and whether it's normal or it's a bug), but obviously switching between two completely mess up the tracing.",0.113,0.0,0.887,-0.5704
Jaeger,66257794,64878686,0,"2021/02/18, 12:11:01",True,"2021/02/25, 01:42:56",1332.0,1687162.0,1,Here is an example to easily reproduce it:,0.0,0.255,0.745,0.34
Jaeger,66257794,64878686,0,"2021/02/18, 12:11:01",True,"2021/02/25, 01:42:56",1332.0,1687162.0,1,Here is a screenshot that show the issue,0.0,0.0,1.0,0.0
Jaeger,66257794,64878686,0,"2021/02/18, 12:11:01",True,"2021/02/25, 01:42:56",1332.0,1687162.0,1,"As you can see, as soon as the JAX-RS resource is it and executed on one of the two threads available, it &quot;corrupts&quot; it, messing the trace_id reported (i don't know if it's the generation or the reporting on logs that is broken) on logs for the next calls of the reactive route.",0.0,0.0,1.0,0.0
Jaeger,66257794,64878686,0,"2021/02/18, 12:11:01",True,"2021/02/25, 01:42:56",1332.0,1687162.0,1,"This does not happen on the JAX-RS resource, as you can notice on the screenshot as well.",0.0,0.116,0.884,0.2732
Jaeger,66257794,64878686,0,"2021/02/18, 12:11:01",True,"2021/02/25, 01:42:56",1332.0,1687162.0,1,So it seems to be related to reactive routes only.,0.0,0.0,1.0,0.0
Jaeger,66257794,64878686,0,"2021/02/18, 12:11:01",True,"2021/02/25, 01:42:56",1332.0,1687162.0,1,Another point here is the fact that JAX-RS Reactive resources are incorrectly reported on Jaeger.,0.0,0.0,1.0,0.0
Jaeger,66257794,64878686,0,"2021/02/18, 12:11:01",True,"2021/02/25, 01:42:56",1332.0,1687162.0,1,(with a mention to a missing root span) Not sure if it's related to the issue but that's also another annoying point.,0.281,0.0,0.719,-0.6839
Jaeger,66257794,64878686,0,"2021/02/18, 12:11:01",True,"2021/02/25, 01:42:56",1332.0,1687162.0,1,I'm thinking to completely remove the JAX-RS Reactive endpoint and replace them by normal reactive route to eliminate this bug.,0.0,0.0,1.0,0.0
Jaeger,66257794,64878686,0,"2021/02/18, 12:11:01",True,"2021/02/25, 01:42:56",1332.0,1687162.0,1,I would appreciate if someone with more experience than me could verify this or tell me what i did wrong :),0.125,0.23,0.645,0.3818
Jaeger,66257794,64878686,0,"2021/02/18, 12:11:01",True,"2021/02/25, 01:42:56",1332.0,1687162.0,1,"EDIT 1: I added a route filter with priority 500 to clear the MDC and the bug is still there, so definitely not coming from MDC.",0.0,0.217,0.783,0.7262
Jaeger,66257794,64878686,0,"2021/02/18, 12:11:01",True,"2021/02/25, 01:42:56",1332.0,1687162.0,1,EDIT 2: I opened a  bug report  on Quarkus,0.0,0.0,1.0,0.0
Jaeger,66257794,64878686,0,"2021/02/18, 12:11:01",True,"2021/02/25, 01:42:56",1332.0,1687162.0,1,"EDIT 3: It seems related to how both implementations works (thread locals versus context propagation in actor based context)
So, unless JAX-RS reactive resources are marked @Blocking (and get executed in a separated thread pool), JAX-RS reactive and Vertx reactive routes are incompatible when it comes to tracing (but also probably the same for MDC related informations since MDC is also thread related)",0.0,0.0,1.0,0.0
Jaeger,64067421,64051610,0,"2020/09/25, 18:50:14",False,"2020/09/25, 18:50:14",1532.0,5491213.0,0,"I tried applying your file on a Kubernetes 1.16 cluster, and there are a couple of issues with it:",0.0,0.0,1.0,0.0
Jaeger,64067421,64051610,0,"2020/09/25, 18:50:14",False,"2020/09/25, 18:50:14",1532.0,5491213.0,0,The .spec.selector field defines how the Deployment finds which Pods to manage.,0.0,0.0,1.0,0.0
Jaeger,64067421,64051610,0,"2020/09/25, 18:50:14",False,"2020/09/25, 18:50:14",1532.0,5491213.0,0,It seems like you are applying something that is super old.,0.0,0.416,0.584,0.7506
Jaeger,64067421,64051610,0,"2020/09/25, 18:50:14",False,"2020/09/25, 18:50:14",1532.0,5491213.0,0,"Kubernetes notes the below in its doc, and so I wonder if this used to work on older versions of Kubernetes where selectors were defaulted.",0.0,0.0,1.0,0.0
Jaeger,64067421,64051610,0,"2020/09/25, 18:50:14",False,"2020/09/25, 18:50:14",1532.0,5491213.0,0,"As of Kubernetes 1.8, you must specify a pod selector that matches the labels of the .spec.template.",0.0,0.0,1.0,0.0
Jaeger,64067421,64051610,0,"2020/09/25, 18:50:14",False,"2020/09/25, 18:50:14",1532.0,5491213.0,0,The pod selector will no longer be defaulted when left empty.,0.308,0.0,0.692,-0.4588
Jaeger,64067421,64051610,0,"2020/09/25, 18:50:14",False,"2020/09/25, 18:50:14",1532.0,5491213.0,0,"It seems like you should take a new approach-- in looking around, I found a couple of good tutorials  here  and  here , and Jaeger themselves offer a similar approach  here .",0.0,0.184,0.816,0.6597
Jaeger,64067421,64051610,0,"2020/09/25, 18:50:14",False,"2020/09/25, 18:50:14",1532.0,5491213.0,0,They all make use of  Kubernetes Operators .,0.0,0.0,1.0,0.0
Jaeger,64067421,64051610,0,"2020/09/25, 18:50:14",False,"2020/09/25, 18:50:14",1532.0,5491213.0,0,"A Kubernetes operator is an application-specific controller that extends the functionality of the Kubernetes API to create, configure, and manage instances of complex applications on behalf of a Kubernetes user",0.0,0.122,0.878,0.3818
Jaeger,64067421,64051610,0,"2020/09/25, 18:50:14",False,"2020/09/25, 18:50:14",1532.0,5491213.0,0,I don't know what you mean by  &quot;So previously I had a binary executable (jaeger-agent) running on a Linux CentOS 8 box alongside a server side application&quot;,0.0,0.0,1.0,0.0
Jaeger,64067421,64051610,0,"2020/09/25, 18:50:14",False,"2020/09/25, 18:50:14",1532.0,5491213.0,0,"The file you are applying looks like it  deploys the agent as a daemonset , which means the agent is run as a pod on each node of your cluster.",0.0,0.088,0.912,0.3612
Jaeger,64067421,64051610,0,"2020/09/25, 18:50:14",False,"2020/09/25, 18:50:14",1532.0,5491213.0,0,"If it is running in your k8's cluster, then  this is how I normally approach troubleshooting kubernetes services .",0.0,0.096,0.904,0.1779
Jaeger,64067421,64051610,0,"2020/09/25, 18:50:14",False,"2020/09/25, 18:50:14",1532.0,5491213.0,0,"If it is running outside of your cluster entirely, then you need to make sure the Service it talks to is exposed outside of the cluster probably using type LoadBalancer.",0.041,0.073,0.886,0.25
Jaeger,62430615,62418175,0,"2020/06/17, 16:50:21",True,"2020/06/17, 16:50:21",213.0,7933630.0,0,"Metrics in OpenTelemetry are currently undergoing continued development and refinement, so they aren't necessarily available for each language yet (see  this tag in the OpenTelemetry JS repo  for an example of metric instruments that aren't up to date yet with spec), but once they are, I'd expect for metrics to be added to the existing node/web instrumentation packages.",0.0,0.0,1.0,0.0
Jaeger,62430615,62418175,0,"2020/06/17, 16:50:21",True,"2020/06/17, 16:50:21",213.0,7933630.0,0,"That said, I would still advise you to try out OpenTelemetry for traces at this point, as it's pretty stable for tracing.",0.0,0.221,0.779,0.6597
Jaeger,62430615,62418175,0,"2020/06/17, 16:50:21",True,"2020/06/17, 16:50:21",213.0,7933630.0,0,"You can use a Prometheus client to export metrics separately, and once OpenTelemetry metrics are fully supported in the JS library, switch over to that.",0.0,0.101,0.899,0.3804
Jaeger,60989810,60978505,3,"2020/04/02, 13:49:29",False,"2020/04/02, 13:49:29",2576.0,12014434.0,0,There are a few different reasons why You could be experiencing this issue.,0.0,0.0,1.0,0.0
Jaeger,60989810,60978505,3,"2020/04/02, 13:49:29",False,"2020/04/02, 13:49:29",2576.0,12014434.0,0,From  istio  documentation:,0.0,0.0,1.0,0.0
Jaeger,60989810,60978505,3,"2020/04/02, 13:49:29",False,"2020/04/02, 13:49:29",2576.0,12014434.0,0,"Since Istio 1.0.3, the sampling rate for tracing has been reduced to 1% in the   default    configuration profile .",0.0,0.0,1.0,0.0
Jaeger,60989810,60978505,3,"2020/04/02, 13:49:29",False,"2020/04/02, 13:49:29",2576.0,12014434.0,0,This means that only 1 out of 100 trace instances captured by Istio will be reported to the tracing backend.,0.0,0.0,1.0,0.0
Jaeger,60989810,60978505,3,"2020/04/02, 13:49:29",False,"2020/04/02, 13:49:29",2576.0,12014434.0,0,The sampling rate in the   demo   profile is still set to 100%.,0.0,0.0,1.0,0.0
Jaeger,60989810,60978505,3,"2020/04/02, 13:49:29",False,"2020/04/02, 13:49:29",2576.0,12014434.0,0,See   this section   for more information on how to set the sampling rate.,0.0,0.0,1.0,0.0
Jaeger,60989810,60978505,3,"2020/04/02, 13:49:29",False,"2020/04/02, 13:49:29",2576.0,12014434.0,0,"If you still do not see any trace data, please confirm that your ports conform to the Istio   port naming conventions   and that the appropriate container port is exposed (via pod spec, for example) to enable traffic capture by the sidecar proxy (Envoy).",0.029,0.052,0.919,0.25
Jaeger,60989810,60978505,3,"2020/04/02, 13:49:29",False,"2020/04/02, 13:49:29",2576.0,12014434.0,0,"If you only see trace data associated with the egress proxy, but not the ingress proxy, it may still be related to the Istio   port naming conventions .",0.0,0.0,1.0,0.0
Jaeger,60989810,60978505,3,"2020/04/02, 13:49:29",False,"2020/04/02, 13:49:29",2576.0,12014434.0,0,Starting with   Istio 1.3   the protocol for   outbound   traffic is automatically detected.,0.0,0.0,1.0,0.0
Jaeger,60989810,60978505,3,"2020/04/02, 13:49:29",False,"2020/04/02, 13:49:29",2576.0,12014434.0,0,Hope it helps.,0.0,0.846,0.154,0.6705
Jaeger,59894312,59875985,0,"2020/01/24, 12:11:13",True,"2020/01/24, 12:23:57",4418.0,127670.0,0,"The short answer is ""you can't.""",0.0,0.0,1.0,0.0
Jaeger,59894312,59875985,0,"2020/01/24, 12:11:13",True,"2020/01/24, 12:23:57",4418.0,127670.0,0,My question was based on a very fundamental misunderstanding of what opentracing does.,0.219,0.0,0.781,-0.4728
Jaeger,59894312,59875985,0,"2020/01/24, 12:11:13",True,"2020/01/24, 12:23:57",4418.0,127670.0,0,"Tracing context is only propagated downstream, not upstream.",0.0,0.0,1.0,0.0
Jaeger,59894312,59875985,0,"2020/01/24, 12:11:13",True,"2020/01/24, 12:23:57",4418.0,127670.0,0,From the same discussion thread:,0.0,0.0,1.0,0.0
Jaeger,59894312,59875985,0,"2020/01/24, 12:11:13",True,"2020/01/24, 12:23:57",4418.0,127670.0,0,"On the wire propagation is only meant to carry ""span context"", which
  is a small set of ID fields and possible baggage.",0.0,0.0,1.0,0.0
Jaeger,59894312,59875985,0,"2020/01/24, 12:11:13",True,"2020/01/24, 12:23:57",4418.0,127670.0,0,"Returning the whole
  trace as part of the request is not a use case that was considered.",0.0,0.0,1.0,0.0
Jaeger,59894312,59875985,0,"2020/01/24, 12:11:13",True,"2020/01/24, 12:23:57",4418.0,127670.0,0,and,0.0,0.0,1.0,0.0
Jaeger,59894312,59875985,0,"2020/01/24, 12:11:13",True,"2020/01/24, 12:23:57",4418.0,127670.0,0,The trace collection is meant to be asynchronous and out of process.,0.0,0.0,1.0,0.0
Jaeger,59894312,59875985,0,"2020/01/24, 12:11:13",True,"2020/01/24, 12:23:57",4418.0,127670.0,0,So my understanding is now thus:,0.0,0.0,1.0,0.0
Jaeger,59894312,59875985,0,"2020/01/24, 12:11:13",True,"2020/01/24, 12:23:57",4418.0,127670.0,0,"Each individual software component creates its own tracing data, bundles it up, and sends it off to the tracing server (e.g.",0.0,0.095,0.905,0.2732
Jaeger,59894312,59875985,0,"2020/01/24, 12:11:13",True,"2020/01/24, 12:23:57",4418.0,127670.0,0,Jaeger).,0.0,0.0,1.0,0.0
Jaeger,59894312,59875985,0,"2020/01/24, 12:11:13",True,"2020/01/24, 12:23:57",4418.0,127670.0,0,Each software component  must  be configured to use the same tracing provider and the same tracing server - an RPC client cannot tell an RPC server that for a particular trace it should use the Jaeger tracing provider and a Jaeger server at such-and-such an address.,0.0,0.0,1.0,0.0
Jaeger,59894312,59875985,0,"2020/01/24, 12:11:13",True,"2020/01/24, 12:23:57",4418.0,127670.0,0,"(At least, the opentracing standard doesn't provide a way to do this.)",0.0,0.0,1.0,0.0
Jaeger,59894312,59875985,0,"2020/01/24, 12:11:13",True,"2020/01/24, 12:23:57",4418.0,127670.0,0,The tracing information injected into a RPC request by the client allows the RPC server to embed a 'parent' ID field into the tracing information.,0.0,0.0,1.0,0.0
Jaeger,59894312,59875985,0,"2020/01/24, 12:11:13",True,"2020/01/24, 12:23:57",4418.0,127670.0,0,It's then the responsibility of the tracer (e.g.,0.0,0.0,1.0,0.0
Jaeger,59894312,59875985,0,"2020/01/24, 12:11:13",True,"2020/01/24, 12:23:57",4418.0,127670.0,0,Jaeger) to figure out the relationships between the various traces it has received from various software components by matching up ID codes embedded in them.,0.0,0.0,1.0,0.0
Jaeger,59894312,59875985,0,"2020/01/24, 12:11:13",True,"2020/01/24, 12:23:57",4418.0,127670.0,0,So what I wanted to do is not a use case considered by opentracing and is not possible.,0.0,0.0,1.0,0.0
Jaeger,59841411,59839687,0,"2020/01/21, 14:41:01",False,"2020/01/21, 14:41:01",2576.0,12014434.0,0,My interpretation of this is that We need to keep in mind that communication between services needs to support forwarding/&quot;passing along&quot; the trace ID's so that the tracing works correctly.,0.0,0.085,0.915,0.4019
Jaeger,59841411,59839687,0,"2020/01/21, 14:41:01",False,"2020/01/21, 14:41:01",2576.0,12014434.0,0,So it warns us against situations where:,0.219,0.0,0.781,-0.1725
Jaeger,59841411,59839687,0,"2020/01/21, 14:41:01",False,"2020/01/21, 14:41:01",2576.0,12014434.0,0,Client calls -&gt; Service A #using http request with trace ID in header.,0.0,0.0,1.0,0.0
Jaeger,59841411,59839687,0,"2020/01/21, 14:41:01",False,"2020/01/21, 14:41:01",2576.0,12014434.0,0,Service A -&gt; Service B #using tcp request that does not support headers and the trace ID header is lost.,0.222,0.0,0.778,-0.5511
Jaeger,59841411,59839687,0,"2020/01/21, 14:41:01",False,"2020/01/21, 14:41:01",2576.0,12014434.0,0,This situation could break or limit tracing functionality.,0.0,0.0,1.0,0.0
Jaeger,59841411,59839687,0,"2020/01/21, 14:41:01",False,"2020/01/21, 14:41:01",2576.0,12014434.0,0,On the other hand If we have situation where:,0.0,0.286,0.714,0.4939
Jaeger,59841411,59839687,0,"2020/01/21, 14:41:01",False,"2020/01/21, 14:41:01",2576.0,12014434.0,0,Client calls -&gt; Service A #using http request with trace ID in header.,0.0,0.0,1.0,0.0
Jaeger,59841411,59839687,0,"2020/01/21, 14:41:01",False,"2020/01/21, 14:41:01",2576.0,12014434.0,0,Service A -&gt; Service B #using http request the trace ID is forwarded to service B.,0.0,0.0,1.0,0.0
Jaeger,59841411,59839687,0,"2020/01/21, 14:41:01",False,"2020/01/21, 14:41:01",2576.0,12014434.0,0,This situation allows for the trace ID header to be present in both connections so the tracing can be logged and then viewed in tracing service dashboard.,0.0,0.0,1.0,0.0
Jaeger,59841411,59839687,0,"2020/01/21, 14:41:01",False,"2020/01/21, 14:41:01",2576.0,12014434.0,0,Then We can explore the path taken by the request and view the latency incurred at each hop.,0.0,0.0,1.0,0.0
Jaeger,59841411,59839687,0,"2020/01/21, 14:41:01",False,"2020/01/21, 14:41:01",2576.0,12014434.0,0,Hope it helps.,0.0,0.846,0.154,0.6705
Jaeger,60078995,59839687,0,"2020/02/05, 17:23:23",False,"2020/02/05, 17:23:23",461.0,3707487.0,3,"When doing tracing in a service mesh, behind proxies, the traceID generated upon the initial client call is propagated automatically only so long as the call goes from proxy- proxy.",0.0,0.0,1.0,0.0
Jaeger,60078995,59839687,0,"2020/02/05, 17:23:23",False,"2020/02/05, 17:23:23",461.0,3707487.0,3,So:,0.0,0.0,1.0,0.0
Jaeger,60078995,59839687,0,"2020/02/05, 17:23:23",False,"2020/02/05, 17:23:23",461.0,3707487.0,3,"To get around this, Microservice A just needs to know which headers represent the traceIDs, how to append into it, and some state to make sure it makes it to outgoing requests.",0.0,0.134,0.866,0.5423
Jaeger,60078995,59839687,0,"2020/02/05, 17:23:23",False,"2020/02/05, 17:23:23",461.0,3707487.0,3,Then you'll get a full transaction chain.,0.0,0.0,1.0,0.0
Jaeger,60078995,59839687,0,"2020/02/05, 17:23:23",False,"2020/02/05, 17:23:23",461.0,3707487.0,3,"Without the service propagating the headers, tracing basically gives you each path that ends in a microservice.",0.0,0.0,1.0,0.0
Jaeger,60078995,59839687,0,"2020/02/05, 17:23:23",False,"2020/02/05, 17:23:23",461.0,3707487.0,3,"Still useful, but not as complete of a picture.",0.0,0.218,0.782,0.2382
Jaeger,59577425,59576942,0,"2020/01/03, 13:02:50",False,"2020/01/03, 13:02:50",5244.0,5564578.0,2,"By default, you don't have a logging system on Istio.",0.0,0.0,1.0,0.0
Jaeger,59577425,59576942,0,"2020/01/03, 13:02:50",False,"2020/01/03, 13:02:50",5244.0,5564578.0,2,"I mean, besides the native logging of Kubernetes.",0.0,0.0,1.0,0.0
Jaeger,59577425,59576942,0,"2020/01/03, 13:02:50",False,"2020/01/03, 13:02:50",5244.0,5564578.0,2,"Zipkin and Jaeger are tracing systems, meaning for latency, not for logging.",0.0,0.0,1.0,0.0
Jaeger,59577425,59576942,0,"2020/01/03, 13:02:50",False,"2020/01/03, 13:02:50",5244.0,5564578.0,2,"You can definitely get this info through Istio components, but you will have to set it up first.",0.0,0.098,0.902,0.2144
Jaeger,59577425,59576942,0,"2020/01/03, 13:02:50",False,"2020/01/03, 13:02:50",5244.0,5564578.0,2,I found  this  articles; in Istio website about how to collect logs.,0.0,0.0,1.0,0.0
Jaeger,59577425,59576942,0,"2020/01/03, 13:02:50",False,"2020/01/03, 13:02:50",5244.0,5564578.0,2,I would say  Fluentd  +  Elasticsearch  would give you something as powerful as you need.,0.0,0.189,0.811,0.4215
Jaeger,59577425,59576942,0,"2020/01/03, 13:02:50",False,"2020/01/03, 13:02:50",5244.0,5564578.0,2,Unfortunately I don't have any examples.,0.375,0.0,0.625,-0.34
Jaeger,59580066,59574885,1,"2020/01/03, 16:14:30",True,"2020/01/03, 17:50:28",2576.0,12014434.0,1,According to envoy proxy documentation for envoy  v1.12.0  used by istio  1.3 :,0.0,0.0,1.0,0.0
Jaeger,59580066,59574885,1,"2020/01/03, 16:14:30",True,"2020/01/03, 17:50:28",2576.0,12014434.0,1,Envoy provides the capability for reporting tracing information regarding communications between services in the mesh.,0.0,0.0,1.0,0.0
Jaeger,59580066,59574885,1,"2020/01/03, 16:14:30",True,"2020/01/03, 17:50:28",2576.0,12014434.0,1,"However, to be able to correlate the pieces of tracing information generated by the various proxies within a call flow, the services must propagate certain trace context between the inbound and outbound requests.",0.0,0.063,0.937,0.2732
Jaeger,59580066,59574885,1,"2020/01/03, 16:14:30",True,"2020/01/03, 17:50:28",2576.0,12014434.0,1,"Whichever tracing provider is being used, the service should propagate the   x-request-id   to enable logging across the invoked services to be correlated.",0.0,0.0,1.0,0.0
Jaeger,59580066,59574885,1,"2020/01/03, 16:14:30",True,"2020/01/03, 17:50:28",2576.0,12014434.0,1,"The tracing providers also require additional context, to enable the parent/child relationships between the spans (logical units of work) to be understood.",0.0,0.0,1.0,0.0
Jaeger,59580066,59574885,1,"2020/01/03, 16:14:30",True,"2020/01/03, 17:50:28",2576.0,12014434.0,1,"This can be achieved by using the LightStep (via OpenTracing API) or Zipkin tracer directly within the service itself, to extract the trace context from the inbound request and inject it into any subsequent outbound requests.",0.0,0.0,1.0,0.0
Jaeger,59580066,59574885,1,"2020/01/03, 16:14:30",True,"2020/01/03, 17:50:28",2576.0,12014434.0,1,"This approach would also enable the service to create additional spans, describing work being done internally within the service, that may be useful when examining the end-to-end trace.",0.0,0.161,0.839,0.6124
Jaeger,59580066,59574885,1,"2020/01/03, 16:14:30",True,"2020/01/03, 17:50:28",2576.0,12014434.0,1,Alternatively the trace context can be manually propagated by the service:,0.0,0.0,1.0,0.0
Jaeger,59580066,59574885,1,"2020/01/03, 16:14:30",True,"2020/01/03, 17:50:28",2576.0,12014434.0,1,"When using the LightStep tracer, Envoy relies on the service to propagate the 
   x-ot-span-context 
  HTTP header while sending HTTP requests to other services.",0.0,0.0,1.0,0.0
Jaeger,59580066,59574885,1,"2020/01/03, 16:14:30",True,"2020/01/03, 17:50:28",2576.0,12014434.0,1,"When using the Zipkin tracer, Envoy relies on the service to propagate the B3 HTTP headers ( 
   x-b3-traceid ,
   x-b3-spanid ,
   x-b3-parentspanid ,
   x-b3-sampled ,
  and 
   x-b3-flags ).",0.0,0.0,1.0,0.0
Jaeger,59580066,59574885,1,"2020/01/03, 16:14:30",True,"2020/01/03, 17:50:28",2576.0,12014434.0,1,"The 
   x-b3-sampled 
  header can also be supplied by an external client to either enable or
  disable tracing for a particular request.",0.0,0.0,1.0,0.0
Jaeger,59580066,59574885,1,"2020/01/03, 16:14:30",True,"2020/01/03, 17:50:28",2576.0,12014434.0,1,"In addition, the single 
   b3 
  header propagation format is supported, which is a more compressed
  format.",0.0,0.141,0.859,0.3182
Jaeger,59580066,59574885,1,"2020/01/03, 16:14:30",True,"2020/01/03, 17:50:28",2576.0,12014434.0,1,"When using the Datadog tracer, Envoy relies on the service to propagate the Datadog-specific HTTP headers ( 
   x-datadog-trace-id ,
   x-datadog-parent-id ,
   x-datadog-sampling-priority ).",0.0,0.0,1.0,0.0
Jaeger,59580066,59574885,1,"2020/01/03, 16:14:30",True,"2020/01/03, 17:50:28",2576.0,12014434.0,1,TLDR: traceId headers need to be manually added to B3 HTTP headers.,0.0,0.0,1.0,0.0
Jaeger,59580066,59574885,1,"2020/01/03, 16:14:30",True,"2020/01/03, 17:50:28",2576.0,12014434.0,1,Additional information:  https://github.com/openzipkin/b3-propagation,0.0,0.0,1.0,0.0
Jaeger,58269994,58228451,2,"2019/10/07, 15:54:44",True,"2019/10/07, 15:54:44",6049.0,3156333.0,1,If you have sampling rate set to 1% then error will be seen in Jaeger once it occurs 100 times.,0.124,0.0,0.876,-0.4019
Jaeger,58269994,58228451,2,"2019/10/07, 15:54:44",True,"2019/10/07, 15:54:44",6049.0,3156333.0,1,This is mentioned at  Distributed Tracing - Jaeger :,0.0,0.0,1.0,0.0
Jaeger,58269994,58228451,2,"2019/10/07, 15:54:44",True,"2019/10/07, 15:54:44",6049.0,3156333.0,1,"To see trace data, you must send requests to your service.",0.0,0.0,1.0,0.0
Jaeger,58269994,58228451,2,"2019/10/07, 15:54:44",True,"2019/10/07, 15:54:44",6049.0,3156333.0,1,The number of requests depends on Istio’s sampling rate.,0.0,0.14,0.86,0.0772
Jaeger,58269994,58228451,2,"2019/10/07, 15:54:44",True,"2019/10/07, 15:54:44",6049.0,3156333.0,1,You set this rate when you install Istio.,0.0,0.0,1.0,0.0
Jaeger,58269994,58228451,2,"2019/10/07, 15:54:44",True,"2019/10/07, 15:54:44",6049.0,3156333.0,1,The default sampling rate is 1%.,0.0,0.0,1.0,0.0
Jaeger,58269994,58228451,2,"2019/10/07, 15:54:44",True,"2019/10/07, 15:54:44",6049.0,3156333.0,1,You need to send at least 100 requests before the first trace is visible.,0.0,0.0,1.0,0.0
Jaeger,58269994,58228451,2,"2019/10/07, 15:54:44",True,"2019/10/07, 15:54:44",6049.0,3156333.0,1,"To send a 100 requests to the   productpage   service, use the following command:",0.0,0.0,1.0,0.0
Jaeger,58269994,58228451,2,"2019/10/07, 15:54:44",True,"2019/10/07, 15:54:44",6049.0,3156333.0,1,$ for i in  `seq 1 100`;  do  curl -s -o /dev/null http://$GATEWAY_URL/productpage;  done,0.0,0.0,1.0,0.0
Jaeger,58269994,58228451,2,"2019/10/07, 15:54:44",True,"2019/10/07, 15:54:44",6049.0,3156333.0,1,"If you are not seeing the error in the current sample, I would advice make the sample higher.",0.0,0.124,0.876,0.3089
Jaeger,58269994,58228451,2,"2019/10/07, 15:54:44",True,"2019/10/07, 15:54:44",6049.0,3156333.0,1,You can read about  Tracing context propagation  which is being done by  Envoy .,0.0,0.0,1.0,0.0
Jaeger,58269994,58228451,2,"2019/10/07, 15:54:44",True,"2019/10/07, 15:54:44",6049.0,3156333.0,1,Envoy automatically sends spans to tracing collectors,0.0,0.0,1.0,0.0
Jaeger,58269994,58228451,2,"2019/10/07, 15:54:44",True,"2019/10/07, 15:54:44",6049.0,3156333.0,1,Alternatively the trace context can be manually propagated by the service:,0.0,0.0,1.0,0.0
Jaeger,57918375,57913923,1,"2019/09/13, 09:31:04",True,"2019/09/13, 09:31:04",269.0,12020494.0,6,"Just mentioning beforehand (you might already know) that a Kubernetes Service is not a ""service"" as in a piece of code.",0.0,0.0,1.0,0.0
Jaeger,57918375,57913923,1,"2019/09/13, 09:31:04",True,"2019/09/13, 09:31:04",269.0,12020494.0,6,"It is a way for Kubernetes components &amp; deployments to communicate with one another through an interface which always stays the same, regardless of how many pods or servers there are.",0.0,0.0,1.0,0.0
Jaeger,57918375,57913923,1,"2019/09/13, 09:31:04",True,"2019/09/13, 09:31:04",269.0,12020494.0,6,"When Istio deploys it's tracing mechanism, it deploys modular parts so it can deploy them independently, and also scale them independently, very much like micro-services.",0.0,0.104,0.896,0.4173
Jaeger,57918375,57913923,1,"2019/09/13, 09:31:04",True,"2019/09/13, 09:31:04",269.0,12020494.0,6,Generally a Kubernetes deployed utility will be deployed as a few parts which make up the bigger picture.,0.0,0.0,1.0,0.0
Jaeger,57918375,57913923,1,"2019/09/13, 09:31:04",True,"2019/09/13, 09:31:04",269.0,12020494.0,6,For instance in your case:,0.0,0.0,1.0,0.0
Jaeger,57918375,57913923,1,"2019/09/13, 09:31:04",True,"2019/09/13, 09:31:04",269.0,12020494.0,6,jaeger-agent - This is the components which will collect all the traffic and tracing from your nodes.,0.0,0.0,1.0,0.0
Jaeger,57918375,57913923,1,"2019/09/13, 09:31:04",True,"2019/09/13, 09:31:04",269.0,12020494.0,6,"jaeger-collector - This is the place where all of the jaeger-agents will push the logs and traces they find on the node, and the collector will aggregate these as a trace may span multiple nodes.",0.0,0.0,1.0,0.0
Jaeger,57918375,57913923,1,"2019/09/13, 09:31:04",True,"2019/09/13, 09:31:04",269.0,12020494.0,6,tracing - might be the component which injects the tracing ID's into network traffic for the agent to watch.,0.0,0.0,1.0,0.0
Jaeger,57918375,57913923,1,"2019/09/13, 09:31:04",True,"2019/09/13, 09:31:04",269.0,12020494.0,6,"zipkin - could be the UI which allows debugging with traces, or replaying requests etc.",0.0,0.0,1.0,0.0
Jaeger,57918375,57913923,1,"2019/09/13, 09:31:04",True,"2019/09/13, 09:31:04",269.0,12020494.0,6,"The above might not be absolutely correct, but I hope you get the idea of why multiple parts would be deployed.",0.0,0.183,0.817,0.6423
Jaeger,57918375,57913923,1,"2019/09/13, 09:31:04",True,"2019/09/13, 09:31:04",269.0,12020494.0,6,"In the same way we deploy mysql, and our containers separately, Kubernetes projects are generally deployed as a set of deployments or pods.",0.0,0.0,1.0,0.0
Jaeger,58058841,57913923,0,"2019/09/23, 11:52:53",False,"2019/09/23, 11:52:53",13313.0,524946.0,1,"To complement @christiaan-vermeulen's answer: the  tracing  service is Jaeger's UI (jaeger-query) so that the same URL can be used for alternative backends, whereas the Zipkin service is a convenience service, allowing applications using Zipkin tracers (like Brave) to send data to Jaeger without requiring complex changes.",0.0,0.0,1.0,0.0
Jaeger,58058841,57913923,0,"2019/09/23, 11:52:53",False,"2019/09/23, 11:52:53",13313.0,524946.0,1,"If you look closely, the Zipkin service is backed by the jaeger-collector as well.",0.0,0.211,0.789,0.296
Jaeger,57474533,57473932,2,"2019/08/13, 12:10:40",True,"2019/08/13, 12:10:40",246.0,10967175.0,0,I hope you have followed the official documentation of the jager with istio.,0.0,0.209,0.791,0.4404
Jaeger,57474533,57473932,2,"2019/08/13, 12:10:40",True,"2019/08/13, 12:10:40",246.0,10967175.0,0,If you are using the helm chart make the following changes required.,0.0,0.0,1.0,0.0
Jaeger,57474533,57473932,2,"2019/08/13, 12:10:40",True,"2019/08/13, 12:10:40",246.0,10967175.0,0,Export the dashboard via Kube port-forward or ingress.,0.0,0.0,1.0,0.0
Jaeger,57474533,57473932,2,"2019/08/13, 12:10:40",True,"2019/08/13, 12:10:40",246.0,10967175.0,0,Official Documentation.,0.0,0.0,1.0,0.0
Jaeger,57474533,57473932,2,"2019/08/13, 12:10:40",True,"2019/08/13, 12:10:40",246.0,10967175.0,0,https://istio.io/docs/tasks/telemetry/distributed-tracing/jaeger/,0.0,0.0,1.0,0.0
Jaeger,57474533,57473932,2,"2019/08/13, 12:10:40",True,"2019/08/13, 12:10:40",246.0,10967175.0,0,NOTE: The important thing by default jaeger will trace something like 0.1% request i.e.,0.0,0.264,0.736,0.5106
Jaeger,57474533,57473932,2,"2019/08/13, 12:10:40",True,"2019/08/13, 12:10:40",246.0,10967175.0,0,1 request out of 100 so put a lot of requests only then you can see a trace in UI.,0.0,0.0,1.0,0.0
Jaeger,57397006,57380205,0,"2019/08/07, 17:43:53",False,"2019/08/07, 17:43:53",527.0,3966540.0,0,I had a wrong opencensus collector configuration.,0.437,0.0,0.563,-0.4767
Jaeger,57397006,57380205,0,"2019/08/07, 17:43:53",False,"2019/08/07, 17:43:53",527.0,3966540.0,0,The docker container network cannot see port 9411 as it was on the host network.,0.0,0.0,1.0,0.0
Jaeger,57397006,57380205,0,"2019/08/07, 17:43:53",False,"2019/08/07, 17:43:53",527.0,3966540.0,0,I was able to fix the issue after noticing this misconfiguration.,0.0,0.0,1.0,0.0
Jaeger,57295589,57273076,2,"2019/07/31, 19:48:01",False,"2019/07/31, 19:48:01",13313.0,524946.0,0,"OpenTracing does not define the concrete data model, how the data should be collected, nor how it should be transported.",0.0,0.0,1.0,0.0
Jaeger,57295589,57273076,2,"2019/07/31, 19:48:01",False,"2019/07/31, 19:48:01",13313.0,524946.0,0,"As such, there's no specification for the endpoint.",0.239,0.0,0.761,-0.296
Jaeger,57295589,57273076,2,"2019/07/31, 19:48:01",False,"2019/07/31, 19:48:01",13313.0,524946.0,0,"This allows implementations like Jaeger to use non-HTTP transport by default when sending data from the client (tracer) to the backend, by sending UDP packets to an intermediate ""Jaeger Agent"".",0.0,0.079,0.921,0.3612
Jaeger,57295589,57273076,2,"2019/07/31, 19:48:01",False,"2019/07/31, 19:48:01",13313.0,524946.0,0,"Given that the base model is pretty much similar among implementations, it's common to have tracing solutions to support each other's endpoints.",0.0,0.286,0.714,0.765
Jaeger,57295589,57273076,2,"2019/07/31, 19:48:01",False,"2019/07/31, 19:48:01",13313.0,524946.0,0,"For instance, Jaeger is able to expose an endpoint with  Zipkin compatibility .",0.127,0.0,0.873,-0.1531
Jaeger,57295589,57273076,2,"2019/07/31, 19:48:01",False,"2019/07/31, 19:48:01",13313.0,524946.0,0,"Based on your question, I think you might be interested in the OpenTelemetry project, a successor to the OpenTracing project, as the result of a merge with the OpenCensus project.",0.0,0.155,0.845,0.5574
Jaeger,57295589,57273076,2,"2019/07/31, 19:48:01",False,"2019/07/31, 19:48:01",13313.0,524946.0,0,"OpenTelemetry provides its own tracer and is able to ""receive"" data in several formats (including Jaeger), and ""export"" to several backends.",0.0,0.0,1.0,0.0
Jaeger,56462471,56202494,2,"2019/06/05, 17:23:36",True,"2019/06/05, 17:23:36",3681.0,11300382.0,2,Assuming that your services are  defined in Istio’s internal service registry.,0.0,0.0,1.0,0.0
Jaeger,56462471,56202494,2,"2019/06/05, 17:23:36",True,"2019/06/05, 17:23:36",3681.0,11300382.0,2,If not please configure it according to instruction  service-defining .,0.197,0.0,0.803,-0.2411
Jaeger,56462471,56202494,2,"2019/06/05, 17:23:36",True,"2019/06/05, 17:23:36",3681.0,11300382.0,2,"In HTTPS all the HTTP-related information like method, URL path, response code, is encrypted so Istio  cannot  see and cannot monitor that information for HTTPS.",0.0,0.094,0.906,0.3612
Jaeger,56462471,56202494,2,"2019/06/05, 17:23:36",True,"2019/06/05, 17:23:36",3681.0,11300382.0,2,"If you need to monitor HTTP-related information in access to external HTTPS services, you may want to let your applications issue HTTP requests and configure Istio to perform TLS origination.",0.0,0.043,0.957,0.0772
Jaeger,56462471,56202494,2,"2019/06/05, 17:23:36",True,"2019/06/05, 17:23:36",3681.0,11300382.0,2,First you have to  redefine  your ServiceEntry and create VirtualService  to rewrite the HTTP request port and add a DestinationRule to perform TLS origination.,0.0,0.087,0.913,0.2732
Jaeger,56462471,56202494,2,"2019/06/05, 17:23:36",True,"2019/06/05, 17:23:36",3681.0,11300382.0,2,The VirtualService redirects HTTP requests on port 80 to port 443 where the corresponding DestinationRule then performs the TLS origination.,0.0,0.0,1.0,0.0
Jaeger,56462471,56202494,2,"2019/06/05, 17:23:36",True,"2019/06/05, 17:23:36",3681.0,11300382.0,2,"Unlike the previous ServiceEntry, this time the protocol on port 443 is HTTP, instead of HTTPS, because clients will only send HTTP requests and Istio will upgrade the connection to HTTPS.",0.0,0.0,1.0,0.0
Jaeger,56462471,56202494,2,"2019/06/05, 17:23:36",True,"2019/06/05, 17:23:36",3681.0,11300382.0,2,I hope it helps.,0.0,0.846,0.154,0.6705
Jaeger,54367053,54078937,0,"2019/01/25, 16:16:32",True,"2019/01/25, 16:16:32",13313.0,524946.0,2,"Note that tracing data (spans) are not the same as ""metrics"", although there could be some overlap in some cases.",0.0,0.0,1.0,0.0
Jaeger,54367053,54078937,0,"2019/01/25, 16:16:32",True,"2019/01/25, 16:16:32",13313.0,524946.0,2,"I recommend the following blog post on what is the purpose of each, including logging:",0.0,0.161,0.839,0.3612
Jaeger,54367053,54078937,0,"2019/01/25, 16:16:32",True,"2019/01/25, 16:16:32",13313.0,524946.0,2,https://peter.bourgon.org/blog/2017/02/21/metrics-tracing-and-logging.html,0.0,0.0,1.0,0.0
Jaeger,54367053,54078937,0,"2019/01/25, 16:16:32",True,"2019/01/25, 16:16:32",13313.0,524946.0,2,"That said, there is the OpenTracing library mentioned in the blog post you linked, called  opentracing-contrib/java-metrics .",0.0,0.0,1.0,0.0
Jaeger,54367053,54078937,0,"2019/01/25, 16:16:32",True,"2019/01/25, 16:16:32",13313.0,524946.0,2,It allows you to pick specific spans and record them as data points (metrics).,0.0,0.0,1.0,0.0
Jaeger,54367053,54078937,0,"2019/01/25, 16:16:32",True,"2019/01/25, 16:16:32",13313.0,524946.0,2,"It works as a decorator of a concrete tracer, so, your spans would reach a concrete backend like Jaeger and, additionally, create data points based on the configured spans.",0.0,0.199,0.801,0.5719
Jaeger,54367053,54078937,0,"2019/01/25, 16:16:32",True,"2019/01/25, 16:16:32",13313.0,524946.0,2,"The data points are then reported via  Micrometer , which can be configured to expose this data in Prometheus format.",0.082,0.0,0.918,-0.1531
Jaeger,54367053,54078937,0,"2019/01/25, 16:16:32",True,"2019/01/25, 16:16:32",13313.0,524946.0,2,"The problem is, I haven't found any consistent solution to do this, all the examples that I have found so far are outdated, deprecated or lacks documentation.",0.096,0.082,0.821,-0.1027
Jaeger,54367053,54078937,0,"2019/01/25, 16:16:32",True,"2019/01/25, 16:16:32",13313.0,524946.0,2,"Please, open an issue on the  java-metrics  repository with the problems you are facing.",0.159,0.135,0.706,-0.1027
Jaeger,53600710,53095177,1,"2018/12/03, 21:38:34",True,"2018/12/03, 21:38:34",467.0,1404502.0,1,Just to close this question out for the solution to the problem in my instance.,0.15,0.128,0.722,-0.1027
Jaeger,53600710,53095177,1,"2018/12/03, 21:38:34",True,"2018/12/03, 21:38:34",467.0,1404502.0,1,The mistake in configuration started all the way back in the Kubernetes cluster initialisation.,0.156,0.0,0.844,-0.34
Jaeger,53600710,53095177,1,"2018/12/03, 21:38:34",True,"2018/12/03, 21:38:34",467.0,1404502.0,1,I had applied:,0.0,0.0,1.0,0.0
Jaeger,53600710,53095177,1,"2018/12/03, 21:38:34",True,"2018/12/03, 21:38:34",467.0,1404502.0,1,the pod-network-cidr using the same address range as the local LAN on which the Kubernetes installation was deployed i.e.,0.0,0.0,1.0,0.0
Jaeger,53600710,53095177,1,"2018/12/03, 21:38:34",True,"2018/12/03, 21:38:34",467.0,1404502.0,1,the desktop for the Ubuntu host used the same IP subnet as what I'd assigned the container network.,0.0,0.0,1.0,0.0
Jaeger,53600710,53095177,1,"2018/12/03, 21:38:34",True,"2018/12/03, 21:38:34",467.0,1404502.0,1,"For the most part, everything operated fine as detailed above, until the Istio proxy was trying to route packets from an external load-balancer IP address to an internal IP address which happened to be on the same subnet.",0.0,0.046,0.954,0.2023
Jaeger,53600710,53095177,1,"2018/12/03, 21:38:34",True,"2018/12/03, 21:38:34",467.0,1404502.0,1,Project Calico with Kubernetes seemed to be able to cope with it as that's effectively Layer 3/4 policy but Istio had a problem with it a L7 (even though it was sitting on Calico underneath).,0.097,0.053,0.849,-0.3818
Jaeger,53600710,53095177,1,"2018/12/03, 21:38:34",True,"2018/12/03, 21:38:34",467.0,1404502.0,1,The solution was to tear down my entire Kubernetes deployment.,0.0,0.204,0.796,0.3182
Jaeger,53600710,53095177,1,"2018/12/03, 21:38:34",True,"2018/12/03, 21:38:34",467.0,1404502.0,1,I was paranoid and went so far as to uninstall Kubernetes and deploy again and redeploy with a pod network in the 172 range which wasn't anything to do with my local lan.,0.062,0.0,0.938,-0.25
Jaeger,53600710,53095177,1,"2018/12/03, 21:38:34",True,"2018/12/03, 21:38:34",467.0,1404502.0,1,I also made the same changes in the Project Calico configuration file to match pod networks.,0.0,0.0,1.0,0.0
Jaeger,53600710,53095177,1,"2018/12/03, 21:38:34",True,"2018/12/03, 21:38:34",467.0,1404502.0,1,"After that change, everything worked as expected.",0.0,0.0,1.0,0.0
Jaeger,53600710,53095177,1,"2018/12/03, 21:38:34",True,"2018/12/03, 21:38:34",467.0,1404502.0,1,I suspect that in a more public configuration where your cluster was directly attached to a BGP router as opposed to using MetalLB with an L2 configuration as a subset of your LAN wouldn't exhibit this issue either.,0.062,0.0,0.937,-0.296
Jaeger,53600710,53095177,1,"2018/12/03, 21:38:34",True,"2018/12/03, 21:38:34",467.0,1404502.0,1,I've documented it more in this post:,0.0,0.0,1.0,0.0
Jaeger,53600710,53095177,1,"2018/12/03, 21:38:34",True,"2018/12/03, 21:38:34",467.0,1404502.0,1,"Microservices: .Net, Linux, Kubernetes and Istio make a powerful combination",0.0,0.259,0.741,0.4215
Jaeger,17618211,17616323,0,"2013/07/12, 18:12:11",False,"2013/07/12, 18:12:11",67.0,1278255.0,0,"Actually, there is no error.",0.62,0.0,0.38,-0.5994
Jaeger,17618211,17616323,0,"2013/07/12, 18:12:11",False,"2013/07/12, 18:12:11",67.0,1278255.0,0,The code was changing the color of a yellow texture to a red tint inline.,0.0,0.0,1.0,0.0
Jaeger,12318996,12318639,3,"2012/09/07, 16:23:31",True,"2012/09/07, 16:23:31",7480.0,1566232.0,1,Try this : (if you can gives us all the variants of the url it would be better),0.0,0.0,1.0,0.0
Jaeger,54075526,54061611,0,"2019/01/07, 15:41:12",True,"2019/01/07, 15:41:12",213.0,7933630.0,2,"Unlike Jaeger, LightStep is a commercial SaaS offering.",0.0,0.0,1.0,0.0
Jaeger,54075526,54061611,0,"2019/01/07, 15:41:12",True,"2019/01/07, 15:41:12",213.0,7933630.0,2,"If you wanted to try out their service, you'd need to contact their sales team.",0.0,0.0,1.0,0.0
Jaeger,49580258,49571999,0,"2018/03/30, 22:27:46",False,"2018/03/30, 22:27:46",1.0,9573945.0,0,Managed to create the desired capturing groups:,0.0,0.457,0.543,0.4939
Jaeger,49580258,49571999,0,"2018/03/30, 22:27:46",False,"2018/03/30, 22:27:46",1.0,9573945.0,0,"Then I could write out the files, it looks correct as for these few occurences.",0.0,0.0,1.0,0.0
InspectIT,54902800,52150469,0,"2019/02/27, 12:03:53",False,"2019/02/27, 12:03:53",479.0,715518.0,0,"You need to add instrumentation rules for your application to ""dig deeper"".",0.0,0.0,1.0,0.0
InspectIT,54902800,52150469,0,"2019/02/27, 12:03:53",False,"2019/02/27, 12:03:53",479.0,715518.0,0,The  doFilter  and  service  methods are instrumented by default as part of the HTTP instrumentation profile.,0.0,0.0,1.0,0.0
InspectIT,54902800,52150469,0,"2019/02/27, 12:03:53",False,"2019/02/27, 12:03:53",479.0,715518.0,0,"In addition to the HTTP profile, by default inspectIT instruments SQL statement executions, other entry points into your application (like JMS), the places where external HTTP/JMS calls are done and Java Executors.",0.0,0.0,1.0,0.0
InspectIT,54902800,52150469,0,"2019/02/27, 12:03:53",False,"2019/02/27, 12:03:53",479.0,715518.0,0,"There are some other common instrumentation profiles (Hibernate, SQL parameters, Spring, etc), but you need to enable them by yourself.",0.0,0.0,1.0,0.0
InspectIT,54902800,52150469,0,"2019/02/27, 12:03:53",False,"2019/02/27, 12:03:53",479.0,715518.0,0,"Usually, you would start with the default settings and then, in addition, add instrumentation rules related to your application.",0.0,0.0,1.0,0.0
Instana,55646922,55606472,0,"2019/04/12, 10:53:10",False,"2019/04/12, 10:53:10",294930.0,208809.0,3,Disclaimer: I work for Instana.,0.0,0.0,1.0,0.0
Instana,55646922,55606472,0,"2019/04/12, 10:53:10",False,"2019/04/12, 10:53:10",294930.0,208809.0,3,There is not much to setup.,0.0,0.0,1.0,0.0
Instana,55646922,55606472,0,"2019/04/12, 10:53:10",False,"2019/04/12, 10:53:10",294930.0,208809.0,3,Instana provides out-of-the-box support for Kafka and Zookeeper nodes.,0.0,0.252,0.748,0.4019
Instana,55646922,55606472,0,"2019/04/12, 10:53:10",False,"2019/04/12, 10:53:10",294930.0,208809.0,3,So all you need to do is to install the Instana agent on the server(s) you want to monitor.,0.0,0.067,0.933,0.0772
Instana,55646922,55606472,0,"2019/04/12, 10:53:10",False,"2019/04/12, 10:53:10",294930.0,208809.0,3,It will automatically detect your Kafka and Zookeeper installations and start reporting metrics for them to your tenant unit.,0.0,0.0,1.0,0.0
Instana,55646922,55606472,0,"2019/04/12, 10:53:10",False,"2019/04/12, 10:53:10",294930.0,208809.0,3,"If you don't have a tenant unit yet, you can register for a free trial at  https://www.instana.com/trial/  or contact Sales.",0.0,0.163,0.837,0.5106
Instana,55646922,55606472,0,"2019/04/12, 10:53:10",False,"2019/04/12, 10:53:10",294930.0,208809.0,3,"If you need additional help, I suggest to open a ticket at  https://instana.zendesk.com  to get dedicated support.",0.0,0.412,0.588,0.8126
Instana,57088646,55974537,0,"2019/07/18, 09:53:32",True,"2019/07/18, 09:53:32",465.0,469028.0,3,Instana has a  demo application  that shows to do this.,0.0,0.0,1.0,0.0
Instana,57088646,55974537,0,"2019/07/18, 09:53:32",True,"2019/07/18, 09:53:32",465.0,469028.0,3,To summarize the parts that you would need:,0.0,0.0,1.0,0.0
Instana,57088646,55974537,0,"2019/07/18, 09:53:32",True,"2019/07/18, 09:53:32",465.0,469028.0,3,The combination of these two steps will make TypeScript aware of the function.,0.0,0.0,1.0,0.0
Instana,57088646,55974537,0,"2019/07/18, 09:53:32",True,"2019/07/18, 09:53:32",465.0,469028.0,3,Now you can use  ineum  just like any other global.,0.0,0.217,0.783,0.3612
Instana,59945966,59921305,1,"2020/01/28, 11:46:22",False,"2020/01/28, 14:25:45",1628.0,1847951.0,2,Instana will use the same protocol to make the sourcemap request.,0.0,0.0,1.0,0.0
Instana,59945966,59921305,1,"2020/01/28, 11:46:22",False,"2020/01/28, 14:25:45",1628.0,1847951.0,2,"The documentation example uses http, but it will work with https the same way.",0.0,0.0,1.0,0.0
Instana,59945966,59921305,1,"2020/01/28, 11:46:22",False,"2020/01/28, 14:25:45",1628.0,1847951.0,2,The most likely reason for your problem is that the sourcemap is not readable from the public internet.,0.137,0.0,0.863,-0.4019
Instana,59945966,59921305,1,"2020/01/28, 11:46:22",False,"2020/01/28, 14:25:45",1628.0,1847951.0,2,"In your case, the sourcemap file requires http session authentication and redirects to a login page.",0.0,0.0,1.0,0.0
Instana,60976067,57292600,0,"2020/04/01, 19:46:26",False,"2020/04/01, 19:46:26",121.0,2001962.0,1,"You could remove the location /nginx_status in that server, and add a new server section like this:",0.0,0.143,0.857,0.3612
Instana,58527533,58518001,2,"2019/10/23, 19:43:00",False,"2019/10/23, 19:43:00",51.0,10645824.0,3,"That endpoint requires a POST, it appears you are using GET.",0.0,0.0,1.0,0.0
Instana,58527533,58518001,2,"2019/10/23, 19:43:00",False,"2019/10/23, 19:43:00",51.0,10645824.0,3,Hence method not allowed.,0.0,0.0,1.0,0.0
Instana,64642758,64613844,0,"2020/11/02, 11:18:54",False,"2020/11/02, 11:18:54",11.0,14562868.0,1,"Instana offers an agent tailored to React native, which simplifies the integration.",0.0,0.0,1.0,0.0
Instana,64642758,64613844,0,"2020/11/02, 11:18:54",False,"2020/11/02, 11:18:54",11.0,14562868.0,1,The  React native agent  is different than the one used for website monitoring.,0.0,0.0,1.0,0.0
Instana,64642758,64613844,0,"2020/11/02, 11:18:54",False,"2020/11/02, 11:18:54",11.0,14562868.0,1,You can get started with React native monitoring by creating a mobile app within Instana's user interface under  Websites &amp; Mobile Apps -&gt; Mobile Apps .,0.0,0.087,0.913,0.296
Instana,64642758,64613844,0,"2020/11/02, 11:18:54",False,"2020/11/02, 11:18:54",11.0,14562868.0,1,For the React native agent you can find  dedicated documentation and installation instructions  on Instana's documentation site.,0.0,0.158,0.842,0.4588
Instana,64642758,64613844,0,"2020/11/02, 11:18:54",False,"2020/11/02, 11:18:54",11.0,14562868.0,1,"For further questions and support, I suggest leveraging  Instana's support portal .",0.0,0.403,0.597,0.6597
Instana,57372845,57354975,0,"2019/08/06, 12:22:05",False,"2019/08/06, 12:22:05",1628.0,1847951.0,1,the Instana repository has been upgraded to support Disco Dingo as well.,0.0,0.324,0.676,0.5859
Instana,62222959,62075759,0,"2020/06/05, 22:35:52",True,"2020/06/05, 22:35:52",4812.0,4828463.0,1,To whoever removed his/her answer: It was a correct answer.,0.0,0.0,1.0,0.0
Instana,62222959,62075759,0,"2020/06/05, 22:35:52",True,"2020/06/05, 22:35:52",4812.0,4828463.0,1,I don't know why you deleted it.,0.0,0.0,1.0,0.0
Instana,62222959,62075759,0,"2020/06/05, 22:35:52",True,"2020/06/05, 22:35:52",4812.0,4828463.0,1,"Anyhow, I am posting again in case someone stumbles here.",0.0,0.0,1.0,0.0
Instana,62222959,62075759,0,"2020/06/05, 22:35:52",True,"2020/06/05, 22:35:52",4812.0,4828463.0,1,You can control frequency and time by using  INSTANA_AGENT_UPDATES_FREQUENCY  and  INSTANA_AGENT_UPDATES_TIME  environment variables.,0.0,0.0,1.0,0.0
Instana,62222959,62075759,0,"2020/06/05, 22:35:52",True,"2020/06/05, 22:35:52",4812.0,4828463.0,1,Updating  mode  via env variable is still unknown at this point.,0.0,0.0,1.0,0.0
Instana,62222959,62075759,0,"2020/06/05, 22:35:52",True,"2020/06/05, 22:35:52",4812.0,4828463.0,1,Look at this page for more info:  https://www.instana.com/docs/setup_and_manage/host_agent/on/docker/#updates-and-version-pinning,0.0,0.0,1.0,0.0
Instana,64377786,62075759,1,"2020/10/15, 21:37:15",False,"2020/10/19, 07:30:18",11.0,13617981.0,1,"Most agent settings that one may want to change quickly are available as environment variables, see  https://www.instana.com/docs/setup_and_manage/host_agent/on/docker .",0.0,0.075,0.925,0.0772
Instana,64377786,62075759,1,"2020/10/15, 21:37:15",False,"2020/10/19, 07:30:18",11.0,13617981.0,1,"For example, setting the mode via environment variable is supported as well with  INSTANA_AGENT_MODE , see e.g.,  https://hub.docker.com/r/instana/agent .",0.0,0.227,0.773,0.5267
Instana,64377786,62075759,1,"2020/10/15, 21:37:15",False,"2020/10/19, 07:30:18",11.0,13617981.0,1,The valid values are:,0.0,0.474,0.526,0.4019
Instana,64377786,62075759,1,"2020/10/15, 21:37:15",False,"2020/10/19, 07:30:18",11.0,13617981.0,1,"On Kubernetes, it is also of course possible to use a ConfigMap to override files in the agent container.",0.0,0.0,1.0,0.0
Instana,57736812,57580097,0,"2019/08/31, 12:35:09",True,"2019/08/31, 12:35:09",49.0,10826472.0,0,Solved.,0.0,1.0,0.0,0.2732
Instana,57736812,57580097,0,"2019/08/31, 12:35:09",True,"2019/08/31, 12:35:09",49.0,10826472.0,0,"Added flags to my run configuration, and increase XMS and XMX twice.",0.0,0.173,0.827,0.3182
Instana,41056601,41053682,3,"2016/12/09, 10:58:46",False,"2016/12/09, 10:58:46",8454.0,958529.0,0,"pod install  is  cocoapods  command, not part of  ruby  or  gem .",0.0,0.0,1.0,0.0
Instana,41056601,41053682,3,"2016/12/09, 10:58:46",False,"2016/12/09, 10:58:46",8454.0,958529.0,0,The error means there is no  pod  or  install  package in  ruby  package repository.,0.29,0.0,0.71,-0.5994
Instana,41056601,41053682,3,"2016/12/09, 10:58:46",False,"2016/12/09, 10:58:46",8454.0,958529.0,0,"After writing a proper  Podfile  in your Xcode project dir, just run  pod install .",0.0,0.0,1.0,0.0
LightStep,44529216,43796314,1,"2017/06/13, 21:43:41",True,"2017/06/13, 23:47:13",592.0,1359408.0,1,[Disclaimer: I work at  LightStep],0.0,0.0,1.0,0.0
LightStep,44529216,43796314,1,"2017/06/13, 21:43:41",True,"2017/06/13, 23:47:13",592.0,1359408.0,1,Sorry you're having trouble getting Java and Go to play well together.,0.242,0.273,0.485,0.128
LightStep,44529216,43796314,1,"2017/06/13, 21:43:41",True,"2017/06/13, 23:47:13",592.0,1359408.0,1,I suspect this is caused by time-correction being enabled in Java but not being used in Go.,0.096,0.0,0.904,-0.1531
LightStep,44529216,43796314,1,"2017/06/13, 21:43:41",True,"2017/06/13, 23:47:13",592.0,1359408.0,1,"You can disable time correction in Java using the  withClockSkewCorrection(boolean clockCorrection)  
option to turn off clockCorrection when passing in options to the LightStep tracer",0.0,0.0,1.0,0.0
LightStep,44529216,43796314,1,"2017/06/13, 21:43:41",True,"2017/06/13, 23:47:13",592.0,1359408.0,1,Here is the updated  README  and a link to the  option code,0.0,0.0,1.0,0.0
LightStep,44529216,43796314,1,"2017/06/13, 21:43:41",True,"2017/06/13, 23:47:13",592.0,1359408.0,1,"If you contact us via the [Support] button in LightStep, we should be able to get you sorted out.",0.0,0.0,1.0,0.0
LightStep,44529216,43796314,1,"2017/06/13, 21:43:41",True,"2017/06/13, 23:47:13",592.0,1359408.0,1,Please send us a note so that we can confirm that this is solved for you.,0.0,0.253,0.747,0.5267
LightStep,44529216,43796314,1,"2017/06/13, 21:43:41",True,"2017/06/13, 23:47:13",592.0,1359408.0,1,We'll start monitoring SO more carefully so that we catch these things earlier.,0.0,0.187,0.813,0.4152
LightStep,44529216,43796314,1,"2017/06/13, 21:43:41",True,"2017/06/13, 23:47:13",592.0,1359408.0,1,Thanks and happy tracing!,0.0,0.775,0.225,0.784
LightStep,44529216,43796314,1,"2017/06/13, 21:43:41",True,"2017/06/13, 23:47:13",592.0,1359408.0,1,Will,0.0,0.0,1.0,0.0
LightStep,63725459,63724073,2,"2020/09/03, 17:13:18",True,"2020/09/03, 17:13:18",91.0,1548178.0,2,lightstep-opentelemetry-launcher-node  basically bundles the required things for you for easier configuration so this is not an exporter.,0.0,0.149,0.851,0.4215
LightStep,63725459,63724073,2,"2020/09/03, 17:13:18",True,"2020/09/03, 17:13:18",91.0,1548178.0,2,If you were to simply replace the &quot;LightstepExporter&quot; with &quot;OpenTelemetry Collector Exporter&quot; in your code you can simply do this,0.0,0.0,1.0,0.0
LightStep,63725459,63724073,2,"2020/09/03, 17:13:18",True,"2020/09/03, 17:13:18",91.0,1548178.0,2,The default  YOUR_DIGETS_URL  from  lightstep/otel-launcher-node  is  https://ingest.lightstep.com:443/api/v2/otel/trace,0.0,0.0,1.0,0.0
LightStep,49949656,49949470,0,"2018/04/21, 00:00:46",False,"2018/04/21, 00:00:46",8336.0,1773866.0,1,"If you go to the latest snapshot documentation (or milestone) and you search for the word OpenTracing, you would get your answer.",0.0,0.0,1.0,0.0
LightStep,49949656,49949470,0,"2018/04/21, 00:00:46",False,"2018/04/21, 00:00:46",8336.0,1773866.0,1,It's here  https://cloud.spring.io/spring-cloud-sleuth/single/spring-cloud-sleuth.html#_opentracing,0.0,0.0,1.0,0.0
LightStep,49949656,49949470,0,"2018/04/21, 00:00:46",False,"2018/04/21, 00:00:46",8336.0,1773866.0,1,Spring Cloud Sleuth is compatible with OpenTracing.,0.0,0.0,1.0,0.0
LightStep,49949656,49949470,0,"2018/04/21, 00:00:46",False,"2018/04/21, 00:00:46",8336.0,1773866.0,1,"If you have OpenTracing on the classpath, we automatically register the OpenTracing Tracer bean.",0.0,0.0,1.0,0.0
LightStep,49949656,49949470,0,"2018/04/21, 00:00:46",False,"2018/04/21, 00:00:46",8336.0,1773866.0,1,"If you wish to disable this, set  spring.sleuth.opentracing.enabled  to false",0.0,0.231,0.769,0.4019
LightStep,49949656,49949470,0,"2018/04/21, 00:00:46",False,"2018/04/21, 00:00:46",8336.0,1773866.0,1,So it's enough to just have OpenTracing on the classpath and Sleuth will work out of the box,0.0,0.0,1.0,0.0
SkyWalking,58586629,58314080,0,"2019/10/28, 08:40:49",True,"2019/10/28, 08:40:49",395.0,9571426.0,0,"Explanation how to set up  skywalking  properly:
 https://github.com/apache/skywalking/issues/3589#issuecomment-543268029",0.0,0.0,1.0,0.0
SkyWalking,62389599,60476903,0,"2020/06/15, 16:48:11",False,"2020/06/15, 16:48:11",6436.0,2628868.0,0,"It is the dashboard default time filter value problem, the time range did not contains data:",0.141,0.126,0.733,-0.0772
SkyWalking,62389599,60476903,0,"2020/06/15, 16:48:11",False,"2020/06/15, 16:48:11",6436.0,2628868.0,0,change the time start and end to having collection data area.,0.0,0.0,1.0,0.0
SkyWalking,62358989,62358639,0,"2020/06/13, 14:19:26",True,"2020/06/13, 14:19:26",6436.0,2628868.0,1,Finally I build the side car image by myself:,0.0,0.0,1.0,0.0
SkyWalking,62358989,62358639,0,"2020/06/13, 14:19:26",True,"2020/06/13, 14:19:26",6436.0,2628868.0,1,this is the docker file:,0.0,0.0,1.0,0.0
SkyWalking,63516070,63509093,0,"2020/08/21, 06:24:43",True,"2020/08/21, 06:24:43",22293.0,225016.0,3,how to add the jdbc driver jar into the image file?,0.0,0.0,1.0,0.0
SkyWalking,63516070,63509093,0,"2020/08/21, 06:24:43",True,"2020/08/21, 06:24:43",22293.0,225016.0,3,One way would be an  initContainer:  and then artificially inject the jdbc driver via  -Xbootclasspath,0.0,0.0,1.0,0.0
SkyWalking,63516070,63509093,0,"2020/08/21, 06:24:43",True,"2020/08/21, 06:24:43",22293.0,225016.0,3,"a similar, although slightly riskier way, is to find a path that is already on the classpath of the image, and attempt to volume mount the jar path into that directory",0.07,0.0,0.93,-0.2748
SkyWalking,63516070,63509093,0,"2020/08/21, 06:24:43",True,"2020/08/21, 06:24:43",22293.0,225016.0,3,"All of this seems kind of moot given that your image looks like one that is custom built, and therefore the correct action is to update the  Dockerfile  for it to download the jar at build time",0.0,0.065,0.935,0.3612
SkyWalking,65282846,65259756,0,"2020/12/14, 04:23:10",True,"2020/12/14, 04:23:10",1070.0,5196039.0,0,"I created a lifecycle that performs the delete action after a set time, and then I added this configuration to the skywalking application.yml under  storage.elasticsearch7 :",0.0,0.091,0.909,0.25
SkyWalking,65282846,65259756,0,"2020/12/14, 04:23:10",True,"2020/12/14, 04:23:10",1070.0,5196039.0,0,"SW creates index templates, and now I see that this is part of the template, and indeed the indexes have this sw-policy attached.",0.0,0.091,0.909,0.2732
SkyWalking,60472081,60465004,0,"2020/03/01, 06:50:19",False,"2020/03/01, 06:50:19",43078.0,78722.0,2,So you can see this more clearly in the output.,0.0,0.25,0.75,0.4576
SkyWalking,60472081,60465004,0,"2020/03/01, 06:50:19",False,"2020/03/01, 06:50:19",43078.0,78722.0,2,The pod is Running but the Ready flag is false meaning the container is up but is failing the Readiness Probe.,0.158,0.204,0.638,0.0772
SkyWalking,60463565,60462786,0,"2020/02/29, 11:08:36",True,"2020/02/29, 11:08:36",33843.0,498256.0,2,So you should go throught this document first,0.0,0.0,1.0,0.0
SkyWalking,60463565,60462786,0,"2020/02/29, 11:08:36",True,"2020/02/29, 11:08:36",33843.0,498256.0,2,https://kubernetes.io/docs/concepts/storage/volumes/#hostpath,0.0,0.0,1.0,0.0
SkyWalking,60463565,60462786,0,"2020/02/29, 11:08:36",True,"2020/02/29, 11:08:36",33843.0,498256.0,2,use  hostPath  as sample,0.0,0.0,1.0,0.0
SkyWalking,60463565,60462786,0,"2020/02/29, 11:08:36",True,"2020/02/29, 11:08:36",33843.0,498256.0,2,You need reference it for both init container and normal container.,0.0,0.0,1.0,0.0
Stagemonitor,44376892,44347364,0,"2017/06/05, 23:09:57",True,"2017/06/06, 08:46:33",4936.0,1125055.0,1,Most of the metrics stagemonitor collects are not available via JMX.,0.0,0.0,1.0,0.0
Stagemonitor,44376892,44347364,0,"2017/06/05, 23:09:57",True,"2017/06/06, 08:46:33",4936.0,1125055.0,1,"For example, response time statistics grouped by the endpoint of your application.",0.0,0.0,1.0,0.0
Stagemonitor,44376892,44347364,0,"2017/06/05, 23:09:57",True,"2017/06/06, 08:46:33",4936.0,1125055.0,1,"Also, stagemonitor is much more than just metrics.",0.0,0.0,1.0,0.0
Stagemonitor,44376892,44347364,0,"2017/06/05, 23:09:57",True,"2017/06/06, 08:46:33",4936.0,1125055.0,1,"It is also a profiler, you can use to see which methods caused a request to be slow.",0.0,0.0,1.0,0.0
Stagemonitor,44376892,44347364,0,"2017/06/05, 23:09:57",True,"2017/06/06, 08:46:33",4936.0,1125055.0,1,"Further more, it can (soon) do distributed tracing which helps you to analyze and debug latency problems in a microservice environment by correlating related requests.",0.099,0.095,0.806,-0.0258
Stagemonitor,44376892,44347364,0,"2017/06/05, 23:09:57",True,"2017/06/06, 08:46:33",4936.0,1125055.0,1,It also offers you a Kibana dashboard you can use to drill into the requests your application serves to find out about causes of errors or latency.,0.088,0.0,0.912,-0.34
Stagemonitor,44376892,44347364,0,"2017/06/05, 23:09:57",True,"2017/06/06, 08:46:33",4936.0,1125055.0,1,Another use case is lightweight web analytics to identify which devices and operating systems your customers use to access your site.,0.0,0.0,1.0,0.0
Stagemonitor,48148146,48147866,0,"2018/01/08, 12:12:45",False,"2018/01/08, 12:12:45",128.0,7445902.0,0,Two possible solutions.,0.0,0.459,0.541,0.1779
Stagemonitor,48148146,48147866,0,"2018/01/08, 12:12:45",False,"2018/01/08, 12:12:45",128.0,7445902.0,0,"You can have a button 'hide', that will hide the metrics using some javascript code.",0.116,0.0,0.884,-0.1779
Stagemonitor,48148146,48147866,0,"2018/01/08, 12:12:45",False,"2018/01/08, 12:12:45",128.0,7445902.0,0,Or in the same button you do the following:,0.0,0.0,1.0,0.0
Stagemonitor,30496430,30493074,0,"2015/05/28, 05:55:25",True,"2015/05/28, 05:55:25",74274.0,160313.0,1,It doesn't appear to be compatible with Grails.,0.0,0.0,1.0,0.0
Stagemonitor,30496430,30493074,0,"2015/05/28, 05:55:25",True,"2015/05/28, 05:55:25",74274.0,160313.0,1,If you enable logging,0.0,0.0,1.0,0.0
Stagemonitor,30496430,30493074,0,"2015/05/28, 05:55:25",True,"2015/05/28, 05:55:25",74274.0,160313.0,1,you'll see a bunch of error stacktraces that appear to imply that the way they're using Javassist to wire in tracing code isn't compatible with Groovy and/or the AST transformations that Grails uses:,0.08,0.0,0.92,-0.4019
Stagemonitor,38195456,38184669,0,"2016/07/05, 07:59:13",True,"2017/02/13, 14:44:38",3568.0,1679544.0,1,Finally I found out how to disable the browser widget.,0.0,0.0,1.0,0.0
Stagemonitor,38195456,38184669,0,"2016/07/05, 07:59:13",True,"2017/02/13, 14:44:38",3568.0,1679544.0,1,Set:,0.0,0.0,1.0,0.0
Stagemonitor,38195456,38184669,0,"2016/07/05, 07:59:13",True,"2017/02/13, 14:44:38",3568.0,1679544.0,1,You can see more information about it  here .,0.0,0.0,1.0,0.0
Stagemonitor,43811524,43806990,4,"2017/05/05, 21:23:27",False,"2017/05/05, 21:23:27",104968.0,1766831.0,0,MySQL does not provide anything more than how much data each tenant has.,0.0,0.0,1.0,0.0
Stagemonitor,43811524,43806990,4,"2017/05/05, 21:23:27",False,"2017/05/05, 21:23:27",104968.0,1766831.0,0,That can be found in  information_schema .,0.0,0.0,1.0,0.0
Stagemonitor,43811524,43806990,4,"2017/05/05, 21:23:27",False,"2017/05/05, 21:23:27",104968.0,1766831.0,0,"If you need CPU/IO/etc., you need to set up multiple instances of MySQL in VMs or cgroups and have the OS / VM-manager provide the data.",0.0,0.0,1.0,0.0
Stagemonitor,43811524,43806990,4,"2017/05/05, 21:23:27",False,"2017/05/05, 21:23:27",104968.0,1766831.0,0,"This will cost extra RAM, so it may not be worth it.",0.132,0.0,0.868,-0.1695
Stagemonitor,48163586,48162254,0,"2018/01/09, 09:52:48",True,"2018/01/09, 09:52:48",4936.0,1125055.0,1,I'm afraid this is currently not possible.,0.0,0.0,1.0,0.0
Stagemonitor,48163586,48162254,0,"2018/01/09, 09:52:48",True,"2018/01/09, 09:52:48",4936.0,1125055.0,1,"However, stagemonitor offers a ""Custom Metrics"" dashboard for Grafana.",0.0,0.0,1.0,0.0
Stagemonitor,48163586,48162254,0,"2018/01/09, 09:52:48",True,"2018/01/09, 09:52:48",4936.0,1125055.0,1,"To see the metrics locally, currently the only way is to enable periodic logging of all metrics.",0.0,0.0,1.0,0.0
Stagemonitor,26004106,25204392,1,"2014/09/23, 23:23:43",False,"2014/09/23, 23:23:43",4936.0,1125055.0,3,Stagemonitor now features a in browser widget that is automatically injected in your web page.,0.0,0.0,1.0,0.0
Stagemonitor,26004106,25204392,1,"2014/09/23, 23:23:43",False,"2014/09/23, 23:23:43",4936.0,1125055.0,3,You don't need any infrastructure or docker for this and the configuration and set up is easy.,0.0,0.153,0.847,0.4404
Stagemonitor,26004106,25204392,1,"2014/09/23, 23:23:43",False,"2014/09/23, 23:23:43",4936.0,1125055.0,3,"For more information, visit  http://www.stagemonitor.org/ .",0.0,0.0,1.0,0.0
Stagemonitor,26004106,25204392,1,"2014/09/23, 23:23:43",False,"2014/09/23, 23:23:43",4936.0,1125055.0,3,This is how you enable the widget:  https://github.com/stagemonitor/stagemonitor/wiki/Step-2%3A-Log-Only-Monitoring-In-Browser-Widget#in-browser-widget .,0.0,0.0,1.0,0.0
Wavefront VMware,63893372,63635003,0,"2020/09/15, 03:17:06",False,"2020/09/15, 03:17:06",5847.0,3390417.0,0,"The problem(s) (as noted in  GEODE-788 ,  GEODE-7665 ,  GEODE-7666 ,  GEODE-7670 ,  GEODE-7672  and  GEODE-7676 ) is, is that GemFire/Geode does not support  Region.clear()  for  PARTITION   Regions  (yet).",0.089,0.0,0.911,-0.3089
Wavefront VMware,63893372,63635003,0,"2020/09/15, 03:17:06",False,"2020/09/15, 03:17:06",5847.0,3390417.0,0,"When you declare the  @CacheEvent(allEntries = true)  annotation/attribute on your managed application component, for example...",0.0,0.0,1.0,0.0
Wavefront VMware,63893372,63635003,0,"2020/09/15, 03:17:06",False,"2020/09/15, 03:17:06",5847.0,3390417.0,0,This in effect calls  Region.clear()  (see  here ).,0.0,0.0,1.0,0.0
Wavefront VMware,63893372,63635003,0,"2020/09/15, 03:17:06",False,"2020/09/15, 03:17:06",5847.0,3390417.0,0,"This behavior works for  REPLICATE  and  LOCAL   Regions , however not for  PARTITION   Regions , given the numerous GemFire/Geode problems.",0.137,0.0,0.863,-0.4019
Wavefront VMware,63893372,63635003,0,"2020/09/15, 03:17:06",False,"2020/09/15, 03:17:06",5847.0,3390417.0,0,"It is currently a WIP, though.",0.0,0.0,1.0,0.0
Wavefront VMware,63893372,63635003,0,"2020/09/15, 03:17:06",False,"2020/09/15, 03:17:06",5847.0,3390417.0,0,"There was (partly, still is) an intention in Spring Data for Apache Geode &amp; VMware Tanzu (Pivotal) GemFire to handle cache clear operations.",0.0,0.106,0.894,0.3818
Wavefront VMware,63893372,63635003,0,"2020/09/15, 03:17:06",False,"2020/09/15, 03:17:06",5847.0,3390417.0,0,https://jira.spring.io/browse/DATAGEODE-265,0.0,0.0,1.0,0.0
Wavefront VMware,63893372,63635003,0,"2020/09/15, 03:17:06",False,"2020/09/15, 03:17:06",5847.0,3390417.0,0,"However, this is on hold until the above GEODE JIRA tickets get sorted out.",0.0,0.0,1.0,0.0
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201.0,1585136.0,0,The short answer is no.,0.355,0.0,0.645,-0.296
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201.0,1585136.0,0,"You really, really want to have DNS set up properly.",0.0,0.172,0.828,0.2195
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201.0,1585136.0,0,Here's the long answer that is more nuanced.,0.0,0.0,1.0,0.0
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201.0,1585136.0,0,All requests to your foundation go through the Gorouter.,0.0,0.0,1.0,0.0
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201.0,1585136.0,0,"Gorouter will take the incoming request, look at the  Host  header and use that to determine where to send the request.",0.0,0.0,1.0,0.0
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201.0,1585136.0,0,This happens the same for system services like CAPI and UAA as it does for apps you deploy to the foundation.,0.0,0.111,0.889,0.3612
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201.0,1585136.0,0,DNS is a requirement because of the  Host  header.,0.0,0.0,1.0,0.0
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201.0,1585136.0,0,A browser trying to access CAPI or an application on your foundation is going to set the  Host  header based on the DNS entry you type into your browser's address bar.,0.0,0.0,1.0,0.0
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201.0,1585136.0,0,The cf CLI is going to do the same thing.,0.0,0.0,1.0,0.0
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201.0,1585136.0,0,There are some ways to work around this:,0.0,0.0,1.0,0.0
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201.0,1585136.0,0,If you are strictly using a client like  curl  where you can set the  Host  header to arbitrary values.,0.0,0.245,0.755,0.6369
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201.0,1585136.0,0,"In that way, you could set the host header to  api.system_domain  and at the same time connect to the IP address of your foundation.",0.0,0.0,1.0,0.0
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201.0,1585136.0,0,That's not a very elegant way to use CF though.,0.257,0.0,0.743,-0.4158
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201.0,1585136.0,0,You can manually set entries in your /etc/hosts` (or similar on Windows).,0.0,0.0,1.0,0.0
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201.0,1585136.0,0,This is basically a way to override DNS resolution and supply your own custom IP.,0.0,0.0,1.0,0.0
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201.0,1585136.0,0,"You would need to do this for  uaa.system_domain ,  login.system_domain ,  api.system_domain  and any host names you want to use for apps deployed to your foundation, like  my-super-cool-app.apps_domain .",0.0,0.137,0.863,0.4215
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201.0,1585136.0,0,These should all point to the IP of the load balancer that's in front of your pool of Gorouters.,0.0,0.0,1.0,0.0
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201.0,1585136.0,0,If you add enough entries into  /etc/hosts  you can make the cf CLI work.,0.0,0.0,1.0,0.0
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201.0,1585136.0,0,I have done this on occasion to bypass the load balancer layer for troubleshooting purposes.,0.0,0.116,0.884,0.1779
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201.0,1585136.0,0,"Where this won't work is on systems where you can't edit  /etc/hosts , like customers or external users of software running on your foundation or if you're trying to deploy apps on your foundation that talk to each other using routes on CF (because you can't edit  /etc/hosts  in the container).",0.041,0.0,0.959,-0.2755
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201.0,1585136.0,0,Like if you have  app-a.apps_domain  and  app-b.apps_domain  and  app-a  needs to talk to  app-b .,0.0,0.161,0.839,0.3612
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201.0,1585136.0,0,That won't work because you have no DNS resolution for  apps_domain .,0.18,0.0,0.82,-0.296
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201.0,1585136.0,0,You can probably make app-to-app communication work if you are able to use container-to-container networking and the  apps.internal  domain though.,0.0,0.0,1.0,0.0
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201.0,1585136.0,0,The resolution for that domain is provided by Bosh DNS.,0.0,0.0,1.0,0.0
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201.0,1585136.0,0,"You have to be aware of this difference though when deploying your apps and map routes on the  apps.internal  domain, as well as setting network policy to allow traffic to flow between the two.",0.0,0.111,0.889,0.4588
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201.0,1585136.0,0,"Anyway, there might be other hiccups.",0.0,0.0,1.0,0.0
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201.0,1585136.0,0,This is just off the top of my head.,0.0,0.184,0.816,0.2023
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201.0,1585136.0,0,You can see it's a lot better if you can set up DNS.,0.0,0.209,0.791,0.4404
Wavefront VMware,66634194,66545598,0,"2021/03/15, 09:54:59",False,"2021/03/15, 09:54:59",31.0,7582877.0,0,The most easy way to achieve a portable solution is a service like  xip.io  that will work out of the box.,0.0,0.333,0.667,0.7902
Wavefront VMware,66634194,66545598,0,"2021/03/15, 09:54:59",False,"2021/03/15, 09:54:59",31.0,7582877.0,0,"I have setup and run a lot of PoCs that way, when wildcard DNS was something that enterprise IT was still oblivious about.",0.0,0.0,1.0,0.0
Wavefront VMware,66634194,66545598,0,"2021/03/15, 09:54:59",False,"2021/03/15, 09:54:59",31.0,7582877.0,0,It works like this (excerpt from their site):,0.0,0.263,0.737,0.3612
Wavefront VMware,66634194,66545598,0,"2021/03/15, 09:54:59",False,"2021/03/15, 09:54:59",31.0,7582877.0,0,What is xip.io?,0.0,0.0,1.0,0.0
Wavefront VMware,66634194,66545598,0,"2021/03/15, 09:54:59",False,"2021/03/15, 09:54:59",31.0,7582877.0,0,"xip.io is a magic domain name that provides wildcard DNS
for any IP address.",0.0,0.0,1.0,0.0
Wavefront VMware,66634194,66545598,0,"2021/03/15, 09:54:59",False,"2021/03/15, 09:54:59",31.0,7582877.0,0,Say your LAN IP address is 10.0.0.1.,0.0,0.0,1.0,0.0
Wavefront VMware,66634194,66545598,0,"2021/03/15, 09:54:59",False,"2021/03/15, 09:54:59",31.0,7582877.0,0,"Using xip.io,",0.0,0.0,1.0,0.0
Wavefront VMware,66634194,66545598,0,"2021/03/15, 09:54:59",False,"2021/03/15, 09:54:59",31.0,7582877.0,0,"mysite.10.0.0.1.xip.io   resolves to   10.0.0.1
foo.bar.10.0.0.1.xip.io   resolves to   10.0.0.1",0.0,0.362,0.638,0.34
Wavefront VMware,66634194,66545598,0,"2021/03/15, 09:54:59",False,"2021/03/15, 09:54:59",31.0,7582877.0,0,...and so on.,0.0,0.0,1.0,0.0
Wavefront VMware,66634194,66545598,0,"2021/03/15, 09:54:59",False,"2021/03/15, 09:54:59",31.0,7582877.0,0,"You can use these domains to access virtual
hosts on your development web server from devices on your
local network, like iPads, iPhones, and other computers.",0.0,0.091,0.909,0.3612
Wavefront VMware,66634194,66545598,0,"2021/03/15, 09:54:59",False,"2021/03/15, 09:54:59",31.0,7582877.0,0,No configuration required!,0.555,0.0,0.445,-0.3595
Wavefront VMware,67007504,67007310,3,"2021/04/08, 18:47:38",False,"2021/04/08, 18:47:38",5028.0,5965430.0,1,It is unclear whether you're using the SCDF tile or the SCDF OSS (via  manfest.yml ) on PCF.,0.111,0.0,0.889,-0.25
Wavefront VMware,67007504,67007310,3,"2021/04/08, 18:47:38",False,"2021/04/08, 18:47:38",5028.0,5965430.0,1,"Suppose you're using the OSS, AFA.",0.0,0.0,1.0,0.0
Wavefront VMware,67007504,67007310,3,"2021/04/08, 18:47:38",False,"2021/04/08, 18:47:38",5028.0,5965430.0,1,"In that case, you are providing the right RMQ service-instance configuration (that you pre-created) in the  manifest.yml , then SCDF would automatically propagate that RMQ service instance and bind it to the apps it is deploying to your ORG/Space.",0.0,0.0,1.0,0.0
Wavefront VMware,67007504,67007310,3,"2021/04/08, 18:47:38",False,"2021/04/08, 18:47:38",5028.0,5965430.0,1,You don't need to muck around with connection credentials manually.,0.0,0.0,1.0,0.0
Wavefront VMware,67007504,67007310,3,"2021/04/08, 18:47:38",False,"2021/04/08, 18:47:38",5028.0,5965430.0,1,"On the other hand, if you are using the SCDF Tile, the SCDF service broker will auto-create the RMQ SI and automatically bind it to the apps it deploys.",0.0,0.103,0.897,0.4939
Wavefront VMware,67007504,67007310,3,"2021/04/08, 18:47:38",False,"2021/04/08, 18:47:38",5028.0,5965430.0,1,"In summary, there's no reason to manually pass the connection credentials or pack them as application properties inside your apps.",0.104,0.0,0.896,-0.296
Wavefront VMware,67007504,67007310,3,"2021/04/08, 18:47:38",False,"2021/04/08, 18:47:38",5028.0,5965430.0,1,You can automate all this provided you're configuring all this correctly.,0.0,0.0,1.0,0.0
Wavefront VMware,63456882,63327611,0,"2020/08/17, 21:44:26",False,"2020/08/17, 21:44:26",432.0,6689024.0,0,"&quot; In this case it will wait till the processing completes or it
forcibly reduces the instance count when reached threshold.",0.0,0.065,0.935,0.1027
Wavefront VMware,63456882,63327611,0,"2020/08/17, 21:44:26",False,"2020/08/17, 21:44:26",432.0,6689024.0,0,&quot;,0.0,0.0,1.0,0.0
Wavefront VMware,63456882,63327611,0,"2020/08/17, 21:44:26",False,"2020/08/17, 21:44:26",432.0,6689024.0,0,"Answer: 
No, the App Autoscaler will not force anything, after the decision cycle, it will prepare the instance to be escalated-down (shutdown), so the intention is to avoid lose requests or data during this process.",0.182,0.0,0.818,-0.7269
Wavefront VMware,63456882,63327611,0,"2020/08/17, 21:44:26",False,"2020/08/17, 21:44:26",432.0,6689024.0,0,"Please, take a look into the documentation below, it will help you to understand better the App Autoscaler mechanism.",0.0,0.345,0.655,0.7845
Wavefront VMware,63456882,63327611,0,"2020/08/17, 21:44:26",False,"2020/08/17, 21:44:26",432.0,6689024.0,0,How App Autoscaler Determines When to Scale:,0.0,0.0,1.0,0.0
Wavefront VMware,63456882,63327611,0,"2020/08/17, 21:44:26",False,"2020/08/17, 21:44:26",432.0,6689024.0,0,"Every 35 seconds, App Autoscaler makes a decision about whether to
scale up, scale down, or keep the same number of instances.",0.0,0.061,0.939,0.0772
Wavefront VMware,63456882,63327611,0,"2020/08/17, 21:44:26",False,"2020/08/17, 21:44:26",432.0,6689024.0,0,"To make a scaling decision, App Autoscaler averages the values of a
given metric for the most recent 120 seconds.",0.0,0.137,0.863,0.4019
Wavefront VMware,63456882,63327611,0,"2020/08/17, 21:44:26",False,"2020/08/17, 21:44:26",432.0,6689024.0,0,The following diagram provides an example of how App Autoscaler makes scaling decisions:,0.0,0.0,1.0,0.0
Wavefront VMware,63456882,63327611,0,"2020/08/17, 21:44:26",False,"2020/08/17, 21:44:26",432.0,6689024.0,0,"Reference: 
 VMWare Tanzu App Autoscaler documentation",0.0,0.0,1.0,0.0
Wavefront VMware,63456882,63327611,0,"2020/08/17, 21:44:26",False,"2020/08/17, 21:44:26",432.0,6689024.0,0,VMWare Tanzu is the former Pivotal Cloud Foundry (PCF).,0.0,0.0,1.0,0.0
Wavefront VMware,67134760,63327611,0,"2021/04/17, 07:49:29",False,"2021/04/17, 07:49:29",11.0,15668534.0,1,I have the same question and as far as I understood from  App Container Lifecycle  it’s up to your app to gracefully shutdown but that might not be possible in given 10 seconds as some processes might take longer.,0.0,0.058,0.942,0.296
Wavefront VMware,67134760,63327611,0,"2021/04/17, 07:49:29",False,"2021/04/17, 07:49:29",11.0,15668534.0,1,"Shutdown 
CF requests a shutdown of your app instance in the following scenarios:
When a user runs  cf scale , cf stop, cf push, cf delete, or cf restart-app-instance
As a result of a system event, such as the replacement procedure during Diego Cell evacuation or when an app instance stops because of a failed health check probe
To shut down the app, CF sends the app process in the container a SIGTERM.",0.101,0.0,0.899,-0.7269
Wavefront VMware,67134760,63327611,0,"2021/04/17, 07:49:29",False,"2021/04/17, 07:49:29",11.0,15668534.0,1,"By default, the process has ten seconds to shut down gracefully.",0.0,0.254,0.746,0.5267
Wavefront VMware,67134760,63327611,0,"2021/04/17, 07:49:29",False,"2021/04/17, 07:49:29",11.0,15668534.0,1,"If the process has not exited after ten seconds, CF sends a SIGKILL.",0.0,0.0,1.0,0.0
Wavefront VMware,67134760,63327611,0,"2021/04/17, 07:49:29",False,"2021/04/17, 07:49:29",11.0,15668534.0,1,"By default, apps must finish their in-flight jobs within ten seconds of receiving the SIGTERM before CF terminates the app with a SIGKILL.",0.0,0.0,1.0,0.0
Wavefront VMware,67134760,63327611,0,"2021/04/17, 07:49:29",False,"2021/04/17, 07:49:29",11.0,15668534.0,1,"For instance, a web app must finish processing existing requests  and stop accepting new requests.",0.131,0.155,0.714,0.1027
Wavefront VMware,67134760,63327611,0,"2021/04/17, 07:49:29",False,"2021/04/17, 07:49:29",11.0,15668534.0,1,Note: One exception to the cases mentioned above is when monit restarts a crashed Diego Cell rep or Garden server.,0.0,0.0,1.0,0.0
Wavefront VMware,67134760,63327611,0,"2021/04/17, 07:49:29",False,"2021/04/17, 07:49:29",11.0,15668534.0,1,"In this case, CF immediately stops the apps that are still running using SIGKILL.",0.11,0.0,0.89,-0.1531
Wavefront VMware,59697948,59047752,0,"2020/01/11, 21:31:36",False,"2020/01/11, 21:31:36",334.0,1147487.0,0,I think there's a workaround for kubernetes versions prior to 1.17.,0.0,0.0,1.0,0.0
Wavefront VMware,59697948,59047752,0,"2020/01/11, 21:31:36",False,"2020/01/11, 21:31:36",334.0,1147487.0,0,On  kubernetes version v1.16  you can run Sonobuoy (Sonobuoy version v0.16.1 or higher) with providing the test framework flag:  --allowed-not-ready-nodes=1,0.0,0.0,1.0,0.0
Wavefront VMware,59697948,59047752,0,"2020/01/11, 21:31:36",False,"2020/01/11, 21:31:36",334.0,1147487.0,0,And on  kubernetes version prior to v1.16  it was more complicated.,0.0,0.0,1.0,0.0
Wavefront VMware,59697948,59047752,0,"2020/01/11, 21:31:36",False,"2020/01/11, 21:31:36",334.0,1147487.0,0,I haven't tested this but according to docs:,0.0,0.0,1.0,0.0
Wavefront VMware,64468853,64453467,1,"2020/10/21, 20:36:57",False,"2020/10/21, 20:36:57",1170.0,4462517.0,1,Pivotal Web Services is not the same as Pivotal Cloud Foundry.,0.0,0.0,1.0,0.0
Wavefront VMware,64468853,64453467,1,"2020/10/21, 20:36:57",False,"2020/10/21, 20:36:57",1170.0,4462517.0,1,"Pivotal Web Services has been sunset, yes.",0.0,0.31,0.69,0.4019
Wavefront VMware,64468853,64453467,1,"2020/10/21, 20:36:57",False,"2020/10/21, 20:36:57",1170.0,4462517.0,1,"Tanzu Application Service is VMware's enterprise solution that is, if you want to think about it this way, a self-hosted Pivotal Web Services (this is a gross understatement, but works for this situation).",0.062,0.085,0.852,-0.0644
Wavefront VMware,64468853,64453467,1,"2020/10/21, 20:36:57",False,"2020/10/21, 20:36:57",1170.0,4462517.0,1,Are you looking to test Cloud Foundry for its suitability?,0.0,0.0,1.0,0.0
Wavefront VMware,65097195,64327529,0,"2020/12/01, 21:27:18",False,"2020/12/01, 21:27:18",43.0,13300061.0,0,Add the AzureIdentity and AzureIdentityBinding roles to cluster as mentioned in docs.,0.0,0.0,1.0,0.0
Wavefront VMware,65097195,64327529,0,"2020/12/01, 21:27:18",False,"2020/12/01, 21:27:18",43.0,13300061.0,0,"once done, use the selector defined AzureIdentityBinding as label while deploying helm chart.",0.0,0.0,1.0,0.0
Wavefront VMware,65097195,64327529,0,"2020/12/01, 21:27:18",False,"2020/12/01, 21:27:18",43.0,13300061.0,0,Check for the actual syntax for podLabels using with --set in helm install command.,0.0,0.0,1.0,0.0
Wavefront VMware,65097195,64327529,0,"2020/12/01, 21:27:18",False,"2020/12/01, 21:27:18",43.0,13300061.0,0,Or you can clone the charts and make changes to values.yaml below and install it from local charts.,0.0,0.0,1.0,0.0
Wavefront VMware,65097195,64327529,0,"2020/12/01, 21:27:18",False,"2020/12/01, 21:27:18",43.0,13300061.0,0,https://github.com/vmware-tanzu/helm-charts/blob/04615803a7d976ce15a6eeb4b1bd6a1cfb9a02c5/charts/velero/values.yaml#L27,0.0,0.0,1.0,0.0
Wavefront VMware,65097195,64327529,0,"2020/12/01, 21:27:18",False,"2020/12/01, 21:27:18",43.0,13300061.0,0,"Just for help:
 https://medium.com/@kimvisscher/using-aad-pod-identity-in-an-aks-cluster-117c08565692",0.0,0.474,0.526,0.4019
Wavefront VMware,65372173,64327529,0,"2020/12/19, 18:50:18",False,"2020/12/19, 18:50:18",73.0,3389881.0,0,It seems that you are struggling with how to format the  secretContents  section of the values.yaml file.,0.149,0.0,0.851,-0.4215
Wavefront VMware,65372173,64327529,0,"2020/12/19, 18:50:18",False,"2020/12/19, 18:50:18",73.0,3389881.0,0,"If that is so, take a look at a recent update to the documentation.",0.0,0.0,1.0,0.0
Wavefront VMware,65372173,64327529,0,"2020/12/19, 18:50:18",False,"2020/12/19, 18:50:18",73.0,3389881.0,0,It lays out and documents exactly how to format it:,0.0,0.0,1.0,0.0
Wavefront VMware,65372173,64327529,0,"2020/12/19, 18:50:18",False,"2020/12/19, 18:50:18",73.0,3389881.0,0,https://github.com/vmware-tanzu/helm-charts/blob/1ea07c7fb9c7ba910ba52801c536a6f8dcee096d/charts/velero/values.yaml#L219-L232,0.0,0.0,1.0,0.0
Zipkin,47672314,47670883,2,"2017/12/06, 12:42:50",True,"2017/12/06, 12:42:50",3566.0,1358676.0,11,I found that I need to add a sampler percentage.,0.0,0.0,1.0,0.0
Zipkin,47672314,47670883,2,"2017/12/06, 12:42:50",True,"2017/12/06, 12:42:50",3566.0,1358676.0,11,By default zero percentage of the samples are sent and that is why the sleuth was not sending anything to zipkin.,0.0,0.0,1.0,0.0
Zipkin,47672314,47670883,2,"2017/12/06, 12:42:50",True,"2017/12/06, 12:42:50",3566.0,1358676.0,11,"when I added  spring.sleuth.sampler.percentage=1.0  in the properties files, it started working.",0.0,0.0,1.0,0.0
Zipkin,49838749,47670883,0,"2018/04/15, 08:20:04",False,"2018/04/15, 08:20:04",201.0,7956609.0,2,"If you are exporting all the span data to Zipkin, sampler can be installed  by creating a bean definition in the Spring boot main class",0.0,0.087,0.913,0.296
Zipkin,54384025,47670883,0,"2019/01/27, 02:18:35",False,"2019/01/27, 02:18:35",339.0,1405291.0,9,"For the latest version of cloud dependencies  &lt;version&gt;Finchley.SR2&lt;/version&gt; 
The correct property to send traces to zipkin is:  spring.sleuth.sampler.probability=1.0 
Which has changed from percentage to probability.",0.0,0.0,1.0,0.0
Zipkin,42580765,34272334,0,"2017/03/03, 15:55:09",False,"2017/03/03, 15:55:09",1247.0,1232692.0,1,"It is a very long time ago, but it looks like it was moved here:",0.0,0.2,0.8,0.5023
Zipkin,42580765,34272334,0,"2017/03/03, 15:55:09",False,"2017/03/03, 15:55:09",1247.0,1232692.0,1,http://zipkin.io/pages/quickstart,0.0,0.0,1.0,0.0
Zipkin,57852082,34272334,0,"2019/09/09, 13:21:48",False,"2019/09/09, 13:21:48",3190.0,2987755.0,0,Found multiple language examples at  github .,0.0,0.0,1.0,0.0
Zipkin,57852082,34272334,0,"2019/09/09, 13:21:48",False,"2019/09/09, 13:21:48",3190.0,2987755.0,0,"If you need basic setup steps:  https://zipkin.io/ 
Integrated zipkin with spring boot 2 and mysql 
 Steps 
 example 
Here is sample",0.0,0.0,1.0,0.0
Zipkin,49724310,45046528,0,"2018/04/09, 04:06:02",False,"2018/04/09, 04:06:02",765.0,6466540.0,0,Lately I have been trying the same and couldn't find that option in initializer.,0.0,0.0,1.0,0.0
Zipkin,49724310,45046528,0,"2018/04/09, 04:06:02",False,"2018/04/09, 04:06:02",765.0,6466540.0,0,I am just posting this if anyone encounters the same issues and lands on this page.,0.0,0.0,1.0,0.0
Zipkin,49724310,45046528,0,"2018/04/09, 04:06:02",False,"2018/04/09, 04:06:02",765.0,6466540.0,0,"You can refer below sample GitHub project which is consists of four micro services ( zipkin server, client, rest service, and Eureka ) using Edgware release with latest version of sleuth.",0.0,0.0,1.0,0.0
Zipkin,49724310,45046528,0,"2018/04/09, 04:06:02",False,"2018/04/09, 04:06:02",765.0,6466540.0,0, Sample Zipkin Server/Client,0.0,0.0,1.0,0.0
Zipkin,50127150,45046528,0,"2018/05/02, 07:24:17",False,"2018/05/02, 07:24:17",1767.0,2498986.0,3,Zipkin Server is not part of Spring initializers.,0.0,0.0,1.0,0.0
Zipkin,50127150,45046528,0,"2018/05/02, 07:24:17",False,"2018/05/02, 07:24:17",1767.0,2498986.0,3,You have to use the official release of the Zipkin server,0.0,0.0,1.0,0.0
Zipkin,50127150,45046528,0,"2018/05/02, 07:24:17",False,"2018/05/02, 07:24:17",1767.0,2498986.0,3,https://github.com/openzipkin/zipkin#quick-start,0.0,0.0,1.0,0.0
Zipkin,50127150,45046528,0,"2018/05/02, 07:24:17",False,"2018/05/02, 07:24:17",1767.0,2498986.0,3,And custom servers are not supported anymore meaning you can't use  @EnableZipkinServer  anymore since 2.7,0.123,0.0,0.877,-0.2411
Zipkin,50127150,45046528,0,"2018/05/02, 07:24:17",False,"2018/05/02, 07:24:17",1767.0,2498986.0,3,https://github.com/openzipkin/zipkin#quick-start,0.0,0.0,1.0,0.0
Zipkin,42403962,42402849,1,"2017/02/23, 01:04:02",False,"2017/02/23, 01:04:02",84586.0,1384297.0,3,"I can't explain the error that you got with Dalston.BUILD-SNAPSHOT, but the error with Camden.SR4 is because it's not compatible with Spring Boot 1.5.",0.0,0.177,0.823,0.5448
Zipkin,42403962,42402849,1,"2017/02/23, 01:04:02",False,"2017/02/23, 01:04:02",84586.0,1384297.0,3,I'd recommend upgrading to Camden.SR5  which is compatible with Spring Boot 1.5 .,0.0,0.185,0.815,0.3612
Zipkin,47165778,42402849,0,"2017/11/07, 21:14:23",False,"2017/11/07, 21:14:23",349.0,961275.0,1,Even I got this error while setting up my project.,0.281,0.0,0.719,-0.481
Zipkin,47165778,42402849,0,"2017/11/07, 21:14:23",False,"2017/11/07, 21:14:23",349.0,961275.0,1,I was using Spring boot 1.5.8 with the Brixton.SR6 release.,0.0,0.0,1.0,0.0
Zipkin,47165778,42402849,0,"2017/11/07, 21:14:23",False,"2017/11/07, 21:14:23",349.0,961275.0,1,"However, when I consulted the site  http://projects.spring.io/spring-cloud/  I got to know the issue and I updated my dependency to Dalston.SR4 and then the application started working.",0.0,0.0,1.0,0.0
Zipkin,39174580,39174579,2,"2016/08/27, 00:02:27",True,"2016/08/27, 00:02:27",473.0,2733462.0,8,"The official documentation was helpful, but I think it didn't include all the dependencies explicitly (at least as of now).",0.0,0.095,0.905,0.2263
Zipkin,39174580,39174579,2,"2016/08/27, 00:02:27",True,"2016/08/27, 00:02:27",473.0,2733462.0,8,I had to do some extra research for samples to get all the required dependencies and configuration together.,0.0,0.0,1.0,0.0
Zipkin,39174580,39174579,2,"2016/08/27, 00:02:27",True,"2016/08/27, 00:02:27",473.0,2733462.0,8,"I wanted to share it, because I believe it could be helpful for someone else.",0.0,0.312,0.688,0.6124
Zipkin,39174580,39174579,2,"2016/08/27, 00:02:27",True,"2016/08/27, 00:02:27",473.0,2733462.0,8,Spring Boot version:   1.4.0.RELEASE,0.0,0.0,1.0,0.0
Zipkin,39174580,39174579,2,"2016/08/27, 00:02:27",True,"2016/08/27, 00:02:27",473.0,2733462.0,8,Spring Cloud version:   Brixton.SR4,0.0,0.0,1.0,0.0
Zipkin,39174580,39174579,2,"2016/08/27, 00:02:27",True,"2016/08/27, 00:02:27",473.0,2733462.0,8,POM:,0.0,0.0,1.0,0.0
Zipkin,39174580,39174579,2,"2016/08/27, 00:02:27",True,"2016/08/27, 00:02:27",473.0,2733462.0,8,Java:,0.0,0.0,1.0,0.0
Zipkin,39174580,39174579,2,"2016/08/27, 00:02:27",True,"2016/08/27, 00:02:27",473.0,2733462.0,8,application.yml:,0.0,0.0,1.0,0.0
Zipkin,39174580,39174579,2,"2016/08/27, 00:02:27",True,"2016/08/27, 00:02:27",473.0,2733462.0,8,References:,0.0,0.0,1.0,0.0
Zipkin,39174580,39174579,2,"2016/08/27, 00:02:27",True,"2016/08/27, 00:02:27",473.0,2733462.0,8,https://cloud.spring.io/spring-cloud-sleuth/,0.0,0.0,1.0,0.0
Zipkin,63987464,60023762,0,"2020/09/21, 09:54:36",False,"2021/03/27, 13:21:49",1.0,13026705.0,0,"Your application starts in Tomcat Server but Zipkin use another server but i dont know that name , i include this code to spring boot starter web dependecy for ignore tomcat server",0.104,0.0,0.896,-0.5023
Zipkin,63987464,60023762,0,"2020/09/21, 09:54:36",False,"2021/03/27, 13:21:49",1.0,13026705.0,0,This worked for me try it,0.0,0.0,1.0,0.0
Zipkin,59544246,59444729,0,"2019/12/31, 14:36:17",True,"2019/12/31, 19:46:58",3932.0,1371995.0,1,I have tested this with the official  opencensus-node  example at github.,0.0,0.0,1.0,0.0
Zipkin,59544246,59444729,0,"2019/12/31, 14:36:17",True,"2019/12/31, 19:46:58",3932.0,1371995.0,1,"Zipkin UI displays 2 spans with same name MyApplication(see image),
  but expected 2 different span names",0.0,0.0,1.0,0.0
Zipkin,59544246,59444729,0,"2019/12/31, 14:36:17",True,"2019/12/31, 19:46:58",3932.0,1371995.0,1,"Just to be clear,  MyApplication  is the service name you set in your app.js, and the span names are those which you selected on the image  /service1 ,  /service1 ,  /external_service_2 .",0.0,0.085,0.915,0.3818
Zipkin,59544246,59444729,0,"2019/12/31, 14:36:17",True,"2019/12/31, 19:46:58",3932.0,1371995.0,1,"I think this is the intended behavior, you got one service ( MyApplication ), a root span ( /service1 ) and a child span ( /external_service_2 ).",0.0,0.0,1.0,0.0
Zipkin,59544246,59444729,0,"2019/12/31, 14:36:17",True,"2019/12/31, 19:46:58",3932.0,1371995.0,1,If you got multiple services connected to the same Zipkin server then you'll have multiple names for the services.,0.0,0.0,1.0,0.0
Zipkin,59544246,59444729,0,"2019/12/31, 14:36:17",True,"2019/12/31, 19:46:58",3932.0,1371995.0,1,From the Zipkin's  documentation :,0.0,0.0,1.0,0.0
Zipkin,59544246,59444729,0,"2019/12/31, 14:36:17",True,"2019/12/31, 19:46:58",3932.0,1371995.0,1,Span,0.0,0.0,1.0,0.0
Zipkin,59544246,59444729,0,"2019/12/31, 14:36:17",True,"2019/12/31, 19:46:58",3932.0,1371995.0,1,A set of Annotations and BinaryAnnotations that correspond to a particular RPC.,0.0,0.0,1.0,0.0
Zipkin,59544246,59444729,0,"2019/12/31, 14:36:17",True,"2019/12/31, 19:46:58",3932.0,1371995.0,1,"Spans contain identifying information such as traceId, spanId, parentId, and RPC name.",0.0,0.0,1.0,0.0
Zipkin,59544246,59444729,0,"2019/12/31, 14:36:17",True,"2019/12/31, 19:46:58",3932.0,1371995.0,1,Trace,0.0,0.0,1.0,0.0
Zipkin,59544246,59444729,0,"2019/12/31, 14:36:17",True,"2019/12/31, 19:46:58",3932.0,1371995.0,1,A set of spans that share a single root span.,0.0,0.239,0.761,0.296
Zipkin,59544246,59444729,0,"2019/12/31, 14:36:17",True,"2019/12/31, 19:46:58",3932.0,1371995.0,1,Traces are built by collecting all Spans that share a traceId.,0.0,0.196,0.804,0.296
Zipkin,59544246,59444729,0,"2019/12/31, 14:36:17",True,"2019/12/31, 19:46:58",3932.0,1371995.0,1,The spans are then arranged in a tree based on spanId and parentId thus providing an overview of the path a request takes through the system.,0.0,0.0,1.0,0.0
Zipkin,59544246,59444729,0,"2019/12/31, 14:36:17",True,"2019/12/31, 19:46:58",3932.0,1371995.0,1,"As far Zipkin UI displays 2 spans with same name, service dependencies
  page contains one Service only(see image)",0.0,0.0,1.0,0.0
Zipkin,59544246,59444729,0,"2019/12/31, 14:36:17",True,"2019/12/31, 19:46:58",3932.0,1371995.0,1,"Again, this is the intended behavior, since you got only one service and the external request you made goes through it.",0.0,0.0,1.0,0.0
Zipkin,59544246,59444729,0,"2019/12/31, 14:36:17",True,"2019/12/31, 19:46:58",3932.0,1371995.0,1,"If you mean the framed names on the first image, at the top it shows only the root span you clicked on the previous screen.",0.0,0.07,0.93,0.2023
Zipkin,59544246,59444729,0,"2019/12/31, 14:36:17",True,"2019/12/31, 19:46:58",3932.0,1371995.0,1,"However, you can write custom span names after a little change in your code.",0.0,0.0,1.0,0.0
Zipkin,59544246,59444729,0,"2019/12/31, 14:36:17",True,"2019/12/31, 19:46:58",3932.0,1371995.0,1,From the  tracing documentation  (with your code):,0.0,0.0,1.0,0.0
Zipkin,59544246,59444729,0,"2019/12/31, 14:36:17",True,"2019/12/31, 19:46:58",3932.0,1371995.0,1,"Now you can use  tracer.startRootSpan , I used it in the express sample with a request:",0.0,0.0,1.0,0.0
Zipkin,59544246,59444729,0,"2019/12/31, 14:36:17",True,"2019/12/31, 19:46:58",3932.0,1371995.0,1,A span must be closed.,0.0,0.0,1.0,0.0
Zipkin,59544246,59444729,0,"2019/12/31, 14:36:17",True,"2019/12/31, 19:46:58",3932.0,1371995.0,1,"For more information, check the  test file  of the tracer.",0.0,0.0,1.0,0.0
Zipkin,38537987,35215821,0,"2016/07/23, 06:56:00",False,"2016/07/23, 06:56:00",762.0,1227937.0,0,The best way to ask for a feature is using github issues.,0.0,0.296,0.704,0.6369
Zipkin,38537987,35215821,0,"2016/07/23, 06:56:00",False,"2016/07/23, 06:56:00",762.0,1227937.0,0,"To add a new transport such as RabbitMQ, you'd have to affect Brave (reporter) and Zipkin (collector)",0.0,0.185,0.815,0.5267
Zipkin,38537987,35215821,0,"2016/07/23, 06:56:00",False,"2016/07/23, 06:56:00",762.0,1227937.0,0,"https://github.com/openzipkin/zipkin/issues 
 https://github.com/openzipkin/brave/issues",0.0,0.0,1.0,0.0
Zipkin,16981952,16981751,0,"2013/06/07, 13:29:08",True,"2018/10/17, 13:15:26",18596.0,69875.0,1,"It's not suitable, Zipkin is about tracing in distributed systems.",0.0,0.0,1.0,0.0
Zipkin,16981952,16981751,0,"2013/06/07, 13:29:08",True,"2018/10/17, 13:15:26",18596.0,69875.0,1,"I would say you would want something like a profiler, such as  Visual VM ,  - free and included with the JDK, or  YourKit .",0.0,0.295,0.705,0.7269
Zipkin,16981952,16981751,0,"2013/06/07, 13:29:08",True,"2018/10/17, 13:15:26",18596.0,69875.0,1,Other profilers are available.,0.0,0.0,1.0,0.0
Zipkin,18345991,16981751,0,"2013/08/21, 01:31:07",False,"2013/08/21, 01:31:07",60071.0,298389.0,1,"No, it  is not suitable at all .",0.268,0.0,0.732,-0.296
Zipkin,18345991,16981751,0,"2013/08/21, 01:31:07",False,"2013/08/21, 01:31:07",60071.0,298389.0,1,Why?,0.0,0.0,1.0,0.0
Zipkin,18345991,16981751,0,"2013/08/21, 01:31:07",False,"2013/08/21, 01:31:07",60071.0,298389.0,1,"Because the architecture of Zipkin have multiple levels of indirection: you have to send your spans into collector service and even if collector is running on your own machine there is a huge overhead -- imagine you are traveling from two neighbour cities somewhere in Europe and somebody proposes you instead of going directly from A to B, fly from A to New York, USA and back to B.",0.0,0.034,0.966,0.3182
Zipkin,18345991,16981751,0,"2013/08/21, 01:31:07",False,"2013/08/21, 01:31:07",60071.0,298389.0,1,That is simply ridiculous.,0.455,0.0,0.545,-0.3612
Zipkin,18345991,16981751,0,"2013/08/21, 01:31:07",False,"2013/08/21, 01:31:07",60071.0,298389.0,1,As for the tools that may suit your needs: VisualVM and Yourkit mentioned by @Jonathan are good for looking at average situation in your program -- if you need to carefully inspect low level paths in your program  InTrace  might be a better choice.,0.043,0.151,0.806,0.6369
Zipkin,31348098,16981751,1,"2015/07/10, 21:43:33",False,"2015/07/10, 21:43:33",17932.0,433344.0,1,Zipkin is foremost intended to provide observability into a complex distributed network of services (aka Microservice Architecture).,0.0,0.0,1.0,0.0
Zipkin,31348098,16981751,1,"2015/07/10, 21:43:33",False,"2015/07/10, 21:43:33",17932.0,433344.0,1,It wasn't intended to support just a single application.,0.244,0.0,0.756,-0.3089
Zipkin,31348098,16981751,1,"2015/07/10, 21:43:33",False,"2015/07/10, 21:43:33",17932.0,433344.0,1,"So a profiler can often be a better way to go, particularly if you have an urgent issue to diagnose.",0.0,0.227,0.773,0.5719
Zipkin,31348098,16981751,1,"2015/07/10, 21:43:33",False,"2015/07/10, 21:43:33",17932.0,433344.0,1,"That said, most profilers will only provide realtime data, where Zipkin persists, allowing for analysis over large datasets.",0.0,0.0,1.0,0.0
Zipkin,31348098,16981751,1,"2015/07/10, 21:43:33",False,"2015/07/10, 21:43:33",17932.0,433344.0,1,"If Zipkin is already in your stack, it's not a bad idea to enrich higher level Zipkin spans (ie a complete REST requests) with specific method calls as child-spans, particularly those that do I/O.",0.0,0.084,0.916,0.431
Zipkin,31348098,16981751,1,"2015/07/10, 21:43:33",False,"2015/07/10, 21:43:33",17932.0,433344.0,1,This way you can drill down within Zipkin to more quickly determine bottlenecks.,0.0,0.0,1.0,0.0
Zipkin,31348098,16981751,1,"2015/07/10, 21:43:33",False,"2015/07/10, 21:43:33",17932.0,433344.0,1,"Otherwise you'd have to first look at the Zipkin span for high-level bottlenecks, then separately look at other logs or run a profiler, which is inefficient.",0.0,0.0,1.0,0.0
Zipkin,31348098,16981751,1,"2015/07/10, 21:43:33",False,"2015/07/10, 21:43:33",17932.0,433344.0,1,** If it's already in your stack **,0.0,0.0,1.0,0.0
Zipkin,54816556,54262815,1,"2019/02/21, 23:32:10",True,"2019/02/21, 23:32:10",106.0,3956619.0,3,It's hard to tell without more information.,0.189,0.0,0.811,-0.1027
Zipkin,54816556,54262815,1,"2019/02/21, 23:32:10",True,"2019/02/21, 23:32:10",106.0,3956619.0,3,But it can be related to  incompatible libraries .,0.0,0.0,1.0,0.0
Zipkin,54816556,54262815,1,"2019/02/21, 23:32:10",True,"2019/02/21, 23:32:10",106.0,3956619.0,3,Can you post your dependencies?,0.0,0.0,1.0,0.0
Zipkin,54816556,54262815,1,"2019/02/21, 23:32:10",True,"2019/02/21, 23:32:10",106.0,3956619.0,3,In case you are using  older version  of okhttpclient with  latest  spring cloud:greenwich it can cause this issue.,0.0,0.0,1.0,0.0
Zipkin,54816556,54262815,1,"2019/02/21, 23:32:10",True,"2019/02/21, 23:32:10",106.0,3956619.0,3,I'm using  Greenwich.RELEASE  with  okhttpclient:10.2.0  which works without problems,0.0,0.22,0.78,0.3089
Zipkin,59579899,54262815,0,"2020/01/03, 16:02:23",False,"2020/01/03, 16:02:23",11.0,12647380.0,1,Use the below dependency Management for spring-boot to download the suitable versions for cloud version,0.0,0.0,1.0,0.0
Zipkin,59579899,54262815,0,"2020/01/03, 16:02:23",False,"2020/01/03, 16:02:23",11.0,12647380.0,1,"I am using Java 10, cloud.version is  Finchley.SR2  and sprinb-boot:2.2.0 and spring-cloud-starter-openfeign :2.1.2.RELEASE.",0.0,0.0,1.0,0.0
Zipkin,59579899,54262815,0,"2020/01/03, 16:02:23",False,"2020/01/03, 16:02:23",11.0,12647380.0,1,and this combination worked for me to fix the issue.,0.0,0.0,1.0,0.0
Zipkin,59579899,54262815,0,"2020/01/03, 16:02:23",False,"2020/01/03, 16:02:23",11.0,12647380.0,1,Acctual problem was 10.x.x feign-core  was not working only  and io.github.openfeign:feign-core:jar:9.7.0:compile was working.,0.184,0.0,0.816,-0.4019
Zipkin,62180529,54262815,0,"2020/06/03, 21:51:55",False,"2020/06/03, 21:51:55",116.0,10783374.0,1,"I faced this problem using java 11, springboot 2.3.0.RELEASE, and spring-cloud version Greenwich.RELEASE.",0.197,0.0,0.803,-0.4019
Zipkin,62180529,54262815,0,"2020/06/03, 21:51:55",False,"2020/06/03, 21:51:55",116.0,10783374.0,1,Adding the following dependences saved me:,0.0,0.359,0.641,0.4215
Zipkin,62180529,54262815,0,"2020/06/03, 21:51:55",False,"2020/06/03, 21:51:55",116.0,10783374.0,1,Hope this helps someone.,0.0,0.733,0.267,0.6705
Zipkin,49450864,43025795,0,"2018/03/23, 15:33:24",False,"2018/03/23, 15:33:24",485.0,1287376.0,1,It seems to be a timing issue.,0.0,0.0,1.0,0.0
Zipkin,49450864,43025795,0,"2018/03/23, 15:33:24",False,"2018/03/23, 15:33:24",485.0,1287376.0,1,"If we add some delay, for instance, between children spans execution like",0.155,0.169,0.676,0.0516
Zipkin,49450864,43025795,0,"2018/03/23, 15:33:24",False,"2018/03/23, 15:33:24",485.0,1287376.0,1,In between,0.0,0.0,1.0,0.0
Zipkin,49450864,43025795,0,"2018/03/23, 15:33:24",False,"2018/03/23, 15:33:24",485.0,1287376.0,1,Then we get to see spans:,0.0,0.0,1.0,0.0
Zipkin,49450864,43025795,0,"2018/03/23, 15:33:24",False,"2018/03/23, 15:33:24",485.0,1287376.0,1,I've faced something like this before when Zipkin dropped spans I was (mistakenly) assigning wrong timestamps to.,0.158,0.128,0.714,-0.1531
Zipkin,49450864,43025795,0,"2018/03/23, 15:33:24",False,"2018/03/23, 15:33:24",485.0,1287376.0,1,"For reference and ease of reproduction: I've setup a  project  for reproducing this issue / ""fix"".",0.0,0.161,0.839,0.3612
Zipkin,41628945,41508180,0,"2017/01/13, 08:52:38",True,"2017/01/13, 08:52:38",762.0,1227937.0,5,"In zipkin lingo, what you are asking about is often called ""local spans"" or ""local tracing"", basically an operation that neither originated, nor resulted in a remote call.",0.0,0.0,1.0,0.0
Zipkin,41628945,41508180,0,"2017/01/13, 08:52:38",True,"2017/01/13, 08:52:38",762.0,1227937.0,5,"I'm not aware of anything at the syscall level, but many tracers support explicit instrumentation of function calls.",0.0,0.173,0.827,0.5499
Zipkin,41628945,41508180,0,"2017/01/13, 08:52:38",True,"2017/01/13, 08:52:38",762.0,1227937.0,5,"For example, using  py_zipkin 
 
@zipkin_span(service_name='my_service', span_name='some_function')
def some_function(a, b):
    return do_stuff(a, b)",0.0,0.0,1.0,0.0
Zipkin,41628945,41508180,0,"2017/01/13, 08:52:38",True,"2017/01/13, 08:52:38",762.0,1227937.0,5,"Besides explicit instrumentation like this, one could also export data to zipkin.",0.0,0.185,0.815,0.3612
Zipkin,41628945,41508180,0,"2017/01/13, 08:52:38",True,"2017/01/13, 08:52:38",762.0,1227937.0,5,"For example, one could convert trace data that is made in another tool to  zipkin's json format .",0.0,0.0,1.0,0.0
Zipkin,41628945,41508180,0,"2017/01/13, 08:52:38",True,"2017/01/13, 08:52:38",762.0,1227937.0,5,"This probably doesn't answer your question, but I hope it helps.",0.0,0.475,0.525,0.8047
Zipkin,65881898,65873809,0,"2021/01/25, 11:19:25",False,"2021/01/25, 11:19:25",4007.0,4255107.0,0,The easiest way to get it working is to use the Micrometer library and configure the micrometer to send this data to the Zipkin server.,0.0,0.104,0.896,0.4215
Zipkin,65881898,65873809,0,"2021/01/25, 11:19:25",False,"2021/01/25, 11:19:25",4007.0,4255107.0,0,"Enabling metrics using micrometer is very simple, you need to just add a micrometer-core and spring-cloud-starter-zipkin libraries.",0.0,0.0,1.0,0.0
Zipkin,65881898,65873809,0,"2021/01/25, 11:19:25",False,"2021/01/25, 11:19:25",4007.0,4255107.0,0,See this tutorial for details about configuration and code  https://www.baeldung.com/tracing-services-with-zipkin,0.0,0.0,1.0,0.0
Zipkin,65881898,65873809,0,"2021/01/25, 11:19:25",False,"2021/01/25, 11:19:25",4007.0,4255107.0,0,Micrometer will report consumer/producer metrics to Zipkin,0.0,0.0,1.0,0.0
Zipkin,64096381,64094479,0,"2020/09/28, 08:27:01",True,"2020/09/28, 08:27:01",8336.0,1773866.0,2,If you search for zipkin grafana you'll get this as one of the first results  https://grafana.com/docs/grafana/latest/features/datasources/zipkin/,0.0,0.0,1.0,0.0
Zipkin,61609752,61570149,1,"2020/05/05, 12:19:05",False,"2020/05/05, 12:19:05",2015.0,1398228.0,0,You need to reload after adding the subsystem:,0.0,0.0,1.0,0.0
Zipkin,61636753,61570149,6,"2020/05/06, 16:43:39",False,"2020/05/06, 16:43:39",2015.0,1398228.0,1,This jboss-cli script should enable opentracing before starting the server properly.,0.0,0.0,1.0,0.0
Zipkin,61636753,61570149,6,"2020/05/06, 16:43:39",False,"2020/05/06, 16:43:39",2015.0,1398228.0,1,I'm not sure how/when you can execute that with keycloack image,0.164,0.0,0.836,-0.2411
Zipkin,53209830,53208598,0,"2018/11/08, 16:31:04",False,"2018/11/08, 16:31:04",129048.0,1240763.0,1,Connection refused: connect,0.524,0.0,0.476,-0.296
Zipkin,53209830,53208598,0,"2018/11/08, 16:31:04",False,"2018/11/08, 16:31:04",129048.0,1240763.0,1,"Simply means that RabbitMQ is not running on  localhost:5672  (which is the default if you don't provide a host/port, or addresses, for it in your  application.yml ).",0.0,0.0,1.0,0.0
Zipkin,52814922,52759855,2,"2018/10/15, 13:43:25",True,"2018/10/17, 09:29:03",201.0,7956609.0,2,"If service 2 is getting traceId from service 1, you can take the traceId from requestHeader in your java code.",0.0,0.0,1.0,0.0
Zipkin,52814922,52759855,2,"2018/10/15, 13:43:25",True,"2018/10/17, 09:29:03",201.0,7956609.0,2,Otherwise sleuth generate a new traceId in service 2.,0.0,0.0,1.0,0.0
Zipkin,52814922,52759855,2,"2018/10/15, 13:43:25",True,"2018/10/17, 09:29:03",201.0,7956609.0,2,To get the trace Id In java,0.0,0.0,1.0,0.0
Zipkin,52814922,52759855,2,"2018/10/15, 13:43:25",True,"2018/10/17, 09:29:03",201.0,7956609.0,2,Just do,0.0,0.0,1.0,0.0
Zipkin,59216676,52759855,0,"2019/12/06, 18:13:45",False,"2019/12/06, 18:13:45",859.0,4513218.0,1,"Hello you can also get the x-b3-traceid header information from the request, I created a Util class for that -   https://gist.github.com/walterwhites/067dd635986e564aafdb5ac559073b0f",0.0,0.105,0.895,0.25
Zipkin,59216676,52759855,0,"2019/12/06, 18:13:45",False,"2019/12/06, 18:13:45",859.0,4513218.0,1,Then in your main class you just need to call,0.0,0.0,1.0,0.0
Zipkin,52031368,52029459,0,"2018/08/27, 03:52:08",True,"2018/09/16, 13:54:14",391.0,6452043.0,3,"after many searches, i found that there are a version conflicts between the dependencies.",0.191,0.0,0.809,-0.3818
Zipkin,52031368,52029459,0,"2018/08/27, 03:52:08",True,"2018/09/16, 13:54:14",391.0,6452043.0,3,and thanks for  vladimir-vagaytsev,0.0,0.492,0.508,0.4404
Zipkin,52031368,52029459,0,"2018/08/27, 03:52:08",True,"2018/09/16, 13:54:14",391.0,6452043.0,3,"so, i see that  spring-cloud-starter-sleuth  imported as a different version.",0.0,0.0,1.0,0.0
Zipkin,52031368,52029459,0,"2018/08/27, 03:52:08",True,"2018/09/16, 13:54:14",391.0,6452043.0,3,to fix it i have added   sleuth.version  to properties  in pom.xml like so.,0.0,0.185,0.815,0.3612
Zipkin,52031368,52029459,0,"2018/08/27, 03:52:08",True,"2018/09/16, 13:54:14",391.0,6452043.0,3,then in dependency management we need to specify the version like so,0.0,0.185,0.815,0.3612
Zipkin,52031368,52029459,0,"2018/08/27, 03:52:08",True,"2018/09/16, 13:54:14",391.0,6452043.0,3,after then remove unused dependencies build and run.,0.0,0.0,1.0,0.0
Zipkin,52338780,52029459,0,"2018/09/14, 23:20:40",False,"2018/09/14, 23:20:40",373.0,1037492.0,-1,This class comes from zipkin-2.,0.0,0.0,1.0,0.0
Zipkin,52338780,52029459,0,"2018/09/14, 23:20:40",False,"2018/09/14, 23:20:40",373.0,1037492.0,-1,You can try adding this dependency.,0.0,0.0,1.0,0.0
Zipkin,51011690,50027127,0,"2018/06/24, 19:10:20",False,"2018/06/24, 22:07:28",3589.0,5587542.0,2,"Hello seeing your screenshot maybe you are using a spring boot 2.x version, I had the same problem with spring boot 2.0.3 with Finchley.RELEASE.",0.114,0.0,0.886,-0.4019
Zipkin,51011690,50027127,0,"2018/06/24, 19:10:20",False,"2018/06/24, 22:07:28",3589.0,5587542.0,2,"I found that Zipkin custom Server are not any more supported and deprecated for this reason it is not possible to use @EnableZipkinServer in Spring Cloud code and you have the ui but not the server side configured, api endpoint and so on.",0.037,0.0,0.963,-0.1505
Zipkin,51011690,50027127,0,"2018/06/24, 19:10:20",False,"2018/06/24, 22:07:28",3589.0,5587542.0,2,form the Zipkin base code:,0.0,0.0,1.0,0.0
Zipkin,51011690,50027127,0,"2018/06/24, 19:10:20",False,"2018/06/24, 22:07:28",3589.0,5587542.0,2,"the code is available on  official repo of Zipkin 
I solve the my problem using the official docker image with a compose",0.12,0.08,0.8,-0.2263
Zipkin,51011690,50027127,0,"2018/06/24, 19:10:20",False,"2018/06/24, 22:07:28",3589.0,5587542.0,2,How you can see i use the streaming version.,0.0,0.0,1.0,0.0
Zipkin,51011690,50027127,0,"2018/06/24, 19:10:20",False,"2018/06/24, 22:07:28",3589.0,5587542.0,2,it for me work,0.0,0.0,1.0,0.0
Zipkin,51011690,50027127,0,"2018/06/24, 19:10:20",False,"2018/06/24, 22:07:28",3589.0,5587542.0,2,I hope that is can help you,0.0,0.583,0.417,0.6808
Zipkin,54355078,50027127,0,"2019/01/24, 22:47:42",False,"2019/01/24, 22:47:42",21.0,9286335.0,0,try with this:,0.0,0.0,1.0,0.0
Zipkin,50877783,49676752,0,"2018/06/15, 17:39:40",False,"2018/06/15, 17:39:40",140.0,7980867.0,0,I believe you should be able to as long as you use the fully qualified domain name.,0.0,0.0,1.0,0.0
Zipkin,50877783,49676752,0,"2018/06/15, 17:39:40",False,"2018/06/15, 17:39:40",140.0,7980867.0,0,"For instance,  zipkin.mynamespace.svc.cluster.local .",0.0,0.0,1.0,0.0
Zipkin,49182914,49182626,0,"2018/03/08, 23:38:09",False,"2018/03/08, 23:38:09",9818.0,1490322.0,6,Ok we found the problem and also a work around.,0.227,0.185,0.588,-0.128
Zipkin,49182914,49182626,0,"2018/03/08, 23:38:09",False,"2018/03/08, 23:38:09",9818.0,1490322.0,6,"It looks like all the documentation out there is wrong, at least for the version of Spring Cloud Sleuth we are using.",0.121,0.098,0.781,-0.1531
Zipkin,49182914,49182626,0,"2018/03/08, 23:38:09",False,"2018/03/08, 23:38:09",9818.0,1490322.0,6,The correct property is not  spring.sleuth.sampler.percentage .,0.0,0.0,1.0,0.0
Zipkin,49182914,49182626,0,"2018/03/08, 23:38:09",False,"2018/03/08, 23:38:09",9818.0,1490322.0,6,The correct property is  spring.sleuth.sampler.probability,0.0,0.0,1.0,0.0
Zipkin,49182914,49182626,0,"2018/03/08, 23:38:09",False,"2018/03/08, 23:38:09",9818.0,1490322.0,6,And here is a workaround we found right before noticing that the property was wrong.,0.193,0.0,0.807,-0.4767
Zipkin,49182914,49182626,0,"2018/03/08, 23:38:09",False,"2018/03/08, 23:38:09",9818.0,1490322.0,6,Here are some official documentation from Spring Cloud that contain the wrong property.,0.205,0.0,0.795,-0.4767
Zipkin,49182914,49182626,0,"2018/03/08, 23:38:09",False,"2018/03/08, 23:38:09",9818.0,1490322.0,6,https://cloud.spring.io/spring-cloud-static/spring-cloud-sleuth/2.0.0.M5/single/spring-cloud-sleuth.html,0.0,0.0,1.0,0.0
Zipkin,49182914,49182626,0,"2018/03/08, 23:38:09",False,"2018/03/08, 23:38:09",9818.0,1490322.0,6,https://cloud.spring.io/spring-cloud-sleuth/single/spring-cloud-sleuth.html,0.0,0.0,1.0,0.0
Zipkin,49182914,49182626,0,"2018/03/08, 23:38:09",False,"2018/03/08, 23:38:09",9818.0,1490322.0,6,Here is the source code that is being used and it is using  probability  not  percentage .,0.0,0.0,1.0,0.0
Zipkin,49182914,49182626,0,"2018/03/08, 23:38:09",False,"2018/03/08, 23:38:09",9818.0,1490322.0,6,https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/sampler/SamplerProperties.java,0.0,0.0,1.0,0.0
Zipkin,47777632,47764295,0,"2017/12/12, 18:47:54",True,"2017/12/12, 18:47:54",81.0,2232476.0,1,"Creating custom zipkin servers is an unsupported configuration, but if you must all of the configuration options are documented in the project readme:  https://github.com/openzipkin/zipkin/blob/master/zipkin-server/README.md",0.073,0.063,0.864,-0.0644
Zipkin,50200855,47764295,0,"2018/05/06, 17:35:24",False,"2018/05/06, 17:35:24",69.0,2629308.0,1,"For the dependencies part, the most important one is  zipkin-autoconfigure-storage-elasticsearch-http , here's an full maven pom.xml example:",0.0,0.122,0.878,0.2716
Zipkin,50200855,47764295,0,"2018/05/06, 17:35:24",False,"2018/05/06, 17:35:24",69.0,2629308.0,1,"For the configuration part, you will need the following in you  application.yml :",0.0,0.0,1.0,0.0
Zipkin,64168840,47764295,0,"2020/10/02, 11:59:59",False,"2020/10/02, 11:59:59",511.0,2564032.0,0,I configured zipkin to use ES as a data storage on top of kubernetes.,0.0,0.141,0.859,0.2023
Zipkin,64168840,47764295,0,"2020/10/02, 11:59:59",False,"2020/10/02, 11:59:59",511.0,2564032.0,0,If it fits your requirement feel free to download and use  https://github.com/handysofts/zipkin-on-kubernetes,0.0,0.231,0.769,0.5106
Zipkin,47195615,47195614,4,"2017/11/09, 08:50:24",True,"2017/11/09, 08:50:24",3909.0,3067542.0,3,I found that these traces are actually generated by  https://github.com/spring-cloud/spring-cloud-consul/blob/master/spring-cloud-consul-discovery/src/main/java/org/springframework/cloud/consul/discovery/ConsulCatalogWatch.java,0.0,0.0,1.0,0.0
Zipkin,47195615,47195614,4,"2017/11/09, 08:50:24",True,"2017/11/09, 08:50:24",3909.0,3067542.0,3,"And since this is a class annotated with @scheduled , this Sleuth aspect applies :",0.0,0.0,1.0,0.0
Zipkin,47195615,47195614,4,"2017/11/09, 08:50:24",True,"2017/11/09, 08:50:24",3909.0,3067542.0,3,https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/instrument/scheduling/TraceSchedulingAspect.java,0.0,0.0,1.0,0.0
Zipkin,47195615,47195614,4,"2017/11/09, 08:50:24",True,"2017/11/09, 08:50:24",3909.0,3067542.0,3,"And therefore, the property to control the skipped regexp is not spring.sleuth.instrument.web.skipPattern , but spring.sleuth.instrument.",0.0,0.0,1.0,0.0
Zipkin,47195615,47195614,4,"2017/11/09, 08:50:24",True,"2017/11/09, 08:50:24",3909.0,3067542.0,3,scheduled .skip-pattern,0.0,0.0,1.0,0.0
Zipkin,42764225,42764069,1,"2017/03/13, 14:56:55",True,"2017/03/13, 17:25:56",8336.0,1773866.0,3,Of course - you have to just provide your own logging format (e.g.,0.0,0.0,1.0,0.0
Zipkin,42764225,42764069,1,"2017/03/13, 14:56:55",True,"2017/03/13, 17:25:56",8336.0,1773866.0,3,via   logging.pattern.level  - check  https://docs.spring.io/spring-boot/docs/current/reference/html/boot-features-logging.html  for more info).,0.0,0.0,1.0,0.0
Zipkin,42764225,42764069,1,"2017/03/13, 14:56:55",True,"2017/03/13, 17:25:56",8336.0,1773866.0,3,Then you have to register your own  SpanLogger  bean implementation where you will take care of adding the value of a spring profile via MDC  (you can take a look at this as an example  https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/log/Slf4jSpanLogger.java  ),0.0,0.149,0.851,0.6808
Zipkin,42764225,42764069,1,"2017/03/13, 14:56:55",True,"2017/03/13, 17:25:56",8336.0,1773866.0,3,UPDATE:,0.0,0.0,1.0,0.0
Zipkin,42764225,42764069,1,"2017/03/13, 14:56:55",True,"2017/03/13, 17:25:56",8336.0,1773866.0,3,There's another solution for more complex approach that seems much easier than rewriting Sleuth classes.,0.0,0.282,0.718,0.6249
Zipkin,42764225,42764069,1,"2017/03/13, 14:56:55",True,"2017/03/13, 17:25:56",8336.0,1773866.0,3,You can try the  logback-spring.xml  way like here -  https://github.com/spring-cloud-samples/sleuth-documentation-apps/blob/master/service1/src/main/resources/logback-spring.xml#L5-L11  .,0.0,0.238,0.762,0.3612
Zipkin,42764225,42764069,1,"2017/03/13, 14:56:55",True,"2017/03/13, 17:25:56",8336.0,1773866.0,3,I'm resolving the application name there so maybe you could do the same with active profiles and won't need to write any code?,0.0,0.202,0.798,0.6486
Zipkin,40861860,40860284,0,"2016/11/29, 11:14:50",True,"2016/11/29, 11:14:50",42504.0,936832.0,2,"As long as you have the ability to specify VM parameters, you can add the monitoring agent, regardless of the whether the JVM is started as a Windows service.",0.0,0.078,0.922,0.3182
Zipkin,40861860,40860284,0,"2016/11/29, 11:14:50",True,"2016/11/29, 11:14:50",42504.0,936832.0,2,"For perfino, that VM parameter is",0.0,0.0,1.0,0.0
Zipkin,43888530,40654863,0,"2017/05/10, 12:29:15",False,"2017/05/11, 04:27:33",130.0,1427954.0,3,please ensure config your zipkin sever correctly in your spring boot config file.,0.0,0.308,0.692,0.5994
Zipkin,43888530,40654863,0,"2017/05/10, 12:29:15",False,"2017/05/11, 04:27:33",130.0,1427954.0,3,just like this:,0.0,0.556,0.444,0.3612
Zipkin,43888530,40654863,0,"2017/05/10, 12:29:15",False,"2017/05/11, 04:27:33",130.0,1427954.0,3,And add below config in your zipkin client spring boot config file:,0.0,0.0,1.0,0.0
Zipkin,39597817,39597545,0,"2016/09/20, 18:15:25",True,"2016/09/20, 18:15:25",8336.0,1773866.0,2,We have a  LazyTraceExecutor  that you can use -  https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/instrument/async/LazyTraceExecutor.java  .,0.0,0.0,1.0,0.0
Zipkin,41629061,38738337,0,"2017/01/13, 09:02:26",False,"2017/01/13, 09:02:26",762.0,1227937.0,2,"There are a bunch of ways to answer this, but I'll answer it from the ""one-way"" perspective.",0.0,0.0,1.0,0.0
Zipkin,41629061,38738337,0,"2017/01/13, 09:02:26",False,"2017/01/13, 09:02:26",762.0,1227937.0,2,"The short answer though, is I think you have to roll your own right now!",0.0,0.0,1.0,0.0
Zipkin,41629061,38738337,0,"2017/01/13, 09:02:26",False,"2017/01/13, 09:02:26",762.0,1227937.0,2,"While Kafka can be used in many ways, it can be used as a transport for unidirectional single producer single consumer messages.",0.0,0.0,1.0,0.0
Zipkin,41629061,38738337,0,"2017/01/13, 09:02:26",False,"2017/01/13, 09:02:26",762.0,1227937.0,2,"This action is similar to normal one-way RPC, where you have a request, but no response.",0.167,0.0,0.833,-0.4215
Zipkin,41629061,38738337,0,"2017/01/13, 09:02:26",False,"2017/01/13, 09:02:26",762.0,1227937.0,2,"In Zipkin, an RPC span is usually request-response.",0.0,0.0,1.0,0.0
Zipkin,41629061,38738337,0,"2017/01/13, 09:02:26",False,"2017/01/13, 09:02:26",762.0,1227937.0,2,"For example, you see timing of the client sending to the server, and also the way back to the client.",0.0,0.0,1.0,0.0
Zipkin,41629061,38738337,0,"2017/01/13, 09:02:26",False,"2017/01/13, 09:02:26",762.0,1227937.0,2,One-way is where you leave out the other side.,0.13,0.0,0.87,-0.0516
Zipkin,41629061,38738337,0,"2017/01/13, 09:02:26",False,"2017/01/13, 09:02:26",762.0,1227937.0,2,"The span starts with a ""cs"" (client send) and ends with a ""sr"" (server received).",0.0,0.0,1.0,0.0
Zipkin,41629061,38738337,0,"2017/01/13, 09:02:26",False,"2017/01/13, 09:02:26",762.0,1227937.0,2,"Mapping this to Kafka, you would mark client sent when you produce the message and server received when the consumer receives it.",0.0,0.0,1.0,0.0
Zipkin,41629061,38738337,0,"2017/01/13, 09:02:26",False,"2017/01/13, 09:02:26",762.0,1227937.0,2,The trick to Kafka is that there is no nice place to stuff the trace context.,0.177,0.146,0.677,0.1027
Zipkin,41629061,38738337,0,"2017/01/13, 09:02:26",False,"2017/01/13, 09:02:26",762.0,1227937.0,2,"That's because unlike a lot of messaging systems, there are no headers in a Kafka message.",0.145,0.0,0.855,-0.296
Zipkin,41629061,38738337,0,"2017/01/13, 09:02:26",False,"2017/01/13, 09:02:26",762.0,1227937.0,2,"Without a trace context, you don't know which trace (or span for that matter) you are completing!",0.0,0.0,1.0,0.0
Zipkin,41629061,38738337,0,"2017/01/13, 09:02:26",False,"2017/01/13, 09:02:26",762.0,1227937.0,2,"The ""hack"" approach is to stuff trace identifiers as the message key.",0.0,0.0,1.0,0.0
Zipkin,41629061,38738337,0,"2017/01/13, 09:02:26",False,"2017/01/13, 09:02:26",762.0,1227937.0,2,A less hacky way would be to coordinate a body wrapper which you can nest the trace context into.,0.0,0.0,1.0,0.0
Zipkin,41629061,38738337,0,"2017/01/13, 09:02:26",False,"2017/01/13, 09:02:26",762.0,1227937.0,2,Here's an example of the former:,0.0,0.0,1.0,0.0
Zipkin,41629061,38738337,0,"2017/01/13, 09:02:26",False,"2017/01/13, 09:02:26",762.0,1227937.0,2,https://gist.github.com/adriancole/76d94054b77e3be338bd75424ca8ba30,0.0,0.0,1.0,0.0
Zipkin,42805876,38738337,0,"2017/03/15, 11:36:01",False,"2017/03/15, 11:36:01",181.0,6708214.0,0,"I meet the same problem too.Here is my solution, a less hacky way as above said.",0.159,0.135,0.706,-0.1027
Zipkin,42805876,38738337,0,"2017/03/15, 11:36:01",False,"2017/03/15, 11:36:01",181.0,6708214.0,0,"you must implements MyHttpServerRequest and MyRequest.It is easy,you just return something a span need,such as uri,header,method.",0.0,0.0,1.0,0.0
Zipkin,42805876,38738337,0,"2017/03/15, 11:36:01",False,"2017/03/15, 11:36:01",181.0,6708214.0,0,"This is a rough and ugly code example,just offer an idea.",0.268,0.0,0.732,-0.5106
Zipkin,66913758,66911879,3,"2021/04/02, 05:29:08",False,"2021/04/02, 05:29:08",66.0,12105655.0,1,server.address=&lt;ip&gt;  does not work?,0.0,0.0,1.0,0.0
Zipkin,66913758,66911879,3,"2021/04/02, 05:29:08",False,"2021/04/02, 05:29:08",66.0,12105655.0,1,java -jar zipkin.jar --server.address=192.168.0.7,0.0,0.0,1.0,0.0
Zipkin,66913758,66911879,3,"2021/04/02, 05:29:08",False,"2021/04/02, 05:29:08",66.0,12105655.0,1,If it's not working you can add a property file and connects to it when the server starts:,0.0,0.0,1.0,0.0
Zipkin,66913758,66911879,3,"2021/04/02, 05:29:08",False,"2021/04/02, 05:29:08",66.0,12105655.0,1,java -jar zipkin.jar --spring.config.location=./application.properties,0.0,0.0,1.0,0.0
Zipkin,66913758,66911879,3,"2021/04/02, 05:29:08",False,"2021/04/02, 05:29:08",66.0,12105655.0,1,in application.properties:,0.0,0.0,1.0,0.0
Zipkin,66808417,66777772,2,"2021/03/26, 00:23:10",False,"2021/03/26, 00:23:10",46.0,14172753.0,0,"I'm not entirely sure if that's what you mean, but you can use Jeager  https://www.jaegertracing.io/   which checks if trace-id already exist in the invocation metadata and in it generate child trace id.",0.049,0.0,0.951,-0.1505
Zipkin,66808417,66777772,2,"2021/03/26, 00:23:10",False,"2021/03/26, 00:23:10",46.0,14172753.0,0,Based on all trace ids call diagrams are generated,0.0,0.0,1.0,0.0
Zipkin,66654542,66517888,1,"2021/03/16, 13:40:16",False,"2021/03/16, 13:40:16",19.0,12338209.0,0,In your command please try the following -Dotel.traces.exporter=zipkin instead of -Dotel.exporter=zipkin,0.0,0.187,0.813,0.3182
Zipkin,66782647,66517888,1,"2021/03/24, 15:58:38",True,"2021/03/24, 15:58:38",16.0,15470262.0,0,"i had the same issue, but i solved with this jvm arguments:",0.25,0.187,0.563,-0.2263
Zipkin,66782647,66517888,1,"2021/03/24, 15:58:38",True,"2021/03/24, 15:58:38",16.0,15470262.0,0,"-Dotel.traces.exporter=zipkin -Dotel.metrics.exporter=none -Dotel.exporter.zipkin.endpoint=http://localhost:9411/api/v2/spans
Maybe the error is on zipkin.endpoint, try to write the entire url.",0.162,0.0,0.838,-0.4019
Zipkin,66782647,66517888,1,"2021/03/24, 15:58:38",True,"2021/03/24, 15:58:38",16.0,15470262.0,0,"Regards,
Marco",0.0,0.0,1.0,0.0
Zipkin,66350164,66284701,0,"2021/02/24, 13:40:02",False,"2021/02/24, 13:52:09",11.0,15274583.0,1,I get the same problem and below command did the trick.,0.328,0.0,0.672,-0.4404
Zipkin,66350164,66284701,0,"2021/02/24, 13:40:02",False,"2021/02/24, 13:52:09",11.0,15274583.0,1,I checked the source code.,0.0,0.0,1.0,0.0
Zipkin,66350164,66284701,0,"2021/02/24, 13:40:02",False,"2021/02/24, 13:52:09",11.0,15274583.0,1,It looks the property name has been changed:,0.0,0.0,1.0,0.0
Zipkin,66350164,66284701,0,"2021/02/24, 13:40:02",False,"2021/02/24, 13:52:09",11.0,15274583.0,1,https://github.com/open-telemetry/opentelemetry-java/blob/14ace1ec32dbb194b8990763beb3ab6935849547/sdk-extensions/autoconfigure/src/main/java/io/opentelemetry/sdk/autoconfigure/TracerProviderConfiguration.java#L43,0.0,0.0,1.0,0.0
Zipkin,63969131,63489826,0,"2020/09/19, 16:03:50",True,"2020/09/19, 16:03:50",4433.0,243104.0,3,Getting a handle on the distributed tracing space can be a bit confusing.,0.16,0.0,0.84,-0.2263
Zipkin,63969131,63489826,0,"2020/09/19, 16:03:50",True,"2020/09/19, 16:03:50",4433.0,243104.0,3,Here's a quick summary...,0.0,0.0,1.0,0.0
Zipkin,63969131,63489826,0,"2020/09/19, 16:03:50",True,"2020/09/19, 16:03:50",4433.0,243104.0,3,Open Source Tracers,0.0,0.0,1.0,0.0
Zipkin,63969131,63489826,0,"2020/09/19, 16:03:50",True,"2020/09/19, 16:03:50",4433.0,243104.0,3,"There are a number of popular open source tracers, which is where Zipkin sits:",0.0,0.272,0.728,0.4767
Zipkin,63969131,63489826,0,"2020/09/19, 16:03:50",True,"2020/09/19, 16:03:50",4433.0,243104.0,3,Commercial Tracers,0.0,0.0,1.0,0.0
Zipkin,63969131,63489826,0,"2020/09/19, 16:03:50",True,"2020/09/19, 16:03:50",4433.0,243104.0,3,There are also a lot of vendors offering commercial monitoring/observability tools which are either centred around or include distributed tracing:,0.0,0.0,1.0,0.0
Zipkin,63969131,63489826,0,"2020/09/19, 16:03:50",True,"2020/09/19, 16:03:50",4433.0,243104.0,3,Standardisation Efforts,0.0,0.0,1.0,0.0
Zipkin,63969131,63489826,0,"2020/09/19, 16:03:50",True,"2020/09/19, 16:03:50",4433.0,243104.0,3,Alongside all these products are numerous attempts at creating standards around distributed tracing.,0.0,0.155,0.845,0.296
Zipkin,63969131,63489826,0,"2020/09/19, 16:03:50",True,"2020/09/19, 16:03:50",4433.0,243104.0,3,"These typically start by creating a standard API for the trace-recording side of the architecture, and sometimes extend to become prescriptive about the content of traces or even the wire format.",0.0,0.122,0.878,0.4404
Zipkin,63969131,63489826,0,"2020/09/19, 16:03:50",True,"2020/09/19, 16:03:50",4433.0,243104.0,3,This is where OpenTracing fits in.,0.0,0.0,1.0,0.0
Zipkin,63969131,63489826,0,"2020/09/19, 16:03:50",True,"2020/09/19, 16:03:50",4433.0,243104.0,3,"So it is not a tracing solution itself, but an API that can be implemented by the trace recording SDKs of multiple tracers, allowing you to swap between vendors more easily.",0.045,0.107,0.848,0.4693
Zipkin,63969131,63489826,0,"2020/09/19, 16:03:50",True,"2020/09/19, 16:03:50",4433.0,243104.0,3,The most common standards are:,0.0,0.0,1.0,0.0
Zipkin,63969131,63489826,0,"2020/09/19, 16:03:50",True,"2020/09/19, 16:03:50",4433.0,243104.0,3,"Note that the first two in the list have been abandoned, with their contributors joining forces to create the third one together.",0.12,0.084,0.797,-0.2263
Zipkin,63969131,63489826,0,"2020/09/19, 16:03:50",True,"2020/09/19, 16:03:50",4433.0,243104.0,3,[1],0.0,0.0,1.0,0.0
Zipkin,63969131,63489826,0,"2020/09/19, 16:03:50",True,"2020/09/19, 16:03:50",4433.0,243104.0,3,[1]  https://opensource.googleblog.com/2019/05/opentelemetry-merger-of-opencensus-and.html,0.0,0.0,1.0,0.0
Zipkin,66499651,63385866,0,"2021/03/05, 23:09:02",True,"2021/03/05, 23:09:02",1301.0,1423685.0,0,"I was using it with default storage which is discouraged in production use, it can handle only small amount of data and can be treated only as a demo version.",0.091,0.0,0.909,-0.4019
Zipkin,66499651,63385866,0,"2021/03/05, 23:09:02",True,"2021/03/05, 23:09:02",1301.0,1423685.0,0,What helped a little was setting,0.0,0.0,1.0,0.0
Zipkin,66499651,63385866,0,"2021/03/05, 23:09:02",True,"2021/03/05, 23:09:02",1301.0,1423685.0,0,spring.sleuth.sampler.probability: 0.01,0.0,0.0,1.0,0.0
Zipkin,66499651,63385866,0,"2021/03/05, 23:09:02",True,"2021/03/05, 23:09:02",1301.0,1423685.0,0,-- by default it logs all spans.,0.0,0.0,1.0,0.0
Zipkin,63672504,63073886,0,"2020/08/31, 17:15:00",False,"2020/08/31, 17:15:00",29.0,9544181.0,1,You should create application.properties file and after that you should add the following,0.0,0.149,0.851,0.2732
Zipkin,63672504,63073886,0,"2020/08/31, 17:15:00",False,"2020/08/31, 17:15:00",29.0,9544181.0,1,Your application.properties :,0.0,0.0,1.0,0.0
Zipkin,63672504,63073886,0,"2020/08/31, 17:15:00",False,"2020/08/31, 17:15:00",29.0,9544181.0,1,Your main class :,0.0,0.0,1.0,0.0
Zipkin,63672504,63073886,0,"2020/08/31, 17:15:00",False,"2020/08/31, 17:15:00",29.0,9544181.0,1,Your Pom.xml :,0.0,0.0,1.0,0.0
Zipkin,63672504,63073886,0,"2020/08/31, 17:15:00",False,"2020/08/31, 17:15:00",29.0,9544181.0,1,Your zipkin port :,0.0,0.0,1.0,0.0
Zipkin,61807946,61800994,1,"2020/05/15, 00:44:40",False,"2020/05/15, 00:44:40",985.0,598932.0,1,"It looks like it is related to  https://github.com/openzipkin/zipkin-js/pull/498 , could you try with zipkin-context-cls@0.19.2-alpha.7 and change  ctxImpl  into  ctxImpl = new CLSContext('zipkin', true); ?",0.0,0.111,0.889,0.3612
Zipkin,61862789,61800994,0,"2020/05/18, 08:08:34",True,"2020/05/18, 19:34:48",156911.0,4668.0,0,"The problem ended up not being on Zipkin's end, but instead in how I was instrumenting the express server.",0.098,0.0,0.902,-0.2144
Zipkin,61862789,61800994,0,"2020/05/18, 08:08:34",True,"2020/05/18, 19:34:48",156911.0,4668.0,0,I had added the zipkin middleware  after  my call to  app.get .,0.0,0.0,1.0,0.0
Zipkin,61862789,61800994,0,"2020/05/18, 08:08:34",True,"2020/05/18, 19:34:48",156911.0,4668.0,0,Express executes middlwares in order and makes no distinction between middleware for a named route vs. something added via  app.use .,0.109,0.0,0.891,-0.296
Zipkin,61862789,61800994,0,"2020/05/18, 08:08:34",True,"2020/05/18, 19:34:48",156911.0,4668.0,0,Doing things like this,0.0,0.455,0.545,0.3612
Zipkin,61862789,61800994,0,"2020/05/18, 08:08:34",True,"2020/05/18, 19:34:48",156911.0,4668.0,0,Gave me the result I was looking for.,0.0,0.0,1.0,0.0
Zipkin,61401330,61368689,0,"2020/04/24, 07:46:24",True,"2020/04/24, 07:46:24",21.0,11657025.0,1,"The reason behind error was that i forgot to add the kafka dependency in pom.xml
After adding the dependency, error was gone.",0.221,0.0,0.779,-0.6597
Zipkin,63034460,61208227,0,"2020/07/22, 15:42:19",False,"2020/07/22, 15:42:19",46.0,10967262.0,0,"According to Sleuth documentation, AWS SQS is &quot;natively&quot; supported only on the consumer's side:",0.0,0.15,0.85,0.3182
Zipkin,63034460,61208227,0,"2020/07/22, 15:42:19",False,"2020/07/22, 15:42:19",46.0,10967262.0,0,https://docs.spring.io/spring-cloud-sleuth/docs/current-SNAPSHOT/reference/html/#spring-cloud-aws-messaging-sqs,0.0,0.0,1.0,0.0
Zipkin,63034460,61208227,0,"2020/07/22, 15:42:19",False,"2020/07/22, 15:42:19",46.0,10967262.0,0,In order to add seamless tracing over AWS SQS I resorted to Brave SQS instrumentation (aka SqsMessageTracing) and had to add another dependency:,0.0,0.139,0.861,0.5267
Zipkin,63034460,61208227,0,"2020/07/22, 15:42:19",False,"2020/07/22, 15:42:19",46.0,10967262.0,0,and have the following configuration:,0.0,0.0,1.0,0.0
Zipkin,63034460,61208227,0,"2020/07/22, 15:42:19",False,"2020/07/22, 15:42:19",46.0,10967262.0,0,This is just because I didn't want to do the SQS producer instrumentation myself nor add the tracing headers programmatically.,0.064,0.0,0.936,-0.0572
Zipkin,63034460,61208227,0,"2020/07/22, 15:42:19",False,"2020/07/22, 15:42:19",46.0,10967262.0,0,Little reference for the Brave instrumentation can be found here:,0.0,0.274,0.726,0.5267
Zipkin,63034460,61208227,0,"2020/07/22, 15:42:19",False,"2020/07/22, 15:42:19",46.0,10967262.0,0,https://github.com/spring-cloud/spring-cloud-sleuth/issues/1550#issuecomment-589686583,0.0,0.0,1.0,0.0
Zipkin,63034460,61208227,0,"2020/07/22, 15:42:19",False,"2020/07/22, 15:42:19",46.0,10967262.0,0,My SQS message producer looks like this:,0.0,0.294,0.706,0.3612
Zipkin,63034460,61208227,0,"2020/07/22, 15:42:19",False,"2020/07/22, 15:42:19",46.0,10967262.0,0,FINAL NOTE,0.0,0.0,1.0,0.0
Zipkin,63034460,61208227,0,"2020/07/22, 15:42:19",False,"2020/07/22, 15:42:19",46.0,10967262.0,0,Not required but I also excluded the,0.383,0.0,0.617,-0.4767
Zipkin,63034460,61208227,0,"2020/07/22, 15:42:19",False,"2020/07/22, 15:42:19",46.0,10967262.0,0,Since it performs an AWS environment configuration scan at application startup which wasn't required for me (and also raised a lengthy error log),0.114,0.0,0.886,-0.4019
Zipkin,66417136,60631319,0,"2021/03/01, 07:37:26",False,"2021/03/01, 07:37:26",438.0,3404777.0,0,I think you must have found your answer by now.,0.0,0.0,1.0,0.0
Zipkin,66417136,60631319,0,"2021/03/01, 07:37:26",False,"2021/03/01, 07:37:26",438.0,3404777.0,0,But I am posting this for future reference.,0.0,0.0,1.0,0.0
Zipkin,66417136,60631319,0,"2021/03/01, 07:37:26",False,"2021/03/01, 07:37:26",438.0,3404777.0,0,"Take a look at this  Github issue , it basically explains everything and provides a few workarounds.",0.0,0.0,1.0,0.0
Zipkin,58216010,58214695,0,"2019/10/03, 12:18:37",True,"2019/10/03, 12:18:37",381.0,10371480.0,1,According to  this  Spring Cloud Sleuth is the only tracer that supports messaging.,0.0,0.172,0.828,0.3612
Zipkin,58220753,58214695,0,"2019/10/03, 16:58:34",False,"2019/10/03, 16:58:34",81.0,2232476.0,1,"Brave is the library spring cloud sleuth is built on, therefore you could make it work without sleuth:  https://github.com/openzipkin/brave",0.0,0.159,0.841,0.5267
Zipkin,58220753,58214695,0,"2019/10/03, 16:58:34",False,"2019/10/03, 16:58:34",81.0,2232476.0,1,"Just to clarify though, Sleuth doesn't force you to use any of the rest of the spring-cloud components.",0.0,0.0,1.0,0.0
Zipkin,58220753,58214695,0,"2019/10/03, 16:58:34",False,"2019/10/03, 16:58:34",81.0,2232476.0,1,"It is  spring-cloud  because it is one of the ""cloud native"" spring technologies",0.0,0.0,1.0,0.0
Zipkin,57911578,57892994,0,"2019/09/12, 20:10:36",True,"2019/09/12, 20:10:36",99.0,7474991.0,2,ok I finally realized whats the mistake that I have done when I was starting my zipkin server with this command,0.117,0.107,0.777,-0.0516
Zipkin,57911578,57892994,0,"2019/09/12, 20:10:36",True,"2019/09/12, 20:10:36",99.0,7474991.0,2,but I was not specifying my Zipkin server where my Kafka is running so when I did,0.0,0.0,1.0,0.0
Zipkin,57911578,57892994,0,"2019/09/12, 20:10:36",True,"2019/09/12, 20:10:36",99.0,7474991.0,2,it worked,0.0,0.0,1.0,0.0
Zipkin,58040373,57703884,0,"2019/09/21, 15:42:54",False,"2019/09/21, 15:42:54",1477.0,4051158.0,0,"I am new to zipkin and golang, If you want to trace internal process, then you can create span from context",0.0,0.159,0.841,0.34
Zipkin,58040373,57703884,0,"2019/09/21, 15:42:54",False,"2019/09/21, 15:42:54",1477.0,4051158.0,0,"example: say you have api called Login, inside login you might perform database operation or any other operations",0.0,0.0,1.0,0.0
Zipkin,61247710,57180252,0,"2020/04/16, 13:11:53",False,"2020/04/16, 13:11:53",287.0,8631898.0,0,"On my project, we generated the spans manually before sending the events.",0.0,0.0,1.0,0.0
Zipkin,61247710,57180252,0,"2020/04/16, 13:11:53",False,"2020/04/16, 13:11:53",287.0,8631898.0,0,"var span = tracing.tracer().nextSpanWithParent(req -&gt; true,
Void.class, ctx.get(Span.class).context());",0.0,0.318,0.682,0.4215
Zipkin,61247710,57180252,0,"2020/04/16, 13:11:53",False,"2020/04/16, 13:11:53",287.0,8631898.0,0,span.name(&quot;yourSpanName&quot;).start();,0.0,0.0,1.0,0.0
Zipkin,61247710,57180252,0,"2020/04/16, 13:11:53",False,"2020/04/16, 13:11:53",287.0,8631898.0,0,return sendEventPublisher.doOnError(span::error).doOnTerminate(span::finish);,0.0,0.0,1.0,0.0
Zipkin,61247710,57180252,0,"2020/04/16, 13:11:53",False,"2020/04/16, 13:11:53",287.0,8631898.0,0,"This way, we also link the span to the publisher lifecycle as we had problems with webflux sharing spans between threads.",0.11,0.114,0.776,0.0258
Zipkin,61247710,57180252,0,"2020/04/16, 13:11:53",False,"2020/04/16, 13:11:53",287.0,8631898.0,0,"Basically, we create a span and link it to the parent context created by Spring for the request (either from an incoming B3 HTTP header, or generated if absent).",0.0,0.136,0.864,0.4767
Zipkin,61247710,57180252,0,"2020/04/16, 13:11:53",False,"2020/04/16, 13:11:53",287.0,8631898.0,0,&quot;ctx&quot; is the subscriber context here.,0.0,0.0,1.0,0.0
Zipkin,61247710,57180252,0,"2020/04/16, 13:11:53",False,"2020/04/16, 13:11:53",287.0,8631898.0,0,This also implied to tell sleuth not to generate the spans for async operations in application.properties:,0.0,0.0,1.0,0.0
Zipkin,61247710,57180252,0,"2020/04/16, 13:11:53",False,"2020/04/16, 13:11:53",287.0,8631898.0,0,spring.sleuth.async.enabled=false,0.0,0.0,1.0,0.0
Zipkin,60081185,55001316,0,"2020/02/05, 19:34:06",True,"2020/02/05, 19:34:06",524.0,746528.0,1,"For basic authentication, the username and password are required to be sent as part of the HTTP Header  Authorization .",0.0,0.0,1.0,0.0
Zipkin,60081185,55001316,0,"2020/02/05, 19:34:06",True,"2020/02/05, 19:34:06",524.0,746528.0,1,"The header value is computed as Base64 encoding of the string  username:password .So if the username is  abcd  and password is  1234 , the header will look something like this (Chatset used: UTF-8).",0.0,0.14,0.86,0.5994
Zipkin,60081185,55001316,0,"2020/02/05, 19:34:06",True,"2020/02/05, 19:34:06",524.0,746528.0,1,Authorization: Basic YWJjZDoxMjM0,0.0,0.0,1.0,0.0
Zipkin,60081185,55001316,0,"2020/02/05, 19:34:06",True,"2020/02/05, 19:34:06",524.0,746528.0,1,Sleuth cloud project provides  ZipkinRestTemplateCustomizer  to configure the  RestTemplate  used to communicate with the Zipkin server.,0.0,0.0,1.0,0.0
Zipkin,60081185,55001316,0,"2020/02/05, 19:34:06",True,"2020/02/05, 19:34:06",524.0,746528.0,1,"Refer to the documentation for the same: 
 https://cloud.spring.io/spring-cloud-sleuth/reference/html/#sending-spans-to-zipkin",0.0,0.0,1.0,0.0
Zipkin,60081185,55001316,0,"2020/02/05, 19:34:06",True,"2020/02/05, 19:34:06",524.0,746528.0,1,Note: Base64 encoding is reversible and hence Basic auth credentials are not secured.,0.158,0.0,0.842,-0.3089
Zipkin,60081185,55001316,0,"2020/02/05, 19:34:06",True,"2020/02/05, 19:34:06",524.0,746528.0,1,HTTPS communication should be used along with Basic Authentication.,0.0,0.0,1.0,0.0
Zipkin,63461779,54986635,0,"2020/08/18, 07:12:18",False,"2020/08/18, 07:12:18",3.0,12639023.0,0,I got same problem.,0.574,0.0,0.426,-0.4019
Zipkin,63461779,54986635,0,"2020/08/18, 07:12:18",False,"2020/08/18, 07:12:18",3.0,12639023.0,0,Seem Spring boot  2.1.2.RELEASE  not work with Zipkin.,0.0,0.0,1.0,0.0
Zipkin,63461779,54986635,0,"2020/08/18, 07:12:18",False,"2020/08/18, 07:12:18",3.0,12639023.0,0,Please upgrade to Spring boot version &gt;  2.1.2.RELEASE .,0.0,0.247,0.753,0.3182
Zipkin,52585202,52550644,0,"2018/10/01, 08:47:20",True,"2018/10/01, 08:47:20",568.0,5238035.0,2,"For those who could come across with a same scenario like this,",0.0,0.2,0.8,0.3612
Zipkin,52585202,52550644,0,"2018/10/01, 08:47:20",True,"2018/10/01, 08:47:20",568.0,5238035.0,2,github  has given  APIs  to get details on the repository tag set of each project release as a json object ( https://api.github.com/repos/openzipkin/zipkin/tags  ).,0.0,0.0,1.0,0.0
Zipkin,52585202,52550644,0,"2018/10/01, 08:47:20",True,"2018/10/01, 08:47:20",568.0,5238035.0,2,So that can be used to get the latest version of zipkin.,0.0,0.0,1.0,0.0
Zipkin,52585202,52550644,0,"2018/10/01, 08:47:20",True,"2018/10/01, 08:47:20",568.0,5238035.0,2,"To get the currently running version of my system, zipkin has given an actuator/info end point ( http://localhost:9411/actuator/info ).",0.0,0.0,1.0,0.0
Zipkin,51354529,51151547,3,"2018/07/16, 07:19:17",True,"2018/07/16, 07:19:17",1767.0,2498986.0,1,Yes.,0.0,1.0,0.0,0.4019
Zipkin,51354529,51151547,3,"2018/07/16, 07:19:17",True,"2018/07/16, 07:19:17",1767.0,2498986.0,1,You have to use Brave.,0.0,0.459,0.541,0.5267
Zipkin,51354529,51151547,3,"2018/07/16, 07:19:17",True,"2018/07/16, 07:19:17",1767.0,2498986.0,1,"In fact, Spring cloud sleuth (V2) uses Brave under the hood.",0.0,0.254,0.746,0.5267
Zipkin,51354529,51151547,3,"2018/07/16, 07:19:17",True,"2018/07/16, 07:19:17",1767.0,2498986.0,1,Check the brave web-mvc example to get started.,0.0,0.327,0.673,0.5267
Zipkin,51354529,51151547,3,"2018/07/16, 07:19:17",True,"2018/07/16, 07:19:17",1767.0,2498986.0,1,https://github.com/openzipkin/brave-webmvc-example,0.0,0.0,1.0,0.0
Zipkin,51042639,51040315,0,"2018/06/26, 15:11:31",True,"2018/06/26, 15:11:31",1273.0,4035426.0,1,Try to change all properties with this:,0.0,0.0,1.0,0.0
Zipkin,51042639,51040315,0,"2018/06/26, 15:11:31",True,"2018/06/26, 15:11:31",1273.0,4035426.0,1,"spring.sleuth.sampler.percentage=1.0 is Edgware
so you need that",0.0,0.0,1.0,0.0
Zipkin,51042639,51040315,0,"2018/06/26, 15:11:31",True,"2018/06/26, 15:11:31",1273.0,4035426.0,1,baseUrl by default is localhost,0.0,0.0,1.0,0.0
Zipkin,51073739,50753384,0,"2018/06/28, 05:12:27",True,"2018/06/28, 05:12:27",1728.0,5254103.0,0,"When I change the project root log level to ""debug"", I saw some error report from zipkin.",0.162,0.0,0.838,-0.4019
Zipkin,51073739,50753384,0,"2018/06/28, 05:12:27",True,"2018/06/28, 05:12:27",1728.0,5254103.0,0,Then I realized that the zipkin server I used was very very old.,0.0,0.0,1.0,0.0
Zipkin,51073739,50753384,0,"2018/06/28, 05:12:27",True,"2018/06/28, 05:12:27",1728.0,5254103.0,0,And the zipkin API call returned 404.,0.0,0.0,1.0,0.0
Zipkin,51073739,50753384,0,"2018/06/28, 05:12:27",True,"2018/06/28, 05:12:27",1728.0,5254103.0,0,When I updated my zipkin server to latest version.,0.0,0.0,1.0,0.0
Zipkin,51073739,50753384,0,"2018/06/28, 05:12:27",True,"2018/06/28, 05:12:27",1728.0,5254103.0,0,It worked.,0.0,0.0,1.0,0.0
Zipkin,50599592,50599433,4,"2018/05/30, 11:14:23",False,"2018/05/30, 11:14:23",294930.0,208809.0,0,According to  the section titled Cleanup in the Istio docs :,0.0,0.0,1.0,0.0
Zipkin,50539945,50533178,1,"2018/05/26, 09:21:55",True,"2018/05/26, 09:21:55",8336.0,1773866.0,1,"However, I'd also like to send log messages to Zipkin (either as new spans or annotations to existing spans).",0.0,0.122,0.878,0.3612
Zipkin,50539945,50533178,1,"2018/05/26, 09:21:55",True,"2018/05/26, 09:21:55",8336.0,1773866.0,1,"If I use org.slf4j.Logger to simply LOG.info(""something""), I see the INFO message in console output, with the exportable flag set to true:",0.0,0.128,0.872,0.4215
Zipkin,50539945,50533178,1,"2018/05/26, 09:21:55",True,"2018/05/26, 09:21:55",8336.0,1773866.0,1,You can't send logs to Zipkin.,0.0,0.0,1.0,0.0
Zipkin,50539945,50533178,1,"2018/05/26, 09:21:55",True,"2018/05/26, 09:21:55",8336.0,1773866.0,1,You can send log statements to ELK.,0.0,0.0,1.0,0.0
Zipkin,50539945,50533178,1,"2018/05/26, 09:21:55",True,"2018/05/26, 09:21:55",8336.0,1773866.0,1,"You can check out the sample  https://github.com/marcingrzejszczak/vagrant-elk-box  that has a vagrant box with ELK, uses Sleuth for log correlation and uses ELK to visualize the logs",0.0,0.0,1.0,0.0
Zipkin,50527821,50506432,0,"2018/05/25, 14:08:47",False,"2018/05/25, 14:08:47",201.0,7956609.0,1,To log only request with particular error you can add the log in your exception mapper where you are handling those error.,0.213,0.0,0.787,-0.6597
Zipkin,50527821,50506432,0,"2018/05/25, 14:08:47",False,"2018/05/25, 14:08:47",201.0,7956609.0,1,"To show the log for error response you can set like below,",0.178,0.164,0.658,-0.0516
Zipkin,50527821,50506432,0,"2018/05/25, 14:08:47",False,"2018/05/25, 14:08:47",201.0,7956609.0,1,and set,0.0,0.0,1.0,0.0
Zipkin,50478519,50389884,0,"2018/05/23, 04:20:43",False,"2018/05/23, 04:20:43",201.0,7956609.0,1,You can add the trace id in your logback.xml,0.0,0.0,1.0,0.0
Zipkin,50478519,50389884,0,"2018/05/23, 04:20:43",False,"2018/05/23, 04:20:43",201.0,7956609.0,1,"""request_id"":
                {""trace_id"":""%X{X-B3-TraceId}"",""span_id"":""%X{X-B3-SpanId}"",""parent_span_id"":""%X{X-B3-ParentSpanId}""},",0.0,0.0,1.0,0.0
Zipkin,49407790,49396204,0,"2018/03/21, 15:35:06",False,"2018/03/21, 15:35:06",11.0,7472851.0,0,I found the solution I think.,0.0,0.434,0.566,0.3182
Zipkin,49407790,49396204,0,"2018/03/21, 15:35:06",False,"2018/03/21, 15:35:06",11.0,7472851.0,0,I changed it to this:,0.0,0.0,1.0,0.0
Zipkin,49407790,49396204,0,"2018/03/21, 15:35:06",False,"2018/03/21, 15:35:06",11.0,7472851.0,0,RABBIT_URI=amqp://user:password@tracing:5672,0.0,0.0,1.0,0.0
Zipkin,48628487,48626548,8,"2018/02/05, 19:47:34",True,"2018/02/05, 19:47:34",8336.0,1773866.0,0,Please use latest snapshots.,0.0,0.434,0.566,0.3182
Zipkin,48628487,48626548,8,"2018/02/05, 19:47:34",True,"2018/02/05, 19:47:34",8336.0,1773866.0,0,Sleuth in latest snapshots uses brave internally so integration will be extremely simple.,0.0,0.221,0.779,0.5267
Zipkin,47363721,47343439,2,"2017/11/18, 09:34:23",False,"2017/11/18, 09:34:23",8336.0,1773866.0,3,This feature is available in edgware release train.,0.0,0.0,1.0,0.0
Zipkin,47363721,47343439,2,"2017/11/18, 09:34:23",False,"2017/11/18, 09:34:23",8336.0,1773866.0,3,That corresponds to version 1.3.x of sleuth,0.0,0.0,1.0,0.0
Zipkin,46289584,46287877,0,"2017/09/19, 02:30:27",False,"2017/09/19, 02:30:27",81.0,2232476.0,0,The Zipkin UI is making an AJAX request to the API in order to retrieve the data that is displayed.,0.0,0.0,1.0,0.0
Zipkin,46289584,46287877,0,"2017/09/19, 02:30:27",False,"2017/09/19, 02:30:27",81.0,2232476.0,0,You can find the API definition for zipkin at:,0.0,0.0,1.0,0.0
Zipkin,46289584,46287877,0,"2017/09/19, 02:30:27",False,"2017/09/19, 02:30:27",81.0,2232476.0,0,https://github.com/openzipkin/zipkin-api,0.0,0.0,1.0,0.0
Zipkin,46289584,46287877,0,"2017/09/19, 02:30:27",False,"2017/09/19, 02:30:27",81.0,2232476.0,0,I believe you are looking for the URL:  http://zipkin.iamplus.xyz/api/v1/traces,0.0,0.0,1.0,0.0
Zipkin,46289584,46287877,0,"2017/09/19, 02:30:27",False,"2017/09/19, 02:30:27",81.0,2232476.0,0,From there you will get the traces matching your filter,0.0,0.0,1.0,0.0
Zipkin,45632418,45492904,0,"2017/08/11, 13:02:33",False,"2017/08/11, 13:02:33",86.0,7146447.0,0,I have the same config running on my ingress 9.0-beta.11.,0.0,0.0,1.0,0.0
Zipkin,45632418,45492904,0,"2017/08/11, 13:02:33",False,"2017/08/11, 13:02:33",86.0,7146447.0,0,I guess it's just a misconfiguration.,0.0,0.0,1.0,0.0
Zipkin,45632418,45492904,0,"2017/08/11, 13:02:33",False,"2017/08/11, 13:02:33",86.0,7146447.0,0,First I'll recommend you to not change the template and use the default values and just change when the basic-auth works.,0.0,0.215,0.785,0.6369
Zipkin,45632418,45492904,0,"2017/08/11, 13:02:33",False,"2017/08/11, 13:02:33",86.0,7146447.0,0,What the logs of ingress show to you?,0.0,0.0,1.0,0.0
Zipkin,45632418,45492904,0,"2017/08/11, 13:02:33",False,"2017/08/11, 13:02:33",86.0,7146447.0,0,Did you create the basic-auth file in the same namespace of the ingress resource?,0.0,0.139,0.861,0.2732
Zipkin,43843859,43790619,0,"2017/05/08, 12:12:55",True,"2017/05/08, 12:12:55",8336.0,1773866.0,1,The issue got fixed -  https://github.com/spring-cloud/spring-cloud-sleuth/issues/585  .,0.0,0.0,1.0,0.0
Zipkin,43843859,43790619,0,"2017/05/08, 12:12:55",True,"2017/05/08, 12:12:55",8336.0,1773866.0,1,In the upcoming releases 1.1.5 and 1.2.1 it should work,0.0,0.0,1.0,0.0
Zipkin,41723986,40701663,3,"2017/01/18, 18:04:16",True,"2017/01/18, 18:04:16",8336.0,1773866.0,4,Spring Cloud Zipkin Stream is using Spring Cloud Stream underneath.,0.0,0.0,1.0,0.0
Zipkin,41723986,40701663,3,"2017/01/18, 18:04:16",True,"2017/01/18, 18:04:16",8336.0,1773866.0,4,You need to provide how do you want to send the spans to Zipkin - thus you need a binder.,0.0,0.071,0.929,0.0772
Zipkin,41723986,40701663,3,"2017/01/18, 18:04:16",True,"2017/01/18, 18:04:16",8336.0,1773866.0,4,One possible binder is the RabbitMQ binder.,0.0,0.0,1.0,0.0
Zipkin,41723986,40701663,3,"2017/01/18, 18:04:16",True,"2017/01/18, 18:04:16",8336.0,1773866.0,4,Check out this:  https://github.com/spring-cloud-samples/sleuth-documentation-apps/blob/master/zipkin-server/build.gradle#L6,0.0,0.0,1.0,0.0
Zipkin,37867634,37842256,0,"2016/06/16, 22:15:19",True,"2016/06/16, 22:15:19",911.0,1822028.0,1,It seems that Brave does not support this.,0.194,0.292,0.515,0.2828
Zipkin,37867634,37842256,0,"2016/06/16, 22:15:19",True,"2016/06/16, 22:15:19",911.0,1822028.0,1,An issue has been reported on their GitHub page.,0.0,0.0,1.0,0.0
Zipkin,37867634,37842256,0,"2016/06/16, 22:15:19",True,"2016/06/16, 22:15:19",911.0,1822028.0,1,https://github.com/openzipkin/brave/issues/166,0.0,0.0,1.0,0.0
Zipkin,17684615,17017657,0,"2013/07/16, 21:48:17",False,"2019/09/02, 10:04:21",1124.0,819626.0,1,I don't know much about ActiveMQ but you need to pass the zipkin trace information along.,0.0,0.0,1.0,0.0
Zipkin,17684615,17017657,0,"2013/07/16, 21:48:17",False,"2019/09/02, 10:04:21",1124.0,819626.0,1,"Review the  ActiveMQ Collector  section in this doc 
 https://github.com/openzipkin/zipkin/blob/master/zipkin-server/README.md",0.0,0.0,1.0,0.0
Zipkin,17684615,17017657,0,"2013/07/16, 21:48:17",False,"2019/09/02, 10:04:21",1124.0,819626.0,1,I just set up Zipkin tracing for a stack that includes RabbitMQ.,0.0,0.0,1.0,0.0
Zipkin,17684615,17017657,0,"2013/07/16, 21:48:17",False,"2019/09/02, 10:04:21",1124.0,819626.0,1,"I added the parent_span_id, and span_id to the message header before the message is placed on the queue.",0.0,0.0,1.0,0.0
Zipkin,17684615,17017657,0,"2013/07/16, 21:48:17",False,"2019/09/02, 10:04:21",1124.0,819626.0,1,Then the applications that read the messages get the trace information from the header.,0.0,0.0,1.0,0.0
Zipkin,17684615,17017657,0,"2013/07/16, 21:48:17",False,"2019/09/02, 10:04:21",1124.0,819626.0,1,"And if you need more help, I recommend jumping on IRC #zipkin.",0.0,0.391,0.609,0.6976
Zipkin,65962429,65943408,2,"2021/01/30, 00:17:20",False,"2021/01/30, 00:17:20",1693.0,971735.0,1,"I was not able to reproduce your issue with the  spring-cloud-sleuth-sample-zipkin  app (it worked to me), here's what I did:",0.0,0.0,1.0,0.0
Zipkin,65962429,65943408,2,"2021/01/30, 00:17:20",False,"2021/01/30, 00:17:20",1693.0,971735.0,1,A few pointers to troubleshoot this:,0.0,0.31,0.69,0.2023
Zipkin,65962429,65943408,2,"2021/01/30, 00:17:20",False,"2021/01/30, 00:17:20",1693.0,971735.0,1,Try to make it work using the  sample  and try to bring the working example closer to your app (by adding dependencies) and see what is the difference between the two and where will it break.,0.0,0.0,1.0,0.0
Zipkin,65962429,65943408,2,"2021/01/30, 00:17:20",False,"2021/01/30, 00:17:20",1693.0,971735.0,1,If you can create a minimal sample app (e.g.,0.0,0.231,0.769,0.2732
Zipkin,65962429,65943408,2,"2021/01/30, 00:17:20",False,"2021/01/30, 00:17:20",1693.0,971735.0,1,": based on the zipkin sample) that reproduces the issue, please feel free to create an issue on GH:  https://github.com/spring-cloud/spring-cloud-sleuth  and tag me ( @jonatan-ivanov ), I can take a look.",0.0,0.243,0.757,0.7717
Zipkin,65966458,65943408,0,"2021/01/30, 11:30:34",False,"2021/01/30, 11:30:34",830.0,7349864.0,1,Finally I found it.,0.0,0.0,1.0,0.0
Zipkin,65966458,65943408,0,"2021/01/30, 11:30:34",False,"2021/01/30, 11:30:34",830.0,7349864.0,1,I had 2 problemas,0.0,0.0,1.0,0.0
Zipkin,65966458,65943408,0,"2021/01/30, 11:30:34",False,"2021/01/30, 11:30:34",830.0,7349864.0,1,1 - I was using zipkin-slim docker image for my zip container.,0.0,0.0,1.0,0.0
Zipkin,65966458,65943408,0,"2021/01/30, 11:30:34",False,"2021/01/30, 11:30:34",830.0,7349864.0,1,This image doesn't contain the rabbitmq collector  rabbitmq collector .,0.0,0.0,1.0,0.0
Zipkin,65966458,65943408,0,"2021/01/30, 11:30:34",False,"2021/01/30, 11:30:34",830.0,7349864.0,1,I have replaces by standar zipkin image,0.0,0.0,1.0,0.0
Zipkin,65966458,65943408,0,"2021/01/30, 11:30:34",False,"2021/01/30, 11:30:34",830.0,7349864.0,1,"2 - I do not know why, but the connection from sleuth/zipkin to RabbitMQ is not retrying (I will investigate further).",0.0,0.0,1.0,0.0
Zipkin,65966458,65943408,0,"2021/01/30, 11:30:34",False,"2021/01/30, 11:30:34",830.0,7349864.0,1,"So, if I was in a hurry and test very early (when RabbitMQ is not yet available) it fails, and not retries.",0.128,0.0,0.872,-0.4215
Zipkin,65966458,65943408,0,"2021/01/30, 11:30:34",False,"2021/01/30, 11:30:34",830.0,7349864.0,1,My docker-compose relevant sections now are like this:,0.0,0.263,0.737,0.3612
Zipkin,65966458,65943408,0,"2021/01/30, 11:30:34",False,"2021/01/30, 11:30:34",830.0,7349864.0,1,Thanks again to  Jonatan Ivanov  for helping me!,0.0,0.473,0.527,0.6588
Zipkin,58554326,58551796,0,"2019/10/25, 10:49:15",True,"2019/10/25, 10:49:15",762.0,1227937.0,1,probably best to have the issue you raised in github vs cross posting.,0.0,0.259,0.741,0.6369
Zipkin,58554326,58551796,0,"2019/10/25, 10:49:15",True,"2019/10/25, 10:49:15",762.0,1227937.0,1,it is a bug  https://github.com/honeycombio/honeycomb-opentracing-proxy/issues/37,0.0,0.0,1.0,0.0
Zipkin,61975479,56923548,0,"2020/05/23, 19:50:21",False,"2020/05/23, 19:50:21",83.0,2702332.0,0,There are 2 entries in mysql zipkin_spans table,0.0,0.0,1.0,0.0
Zipkin,61975479,56923548,0,"2020/05/23, 19:50:21",False,"2020/05/23, 19:50:21",83.0,2702332.0,0,Example,0.0,0.0,1.0,0.0
Zipkin,61975479,56923548,0,"2020/05/23, 19:50:21",False,"2020/05/23, 19:50:21",83.0,2702332.0,0,32 character hex trace id  5ec92d0240cd9dee0421f4763e9f674f  displayed in zipkin ui corresponds to,0.0,0.0,1.0,0.0
Zipkin,61975479,56923548,0,"2020/05/23, 19:50:21",False,"2020/05/23, 19:50:21",83.0,2702332.0,0,trace_id_high = 6830039797584469486 in mysql  (5EC92D0240CD9DEE -  upper 16 hex character),0.0,0.0,1.0,0.0
Zipkin,61975479,56923548,0,"2020/05/23, 19:50:21",False,"2020/05/23, 19:50:21",83.0,2702332.0,0,id = 297787839077115727 in mysql  (421F4763E9F674F -  lower 16 hex charecter),0.216,0.0,0.784,-0.296
Zipkin,56032785,56032752,0,"2019/05/08, 05:37:47",False,"2020/08/19, 12:00:44",167.0,486631.0,0,After continue efforts and going throgh core api of spring boot application I got my solution:),0.0,0.0,1.0,0.0
Zipkin,56032785,56032752,0,"2019/05/08, 05:37:47",False,"2020/08/19, 12:00:44",167.0,486631.0,0,Root cause of my issue is below :,0.0,0.0,1.0,0.0
Zipkin,56032785,56032752,0,"2019/05/08, 05:37:47",False,"2020/08/19, 12:00:44",167.0,486631.0,0,"MY application using Spring boot RabitMQ integration and due that
zipkin taking 1st prefrance to RabitMQ sender and my trace are ignored
my zipkin server.",0.087,0.0,0.913,-0.3182
Zipkin,56032785,56032752,0,"2019/05/08, 05:37:47",False,"2020/08/19, 12:00:44",167.0,486631.0,0,"So use below configration is anyone has same issue to avoid lot painless efforts , even we are not getting in logs of server root cause of it",0.075,0.075,0.85,0.0
Zipkin,56032785,56032752,0,"2019/05/08, 05:37:47",False,"2020/08/19, 12:00:44",167.0,486631.0,0,*---,0.0,0.0,1.0,0.0
Zipkin,53370111,53369694,0,"2018/11/19, 09:32:06",True,"2018/11/19, 09:32:06",463.0,1749786.0,2,I was using Finchley.SR2 train of releases.,0.0,0.0,1.0,0.0
Zipkin,53370111,53369694,0,"2018/11/19, 09:32:06",True,"2018/11/19, 09:32:06",463.0,1749786.0,2,"Once I upgraded to the latest Spring Boot and Spring Cloud versions, the issue fixed itself.",0.0,0.0,1.0,0.0
Zipkin,53370111,53369694,0,"2018/11/19, 09:32:06",True,"2018/11/19, 09:32:06",463.0,1749786.0,2,I removed the opentracing-spring-cloud-starter dependency and am now just using,0.0,0.0,1.0,0.0
Zipkin,52739988,52377663,0,"2018/10/10, 15:12:13",False,"2018/10/10, 15:12:13",201.0,7956609.0,0,Check your configuration file and make sure the baseUrl is given properly here,0.0,0.161,0.839,0.3182
Zipkin,50298803,50295992,5,"2018/05/11, 22:10:23",False,"2018/05/11, 22:10:23",91301.0,2756547.0,0,OK!,0.0,1.0,0.0,0.3595
Zipkin,50298803,50295992,5,"2018/05/11, 22:10:23",False,"2018/05/11, 22:10:23",91301.0,2756547.0,0,I see now the problem!,0.499,0.0,0.501,-0.4574
Zipkin,50298803,50295992,5,"2018/05/11, 22:10:23",False,"2018/05/11, 22:10:23",91301.0,2756547.0,0,"So, you say that your HTTP request has these tracing headers:  X-B3-TraceId ,  X-B3-SpanId ,  X-B3-Sampled ,  X-Span-Name ,  X-B3-ParentSpanId .",0.0,0.0,1.0,0.0
Zipkin,50298803,50295992,5,"2018/05/11, 22:10:23",False,"2018/05/11, 22:10:23",91301.0,2756547.0,0,Then you have this code:,0.0,0.0,1.0,0.0
Zipkin,50298803,50295992,5,"2018/05/11, 22:10:23",False,"2018/05/11, 22:10:23",91301.0,2756547.0,0,And that's absolutely natural that your tracing header are not transferred to the RabbitMQ: there is just no those headers to send.,0.088,0.112,0.8,0.1513
Zipkin,50298803,50295992,5,"2018/05/11, 22:10:23",False,"2018/05/11, 22:10:23",91301.0,2756547.0,0,I believe that you can extract those headers in this  @RequestMapping  method and populate them to the AMQP message before sending.,0.0,0.0,1.0,0.0
Zipkin,50298803,50295992,5,"2018/05/11, 22:10:23",False,"2018/05/11, 22:10:23",91301.0,2756547.0,0,See  org.springframework.amqp.core.MessageBuilder .,0.0,0.0,1.0,0.0
Zipkin,50298803,50295992,5,"2018/05/11, 22:10:23",False,"2018/05/11, 22:10:23",91301.0,2756547.0,0,"I also think that Spring Cloud Sleuth should have some mechanism to obtain those headers, e.g.",0.0,0.0,1.0,0.0
Zipkin,50298803,50295992,5,"2018/05/11, 22:10:23",False,"2018/05/11, 22:10:23",91301.0,2756547.0,0,Tracer.currentSpan() :  http://cloud.spring.io/spring-cloud-static/spring-cloud-sleuth/2.0.0.RC1/single/spring-cloud-sleuth.html#_current_span,0.0,0.0,1.0,0.0
Zipkin,48063504,47992456,0,"2018/01/02, 17:22:45",False,"2018/01/02, 17:22:45",81.0,2232476.0,0,"Yes, they are both stateless.",0.0,0.403,0.597,0.4019
Zipkin,48063504,47992456,0,"2018/01/02, 17:22:45",False,"2018/01/02, 17:22:45",81.0,2232476.0,0,You can deploy them using whatever horizontal-scalability construct is available to you.,0.0,0.0,1.0,0.0
Zipkin,47630471,47630294,3,"2017/12/04, 11:45:28",True,"2017/12/04, 13:10:19",32952.0,6603816.0,3,"When connecting to the mysql container while using links, you need to use the container name as a hostname.",0.0,0.0,1.0,0.0
Zipkin,47630471,47630294,3,"2017/12/04, 11:45:28",True,"2017/12/04, 13:10:19",32952.0,6603816.0,3,Change the connection string to:,0.0,0.0,1.0,0.0
Zipkin,47630471,47630294,3,"2017/12/04, 11:45:28",True,"2017/12/04, 13:10:19",32952.0,6603816.0,3,"And when starting the zipkin container, set the env variable:",0.0,0.0,1.0,0.0
Zipkin,45014834,45012331,0,"2017/07/10, 17:32:57",True,"2017/07/10, 17:32:57",8336.0,1773866.0,1,Why are you mocking a span?,0.403,0.0,0.597,-0.4019
Zipkin,45014834,45012331,0,"2017/07/10, 17:32:57",True,"2017/07/10, 17:32:57",8336.0,1773866.0,1,This makes absolutely no sense.,0.384,0.0,0.616,-0.3597
Zipkin,45014834,45012331,0,"2017/07/10, 17:32:57",True,"2017/07/10, 17:32:57",8336.0,1773866.0,1,Also a Span is never a bean.,0.0,0.0,1.0,0.0
Zipkin,45014834,45012331,0,"2017/07/10, 17:32:57",True,"2017/07/10, 17:32:57",8336.0,1773866.0,1,You already create a normal span via a builder and you should leave that.,0.09,0.158,0.752,0.2263
Zipkin,45014834,45012331,0,"2017/07/10, 17:32:57",True,"2017/07/10, 17:32:57",8336.0,1773866.0,1,Assuming that you have set up the Boot context property and  you want to mock out  tracer  bean you should do the following,0.112,0.052,0.837,-0.3612
Zipkin,44636339,44611370,0,"2017/06/19, 20:13:49",False,"2017/06/19, 20:13:49",35.0,440061.0,0,"*sigh, so it turns out, that someone had turned off zipking tracing in a properties file, for no good reason.",0.1,0.131,0.769,0.1779
Zipkin,44636339,44611370,0,"2017/06/19, 20:13:49",False,"2017/06/19, 20:13:49",35.0,440061.0,0,*sigh,0.0,0.0,1.0,0.0
Zipkin,43401515,43394714,2,"2017/04/13, 23:52:30",True,"2017/04/13, 23:52:30",81.0,2232476.0,2,You are trying to run 2 different applications.,0.0,0.0,1.0,0.0
Zipkin,43401515,43394714,2,"2017/04/13, 23:52:30",True,"2017/04/13, 23:52:30",81.0,2232476.0,2,To run the  zipkin  application with with ElasticSearch and Kafka you will need to run it with both sets of environment variables:,0.0,0.0,1.0,0.0
Zipkin,43401515,43394714,2,"2017/04/13, 23:52:30",True,"2017/04/13, 23:52:30",81.0,2232476.0,2,"Once you have the  zipkin  server running with ES, then you can use your second command to generate the data for the dependency graph view",0.0,0.0,1.0,0.0
Zipkin,41842532,41823536,0,"2017/01/25, 04:43:12",False,"2017/01/25, 04:43:12",762.0,1227937.0,0,"At the moment, there's no replacement for the ""thread binder"" apis.",0.18,0.0,0.82,-0.296
Zipkin,41842532,41823536,0,"2017/01/25, 04:43:12",False,"2017/01/25, 04:43:12",762.0,1227937.0,0,There will be in the coming months.,0.0,0.0,1.0,0.0
Zipkin,41842532,41823536,0,"2017/01/25, 04:43:12",False,"2017/01/25, 04:43:12",762.0,1227937.0,0,This is indeed needed to renovate existing instrumentation.,0.0,0.0,1.0,0.0
Zipkin,41842532,41823536,0,"2017/01/25, 04:43:12",False,"2017/01/25, 04:43:12",762.0,1227937.0,0,"Until then, you can re-use thread binders via TracerAdapter or use a different in-process propagation library.",0.0,0.0,1.0,0.0
Zipkin,41842532,41823536,0,"2017/01/25, 04:43:12",False,"2017/01/25, 04:43:12",762.0,1227937.0,0,The following link includes a working example  https://github.com/openzipkin/brave/tree/master/brave#upgrading-from-brave-3,0.0,0.0,1.0,0.0
Zipkin,66524575,66517644,2,"2021/03/08, 07:07:48",True,"2021/03/08, 07:25:55",1693.0,971735.0,1,"It was removed in Sleuth 3.0, though it seems the docs were not updated, I'm going to update the docs soon.",0.0,0.0,1.0,0.0
Zipkin,66524575,66517644,2,"2021/03/08, 07:07:48",True,"2021/03/08, 07:25:55",1693.0,971735.0,1,"To fix the rest with your logs, you can check the logging config  here , the  log integration in the docs  and  this answer .",0.0,0.0,1.0,0.0
Zipkin,66481285,66479206,1,"2021/03/04, 20:52:36",False,"2021/03/04, 20:52:36",1693.0,971735.0,0,This should be done out of the box:  https://docs.spring.io/spring-cloud-sleuth/docs/2.2.7.RELEASE/reference/html/#feign,0.0,0.0,1.0,0.0
Zipkin,66481285,66479206,1,"2021/03/04, 20:52:36",False,"2021/03/04, 20:52:36",1693.0,971735.0,0,"You can take a look at the feign sample (you need to go back in the history, currently it is for 3.x):  https://github.com/spring-cloud/spring-cloud-sleuth/tree/master/spring-cloud-sleuth-samples/spring-cloud-sleuth-sample-feign",0.0,0.0,1.0,0.0
Zipkin,66481285,66479206,1,"2021/03/04, 20:52:36",False,"2021/03/04, 20:52:36",1693.0,971735.0,0,"In order to see if propagation works, look into the outgoing request, it should contain the tracing-related headers.",0.0,0.115,0.885,0.296
Zipkin,66384663,66383029,4,"2021/02/26, 13:04:31",True,"2021/02/26, 13:04:31",8336.0,1773866.0,2,"You could create your own  SpanHandler  bean that takes the  FinishedSpan , converts into JSON and stores it somewhere on your drive.",0.0,0.095,0.905,0.2732
Zipkin,66384663,66383029,4,"2021/02/26, 13:04:31",True,"2021/02/26, 13:04:31",8336.0,1773866.0,2,Then you could just iterate over jsons and upload them to the Zipkin server,0.0,0.0,1.0,0.0
Zipkin,65231143,65228873,0,"2020/12/10, 10:43:15",True,"2020/12/10, 10:43:15",6331.0,2133695.0,1,"No, the tracing SPI will not be backported to Vert.x 3.",0.18,0.0,0.82,-0.296
Zipkin,65231143,65228873,0,"2020/12/10, 10:43:15",True,"2020/12/10, 10:43:15",6331.0,2133695.0,1,I would recommend to check out  Migrate from Vert.x 3 to Vert.x 4 :,0.0,0.217,0.783,0.3612
Zipkin,65231143,65228873,0,"2020/12/10, 10:43:15",True,"2020/12/10, 10:43:15",6331.0,2133695.0,1,"When­ever pos­si­ble, Vert.x 4 APIs have been made avail­able in
Vert.x 3 with a dep­re­ca­tion of the old API, giv­ing the
op­por­tu­nity to im­prove a Vert.x 3 ap­pli­ca­tion with a bet­ter
API while al­low­ing the ap­pli­ca­tion to be ready for a Vert.x 4
mi­gra­tion.",0.0,0.067,0.933,0.3612
Zipkin,65231143,65228873,0,"2020/12/10, 10:43:15",True,"2020/12/10, 10:43:15",6331.0,2133695.0,1,"In other words, one of the Vert.x 4 goals was to minimize the upgrading effort.",0.0,0.0,1.0,0.0
Zipkin,65171650,65169398,0,"2020/12/06, 20:34:26",False,"2020/12/06, 20:34:26",8336.0,1773866.0,0,You should use e.g.,0.0,0.0,1.0,0.0
Zipkin,65171650,65169398,0,"2020/12/06, 20:34:26",False,"2020/12/06, 20:34:26",8336.0,1773866.0,0,openzipkin Brave project or Opentelemetry projects directly.,0.0,0.362,0.638,0.5267
Zipkin,65171650,65169398,0,"2020/12/06, 20:34:26",False,"2020/12/06, 20:34:26",8336.0,1773866.0,0,Sleuth works only with boot based projects,0.0,0.0,1.0,0.0
Zipkin,65347921,65064734,0,"2020/12/17, 22:30:30",True,"2020/12/17, 22:30:30",1693.0,971735.0,0,"I think what you call correlationId is in fact the traceId, if you are new to distributed tracing, I highly recommend reading the docs of  spring-cloud-sleuth , the  introduction  section will give you a basic understanding while the  propagation  will tell you well, how your fields are propagated across services.",0.0,0.1,0.9,0.5984
Zipkin,65347921,65064734,0,"2020/12/17, 22:30:30",True,"2020/12/17, 22:30:30",1693.0,971735.0,0,"I also recommend this talk:  Distributed Tracing: Latency Analysis for Your Microservices - Grzejszczak, Krishna .",0.0,0.172,0.828,0.3612
Zipkin,65347921,65064734,0,"2020/12/17, 22:30:30",True,"2020/12/17, 22:30:30",1693.0,971735.0,0,To answer your exact questions:,0.0,0.0,1.0,0.0
Zipkin,65347921,65064734,0,"2020/12/17, 22:30:30",True,"2020/12/17, 22:30:30",1693.0,971735.0,0,How the correlation id will be passed to Kafka messages?,0.0,0.0,1.0,0.0
Zipkin,65347921,65064734,0,"2020/12/17, 22:30:30",True,"2020/12/17, 22:30:30",1693.0,971735.0,0,"Kafka has headers, I assume the fields are propagated through Kafka headers.",0.0,0.0,1.0,0.0
Zipkin,65347921,65064734,0,"2020/12/17, 22:30:30",True,"2020/12/17, 22:30:30",1693.0,971735.0,0,How the correlation id will be passed to Http requests?,0.0,0.0,1.0,0.0
Zipkin,65347921,65064734,0,"2020/12/17, 22:30:30",True,"2020/12/17, 22:30:30",1693.0,971735.0,0,Through HTTP Headers.,0.0,0.0,1.0,0.0
Zipkin,65347921,65064734,0,"2020/12/17, 22:30:30",True,"2020/12/17, 22:30:30",1693.0,971735.0,0,Is it possible to use existing tracedId from other service?,0.0,0.0,1.0,0.0
Zipkin,65347921,65064734,0,"2020/12/17, 22:30:30",True,"2020/12/17, 22:30:30",1693.0,971735.0,0,"Not just possible, Sleuth does this for you out of the box.",0.0,0.0,1.0,0.0
Zipkin,65347921,65064734,0,"2020/12/17, 22:30:30",True,"2020/12/17, 22:30:30",1693.0,971735.0,0,If there is a traceId in the incoming request/message/event/etc.,0.0,0.0,1.0,0.0
Zipkin,65347921,65064734,0,"2020/12/17, 22:30:30",True,"2020/12/17, 22:30:30",1693.0,971735.0,0,Sleuth will not create a new one but it will use it (see the docs I linked above).,0.086,0.0,0.914,-0.1045
Zipkin,63739507,63737361,2,"2020/09/04, 13:37:27",False,"2020/09/04, 13:37:27",4113.0,12201084.0,0,Your service is expecting following labels on pod:,0.0,0.0,1.0,0.0
Zipkin,63739507,63737361,2,"2020/09/04, 13:37:27",False,"2020/09/04, 13:37:27",4113.0,12201084.0,0,Although it looks like you have only one label on zipkin pods:,0.0,0.185,0.815,0.3612
Zipkin,63739507,63737361,2,"2020/09/04, 13:37:27",False,"2020/09/04, 13:37:27",4113.0,12201084.0,0,"Label selector uses logical AND (&amp;&amp;), and this means that all labels specified must be on pod to match it.",0.0,0.0,1.0,0.0
Zipkin,63778418,63737361,0,"2020/09/07, 16:19:45",True,"2020/09/07, 18:08:33",351.0,10196632.0,0,The following worked.,0.0,0.0,1.0,0.0
Zipkin,63778418,63737361,0,"2020/09/07, 16:19:45",True,"2020/09/07, 18:08:33",351.0,10196632.0,0,"Sorry, I cannot provide info on all details since I don't know them :( Maybe somebody else can.",0.073,0.136,0.791,0.2746
Zipkin,63778418,63737361,0,"2020/09/07, 16:19:45",True,"2020/09/07, 18:08:33",351.0,10196632.0,0,deployment.yaml,0.0,0.0,1.0,0.0
Zipkin,63778418,63737361,0,"2020/09/07, 16:19:45",True,"2020/09/07, 18:08:33",351.0,10196632.0,0,service.yaml,0.0,0.0,1.0,0.0
Zipkin,63778418,63737361,0,"2020/09/07, 16:19:45",True,"2020/09/07, 18:08:33",351.0,10196632.0,0,ingress.yaml,0.0,0.0,1.0,0.0
Zipkin,63384808,63384688,0,"2020/08/13, 00:15:25",True,"2020/08/13, 00:15:25",8336.0,1773866.0,1,It's because of sampling.,0.0,0.0,1.0,0.0
Zipkin,63384808,63384688,0,"2020/08/13, 00:15:25",True,"2020/08/13, 00:15:25",8336.0,1773866.0,1,Please create a bean of sampler type whose value can be Sampler.ALWAYS or set the probability property to 1.0,0.0,0.312,0.688,0.7003
Zipkin,63219184,62894286,0,"2020/08/02, 20:39:09",False,"2020/08/02, 20:46:19",11.0,11424150.0,1,Hi I just resolved this issue ..,0.0,0.254,0.746,0.1779
Zipkin,63219184,62894286,0,"2020/08/02, 20:39:09",False,"2020/08/02, 20:46:19",11.0,11424150.0,1,Step1.verify C:\Programfiles\Err(version) folder including bin folder is created or not.,0.0,0.182,0.818,0.25
Zipkin,63219184,62894286,0,"2020/08/02, 20:39:09",False,"2020/08/02, 20:46:19",11.0,11424150.0,1,Otherwise try to download and install Erlang again.,0.0,0.0,1.0,0.0
Zipkin,63219184,62894286,0,"2020/08/02, 20:39:09",False,"2020/08/02, 20:46:19",11.0,11424150.0,1,reinstall RabbitMQ and try connecting Zipkin.,0.0,0.0,1.0,0.0
Zipkin,63219184,62894286,0,"2020/08/02, 20:39:09",False,"2020/08/02, 20:46:19",11.0,11424150.0,1,Make sure Erlang version and RabbitMQ version is compatible.,0.0,0.223,0.777,0.3182
Zipkin,63219184,62894286,0,"2020/08/02, 20:39:09",False,"2020/08/02, 20:46:19",11.0,11424150.0,1,Step2.Check ERLANG_HOME is set to proper location in environment variables.,0.0,0.0,1.0,0.0
Zipkin,63219184,62894286,0,"2020/08/02, 20:39:09",False,"2020/08/02, 20:46:19",11.0,11424150.0,1,"at this point if RabbitMQ windows installer pointing to any old erlang version installed earlier  try to install RabitMQ windows manually
follow the steps mentioned in below link for manual installation",0.0,0.0,1.0,0.0
Zipkin,63219184,62894286,0,"2020/08/02, 20:39:09",False,"2020/08/02, 20:46:19",11.0,11424150.0,1,https://www.rabbitmq.com/install-windows-manual.html,0.0,0.0,1.0,0.0
Zipkin,61710527,61710421,0,"2020/05/10, 13:32:48",True,"2020/05/10, 13:32:48",,,0,That's an old implementation.,0.0,0.0,1.0,0.0
Zipkin,61710527,61710421,0,"2020/05/10, 13:32:48",True,"2020/05/10, 13:32:48",,,0,Below I have modified your code to work:,0.0,0.0,1.0,0.0
Zipkin,61710527,61710421,0,"2020/05/10, 13:32:48",True,"2020/05/10, 13:32:48",,,0,For more information check this link:  https://gist.github.com/marcingrzejszczak/d3c15a0c11dda71970e42c513c9c0e09,0.0,0.0,1.0,0.0
Zipkin,61332491,61198731,0,"2020/04/21, 00:41:04",False,"2020/04/21, 00:41:04",985.0,598932.0,0,From  zipkin docs :,0.0,0.0,1.0,0.0
Zipkin,61332491,61198731,0,"2020/04/21, 00:41:04",False,"2020/04/21, 00:41:04",985.0,598932.0,0,There is no support for TTL through this SpanStore.,0.185,0.227,0.588,0.128
Zipkin,61332491,61198731,0,"2020/04/21, 00:41:04",False,"2020/04/21, 00:41:04",985.0,598932.0,0,It is recommended instead to use Elastic Curator to remove indices older than the point you are interested in.,0.0,0.209,0.791,0.5423
Zipkin,62824310,60598519,0,"2020/07/10, 00:55:28",False,"2020/07/16, 00:09:39",1.0,13324871.0,0,"I had the same problem and solved it by:
Open windows task manger and kill all java instances java.exe or javaw.exe",0.279,0.079,0.642,-0.743
Zipkin,60213881,60170816,0,"2020/02/13, 19:56:57",False,"2020/02/13, 19:56:57",2576.0,12014434.0,0,I have finally figured out what could be the cause of this issue:,0.0,0.0,1.0,0.0
Zipkin,60213881,60170816,0,"2020/02/13, 19:56:57",False,"2020/02/13, 19:56:57",2576.0,12014434.0,0,The install option:,0.0,0.0,1.0,0.0
Zipkin,60213881,60170816,0,"2020/02/13, 19:56:57",False,"2020/02/13, 19:56:57",2576.0,12014434.0,0,--set values.tracing.provider=zipkin --set values.global.tracer.zipkin.address,0.0,0.0,1.0,0.0
Zipkin,60213881,60170816,0,"2020/02/13, 19:56:57",False,"2020/02/13, 19:56:57",2576.0,12014434.0,0,requires  &lt;zipkin-collector-service&gt;.&lt;zipkin-collector-namespace&gt;:9411  according to  istio  documentation.,0.0,0.0,1.0,0.0
Zipkin,60213881,60170816,0,"2020/02/13, 19:56:57",False,"2020/02/13, 19:56:57",2576.0,12014434.0,0,While You have just IP address and port of external server.,0.0,0.0,1.0,0.0
Zipkin,60213881,60170816,0,"2020/02/13, 19:56:57",False,"2020/02/13, 19:56:57",2576.0,12014434.0,0,This most likely means that the install option requires existing name that is in istio service mesh registry.,0.0,0.0,1.0,0.0
Zipkin,60213881,60170816,0,"2020/02/13, 19:56:57",False,"2020/02/13, 19:56:57",2576.0,12014434.0,0,"So if Your zipkin collector is outside cluster We need to add  ServiceEntry ,  VirtualService  and maybe  DestinationRule  and so the external service can be used within mesh.",0.0,0.0,1.0,0.0
Zipkin,60213881,60170816,0,"2020/02/13, 19:56:57",False,"2020/02/13, 19:56:57",2576.0,12014434.0,0,You can follow  istio  documentation to see how to create these objects for external service.,0.0,0.13,0.87,0.2732
Zipkin,60213881,60170816,0,"2020/02/13, 19:56:57",False,"2020/02/13, 19:56:57",2576.0,12014434.0,0,Here  is another guide.,0.0,0.0,1.0,0.0
Zipkin,60213881,60170816,0,"2020/02/13, 19:56:57",False,"2020/02/13, 19:56:57",2576.0,12014434.0,0,After that We need to update the tracer address value with the  VirtualService  as an endpoint.,0.0,0.138,0.862,0.34
Zipkin,60213881,60170816,0,"2020/02/13, 19:56:57",False,"2020/02/13, 19:56:57",2576.0,12014434.0,0,Hope this helps.,0.0,0.846,0.154,0.6705
Zipkin,59877579,59859303,0,"2020/01/23, 13:31:55",True,"2020/01/23, 13:31:55",2576.0,12014434.0,1,By using the following  commands  I was able to generate the manifests using  istioctl  with parameters You mentioned:,0.0,0.0,1.0,0.0
Zipkin,59877579,59859303,0,"2020/01/23, 13:31:55",True,"2020/01/23, 13:31:55",2576.0,12014434.0,1,Then compared them to see differences made with those parameter modifications.,0.0,0.0,1.0,0.0
Zipkin,59877579,59859303,0,"2020/01/23, 13:31:55",True,"2020/01/23, 13:31:55",2576.0,12014434.0,1,You can try to manually modify those applied settings or apply it to Your cluster.,0.0,0.0,1.0,0.0
Zipkin,59877579,59859303,0,"2020/01/23, 13:31:55",True,"2020/01/23, 13:31:55",2576.0,12014434.0,1,Istioctl I used to generate these manifests:,0.0,0.0,1.0,0.0
Zipkin,59877579,59859303,0,"2020/01/23, 13:31:55",True,"2020/01/23, 13:31:55",2576.0,12014434.0,1,Hope it helps.,0.0,0.846,0.154,0.6705
Zipkin,58754849,58619789,0,"2019/11/07, 20:21:08",False,"2019/11/07, 20:21:08",21.0,9936877.0,0,"Still with using 2.2.0 parent, I still face the whitelable error.",0.231,0.0,0.769,-0.4019
Zipkin,58754849,58619789,0,"2019/11/07, 20:21:08",False,"2019/11/07, 20:21:08",21.0,9936877.0,0,I will check on this latter but by changing the pom defination the Zipkin server work,0.0,0.0,1.0,0.0
Zipkin,58754849,58619789,0,"2019/11/07, 20:21:08",False,"2019/11/07, 20:21:08",21.0,9936877.0,0,And in zipkinserverapplication we need the @Enablezipkinserver,0.0,0.0,1.0,0.0
Zipkin,58755169,58619789,2,"2019/11/07, 20:47:05",False,"2019/11/07, 20:47:05",3589.0,5587542.0,0,Hi if your spring cloud app target is a Spring Boot 2.x base I suggest to do not try to use the @EnableZipkinServer because it is not a raccomanded way as the java doc suggest:,0.0,0.0,1.0,0.0
Zipkin,58755169,58619789,2,"2019/11/07, 20:47:05",False,"2019/11/07, 20:47:05",3589.0,5587542.0,0,form the Zipkin base code:,0.0,0.0,1.0,0.0
Zipkin,58755169,58619789,2,"2019/11/07, 20:47:05",False,"2019/11/07, 20:47:05",3589.0,5587542.0,0,I spoke for personal experience with spring boot application 2.x family.,0.0,0.0,1.0,0.0
Zipkin,58755169,58619789,2,"2019/11/07, 20:47:05",False,"2019/11/07, 20:47:05",3589.0,5587542.0,0,"The my solution,for development, was use a docker compose like below(consider that my application use spring cloud stream in order to push on zipkin the tracing information:",0.0,0.091,0.909,0.3612
Zipkin,58755169,58619789,2,"2019/11/07, 20:47:05",False,"2019/11/07, 20:47:05",3589.0,5587542.0,0,On the other hands if your target is a spring boot 1.5.x you can use the legacy embedded zipkin server like below:,0.0,0.111,0.889,0.3612
Zipkin,58755169,58619789,2,"2019/11/07, 20:47:05",False,"2019/11/07, 20:47:05",3589.0,5587542.0,0,POM:,0.0,0.0,1.0,0.0
Zipkin,58755169,58619789,2,"2019/11/07, 20:47:05",False,"2019/11/07, 20:47:05",3589.0,5587542.0,0,Application:,0.0,0.0,1.0,0.0
Zipkin,58755169,58619789,2,"2019/11/07, 20:47:05",False,"2019/11/07, 20:47:05",3589.0,5587542.0,0,package it.valeriovaudi.emarket;,0.0,0.0,1.0,0.0
Zipkin,58755169,58619789,2,"2019/11/07, 20:47:05",False,"2019/11/07, 20:47:05",3589.0,5587542.0,0,"both solution works for me, spring boot 1.5.x was the base spring boot version for my master thesis and spring boot 2.x version for a my personal distributed system project that I use every day for my persona family budget management.",0.0,0.057,0.943,0.3182
Zipkin,58755169,58619789,2,"2019/11/07, 20:47:05",False,"2019/11/07, 20:47:05",3589.0,5587542.0,0,I hope that this can help you,0.0,0.583,0.417,0.6808
Zipkin,58799041,58619789,0,"2019/11/11, 11:50:52",False,"2019/11/11, 11:50:52",21.0,9936877.0,0,For more details can read the document  https://www.baeldung.com/spring-boot-actuators#boot-2x-actuator,0.0,0.0,1.0,0.0
Zipkin,58298923,58294974,2,"2019/10/09, 10:18:06",True,"2019/10/09, 10:18:06",2382.0,4497840.0,2,I have a working project with spring cloud stream and zipkin using the following configuration (maybe you should set the sender.type):,0.0,0.0,1.0,0.0
Zipkin,58298923,58294974,2,"2019/10/09, 10:18:06",True,"2019/10/09, 10:18:06",2382.0,4497840.0,2,Hope this can help.,0.0,0.737,0.263,0.6808
Zipkin,58172731,58170335,2,"2019/09/30, 20:39:51",False,"2019/09/30, 20:39:51",81.0,2232476.0,0,"The problem lies in your  ES_HOSTS  variable, from the docs  here :",0.379,0.0,0.621,-0.6705
Zipkin,58172731,58170335,2,"2019/09/30, 20:39:51",False,"2019/09/30, 20:39:51",81.0,2232476.0,0,So you will need:  ES_HOSTS=http://storage:9200,0.0,0.0,1.0,0.0
Zipkin,58181776,58170335,0,"2019/10/01, 12:23:56",False,"2019/10/01, 12:23:56",35.0,9795454.0,0,Finally I have this file:,0.0,0.0,1.0,0.0
Zipkin,58181776,58170335,0,"2019/10/01, 12:23:56",False,"2019/10/01, 12:23:56",35.0,9795454.0,0,Main differences are the usage of,0.0,0.0,1.0,0.0
Zipkin,58181776,58170335,0,"2019/10/01, 12:23:56",False,"2019/10/01, 12:23:56",35.0,9795454.0,0,"""ES_HOSTS=elasticsearch:9300""",0.0,0.0,1.0,0.0
Zipkin,58181776,58170335,0,"2019/10/01, 12:23:56",False,"2019/10/01, 12:23:56",35.0,9795454.0,0,instead of,0.0,0.0,1.0,0.0
Zipkin,58181776,58170335,0,"2019/10/01, 12:23:56",False,"2019/10/01, 12:23:56",35.0,9795454.0,0,"""ES_HOSTS=storage:9300""",0.0,0.0,1.0,0.0
Zipkin,58181776,58170335,0,"2019/10/01, 12:23:56",False,"2019/10/01, 12:23:56",35.0,9795454.0,0,and in the dependencies configuration I add the entrypoint in dependencies:,0.0,0.0,1.0,0.0
Zipkin,58181776,58170335,0,"2019/10/01, 12:23:56",False,"2019/10/01, 12:23:56",35.0,9795454.0,0,"entrypoint: crond -f
  This one is really the key to not have the exception when I start docker-compose.",0.0,0.0,1.0,0.0
Zipkin,58181776,58170335,0,"2019/10/01, 12:23:56",False,"2019/10/01, 12:23:56",35.0,9795454.0,0,"To solve this issue, I check the this project:  https://github.com/openzipkin/docker-zipkin",0.0,0.184,0.816,0.2023
Zipkin,58181776,58170335,0,"2019/10/01, 12:23:56",False,"2019/10/01, 12:23:56",35.0,9795454.0,0,The remaining question is: why do I need to use entrypoint: crond -f,0.0,0.0,1.0,0.0
Zipkin,57287189,57284146,0,"2019/07/31, 12:12:05",False,"2019/07/31, 12:12:05",11.0,11602721.0,1,"I found examples from:
 https://github.com/openzipkin/zipkin/tree/master/zipkin-lens/testdata",0.0,0.0,1.0,0.0
Zipkin,57287189,57284146,0,"2019/07/31, 12:12:05",False,"2019/07/31, 12:12:05",11.0,11602721.0,1,It works well.,0.0,0.512,0.488,0.2732
Zipkin,57232063,57219354,0,"2019/07/27, 15:16:24",False,"2019/07/27, 15:16:24",1.0,11841253.0,0,"Set the datasource setting in the application.yml file of the application as follows,",0.0,0.0,1.0,0.0
Zipkin,57232063,57219354,0,"2019/07/27, 15:16:24",False,"2019/07/27, 15:16:24",1.0,11841253.0,0,You can add the zipkin attribute to POM.xml,0.0,0.0,1.0,0.0
Zipkin,57232063,57219354,0,"2019/07/27, 15:16:24",False,"2019/07/27, 15:16:24",1.0,11841253.0,0,Problems can occur due to Spring's auto configuration property.,0.252,0.0,0.748,-0.4019
Zipkin,57232063,57219354,0,"2019/07/27, 15:16:24",False,"2019/07/27, 15:16:24",1.0,11841253.0,0,"Therefore, modify the datasource setting as follows and modify the datasource configuration related source to make the application work normally.",0.0,0.0,1.0,0.0
Zipkin,56070984,56070700,5,"2019/05/10, 08:05:46",True,"2019/05/10, 08:05:46",12864.0,1413240.0,0,It looks like the trace is triggering an old-fashioned inverted-lock-order deadlock freezing threads that are attempting to acquire new Connections.,0.163,0.107,0.73,-0.0772
Zipkin,56070984,56070700,5,"2019/05/10, 08:05:46",True,"2019/05/10, 08:05:46",12864.0,1413240.0,0,The last of the three deadlocked threads is  trying to get a lock on some singleton or bean .,0.0,0.0,1.0,0.0
Zipkin,56070984,56070700,5,"2019/05/10, 08:05:46",True,"2019/05/10, 08:05:46",12864.0,1413240.0,0,It has already passed through and presumably acquired a lock on a  GenericScope .,0.0,0.0,1.0,0.0
Zipkin,56070984,56070700,5,"2019/05/10, 08:05:46",True,"2019/05/10, 08:05:46",12864.0,1413240.0,0,"The other two threads are  trying to acquire a lock on a  GenericScope , which presumably the first thread has.",0.0,0.0,1.0,0.0
Zipkin,56070984,56070700,5,"2019/05/10, 08:05:46",True,"2019/05/10, 08:05:46",12864.0,1413240.0,0,An unexpected reentrance from the  zipkin  code into spring is generating a deadlock.,0.179,0.0,0.821,-0.34
Zipkin,56070984,56070700,5,"2019/05/10, 08:05:46",True,"2019/05/10, 08:05:46",12864.0,1413240.0,0,"c3p0  has a fixed-size thread pool that notices when all its threads (just 3 here,  c3p0 's default) are persistently frozen, then (pretty correctly in this case) declares a deadlock and replaces the blocked threads in hopes of recovering.",0.112,0.069,0.819,-0.1779
Zipkin,56070984,56070700,5,"2019/05/10, 08:05:46",True,"2019/05/10, 08:05:46",12864.0,1413240.0,0,Does c3p0 recover?,0.0,0.0,1.0,0.0
Zipkin,56070984,56070700,5,"2019/05/10, 08:05:46",True,"2019/05/10, 08:05:46",12864.0,1413240.0,0,Is this a rare or frequent deadlock?,0.324,0.0,0.676,-0.34
Zipkin,56070984,56070700,5,"2019/05/10, 08:05:46",True,"2019/05/10, 08:05:46",12864.0,1413240.0,0,"There's not much you can easily do to prevent this deadlock, I think either you'll have to tolerate it or do without the instrumentation.",0.105,0.133,0.762,-0.0644
Zipkin,55801244,55512192,0,"2019/04/23, 00:16:47",False,"2019/04/23, 00:16:47",630.0,10676678.0,2,Application Insights users would also be able to leverage the distributed tracing offered through Zipkin by instrumenting their services using existing libraries.,0.0,0.0,1.0,0.0
Zipkin,55801244,55512192,0,"2019/04/23, 00:16:47",False,"2019/04/23, 00:16:47",630.0,10676678.0,2,"To use the Application Insights back-end store, configure your Zipkin server instance to use the Application Insights  plug-in .",0.0,0.0,1.0,0.0
Zipkin,55801244,55512192,0,"2019/04/23, 00:16:47",False,"2019/04/23, 00:16:47",630.0,10676678.0,2,This integration makes monitoring and debugging your overall end-to-end applications much easier.,0.0,0.203,0.797,0.4215
Zipkin,55801244,55512192,0,"2019/04/23, 00:16:47",False,"2019/04/23, 00:16:47",630.0,10676678.0,2,"Once you have the data in Application Insights, you can always perform  cross-resource log queries  between Application Insights and Log Analytics.",0.0,0.0,1.0,0.0
Zipkin,55801244,55512192,0,"2019/04/23, 00:16:47",False,"2019/04/23, 00:16:47",630.0,10676678.0,2,Additional Documentation Reference -,0.0,0.0,1.0,0.0
Zipkin,55801244,55512192,0,"2019/04/23, 00:16:47",False,"2019/04/23, 00:16:47",630.0,10676678.0,2,Zipkin to Application Insights Module,0.0,0.0,1.0,0.0
Zipkin,55801244,55512192,0,"2019/04/23, 00:16:47",False,"2019/04/23, 00:16:47",630.0,10676678.0,2,Zipkin-Azure,0.0,0.0,1.0,0.0
Zipkin,55801244,55512192,0,"2019/04/23, 00:16:47",False,"2019/04/23, 00:16:47",630.0,10676678.0,2,Send Log Data to Azure Monitor with HTTP Data Collector API (public preview),0.0,0.0,1.0,0.0
Zipkin,55801244,55512192,0,"2019/04/23, 00:16:47",False,"2019/04/23, 00:16:47",630.0,10676678.0,2,Hope the above information helps.,0.0,0.647,0.353,0.6705
Zipkin,55535480,55364711,0,"2019/04/05, 15:32:51",True,"2019/04/05, 15:32:51",8336.0,1773866.0,1,No you can't.,0.524,0.0,0.476,-0.296
Zipkin,55535480,55364711,0,"2019/04/05, 15:32:51",True,"2019/04/05, 15:32:51",8336.0,1773866.0,1,You can use tools like Elasticsearch Logstash Kibana to visualize it.,0.0,0.2,0.8,0.3612
Zipkin,55535480,55364711,0,"2019/04/05, 15:32:51",True,"2019/04/05, 15:32:51",8336.0,1773866.0,1,"You can go to my repo  https://github.com/marcingrzejszczak/docker-elk  and run  ./   getReadyForConference.sh , it will start docker containers with the ELK stack, run the apps, curl the request to the apps so that you can then check them in ELK.",0.0,0.0,1.0,0.0
Zipkin,54758616,54758615,0,"2019/02/19, 05:51:06",True,"2019/02/19, 05:51:06",3909.0,3067542.0,1,"Alright, so after a few hours struggling, I made some progress, and now the app starts - even though the root cause of the issue is not fully clear to me at this time.",0.141,0.13,0.73,-0.1029
Zipkin,54758616,54758615,0,"2019/02/19, 05:51:06",True,"2019/02/19, 05:51:06",3909.0,3067542.0,1,Below are my findings :,0.0,0.0,1.0,0.0
Zipkin,54758616,54758615,0,"2019/02/19, 05:51:06",True,"2019/02/19, 05:51:06",3909.0,3067542.0,1,"one strange thing I noticed : if I change the  sender.type  from  web  to  rabbit , then the application starts with no error.",0.295,0.0,0.705,-0.6908
Zipkin,54758616,54758615,0,"2019/02/19, 05:51:06",True,"2019/02/19, 05:51:06",3909.0,3067542.0,1,"I also found this Spring Boot  issue report , very similar to mine, that was pointing at a JDK bug.",0.0,0.0,1.0,0.0
Zipkin,54758616,54758615,0,"2019/02/19, 05:51:06",True,"2019/02/19, 05:51:06",3909.0,3067542.0,1,"And indeed, upgrading from  jdk1.8.0_25  to  jdk1.8.0_201  .",0.0,0.0,1.0,0.0
Zipkin,54758616,54758615,0,"2019/02/19, 05:51:06",True,"2019/02/19, 05:51:06",3909.0,3067542.0,1,"Finally, I also found that if I was using  jdk1.8.0_25  and wasn't providing the  sender.type  at all, then the app was also starting with no issue.",0.087,0.0,0.913,-0.296
Zipkin,54758616,54758615,0,"2019/02/19, 05:51:06",True,"2019/02/19, 05:51:06",3909.0,3067542.0,1,"For some reason, in the other app that I have and that works, I am able to use  jdk1.8.0_25  and  sender.type: web",0.0,0.0,1.0,0.0
Zipkin,54758616,54758615,0,"2019/02/19, 05:51:06",True,"2019/02/19, 05:51:06",3909.0,3067542.0,1,"If anyone has a methodology to figure out this kind of issue quickly, don't hesitate to add it in the comment or edit this answer.",0.0,0.073,0.927,0.2057
Zipkin,54766304,54645785,3,"2019/02/19, 14:25:08",True,"2019/02/19, 14:25:08",8336.0,1773866.0,0,It makes perfect sense that it's  null .,0.0,0.381,0.619,0.5719
Zipkin,54766304,54645785,3,"2019/02/19, 14:25:08",True,"2019/02/19, 14:25:08",8336.0,1773866.0,0,That's because YOU control the way what happens with the caught exception.,0.0,0.0,1.0,0.0
Zipkin,54766304,54645785,3,"2019/02/19, 14:25:08",True,"2019/02/19, 14:25:08",8336.0,1773866.0,0,"In your case, nothing, cause you swallow that exception.",0.0,0.0,1.0,0.0
Zipkin,54766304,54645785,3,"2019/02/19, 14:25:08",True,"2019/02/19, 14:25:08",8336.0,1773866.0,0,"If you want to do sth better, just add the error tag manually via the  SpanCustomizer .",0.136,0.211,0.653,0.128
Zipkin,54766304,54645785,3,"2019/02/19, 14:25:08",True,"2019/02/19, 14:25:08",8336.0,1773866.0,0,That way you'll add the exception to the given span.,0.0,0.0,1.0,0.0
Zipkin,54766304,54645785,3,"2019/02/19, 14:25:08",True,"2019/02/19, 14:25:08",8336.0,1773866.0,0,It will then automatically get closed and reported to Zipkin (you can do sth else than  ex.toString()  of course.,0.0,0.0,1.0,0.0
Zipkin,54555743,54555618,0,"2019/02/06, 16:19:12",False,"2019/02/06, 23:57:39",2802.0,2810730.0,1,Zipkin  is a solution for distributed tracing.,0.0,0.315,0.685,0.3182
Zipkin,54555743,54555618,0,"2019/02/06, 16:19:12",False,"2019/02/06, 23:57:39",2802.0,2810730.0,1,Specifically it allows to track latency problems in distributed system.,0.231,0.0,0.769,-0.4019
Zipkin,54555743,54555618,0,"2019/02/06, 16:19:12",False,"2019/02/06, 23:57:39",2802.0,2810730.0,1,Also it's a greate tool for debugging/investigating problems in your application.,0.231,0.0,0.769,-0.4019
Zipkin,54555743,54555618,0,"2019/02/06, 16:19:12",False,"2019/02/06, 23:57:39",2802.0,2810730.0,1,So by definition it requires to collect successful and failed traces.,0.205,0.236,0.559,0.128
Zipkin,54555743,54555618,0,"2019/02/06, 16:19:12",False,"2019/02/06, 23:57:39",2802.0,2810730.0,1,However  traces  have nothing to do with logging.,0.0,0.0,1.0,0.0
Zipkin,54555743,54555618,0,"2019/02/06, 16:19:12",False,"2019/02/06, 23:57:39",2802.0,2810730.0,1,"Assuming you mean controlling the logging level of Zipkin server, then you can just set it using  --logging.level.zipkin2=INFO .",0.0,0.0,1.0,0.0
Zipkin,54555923,54555618,0,"2019/02/06, 16:29:05",False,"2019/02/06, 16:29:05",8336.0,1773866.0,0,I don't understand the problem.,0.0,0.429,0.571,0.3089
Zipkin,54555923,54555618,0,"2019/02/06, 16:29:05",False,"2019/02/06, 16:29:05",8336.0,1773866.0,0,You don't send logs to Zipkin.,0.0,0.0,1.0,0.0
Zipkin,54555923,54555618,0,"2019/02/06, 16:29:05",False,"2019/02/06, 16:29:05",8336.0,1773866.0,0,You send spans to Zipkin.,0.0,0.0,1.0,0.0
Zipkin,54555923,54555618,0,"2019/02/06, 16:29:05",False,"2019/02/06, 16:29:05",8336.0,1773866.0,0,Zipkin has nothing to do with logs.,0.0,0.0,1.0,0.0
Zipkin,54468173,54466528,0,"2019/01/31, 21:45:59",True,"2019/01/31, 21:45:59",2296.0,1575416.0,2,Seems to work once I added the Web package.,0.0,0.0,1.0,0.0
Zipkin,54468173,54466528,0,"2019/01/31, 21:45:59",True,"2019/01/31, 21:45:59",2296.0,1575416.0,2,Though I don't recall it being needed previously.,0.0,0.0,1.0,0.0
Zipkin,53468275,53428788,0,"2018/11/25, 16:03:42",False,"2018/11/25, 16:03:42",1767.0,2498986.0,0,"Zipkin currently supports four types of backend storage to store spans in-memory, MySQl, ElasticSearch, Cassandra.",0.0,0.152,0.848,0.3612
Zipkin,53468275,53428788,0,"2018/11/25, 16:03:42",False,"2018/11/25, 16:03:42",1767.0,2498986.0,0,Although for production it is recommended to use ES or Cassandra.,0.0,0.153,0.847,0.2023
Zipkin,53468275,53428788,0,"2018/11/25, 16:03:42",False,"2018/11/25, 16:03:42",1767.0,2498986.0,0,The other two can be used for learning and understanding.,0.0,0.0,1.0,0.0
Zipkin,53468275,53428788,0,"2018/11/25, 16:03:42",False,"2018/11/25, 16:03:42",1767.0,2498986.0,0,Traces stored in the in-memory is ephemeral and won't be available after the restart.,0.0,0.0,1.0,0.0
Zipkin,53468275,53428788,0,"2018/11/25, 16:03:42",False,"2018/11/25, 16:03:42",1767.0,2498986.0,0,"In the zipkin UI there is an option to see the trace and download it, which can be used to view at a later point in time.",0.0,0.0,1.0,0.0
Zipkin,53468275,53428788,0,"2018/11/25, 16:03:42",False,"2018/11/25, 16:03:42",1767.0,2498986.0,0,If you still have further questions drop in to the zipkin  gitter  channel.,0.149,0.0,0.851,-0.2732
Zipkin,53221841,53218692,0,"2018/11/09, 10:01:09",False,"2018/11/09, 10:01:09",136.0,1433218.0,0,we also use use zipkin but can't query with zipkin as elk.,0.0,0.0,1.0,0.0
Zipkin,53221841,53218692,0,"2018/11/09, 10:01:09",False,"2018/11/09, 10:01:09",136.0,1433218.0,0,we can just click on each services which are display on zipkin and get more info as below image.,0.0,0.0,1.0,0.0
Zipkin,53236126,53218692,1,"2018/11/10, 07:02:05",False,"2018/11/10, 07:02:05",1767.0,2498986.0,0,Zipkin is not a business transaction tracking system and it should not be used that way because it is not built for this purpose.,0.0,0.0,1.0,0.0
Zipkin,53236126,53218692,1,"2018/11/10, 07:02:05",False,"2018/11/10, 07:02:05",1767.0,2498986.0,0,There are other tools which are built specifically to cater the needs to business operations which you must consider.,0.0,0.0,1.0,0.0
Zipkin,53236126,53218692,1,"2018/11/10, 07:02:05",False,"2018/11/10, 07:02:05",1767.0,2498986.0,0,P.S.,0.0,0.0,1.0,0.0
Zipkin,53236126,53218692,1,"2018/11/10, 07:02:05",False,"2018/11/10, 07:02:05",1767.0,2498986.0,0,I am a Zipkin contributor.,0.0,0.0,1.0,0.0
Zipkin,53932587,53218692,0,"2018/12/26, 15:12:50",False,"2018/12/26, 15:12:50",3071.0,2979435.0,0,This is not an answer to how achieve this with zipkin but yes for the whole problem.,0.161,0.161,0.679,0.0
Zipkin,53932587,53218692,0,"2018/12/26, 15:12:50",False,"2018/12/26, 15:12:50",3071.0,2979435.0,0,If you have a  transaction that didn't complete it's steps then you probably have two of following issues,0.0,0.0,1.0,0.0
Zipkin,53932587,53218692,0,"2018/12/26, 15:12:50",False,"2018/12/26, 15:12:50",3071.0,2979435.0,0,Some microservice failed to deliver the event to the next one and didn't figure it out,0.18,0.0,0.82,-0.5106
Zipkin,53932587,53218692,0,"2018/12/26, 15:12:50",False,"2018/12/26, 15:12:50",3071.0,2979435.0,0,"You have to make sure delivery at least once here, using Kafka you have to wait until message get flushed to the server for example",0.0,0.087,0.913,0.3182
Zipkin,53932587,53218692,0,"2018/12/26, 15:12:50",False,"2018/12/26, 15:12:50",3071.0,2979435.0,0,The destiny microservice received the message and is not processing it,0.0,0.0,1.0,0.0
Zipkin,53932587,53218692,0,"2018/12/26, 15:12:50",False,"2018/12/26, 15:12:50",3071.0,2979435.0,0,"You have to make sure you application is processing what it's supposed to,  you can monitor the database if the transactions are there or use some tool like LinkedIn burrow to monitor your Kafka message group if you are integrating by using Kafka.",0.0,0.105,0.895,0.5859
Zipkin,53932587,53218692,0,"2018/12/26, 15:12:50",False,"2018/12/26, 15:12:50",3071.0,2979435.0,0,"Conclusion is, instead to try monitor all the thing once it looks like creating specialist monitors for every step will be more assertive and simple to develop.",0.0,0.158,0.842,0.5719
Zipkin,52975826,52972425,3,"2018/10/24, 21:33:48",False,"2018/10/24, 21:33:48",8295.0,4550110.0,0,Here is the related issue:,0.0,0.0,1.0,0.0
Zipkin,52975826,52972425,3,"2018/10/24, 21:33:48",False,"2018/10/24, 21:33:48",8295.0,4550110.0,0,https://github.com/openzipkin/zipkin/issues/1939,0.0,0.0,1.0,0.0
Zipkin,52987258,52972425,0,"2018/10/25, 13:38:05",False,"2018/10/25, 13:38:05",1.0,10552245.0,0,"I opened a issue on the zipkin github, a theme already being treated as a bug.",0.0,0.0,1.0,0.0
Zipkin,52987258,52972425,0,"2018/10/25, 13:38:05",False,"2018/10/25, 13:38:05",1.0,10552245.0,0,"Initial thread:
 https://github.com/openzipkin/zipkin/issues/2218#issuecomment-432876510",0.0,0.0,1.0,0.0
Zipkin,52987258,52972425,0,"2018/10/25, 13:38:05",False,"2018/10/25, 13:38:05",1.0,10552245.0,0,"Bug track:
 https://github.com/openzipkin/zipkin/issues/2219",0.0,0.0,1.0,0.0
Zipkin,52987258,52972425,0,"2018/10/25, 13:38:05",False,"2018/10/25, 13:38:05",1.0,10552245.0,0,Tks for all!,0.0,0.0,1.0,0.0
Zipkin,52815354,52706088,1,"2018/10/15, 14:07:16",False,"2018/10/15, 14:07:16",201.0,7956609.0,2,You have to use  spring.sleuth.web.skipPattern,0.0,0.0,1.0,0.0
Zipkin,52815354,52706088,1,"2018/10/15, 14:07:16",False,"2018/10/15, 14:07:16",201.0,7956609.0,2,sample you will get here  https://www.baeldung.com/tracing-services-with-zipkin,0.0,0.0,1.0,0.0
Zipkin,52739617,52424864,0,"2018/10/10, 14:51:01",False,"2018/10/10, 14:51:01",201.0,7956609.0,0,I think to remove the service names from zipkin you have to Re-deploy the zipkin service,0.0,0.0,1.0,0.0
Zipkin,52776732,52424864,0,"2018/10/12, 12:41:59",True,"2018/10/12, 12:41:59",131.0,6236211.0,0,You will need to remove the spring.zipkin.base-url property from the corresponding applications to remove it from zipkin server list.,0.0,0.0,1.0,0.0
Zipkin,52608036,51661009,0,"2018/10/02, 15:08:15",True,"2018/10/02, 15:08:15",153.0,2999097.0,0,"finally got working after spring verison updated to  5.x 
It already have  Brave Instrument for zipkin trace",0.0,0.175,0.825,0.5267
Zipkin,51580557,51578263,2,"2018/07/29, 15:58:11",True,"2018/07/29, 15:58:11",8336.0,1773866.0,2,If you read the docs or any information starting from edgware you would see that we've removed that support.,0.0,0.13,0.87,0.4019
Zipkin,51580557,51578263,2,"2018/07/29, 15:58:11",True,"2018/07/29, 15:58:11",8336.0,1773866.0,2,You should use native zipkin rabbit / kafka dependencies.,0.0,0.0,1.0,0.0
Zipkin,51580557,51578263,2,"2018/07/29, 15:58:11",True,"2018/07/29, 15:58:11",8336.0,1773866.0,2,Everything is there in the docs.,0.0,0.0,1.0,0.0
Zipkin,51438611,51438273,1,"2018/07/20, 11:46:33",False,"2018/07/20, 11:46:33",8336.0,1773866.0,1,If it comes from the  @Scheduled  method then you can use  https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/instrument/scheduling/SleuthSchedulingProperties.java#L38  ( spring.sleuth.scheduled.skipPattern ) to find the thread and disable it.,0.0,0.0,1.0,0.0
Zipkin,51438611,51438273,1,"2018/07/20, 11:46:33",False,"2018/07/20, 11:46:33",8336.0,1773866.0,1,If you say its name is  async  then it means that it comes from a  TraceRunnable  or  TraceCallable .,0.0,0.0,1.0,0.0
Zipkin,51438611,51438273,1,"2018/07/20, 11:46:33",False,"2018/07/20, 11:46:33",8336.0,1773866.0,1,That can be problematic to get rid off.,0.293,0.0,0.707,-0.4404
Zipkin,51438611,51438273,1,"2018/07/20, 11:46:33",False,"2018/07/20, 11:46:33",8336.0,1773866.0,1,You can file an issue in Sleuth to allow  SpanAdjuster  to actually not send spans to Zipkin (by for example returning  null ).,0.0,0.079,0.921,0.2263
Zipkin,51438611,51438273,1,"2018/07/20, 11:46:33",False,"2018/07/20, 11:46:33",8336.0,1773866.0,1,You can also try to disable async at all  spring.sleuth.async.enabled .,0.0,0.0,1.0,0.0
Zipkin,51438611,51438273,1,"2018/07/20, 11:46:33",False,"2018/07/20, 11:46:33",8336.0,1773866.0,1,If you're not using any other features of async that should not interfere.,0.0,0.0,1.0,0.0
Zipkin,51354519,51244941,1,"2018/07/16, 07:17:28",False,"2018/07/16, 07:17:28",1767.0,2498986.0,0,Brave will work regardless of the server that you choose to use.,0.0,0.236,0.764,0.5267
Zipkin,51354519,51244941,1,"2018/07/16, 07:17:28",False,"2018/07/16, 07:17:28",1767.0,2498986.0,0,Remove the jetty configuration from the pom file and use the Tomcat.,0.0,0.0,1.0,0.0
Zipkin,51354519,51244941,1,"2018/07/16, 07:17:28",False,"2018/07/16, 07:17:28",1767.0,2498986.0,0,If you still have trouble or want to know more about zipkin/brave connect to the community via the gitter channel.,0.123,0.059,0.818,-0.34
Zipkin,51354519,51244941,1,"2018/07/16, 07:17:28",False,"2018/07/16, 07:17:28",1767.0,2498986.0,0,P.S.,0.0,0.0,1.0,0.0
Zipkin,51354519,51244941,1,"2018/07/16, 07:17:28",False,"2018/07/16, 07:17:28",1767.0,2498986.0,0,I contribute to OpenZipkin (Zipkin),0.0,0.0,1.0,0.0
Zipkin,51141657,51068201,0,"2018/07/02, 21:12:12",False,"2018/07/02, 21:12:12",81.0,2232476.0,0,"The UI cannot be password protected without also password protecting the API endpoints, including the one you would send your spans to.",0.103,0.0,0.897,-0.3412
Zipkin,51141657,51068201,0,"2018/07/02, 21:12:12",False,"2018/07/02, 21:12:12",81.0,2232476.0,0,PS: The  @EnableZipkinServer  annotation has been deprecated,0.0,0.0,1.0,0.0
Zipkin,50813959,50813714,8,"2018/06/12, 12:39:34",True,"2018/06/26, 16:13:12",8336.0,1773866.0,1,EDGWARE,0.0,0.0,1.0,0.0
Zipkin,50813959,50813714,8,"2018/06/12, 12:39:34",True,"2018/06/26, 16:13:12",8336.0,1773866.0,1,Have you read the documentation?,0.0,0.0,1.0,0.0
Zipkin,50813959,50813714,8,"2018/06/12, 12:39:34",True,"2018/06/26, 16:13:12",8336.0,1773866.0,1,If you use Spring Cloud Sleuth in Edgware version if you read the Sleuth section you would find this piece of the documentation  https://cloud.spring.io/spring-cloud-static/Edgware.SR3/single/spring-cloud.html#_custom_sa_tag_in_zipkin,0.0,0.0,1.0,0.0
Zipkin,50813959,50813714,8,"2018/06/12, 12:39:34",True,"2018/06/26, 16:13:12",8336.0,1773866.0,1,Let me copy that for you,0.0,0.0,1.0,0.0
Zipkin,50813959,50813714,8,"2018/06/12, 12:39:34",True,"2018/06/26, 16:13:12",8336.0,1773866.0,1,"54.5 Custom SA tag in Zipkin Sometimes you want to create a manual Span that will wrap a call to an external service which is not
  instrumented.",0.0,0.129,0.871,0.34
Zipkin,50813959,50813714,8,"2018/06/12, 12:39:34",True,"2018/06/26, 16:13:12",8336.0,1773866.0,1,"What you can do is to create a span with the
  peer.service tag that will contain a value of the service that you
  want to call.",0.0,0.216,0.784,0.5859
Zipkin,50813959,50813714,8,"2018/06/12, 12:39:34",True,"2018/06/26, 16:13:12",8336.0,1773866.0,1,"Below you can see an example of a call to Redis that is
  wrapped in such a span.",0.0,0.0,1.0,0.0
Zipkin,50813959,50813714,8,"2018/06/12, 12:39:34",True,"2018/06/26, 16:13:12",8336.0,1773866.0,1,"[Important]   Important Remember not to add both peer.service tag and
  the SA tag!",0.0,0.148,0.852,0.2714
Zipkin,50813959,50813714,8,"2018/06/12, 12:39:34",True,"2018/06/26, 16:13:12",8336.0,1773866.0,1,You have to add only peer.service.,0.0,0.0,1.0,0.0
Zipkin,50813959,50813714,8,"2018/06/12, 12:39:34",True,"2018/06/26, 16:13:12",8336.0,1773866.0,1,FINCHLEY,0.0,0.0,1.0,0.0
Zipkin,50813959,50813714,8,"2018/06/12, 12:39:34",True,"2018/06/26, 16:13:12",8336.0,1773866.0,1,The  SA  tag will not work for Finchley.,0.0,0.0,1.0,0.0
Zipkin,50813959,50813714,8,"2018/06/12, 12:39:34",True,"2018/06/26, 16:13:12",8336.0,1773866.0,1,You have to do it in the following manner using the  remoteEndpoint  on the span.,0.0,0.0,1.0,0.0
Zipkin,50798776,50796087,4,"2018/06/11, 16:23:06",True,"2018/06/11, 16:39:48",8336.0,1773866.0,1,That was a bug in Spring Cloud Sleuth in Edgware.,0.0,0.0,1.0,0.0
Zipkin,50798776,50796087,4,"2018/06/11, 16:23:06",True,"2018/06/11, 16:39:48",8336.0,1773866.0,1,The Stream Kafka Binder in Edgware required explicit passing of headers that should get propagated.,0.0,0.0,1.0,0.0
Zipkin,50798776,50796087,4,"2018/06/11, 16:23:06",True,"2018/06/11, 16:39:48",8336.0,1773866.0,1,The side effect of adding  sleuth-stream  on the classpath was exactly that feature.,0.0,0.0,1.0,0.0
Zipkin,50798776,50796087,4,"2018/06/11, 16:23:06",True,"2018/06/11, 16:39:48",8336.0,1773866.0,1,By fixing the  https://github.com/spring-cloud/spring-cloud-sleuth/issues/1005  issue we're adding back the missing feature to core.,0.155,0.0,0.845,-0.296
Zipkin,50798776,50796087,4,"2018/06/11, 16:23:06",True,"2018/06/11, 16:39:48",8336.0,1773866.0,1,This is not ported to Finchley since Stream Kafka Binder in Finchley passes all headers by default.,0.0,0.0,1.0,0.0
Zipkin,50798776,50796087,4,"2018/06/11, 16:23:06",True,"2018/06/11, 16:39:48",8336.0,1773866.0,1,The workaround for Edgware is to pass a list of headers in the following manner:,0.0,0.0,1.0,0.0
Zipkin,50626312,50599475,0,"2018/05/31, 17:09:32",True,"2018/05/31, 17:09:32",3141.0,553720.0,0,The Istio sidecar proxy (Envoy) generates the first headers.,0.0,0.0,1.0,0.0
Zipkin,50626312,50599475,0,"2018/05/31, 17:09:32",True,"2018/05/31, 17:09:32",3141.0,553720.0,0,According to  https://www.envoyproxy.io/docs/envoy/latest/configuration/http_conn_man/headers#x-request-id :,0.0,0.0,1.0,0.0
Zipkin,50626312,50599475,0,"2018/05/31, 17:09:32",True,"2018/05/31, 17:09:32",3141.0,553720.0,0,Envoy will generate an x-request-id header for all external origin requests (the header is sanitized).,0.0,0.0,1.0,0.0
Zipkin,50626312,50599475,0,"2018/05/31, 17:09:32",True,"2018/05/31, 17:09:32",3141.0,553720.0,0,It will also generate an x-request-id header for internal requests that do not already have one.,0.0,0.0,1.0,0.0
Zipkin,50010347,50010083,9,"2018/04/24, 23:34:00",True,"2018/04/24, 23:34:00",8336.0,1773866.0,3,You've mixed almost everything you could have mixed.,0.0,0.0,1.0,0.0
Zipkin,50010347,50010083,9,"2018/04/24, 23:34:00",True,"2018/04/24, 23:34:00",8336.0,1773866.0,3,On the app side you're using both the deprecated zipkin server and the deprecated client.,0.0,0.0,1.0,0.0
Zipkin,50010347,50010083,9,"2018/04/24, 23:34:00",True,"2018/04/24, 23:34:00",8336.0,1773866.0,3,On the server side you're using deprecated zipkin server.,0.0,0.0,1.0,0.0
Zipkin,50010347,50010083,9,"2018/04/24, 23:34:00",True,"2018/04/24, 23:34:00",8336.0,1773866.0,3,My suggestion is that you go through the documentation  https://cloud.spring.io/spring-cloud-static/Edgware.SR3/single/spring-cloud.html#_spring_cloud_sleuth  and read that the  stream servers  are deprecated and you should use the openzipkin zipkin server with rabbitmq support ( https://github.com/openzipkin/zipkin/tree/master/zipkin-collector/rabbitmq ).,0.0,0.083,0.917,0.4019
Zipkin,50010347,50010083,9,"2018/04/24, 23:34:00",True,"2018/04/24, 23:34:00",8336.0,1773866.0,3,On the consumer side use  https://cloud.spring.io/spring-cloud-static/Edgware.SR3/single/spring-cloud.html#_sleuth_with_zipkin_via_rabbitmq_or_kafka  .,0.0,0.0,1.0,0.0
Zipkin,50010347,50010083,9,"2018/04/24, 23:34:00",True,"2018/04/24, 23:34:00",8336.0,1773866.0,3,It really is as simple as that.,0.0,0.0,1.0,0.0
Zipkin,50010347,50010083,9,"2018/04/24, 23:34:00",True,"2018/04/24, 23:34:00",8336.0,1773866.0,3,Also don't forget to turn on the sampling percentage to 1.0,0.0,0.143,0.857,0.1695
Zipkin,53221956,49280873,0,"2018/11/09, 10:11:00",False,"2018/11/09, 10:11:00",136.0,1433218.0,0,"Just add below, it need to be working,",0.0,0.0,1.0,0.0
Zipkin,48982608,48518928,0,"2018/02/26, 08:26:46",False,"2018/02/26, 08:26:46",181.0,6708214.0,0,"Yeah,you should use different libraries for different languages.",0.0,0.0,1.0,0.0
Zipkin,48982608,48518928,0,"2018/02/26, 08:26:46",False,"2018/02/26, 08:26:46",181.0,6708214.0,0,"Brave for Java,Zipkin4net for C# and so on.",0.0,0.327,0.673,0.5267
Zipkin,48982608,48518928,0,"2018/02/26, 08:26:46",False,"2018/02/26, 08:26:46",181.0,6708214.0,0,"For more details,you can visit Zipkin official site:  Zipkin Existing instrumentations .",0.0,0.0,1.0,0.0
Zipkin,48982608,48518928,0,"2018/02/26, 08:26:46",False,"2018/02/26, 08:26:46",181.0,6708214.0,0,Then all you shoud do is following the librarie guide.,0.0,0.0,1.0,0.0
Zipkin,48982608,48518928,0,"2018/02/26, 08:26:46",False,"2018/02/26, 08:26:46",181.0,6708214.0,0,Have fun!,0.0,0.782,0.218,0.5562
Zipkin,48346769,48160588,0,"2018/01/19, 19:46:45",False,"2018/01/19, 19:46:45",1207.0,7376337.0,0,The first request uses v1 of the Zipkin api while the second uses v2 (see  https://github.com/openzipkin/zipkin/issues/1499  for the v2 specification).,0.0,0.0,1.0,0.0
Zipkin,48346769,48160588,0,"2018/01/19, 19:46:45",False,"2018/01/19, 19:46:45",1207.0,7376337.0,0,"Spans are broken up by kind (SERVER and CLIENT) instead of having client receive, server receive, client send, and server send annotations (hence why there are more spans).",0.095,0.105,0.8,0.0772
Zipkin,47664992,47660900,0,"2017/12/06, 02:38:32",True,"2017/12/06, 02:38:32",8336.0,1773866.0,1,I have a client application with multiple channels as SOURCE/SINK.,0.0,0.0,1.0,0.0
Zipkin,47664992,47660900,0,"2017/12/06, 02:38:32",True,"2017/12/06, 02:38:32",8336.0,1773866.0,1,I want to send logs to Zipkin server.,0.0,0.178,0.822,0.0772
Zipkin,47664992,47660900,0,"2017/12/06, 02:38:32",True,"2017/12/06, 02:38:32",8336.0,1773866.0,1,Zipkin is not a tool to store logs,0.0,0.0,1.0,0.0
Zipkin,47664992,47660900,0,"2017/12/06, 02:38:32",True,"2017/12/06, 02:38:32",8336.0,1773866.0,1,"According to my understanding, if spring finds spring cloud stream in classpath, Zipkin client defaults to messaging instead of sending logs through HTTP.",0.0,0.0,1.0,0.0
Zipkin,47664992,47660900,0,"2017/12/06, 02:38:32",True,"2017/12/06, 02:38:32",8336.0,1773866.0,1,No - you need the  sleuth-stream  dependency on the client side and the  zipkin-stream  dependency on the server side (which got deprecated and you should start using the inbuilt rabbitmq support from Zipkin).,0.063,0.077,0.86,0.128
Zipkin,47664992,47660900,0,"2017/12/06, 02:38:32",True,"2017/12/06, 02:38:32",8336.0,1773866.0,1,At client side: Q1.,0.0,0.0,1.0,0.0
Zipkin,47664992,47660900,0,"2017/12/06, 02:38:32",True,"2017/12/06, 02:38:32",8336.0,1773866.0,1,Is there an automatic configuration for zipkin rabbit binding in such scenario?,0.0,0.0,1.0,0.0
Zipkin,47664992,47660900,0,"2017/12/06, 02:38:32",True,"2017/12/06, 02:38:32",8336.0,1773866.0,1,"If not, what is default channel name of zipkin SOURCE channel?",0.0,0.0,1.0,0.0
Zipkin,47664992,47660900,0,"2017/12/06, 02:38:32",True,"2017/12/06, 02:38:32",8336.0,1773866.0,1,"Yes, there is.",0.0,0.574,0.426,0.4019
Zipkin,47664992,47660900,0,"2017/12/06, 02:38:32",True,"2017/12/06, 02:38:32",8336.0,1773866.0,1,The channel is  sleuth,0.0,0.0,1.0,0.0
Zipkin,47664992,47660900,0,"2017/12/06, 02:38:32",True,"2017/12/06, 02:38:32",8336.0,1773866.0,1,Q2.,0.0,0.0,1.0,0.0
Zipkin,47664992,47660900,0,"2017/12/06, 02:38:32",True,"2017/12/06, 02:38:32",8336.0,1773866.0,1,Do I need to configure defaultSampler to AlwaysSampler()?,0.0,0.0,1.0,0.0
Zipkin,47664992,47660900,0,"2017/12/06, 02:38:32",True,"2017/12/06, 02:38:32",8336.0,1773866.0,1,"No, you have the  PercentageBasedSampler  (I'm pretty sure it's written in the docs).",0.124,0.311,0.565,0.5106
Zipkin,47664992,47660900,0,"2017/12/06, 02:38:32",True,"2017/12/06, 02:38:32",8336.0,1773866.0,1,You can tweak its values.,0.0,0.403,0.597,0.4019
Zipkin,47664992,47660900,0,"2017/12/06, 02:38:32",True,"2017/12/06, 02:38:32",8336.0,1773866.0,1,At Server side: Q1.,0.0,0.0,1.0,0.0
Zipkin,47664992,47660900,0,"2017/12/06, 02:38:32",True,"2017/12/06, 02:38:32",8336.0,1773866.0,1,Do I need to create Zipkin server as a spring boot application for my use case or can I use the jar retrieved using: wget -O zipkin.jar ' https://search.maven.org/remote_content?g=io.zipkin.java&amp;a=zipkin-server&amp;v=LATEST&amp;c=exec ' ...as stated on  https://zipkin.io/pages/quickstart.html  ?,0.0,0.081,0.919,0.3527
Zipkin,47664992,47660900,0,"2017/12/06, 02:38:32",True,"2017/12/06, 02:38:32",8336.0,1773866.0,1,You should do the wget.,0.0,0.0,1.0,0.0
Zipkin,47664992,47660900,0,"2017/12/06, 02:38:32",True,"2017/12/06, 02:38:32",8336.0,1773866.0,1,If you want to use the legacy stream support then you should create a zipkin server yourself.,0.0,0.319,0.681,0.6249
Zipkin,47664992,47660900,0,"2017/12/06, 02:38:32",True,"2017/12/06, 02:38:32",8336.0,1773866.0,1,Q2.,0.0,0.0,1.0,0.0
Zipkin,47664992,47660900,0,"2017/12/06, 02:38:32",True,"2017/12/06, 02:38:32",8336.0,1773866.0,1,How do I configure zipkin SINK channel to destination?,0.0,0.0,1.0,0.0
Zipkin,47664992,47660900,0,"2017/12/06, 02:38:32",True,"2017/12/06, 02:38:32",8336.0,1773866.0,1,If you're using the legacy zipkin stream app then it's automatically configured to point to proper destination.,0.0,0.0,1.0,0.0
Zipkin,47664992,47660900,0,"2017/12/06, 02:38:32",True,"2017/12/06, 02:38:32",8336.0,1773866.0,1,You can tweak the destination as you please in the standard way Spring Cloud Stream supports it.,0.0,0.242,0.758,0.5859
Zipkin,47665630,47655299,2,"2017/12/06, 03:58:02",False,"2017/12/06, 03:58:02",693.0,2901325.0,0,I'm not sure I fully understand your question but I can elaborate a bit on how istio works with respect to tracing:,0.065,0.183,0.751,0.5674
Zipkin,47665630,47655299,2,"2017/12/06, 03:58:02",False,"2017/12/06, 03:58:02",693.0,2901325.0,0,"Tracing means identifying every span or node that is part of the original request, so typically an Id is generated by the istio-ingress and your application should  propagate it  so each istio-proxy can capture and forward that information to istio-mixer which then lets you use Zipkin or Jaeger to visualize it.",0.0,0.044,0.956,0.3182
Zipkin,47665630,47655299,2,"2017/12/06, 03:58:02",False,"2017/12/06, 03:58:02",693.0,2901325.0,0,Istio can't know when you make outcalls from your application for which original request it was for unless you do copy the headers.,0.0,0.095,0.905,0.3182
Zipkin,47665630,47655299,2,"2017/12/06, 03:58:02",False,"2017/12/06, 03:58:02",693.0,2901325.0,0,Does that help/makes sense ?,0.0,0.0,1.0,0.0
Zipkin,47592674,47591972,4,"2017/12/01, 13:44:22",False,"2017/12/01, 13:44:22",1176.0,2269535.0,0,"It looks a incompatibility between version in my opinion, something is overridden when you inject the spring-cloud-starter-zipkin dependency",0.0,0.0,1.0,0.0
Zipkin,47592674,47591972,4,"2017/12/01, 13:44:22",False,"2017/12/01, 13:44:22",1176.0,2269535.0,0,What i don't understand from your question is:,0.0,0.0,1.0,0.0
Zipkin,47592674,47591972,4,"2017/12/01, 13:44:22",False,"2017/12/01, 13:44:22",1176.0,2269535.0,0,"Do you need this dependency ""spring-cloud-starter-zipkin"", are you using it?",0.0,0.0,1.0,0.0
Zipkin,47592674,47591972,4,"2017/12/01, 13:44:22",False,"2017/12/01, 13:44:22",1176.0,2269535.0,0,"If no obviously just put it out of the pom, if yes, check which version are you using:",0.105,0.129,0.766,0.128
Zipkin,47592674,47591972,4,"2017/12/01, 13:44:22",False,"2017/12/01, 13:44:22",1176.0,2269535.0,0,mvn dependency:tree and try to align spring-cloud-starter-zipkin with the Spring version you are using.,0.0,0.0,1.0,0.0
Zipkin,47592674,47591972,4,"2017/12/01, 13:44:22",False,"2017/12/01, 13:44:22",1176.0,2269535.0,0,Playing a bit with the version of your artifacts you will find the solution.,0.0,0.272,0.728,0.4767
Zipkin,47592674,47591972,4,"2017/12/01, 13:44:22",False,"2017/12/01, 13:44:22",1176.0,2269535.0,0,Hope it helped.,0.0,0.592,0.408,0.4404
Zipkin,52585722,47026664,0,"2018/10/01, 09:37:07",False,"2020/09/02, 11:35:57",61.0,3548002.0,0,I use &quot;TraceCallable&quot; class from &quot; spring-cloud-sleuth &quot; lib to solve it in my code.,0.0,0.122,0.878,0.2023
Zipkin,52585722,47026664,0,"2018/10/01, 09:37:07",False,"2020/09/02, 11:35:57",61.0,3548002.0,0,My code example is:,0.0,0.0,1.0,0.0
Zipkin,64439116,47026664,0,"2020/10/20, 08:41:48",False,"2020/10/22, 13:48:08",145.0,10936956.0,1,"Attention: This solution does works for logging purposes, but does not work for other Sleuth features like instrumenting RestTemplates to send the tracing headers to other services.",0.0,0.164,0.836,0.5994
Zipkin,64439116,47026664,0,"2020/10/20, 08:41:48",False,"2020/10/22, 13:48:08",145.0,10936956.0,1,"So unfortunately, this is not a fully working solution.",0.448,0.0,0.552,-0.5942
Zipkin,64439116,47026664,0,"2020/10/20, 08:41:48",False,"2020/10/22, 13:48:08",145.0,10936956.0,1,:(,1.0,0.0,0.0,-0.4404
Zipkin,64439116,47026664,0,"2020/10/20, 08:41:48",False,"2020/10/22, 13:48:08",145.0,10936956.0,1,"Some time after adopting @Baca's solution, I discovered that Kotlin Coroutines offer direct integration with slf4j, which is what Spring Sleuth builds on.",0.0,0.099,0.901,0.3182
Zipkin,64439116,47026664,0,"2020/10/20, 08:41:48",False,"2020/10/22, 13:48:08",145.0,10936956.0,1,"Sleuth adds properties  X-B3-TraceId ,  traceId ,  X-B3-SpanId , and  spanId  to the thread's MDC.",0.0,0.0,1.0,0.0
Zipkin,64439116,47026664,0,"2020/10/20, 08:41:48",False,"2020/10/22, 13:48:08",145.0,10936956.0,1,You can retain the parent thread's MDC for a coroutine with the code shown below.,0.0,0.0,1.0,0.0
Zipkin,64439116,47026664,0,"2020/10/20, 08:41:48",False,"2020/10/22, 13:48:08",145.0,10936956.0,1,The coroutine framework will take care of restoring the MDC context on the worker thread whenever the coroutine is executed/resumed.,0.0,0.231,0.769,0.6597
Zipkin,64439116,47026664,0,"2020/10/20, 08:41:48",False,"2020/10/22, 13:48:08",145.0,10936956.0,1,This is the easiest solution I could discover so far.,0.0,0.421,0.579,0.6249
Zipkin,64439116,47026664,0,"2020/10/20, 08:41:48",False,"2020/10/22, 13:48:08",145.0,10936956.0,1,:),0.0,1.0,0.0,0.4588
Zipkin,64439116,47026664,0,"2020/10/20, 08:41:48",False,"2020/10/22, 13:48:08",145.0,10936956.0,1,The launch method takes an optional CoroutineContext and the coroutine-slf4j integration implements the MDCContext.,0.0,0.0,1.0,0.0
Zipkin,64439116,47026664,0,"2020/10/20, 08:41:48",False,"2020/10/22, 13:48:08",145.0,10936956.0,1,This class captures the calling thread's MDC context (creates a copy) and uses that for the coroutine execution.,0.0,0.0,1.0,0.0
Zipkin,64439116,47026664,0,"2020/10/20, 08:41:48",False,"2020/10/22, 13:48:08",145.0,10936956.0,1,Add this dependency to your build.gradle:,0.0,0.0,1.0,0.0
Zipkin,64439116,47026664,0,"2020/10/20, 08:41:48",False,"2020/10/22, 13:48:08",145.0,10936956.0,1,"Project:  https://github.com/Kotlin/kotlinx.coroutines/tree/master/integration/kotlinx-coroutines-slf4j 
Documentation:  https://kotlin.github.io/kotlinx.coroutines/kotlinx-coroutines-slf4j/index.html",0.0,0.0,1.0,0.0
Zipkin,46454753,46432583,1,"2017/09/27, 21:22:45",False,"2017/09/27, 21:22:45",2661.0,1813696.0,0,With Spring boot  Dalston.SR3  (which uses open zipkin 1.28) you can achieve this by setting property  zipkin.storage.mem.max-spans=xxx  This will limit the number of spans and discard old ones.,0.068,0.044,0.887,-0.1779
Zipkin,46454753,46432583,1,"2017/09/27, 21:22:45",False,"2017/09/27, 21:22:45",2661.0,1813696.0,0,pom.xml,0.0,0.0,1.0,0.0
Zipkin,47120862,46164976,1,"2017/11/05, 13:08:17",True,"2017/11/05, 13:08:17",26.0,5772061.0,1,The best way to trace OpenStack project is to use Osprofiler library.,0.0,0.276,0.724,0.6369
Zipkin,47120862,46164976,1,"2017/11/05, 13:08:17",True,"2017/11/05, 13:08:17",26.0,5772061.0,1,If you just want to understand the workflow or just know about the types of calls being made inside OpenStack then Osprofiler is the best and easiest way to get the trace.,0.0,0.223,0.777,0.8074
Zipkin,47120862,46164976,1,"2017/11/05, 13:08:17",True,"2017/11/05, 13:08:17",26.0,5772061.0,1,Now Osprofiler is an accepted OpenStack project and the official project to get traces for OpenStack.,0.0,0.123,0.877,0.2732
Zipkin,47120862,46164976,1,"2017/11/05, 13:08:17",True,"2017/11/05, 13:08:17",26.0,5772061.0,1,"Instead of having to go through the whole code and adding instrumentation points near HTTP request or RPC calls, osprofiler is already integrated in all of the main projects of OpenStack (Nova, Neutron, Keystone, Glance etc..).",0.0,0.0,1.0,0.0
Zipkin,47120862,46164976,1,"2017/11/05, 13:08:17",True,"2017/11/05, 13:08:17",26.0,5772061.0,1,You just have to enable osprofiler in the configuration files of each project in OpenStack to get a trace of that particular project.,0.0,0.0,1.0,0.0
Zipkin,47120862,46164976,1,"2017/11/05, 13:08:17",True,"2017/11/05, 13:08:17",26.0,5772061.0,1,You can go through this link -  https://docs.openstack.org/osprofiler/latest/,0.0,0.0,1.0,0.0
Zipkin,47120862,46164976,1,"2017/11/05, 13:08:17",True,"2017/11/05, 13:08:17",26.0,5772061.0,1,Enabling of Osprofiler in the configuration files can be done by adding these lines at the end of the configuration file (nova.conf or neutron.conf) :,0.0,0.0,1.0,0.0
Zipkin,47120862,46164976,1,"2017/11/05, 13:08:17",True,"2017/11/05, 13:08:17",26.0,5772061.0,1,The connection_string parameter indicates the collector (where the trace information is stored).,0.0,0.0,1.0,0.0
Zipkin,47120862,46164976,1,"2017/11/05, 13:08:17",True,"2017/11/05, 13:08:17",26.0,5772061.0,1,By default it uses Ceilometer.,0.0,0.0,1.0,0.0
Zipkin,47120862,46164976,1,"2017/11/05, 13:08:17",True,"2017/11/05, 13:08:17",26.0,5772061.0,1,You can actually redirect the trace information to other collectors like Elasticsearch by changing the connection_string parameter in the conf file to the elasticsearch server.,0.0,0.094,0.906,0.3612
Zipkin,47120862,46164976,1,"2017/11/05, 13:08:17",True,"2017/11/05, 13:08:17",26.0,5772061.0,1,This is by far the easiest way to get a trace in OpenStack with just minimal effort.,0.0,0.157,0.843,0.4215
Zipkin,45568932,45174992,0,"2017/08/08, 15:53:02",True,"2017/08/08, 15:53:02",45.0,4107097.0,0,After some discussion with Marcin Grzejszczak and Adrien Cole (zipkin and sleuth creators/active developers) I ended up creating a Jersey filter that acts as bridge between sleuth and brave.,0.0,0.183,0.817,0.6808
Zipkin,45568932,45174992,0,"2017/08/08, 15:53:02",True,"2017/08/08, 15:53:02",45.0,4107097.0,0,"Regarding AMQP integration, added a new @StreamListener with a conditional for zipkin format spans (using headers).",0.0,0.0,1.0,0.0
Zipkin,45568932,45174992,0,"2017/08/08, 15:53:02",True,"2017/08/08, 15:53:02",45.0,4107097.0,0,Sending messages to the sleuth exchange with zipkin format will then be valid and consumed by the listener.,0.0,0.0,1.0,0.0
Zipkin,45568932,45174992,0,"2017/08/08, 15:53:02",True,"2017/08/08, 15:53:02",45.0,4107097.0,0,"For javascript (zipkin-js), I ended up creating a new AMQP Logger that sends zipkin spans to a determined exchange.",0.0,0.247,0.753,0.5574
Zipkin,45568932,45174992,0,"2017/08/08, 15:53:02",True,"2017/08/08, 15:53:02",45.0,4107097.0,0,"If someone ends up reading this and needs more detail, you're welcome to reach out to me.",0.0,0.225,0.775,0.5209
Zipkin,45048596,45046700,1,"2017/07/12, 07:41:40",False,"2017/07/12, 07:41:40",4434.0,7122593.0,0,Take a look at Sampling interval in the docs :,0.0,0.0,1.0,0.0
Zipkin,45048596,45046700,1,"2017/07/12, 07:41:40",False,"2017/07/12, 07:41:40",4434.0,7122593.0,0,In distributed tracing the data volumes can be very high so sampling can be important (you usually don’t need to export all spans to get a good picture of what is happening).,0.0,0.139,0.861,0.5719
Zipkin,45048596,45046700,1,"2017/07/12, 07:41:40",False,"2017/07/12, 07:41:40",4434.0,7122593.0,0,Spring Cloud Sleuth has a Sampler strategy that you can implement to take control of the sampling algorithm.,0.0,0.0,1.0,0.0
Zipkin,45048596,45046700,1,"2017/07/12, 07:41:40",False,"2017/07/12, 07:41:40",4434.0,7122593.0,0,"Samplers do not stop span (correlation) ids from being generated, but they do prevent the tags and events being attached and exported.",0.0,0.115,0.885,0.1516
Zipkin,45048596,45046700,1,"2017/07/12, 07:41:40",False,"2017/07/12, 07:41:40",4434.0,7122593.0,0,"By default you get a strategy that continues to trace if a span is already active, but new ones are always marked as non-exportable.",0.0,0.081,0.919,0.2144
Zipkin,45048596,45046700,1,"2017/07/12, 07:41:40",False,"2017/07/12, 07:41:40",4434.0,7122593.0,0,"If all your apps run with this sampler you will see traces in logs, but not in any remote store.",0.0,0.0,1.0,0.0
Zipkin,45048596,45046700,1,"2017/07/12, 07:41:40",False,"2017/07/12, 07:41:40",4434.0,7122593.0,0,"For testing the default is often enough, and it probably is all you need if you are only using the logs (e.g.",0.0,0.0,1.0,0.0
Zipkin,45048596,45046700,1,"2017/07/12, 07:41:40",False,"2017/07/12, 07:41:40",4434.0,7122593.0,0,with an ELK aggregator).,0.0,0.0,1.0,0.0
Zipkin,45048596,45046700,1,"2017/07/12, 07:41:40",False,"2017/07/12, 07:41:40",4434.0,7122593.0,0,"If you are exporting span data to Zipkin or Spring Cloud Stream, there is also an AlwaysSampler that exports everything and a PercentageBasedSampler that samples a fixed fraction of spans.",0.0,0.0,1.0,0.0
Zipkin,45048596,45046700,1,"2017/07/12, 07:41:40",False,"2017/07/12, 07:41:40",4434.0,7122593.0,0,http://cloud.spring.io/spring-cloud-sleuth/spring-cloud-sleuth.html#_sampling,0.0,0.0,1.0,0.0
Zipkin,44395594,44382633,1,"2017/06/06, 19:47:13",False,"2017/06/06, 19:47:13",37971.0,1237575.0,0,You are blending Zipkin autoconfigure version 1.2 with Zipkin 1.26.,0.0,0.0,1.0,0.0
Zipkin,44395594,44382633,1,"2017/06/06, 19:47:13",False,"2017/06/06, 19:47:13",37971.0,1237575.0,0,This results in a version missmatch.,0.0,0.0,1.0,0.0
Zipkin,44403836,44382633,0,"2017/06/07, 07:58:45",False,"2017/06/07, 07:58:45",1.0,7608262.0,0,The problem is casued by two reasons.,0.31,0.0,0.69,-0.4019
Zipkin,44403836,44382633,0,"2017/06/07, 07:58:45",False,"2017/06/07, 07:58:45",1.0,7608262.0,0,First:just as Rafael Winterhalter said the version dismatched; second: you are lacking dependency for zipkin.,0.0,0.0,1.0,0.0
Zipkin,44403836,44382633,0,"2017/06/07, 07:58:45",False,"2017/06/07, 07:58:45",1.0,7608262.0,0,"Finally i fixed the problem by adding the whole 'io.zipkin.java:zipkin"" library.",0.231,0.0,0.769,-0.4019
Zipkin,44403836,44382633,0,"2017/06/07, 07:58:45",False,"2017/06/07, 07:58:45",1.0,7608262.0,0,Here is my final pom file with the storage type of elasticsearch:,0.0,0.0,1.0,0.0
Zipkin,44193861,44147933,0,"2017/05/26, 07:52:41",False,"2017/05/26, 07:52:41",1.0,7983801.0,0,Appears that a sleuth span is not the same as a Zipkin span.,0.0,0.0,1.0,0.0
Zipkin,44193861,44147933,0,"2017/05/26, 07:52:41",False,"2017/05/26, 07:52:41",1.0,7983801.0,0,"Hence, in the above code there is no way to instantiate a default tracer with zipkin span reporter.",0.121,0.0,0.879,-0.296
Zipkin,44193861,44147933,0,"2017/05/26, 07:52:41",False,"2017/05/26, 07:52:41",1.0,7983801.0,0,I converted the sleuth span into a zipkin span and then reported it to zipkin.,0.0,0.0,1.0,0.0
Zipkin,44193861,44147933,0,"2017/05/26, 07:52:41",False,"2017/05/26, 07:52:41",1.0,7983801.0,0,The class to convert it is available in spring-cloud-sleuth-stream.,0.0,0.0,1.0,0.0
Zipkin,44193861,44147933,0,"2017/05/26, 07:52:41",False,"2017/05/26, 07:52:41",1.0,7983801.0,0,I used pretty much the same class with some tweaks.,0.0,0.286,0.714,0.4939
Zipkin,44062121,44027882,0,"2017/05/19, 08:32:21",False,"2017/05/19, 08:32:21",8336.0,1773866.0,2,Here you have a very basic example of Sleuth &amp; HTTP communication.,0.0,0.0,1.0,0.0
Zipkin,44062121,44027882,0,"2017/05/19, 08:32:21",False,"2017/05/19, 08:32:21",8336.0,1773866.0,2,https://github.com/openzipkin/sleuth-webmvc-example  You can set your dependencies in a similar manner and everything should work fine.,0.0,0.122,0.878,0.2023
Zipkin,44062121,44027882,0,"2017/05/19, 08:32:21",False,"2017/05/19, 08:32:21",8336.0,1773866.0,2,In your example you've got Stream but I don't think you're using it so it's better to remove it.,0.0,0.201,0.799,0.6448
Zipkin,45815790,44027882,0,"2017/08/22, 13:58:17",False,"2017/08/22, 13:58:17",253.0,5751473.0,0,As M.Deinum said remove  stream  and  stream-rabbit  dependencies what if you do not need some AMQP server to store the trace message.,0.0,0.0,1.0,0.0
Zipkin,45815790,44027882,0,"2017/08/22, 13:58:17",False,"2017/08/22, 13:58:17",253.0,5751473.0,0,or,0.0,0.0,1.0,0.0
Zipkin,45815790,44027882,0,"2017/08/22, 13:58:17",False,"2017/08/22, 13:58:17",253.0,5751473.0,0,"config the AMQP(rabbitMQ in your code) from application-configuration(both) and add  zipkin-stream  &amp;  stream-rabbit  in  zipkin-server  side, so this time your app( zipkin-client ) will not direct connect with  zipkin-server  
and it will be:",0.0,0.0,1.0,0.0
Zipkin,47431400,43913962,0,"2017/11/22, 11:49:20",False,"2017/11/22, 11:49:20",215.0,3702774.0,1,You may define all needed params via ENV options.,0.0,0.0,1.0,0.0
Zipkin,47431400,43913962,0,"2017/11/22, 11:49:20",False,"2017/11/22, 11:49:20",215.0,3702774.0,1,Here is a cmd for running zipkin in docker:,0.0,0.0,1.0,0.0
Zipkin,47431400,43913962,0,"2017/11/22, 11:49:20",False,"2017/11/22, 11:49:20",215.0,3702774.0,1,All these params can be defined in Deployment (see  Expose Pod Information to Containers Through Environment Variables ),0.091,0.0,0.909,-0.1531
Zipkin,48982440,43782954,0,"2018/02/26, 08:10:42",False,"2018/02/26, 08:10:42",181.0,6708214.0,1,1.You should check if your Zipkin Server is on.,0.0,0.0,1.0,0.0
Zipkin,48982440,43782954,0,"2018/02/26, 08:10:42",False,"2018/02/26, 08:10:42",181.0,6708214.0,1,2.You should check if the Span transfering is async.,0.0,0.0,1.0,0.0
Zipkin,48982440,43782954,0,"2018/02/26, 08:10:42",False,"2018/02/26, 08:10:42",181.0,6708214.0,1,"In HTTP,Zipkin uses in-band transfer,all the information carried in HTTP headers.The cost time of generating Span is about 200 nanosecond.",0.0,0.0,1.0,0.0
Zipkin,43153293,43151368,0,"2017/04/01, 08:46:41",True,"2017/04/01, 08:46:41",762.0,1227937.0,1,"The TL;DR; is that  B3 propagation  was initially designed for fixed size data: carrying data ancillary to tracing isn't in scope, and for this reason any solution that extends B3 in such a fashion wouldn't be compatible with existing code.",0.0,0.093,0.907,0.4215
Zipkin,43153293,43151368,0,"2017/04/01, 08:46:41",True,"2017/04/01, 08:46:41",762.0,1227937.0,1,"So, that means any solution like this will be an extension which means custom handling in the  instrumented apps  which are the things passing headers around.",0.0,0.167,0.833,0.5859
Zipkin,43153293,43151368,0,"2017/04/01, 08:46:41",True,"2017/04/01, 08:46:41",762.0,1227937.0,1,The server won't care as it never sees these headers anyway.,0.208,0.0,0.792,-0.3875
Zipkin,43153293,43151368,0,"2017/04/01, 08:46:41",True,"2017/04/01, 08:46:41",762.0,1227937.0,1,Ways people usually integrate other things like flags with zipkin is to add a tag aka binary annotation including its value (usually in the root span).,0.0,0.176,0.824,0.5994
Zipkin,43153293,43151368,0,"2017/04/01, 08:46:41",True,"2017/04/01, 08:46:41",762.0,1227937.0,1,"This would allow you to query or retrieve these offline, but it doesn't address in-flight lookups from applications.",0.067,0.078,0.856,0.0516
Zipkin,43153293,43151368,0,"2017/04/01, 08:46:41",True,"2017/04/01, 08:46:41",762.0,1227937.0,1,"Let's say that instead of using an intermediary like linkerd, or a platform-specific propagated context, we want to dispatch the responsibility to the tracing layer.",0.0,0.147,0.853,0.4215
Zipkin,43153293,43151368,0,"2017/04/01, 08:46:41",True,"2017/04/01, 08:46:41",762.0,1227937.0,1,"Firstly, what sort of data could work alright?",0.0,0.222,0.778,0.25
Zipkin,43153293,43151368,0,"2017/04/01, 08:46:41",True,"2017/04/01, 08:46:41",762.0,1227937.0,1,The easiest is something set-once (like zipkin's trace id).,0.0,0.259,0.741,0.4215
Zipkin,43153293,43151368,0,"2017/04/01, 08:46:41",True,"2017/04/01, 08:46:41",762.0,1227937.0,1,Anything set and propagated without mutating it is the least mechanics.,0.0,0.0,1.0,0.0
Zipkin,43153293,43151368,0,"2017/04/01, 08:46:41",True,"2017/04/01, 08:46:41",762.0,1227937.0,1,"Next in difficulty is appending new entries mid-stream, and most difficult is mutating/merging entries.",0.302,0.0,0.698,-0.6361
Zipkin,43153293,43151368,0,"2017/04/01, 08:46:41",True,"2017/04/01, 08:46:41",762.0,1227937.0,1,Let's assume this is for inbound flags which never change through the request/trace tree.,0.0,0.0,1.0,0.0
Zipkin,43153293,43151368,0,"2017/04/01, 08:46:41",True,"2017/04/01, 08:46:41",762.0,1227937.0,1,"We see a header when processing trace data, we store it and forward it downstream.",0.0,0.0,1.0,0.0
Zipkin,43153293,43151368,0,"2017/04/01, 08:46:41",True,"2017/04/01, 08:46:41",762.0,1227937.0,1,"If this value doesn't need to be read by the tracing system, it is easiest, as it is largely a transport/propagation concern.",0.0,0.215,0.785,0.6369
Zipkin,43153293,43151368,0,"2017/04/01, 08:46:41",True,"2017/04/01, 08:46:41",762.0,1227937.0,1,"For example, maybe other middleware read that header and it is only a ""side job"" we are adding to the tracer to remember certain things to pass along.",0.0,0.075,0.925,0.2732
Zipkin,43153293,43151368,0,"2017/04/01, 08:46:41",True,"2017/04/01, 08:46:41",762.0,1227937.0,1,"If this was done in a single header, it would be less code than a pattern in each of the places this would be to added.",0.0,0.0,1.0,0.0
Zipkin,43153293,43151368,0,"2017/04/01, 08:46:41",True,"2017/04/01, 08:46:41",762.0,1227937.0,1,"It would be even less code if the flags could be encoded in a number, however unrealistic that may be.",0.0,0.067,0.933,0.0772
Zipkin,43153293,43151368,0,"2017/04/01, 08:46:41",True,"2017/04/01, 08:46:41",762.0,1227937.0,1,"There are libraries with apis to manipulate the propagated context manually, for example,  ""baggage"" from brownsys  and OpenTracing (of which some libraries support zipkin).",0.0,0.105,0.895,0.4019
Zipkin,43153293,43151368,0,"2017/04/01, 08:46:41",True,"2017/04/01, 08:46:41",762.0,1227937.0,1,"The former aims to be a generic layer for any instrumentation (ex monitoring, chargeback, tracing etc) and the latter is specific to tracing.",0.0,0.0,1.0,0.0
Zipkin,43153293,43151368,0,"2017/04/01, 08:46:41",True,"2017/04/01, 08:46:41",762.0,1227937.0,1,OpenTracing has defines abstract types like  injector and extractor  which could be customized to carry other fields.,0.0,0.135,0.865,0.3612
Zipkin,43153293,43151368,0,"2017/04/01, 08:46:41",True,"2017/04/01, 08:46:41",762.0,1227937.0,1,"However, you still would need a concrete implementation (which knows your header format etc) in order to do this.",0.0,0.0,1.0,0.0
Zipkin,43153293,43151368,0,"2017/04/01, 08:46:41",True,"2017/04/01, 08:46:41",762.0,1227937.0,1,"Unless you want applications to read this data, it would need to be a secret detail of that implementation (specifically the trace context).",0.0,0.058,0.942,0.0772
Zipkin,43153293,43151368,0,"2017/04/01, 08:46:41",True,"2017/04/01, 08:46:41",762.0,1227937.0,1,"Certain zipkin-specific libraries like Spring Cloud Sleuth and Brave have means to  customize how headers are parsed , to support variants of B3 or new or site-specific trace formats.",0.0,0.308,0.692,0.8658
Zipkin,43153293,43151368,0,"2017/04/01, 08:46:41",True,"2017/04/01, 08:46:41",762.0,1227937.0,1,"Not all support this at the moment, but I would expect this type of feature to become more common.",0.087,0.0,0.913,-0.1603
Zipkin,43153293,43151368,0,"2017/04/01, 08:46:41",True,"2017/04/01, 08:46:41",762.0,1227937.0,1,This means you may need to do some surgery in order to support all platforms you may need to support.,0.0,0.231,0.769,0.6597
Zipkin,43153293,43151368,0,"2017/04/01, 08:46:41",True,"2017/04/01, 08:46:41",762.0,1227937.0,1,"So long story short is that there are some libraries which are pluggable with regards to propagation, and those will be easiest to modify to support this use case.",0.0,0.169,0.831,0.6705
Zipkin,43153293,43151368,0,"2017/04/01, 08:46:41",True,"2017/04/01, 08:46:41",762.0,1227937.0,1,Some code will be needed regardless as B3 doesn't currently define an expression like this.,0.0,0.152,0.848,0.3612
Zipkin,43106901,43095404,0,"2017/03/30, 04:24:13",True,"2017/03/30, 04:24:13",762.0,1227937.0,0,In general it is better to use the zipkin's http variant of Elasticsearch as it cannot conflict with Spring Boot's elasticsearch library versions.,0.0,0.188,0.812,0.5943
Zipkin,43106901,43095404,0,"2017/03/30, 04:24:13",True,"2017/03/30, 04:24:13",762.0,1227937.0,0,I would set everything in zipkin's group id to latest (currently 1.21.0 which is Spring Boot 1.4.x) and use zipkin-autoconfigure-storage-elasticsearch-http (plus.. the one you are using  will be dropped ),0.0,0.0,1.0,0.0
Zipkin,43106901,43095404,0,"2017/03/30, 04:24:13",True,"2017/03/30, 04:24:13",762.0,1227937.0,0,Make sure your es hosts is specified in url syntax ex.,0.0,0.187,0.813,0.3182
Zipkin,43106901,43095404,0,"2017/03/30, 04:24:13",True,"2017/03/30, 04:24:13",762.0,1227937.0,0,http://host1:9200,0.0,0.0,1.0,0.0
Zipkin,42901789,42884672,0,"2017/03/20, 12:55:54",False,"2017/03/20, 12:55:54",37971.0,1237575.0,0,Zipkin generates traces and communicates them back to a Zipkin server.,0.0,0.0,1.0,0.0
Zipkin,42901789,42884672,0,"2017/03/20, 12:55:54",False,"2017/03/20, 12:55:54",37971.0,1237575.0,0,The latter action is typically performed via HTTP but Zipkin is agnostic to how a span is started and ended.,0.0,0.0,1.0,0.0
Zipkin,42901789,42884672,0,"2017/03/20, 12:55:54",False,"2017/03/20, 12:55:54",37971.0,1237575.0,0,"If you want to measure the execution time of a single method, you would normally create a local span that is nested inside a server span.",0.0,0.139,0.861,0.34
Zipkin,42901789,42884672,0,"2017/03/20, 12:55:54",False,"2017/03/20, 12:55:54",37971.0,1237575.0,0,Zipkin is a distributrd tracing tool for discovering machine-to-machine interaction which is why spans often cover HTTP.,0.0,0.0,1.0,0.0
Zipkin,42901789,42884672,0,"2017/03/20, 12:55:54",False,"2017/03/20, 12:55:54",37971.0,1237575.0,0,"If you want to measure execution time of a method, a tool like metrics might be more suited.",0.0,0.213,0.787,0.4215
Zipkin,41798515,40954585,1,"2017/01/23, 04:38:41",True,"2017/01/23, 04:38:41",762.0,1227937.0,1,This was an issue with MySQL 5.7 and more recently resolved.,0.0,0.165,0.835,0.2449
Zipkin,41798515,40954585,1,"2017/01/23, 04:38:41",True,"2017/01/23, 04:38:41",762.0,1227937.0,1,You can try latest Zipkin.,0.0,0.0,1.0,0.0
Zipkin,39601988,39600581,2,"2016/09/20, 22:06:46",True,"2016/09/20, 22:06:46",8336.0,1773866.0,1,You'd have to implement your own ZipkinSpanReporter that would look more or less like  https://github.com/spring-cloud/spring-cloud-sleuth/blob/v1.0.8.RELEASE/spring-cloud-sleuth-zipkin/src/main/java/org/springframework/cloud/sleuth/zipkin/HttpZipkinSpanReporter.java  .,0.0,0.15,0.85,0.355
Zipkin,39601988,39600581,2,"2016/09/20, 22:06:46",True,"2016/09/20, 22:06:46",8336.0,1773866.0,1,In the next version of Sleuth you will be able to register a bean of ZipkinSpanReporter that can you a custom version of a publisher -  https://github.com/spring-cloud/spring-cloud-sleuth/blob/1.0.x/spring-cloud-sleuth-zipkin/src/main/java/org/springframework/cloud/sleuth/zipkin/HttpZipkinSpanReporter.java,0.0,0.0,1.0,0.0
Zipkin,38537946,38527466,1,"2016/07/23, 06:47:14",False,"2016/07/23, 06:47:14",762.0,1227937.0,1,Instrumenting a library is something that sometimes folks have to do for one reason or another.,0.0,0.0,1.0,0.0
Zipkin,38537946,38527466,1,"2016/07/23, 06:47:14",False,"2016/07/23, 06:47:14",762.0,1227937.0,1,"There are several tracer libraries in Java but the salient points about creating a tracer are either on the website, or issues on the website.",0.0,0.199,0.801,0.6652
Zipkin,38537946,38527466,1,"2016/07/23, 06:47:14",False,"2016/07/23, 06:47:14",762.0,1227937.0,1,"http://zipkin.io/pages/instrumenting.html 
 https://github.com/openzipkin/openzipkin.github.io/issues/11",0.0,0.0,1.0,0.0
Zipkin,38537946,38527466,1,"2016/07/23, 06:47:14",False,"2016/07/23, 06:47:14",762.0,1227937.0,1,OpenTracing also has some nice fundamentals to look at  http://opentracing.io/,0.0,0.237,0.763,0.4215
Zipkin,38537946,38527466,1,"2016/07/23, 06:47:14",False,"2016/07/23, 06:47:14",762.0,1227937.0,1,"This isn't a one-answer type of question, in my experience, as you'll learn you'll need to address other things that tracer libraries address out-of-box.",0.0,0.0,1.0,0.0
Zipkin,38537946,38527466,1,"2016/07/23, 06:47:14",False,"2016/07/23, 06:47:14",762.0,1227937.0,1,"For that reason I'd highly suggest joining gitter so that you
can have a dialogue through your journey  https://gitter.im/openzipkin/zipkin",0.0,0.0,1.0,0.0
Zipkin,37799358,37757200,0,"2016/06/14, 00:23:23",False,"2016/07/26, 21:58:32",13.0,3491416.0,1,I was recording wrong annotation i.e client instead of server.,0.279,0.0,0.721,-0.4767
Zipkin,37799358,37757200,0,"2016/06/14, 00:23:23",False,"2016/07/26, 21:58:32",13.0,3491416.0,1,Just a simple change did the trick.,0.194,0.0,0.806,-0.0516
Zipkin,37799358,37757200,0,"2016/06/14, 00:23:23",False,"2016/07/26, 21:58:32",13.0,3491416.0,1,"Trace.traceService(""Function1"",""Test"")",0.0,0.0,1.0,0.0
Zipkin,37799358,37757200,0,"2016/06/14, 00:23:23",False,"2016/07/26, 21:58:32",13.0,3491416.0,1,Sample working Zipkin example:  https://gist.github.com/AkhilJ876/3e38757c28d43924f296dd2d147c0bd9#file-zipkintracing_example-L34,0.0,0.0,1.0,0.0
Zipkin,37648120,37459261,0,"2016/06/06, 03:30:28",False,"2016/06/06, 16:49:03",91.0,3623286.0,1,"Same problem here with the docker images using Cassandra (1.40.1, 1.40.2, 1.1.4).",0.197,0.0,0.803,-0.4019
Zipkin,37648120,37459261,0,"2016/06/06, 03:30:28",False,"2016/06/06, 16:49:03",91.0,3623286.0,1,This is a problem specific to using Cassandra as the storage tier.,0.213,0.0,0.787,-0.4019
Zipkin,37648120,37459261,0,"2016/06/06, 03:30:28",False,"2016/06/06, 16:49:03",91.0,3623286.0,1,Mysql and the in-memory storage generate the dependency graph on-demand as expected.,0.0,0.0,1.0,0.0
Zipkin,37648120,37459261,0,"2016/06/06, 03:30:28",False,"2016/06/06, 16:49:03",91.0,3623286.0,1,There are references to the following project to generate the Cassandra graph data for the UI to display.,0.0,0.0,1.0,0.0
Zipkin,37648120,37459261,0,"2016/06/06, 03:30:28",False,"2016/06/06, 16:49:03",91.0,3623286.0,1,This looks to be superseded by ongoing work mentioned here,0.0,0.0,1.0,0.0
Zipkin,58025941,37459261,0,"2019/09/20, 12:50:09",False,"2019/09/20, 13:20:39",1592.0,7251967.0,0,"If storage type is other than inline storage, for zipkin dependencies graph you have to start separate cron job/scheduler which reads the storage database and builds the graph.",0.0,0.0,1.0,0.0
Zipkin,58025941,37459261,0,"2019/09/20, 12:50:09",False,"2019/09/20, 13:20:39",1592.0,7251967.0,0,Because zipkin dependencies is separate spark job .,0.0,0.241,0.759,0.2263
Zipkin,58025941,37459261,0,"2019/09/20, 12:50:09",False,"2019/09/20, 13:20:39",1592.0,7251967.0,0,For reference :  https://github.com/openzipkin/docker-zipkin-dependencies,0.0,0.0,1.0,0.0
Zipkin,58025941,37459261,0,"2019/09/20, 12:50:09",False,"2019/09/20, 13:20:39",1592.0,7251967.0,0,I have used zipkin with elastic search as storage type.,0.0,0.0,1.0,0.0
Zipkin,58025941,37459261,0,"2019/09/20, 12:50:09",False,"2019/09/20, 13:20:39",1592.0,7251967.0,0,I will share the steps for setting up the zipkin dependencies with elastic search and cron job for the same:,0.0,0.109,0.891,0.296
Zipkin,58025941,37459261,0,"2019/09/20, 12:50:09",False,"2019/09/20, 13:20:39",1592.0,7251967.0,0,"Other solution is to start a separate service and run the cron job
  using docker",0.0,0.15,0.85,0.3182
Zipkin,58025941,37459261,0,"2019/09/20, 12:50:09",False,"2019/09/20, 13:20:39",1592.0,7251967.0,0,"Steps to get the latest zipkin-dependencies jar try running given
  command on teminal",0.0,0.0,1.0,0.0
Zipkin,58025941,37459261,0,"2019/09/20, 12:50:09",False,"2019/09/20, 13:20:39",1592.0,7251967.0,0,you will get jar file at above mention directory,0.0,0.0,1.0,0.0
Zipkin,58025941,37459261,0,"2019/09/20, 12:50:09",False,"2019/09/20, 13:20:39",1592.0,7251967.0,0,Dockerfile,0.0,0.0,1.0,0.0
Zipkin,58025941,37459261,0,"2019/09/20, 12:50:09",False,"2019/09/20, 13:20:39",1592.0,7251967.0,0,entry.sh,0.0,0.0,1.0,0.0
Zipkin,58025941,37459261,0,"2019/09/20, 12:50:09",False,"2019/09/20, 13:20:39",1592.0,7251967.0,0,script.sh,0.0,0.0,1.0,0.0
Zipkin,58025941,37459261,0,"2019/09/20, 12:50:09",False,"2019/09/20, 13:20:39",1592.0,7251967.0,0,crontab.txt,0.0,0.0,1.0,0.0
Zipkin,21582785,21551629,3,"2014/02/05, 18:27:05",True,"2014/02/05, 18:27:05",26.0,2070089.0,1,This is due to not having an instance of the query server running.,0.0,0.0,1.0,0.0
Zipkin,21582785,21551629,3,"2014/02/05, 18:27:05",True,"2014/02/05, 18:27:05",26.0,2070089.0,1,I'm in the middle of a re-write that'll simplify all of this.,0.0,0.0,1.0,0.0
Zipkin,21582785,21551629,3,"2014/02/05, 18:27:05",True,"2014/02/05, 18:27:05",26.0,2070089.0,1,"Until then, you need to spin up a query server.",0.0,0.0,1.0,0.0
Zipkin,61514513,61514326,0,"2020/04/30, 04:37:29",True,"2020/04/30, 04:37:29",396.0,4641689.0,1,I would say you can have fluentd in multiple namespaces and Elasticsearch in one namespace and fluentd can discover Elasticsearch via K8s internal DNS A/AAAA record e.g.,0.0,0.0,1.0,0.0
Zipkin,61514513,61514326,0,"2020/04/30, 04:37:29",True,"2020/04/30, 04:37:29",396.0,4641689.0,1,elasticsearch.${namespace}.svc.cluster.local .,0.0,0.0,1.0,0.0
Zipkin,61514513,61514326,0,"2020/04/30, 04:37:29",True,"2020/04/30, 04:37:29",396.0,4641689.0,1,"I don't have any link to the best practice, but I would show you a practice I saw from the community.",0.0,0.14,0.86,0.3818
Zipkin,61514513,61514326,0,"2020/04/30, 04:37:29",True,"2020/04/30, 04:37:29",396.0,4641689.0,1,"If you are not familiar with configuring K8s cluster, I recommend to deploy ELK by Helm.",0.0,0.152,0.848,0.3612
Zipkin,61514513,61514326,0,"2020/04/30, 04:37:29",True,"2020/04/30, 04:37:29",396.0,4641689.0,1,It will save you a lot of time and give you enough configuration options.,0.0,0.211,0.789,0.4939
Zipkin,61514513,61514326,0,"2020/04/30, 04:37:29",True,"2020/04/30, 04:37:29",396.0,4641689.0,1,https://github.com/helm/charts/tree/master/stable/elastic-stack .,0.0,0.0,1.0,0.0
Zipkin,61514513,61514326,0,"2020/04/30, 04:37:29",True,"2020/04/30, 04:37:29",396.0,4641689.0,1,"Install your ELK helm release on a  separate  namespace, for example:  logging .",0.0,0.0,1.0,0.0
Zipkin,61514513,61514326,0,"2020/04/30, 04:37:29",True,"2020/04/30, 04:37:29",396.0,4641689.0,1,Install fluentd in any namespaces in your cluster and configure elasticsearch host  https://github.com/helm/charts/tree/master/stable/fluentd-elasticsearch,0.0,0.0,1.0,0.0
Zipkin,57200810,57143690,0,"2019/07/25, 14:23:59",False,"2019/07/25, 14:23:59",9.0,4762878.0,1,"I used an Aspect and from the returned ResponseEntity object, decided whether or not to programatically add an error tag to span.",0.119,0.0,0.881,-0.4019
Zipkin,57200810,57143690,0,"2019/07/25, 14:23:59",False,"2019/07/25, 14:23:59",9.0,4762878.0,1,"With this tag, zipkin will identify and highlight the trace in red colour.",0.0,0.167,0.833,0.34
Zipkin,57200810,57143690,0,"2019/07/25, 14:23:59",False,"2019/07/25, 14:23:59",9.0,4762878.0,1,Below is the code snippet to add error tag to span.,0.213,0.0,0.787,-0.4019
Zipkin,48765679,48668426,0,"2018/02/13, 13:23:09",False,"2018/02/13, 13:23:09",681.0,4255878.0,0,i think i found a suitable way to do this.,0.0,0.0,1.0,0.0
Zipkin,48765679,48668426,0,"2018/02/13, 13:23:09",False,"2018/02/13, 13:23:09",681.0,4255878.0,0,After further thinking my idea went into the direction of using annotations and aspects for intercepting HTTP requests from/to thrifts http client which seems to be quite some work.,0.0,0.0,1.0,0.0
Zipkin,48765679,48668426,0,"2018/02/13, 13:23:09",False,"2018/02/13, 13:23:09",681.0,4255878.0,0,"after further search, i found this library for spring-boot serving exactly my needs:
 https://github.com/aatarasoff/spring-thrift-starter",0.0,0.0,1.0,0.0
Zipkin,58889653,58889547,0,"2019/11/16, 12:16:29",False,"2019/11/16, 12:16:29",1.0,12382349.0,0,I dont kown can you see my pic and I put my code:,0.0,0.0,1.0,0.0
Zipkin,58889653,58889547,0,"2019/11/16, 12:16:29",False,"2019/11/16, 12:16:29",1.0,12382349.0,0,pom.xml:,0.0,0.0,1.0,0.0
Zipkin,58889653,58889547,0,"2019/11/16, 12:16:29",False,"2019/11/16, 12:16:29",1.0,12382349.0,0,ZipkinApplication.java:,0.0,0.0,1.0,0.0
Zipkin,58889653,58889547,0,"2019/11/16, 12:16:29",False,"2019/11/16, 12:16:29",1.0,12382349.0,0,The error:,0.73,0.0,0.27,-0.4019
Zipkin,53692930,53692437,3,"2018/12/09, 15:41:18",False,"2018/12/09, 15:41:18",81.0,2232476.0,0,I can say your YAML has some bad indentation and things are not in the right sections even.,0.179,0.0,0.821,-0.5423
Zipkin,53692930,53692437,3,"2018/12/09, 15:41:18",False,"2018/12/09, 15:41:18",81.0,2232476.0,0,"Otherwise though, you are trying to run Zipkin in an unsupported configuration.",0.197,0.0,0.803,-0.4019
Zipkin,53692930,53692437,3,"2018/12/09, 15:41:18",False,"2018/12/09, 15:41:18",81.0,2232476.0,0,Please check out our quickstart documentation:  https://zipkin.io/pages/quickstart.html,0.0,0.277,0.723,0.3182
Zipkin,53694198,53692437,3,"2018/12/09, 18:13:01",True,"2018/12/13, 16:58:29",5963.0,3625215.0,1,There are 2 approaches to this,0.0,0.0,1.0,0.0
Zipkin,53694198,53692437,3,"2018/12/09, 18:13:01",True,"2018/12/13, 16:58:29",5963.0,3625215.0,1,Looking at your yml file you have added,0.0,0.0,1.0,0.0
Zipkin,53694198,53692437,3,"2018/12/09, 18:13:01",True,"2018/12/13, 16:58:29",5963.0,3625215.0,1,which means your approach is 2.,0.0,0.0,1.0,0.0
Zipkin,53694198,53692437,3,"2018/12/09, 18:13:01",True,"2018/12/13, 16:58:29",5963.0,3625215.0,1,"But then in your pom, you have added  zipkin-server  and  zipkin-autoconfigure-ui  dependencies which is not required.",0.0,0.0,1.0,0.0
Zipkin,53694198,53692437,3,"2018/12/09, 18:13:01",True,"2018/12/13, 16:58:29",5963.0,3625215.0,1,I will try to separate both setups,0.0,0.0,1.0,0.0
Zipkin,53694198,53692437,3,"2018/12/09, 18:13:01",True,"2018/12/13, 16:58:29",5963.0,3625215.0,1,1.,0.0,0.0,1.0,0.0
Zipkin,53694198,53692437,3,"2018/12/09, 18:13:01",True,"2018/12/13, 16:58:29",5963.0,3625215.0,1,To Start Zipkin server with SpringBootApplication,0.0,0.0,1.0,0.0
Zipkin,53694198,53692437,3,"2018/12/09, 18:13:01",True,"2018/12/13, 16:58:29",5963.0,3625215.0,1,pom.xml,0.0,0.0,1.0,0.0
Zipkin,53694198,53692437,3,"2018/12/09, 18:13:01",True,"2018/12/13, 16:58:29",5963.0,3625215.0,1,application.properties,0.0,0.0,1.0,0.0
Zipkin,53694198,53692437,3,"2018/12/09, 18:13:01",True,"2018/12/13, 16:58:29",5963.0,3625215.0,1,Application.java,0.0,0.0,1.0,0.0
Zipkin,53694198,53692437,3,"2018/12/09, 18:13:01",True,"2018/12/13, 16:58:29",5963.0,3625215.0,1,2.,0.0,0.0,1.0,0.0
Zipkin,53694198,53692437,3,"2018/12/09, 18:13:01",True,"2018/12/13, 16:58:29",5963.0,3625215.0,1,To Start Zipkin server as a standalone and use SpringBootApplication as Zipkin Client,0.0,0.0,1.0,0.0
Zipkin,53694198,53692437,3,"2018/12/09, 18:13:01",True,"2018/12/13, 16:58:29",5963.0,3625215.0,1,Start Zipkin server,0.0,0.0,1.0,0.0
Zipkin,53694198,53692437,3,"2018/12/09, 18:13:01",True,"2018/12/13, 16:58:29",5963.0,3625215.0,1,pom.xml,0.0,0.0,1.0,0.0
Zipkin,53694198,53692437,3,"2018/12/09, 18:13:01",True,"2018/12/13, 16:58:29",5963.0,3625215.0,1,application.properties,0.0,0.0,1.0,0.0
Zipkin,53694198,53692437,3,"2018/12/09, 18:13:01",True,"2018/12/13, 16:58:29",5963.0,3625215.0,1,Edit 1:,0.0,0.0,1.0,0.0
Zipkin,53694198,53692437,3,"2018/12/09, 18:13:01",True,"2018/12/13, 16:58:29",5963.0,3625215.0,1,@EnableZipkinServer  is deprecated and unsupported as per Brian Devins's comment.,0.231,0.0,0.769,-0.4019
Zipkin,53694198,53692437,3,"2018/12/09, 18:13:01",True,"2018/12/13, 16:58:29",5963.0,3625215.0,1,"So, please go through the  doc  for more detail info.",0.0,0.224,0.776,0.3804
Zipkin,50837359,50836516,3,"2018/06/13, 15:31:21",False,"2018/06/13, 15:31:21",8336.0,1773866.0,0,You're using an ancient version of Spring CLoud.,0.0,0.0,1.0,0.0
Zipkin,50837359,50836516,3,"2018/06/13, 15:31:21",False,"2018/06/13, 15:31:21",8336.0,1773866.0,0,Please upgrade to latest Edgware.,0.0,0.365,0.635,0.3182
Zipkin,50837359,50836516,3,"2018/06/13, 15:31:21",False,"2018/06/13, 15:31:21",8336.0,1773866.0,0,The RxJava support is very basic so we suggest that you use Project Reactor.,0.0,0.172,0.828,0.4019
Zipkin,50837359,50836516,3,"2018/06/13, 15:31:21",False,"2018/06/13, 15:31:21",8336.0,1773866.0,0,To do that just migrate to Finchley and it should work out of the box with WebFlux.,0.0,0.0,1.0,0.0
Zipkin,50268263,50267380,6,"2018/05/10, 11:01:52",False,"2018/05/10, 11:01:52",8336.0,1773866.0,1,"You're using an ancient version of Sleuth, can you please upgrade?",0.0,0.187,0.813,0.3182
Zipkin,50268263,50267380,6,"2018/05/10, 11:01:52",False,"2018/05/10, 11:01:52",8336.0,1773866.0,1,Why do you provide Zipkin's version manually?,0.0,0.0,1.0,0.0
Zipkin,50268263,50267380,6,"2018/05/10, 11:01:52",False,"2018/05/10, 11:01:52",8336.0,1773866.0,1,Also as far as I see you're using the Sleuth's Zipkin server (that is deprecated in Edgware and removed in Finchley).,0.0,0.0,1.0,0.0
Zipkin,50268263,50267380,6,"2018/05/10, 11:01:52",False,"2018/05/10, 11:01:52",8336.0,1773866.0,1,My suggestion is that you stop using the Sleuth's Stream server (you can read more about this here  https://cloud.spring.io/spring-cloud-static/Edgware.SR3/single/spring-cloud.html#_sleuth_with_zipkin_via_rabbitmq_or_kafka ).,0.104,0.0,0.896,-0.296
Zipkin,50268263,50267380,6,"2018/05/10, 11:01:52",False,"2018/05/10, 11:01:52",8336.0,1773866.0,1,1) In order not to pick versions by yourself it’s much better if you add the dependency management via the Spring BOM,0.0,0.121,0.879,0.4404
Zipkin,50268263,50267380,6,"2018/05/10, 11:01:52",False,"2018/05/10, 11:01:52",8336.0,1773866.0,1,2) Add the dependency to spring-cloud-starter-zipkin - that way all dependent dependencies will be downloaded,0.0,0.0,1.0,0.0
Zipkin,50268263,50267380,6,"2018/05/10, 11:01:52",False,"2018/05/10, 11:01:52",8336.0,1773866.0,1,"3) To automatically configure rabbit, simply add the spring-rabbit dependency",0.0,0.0,1.0,0.0
Zipkin,50203216,50203215,0,"2018/05/06, 21:33:41",True,"2018/05/06, 21:33:41",93.0,6816012.0,0,"the simplest solution i have found for solving broken ui for zipkin thru gateway
is by changing following property of zipkin-server-shared.yml file inside zipkin server",0.108,0.163,0.729,0.1531
Zipkin,50203216,50203215,0,"2018/05/06, 21:33:41",True,"2018/05/06, 21:33:41",93.0,6816012.0,0,"zipkin:
  ui:
   base-path: /zipkin
 
change above property to",0.0,0.0,1.0,0.0
Zipkin,50203216,50203215,0,"2018/05/06, 21:33:41",True,"2018/05/06, 21:33:41",93.0,6816012.0,0,"zipkin:
  ui:
   base-path: /api/tracing/zipkin",0.0,0.0,1.0,0.0
Zipkin,50203216,50203215,0,"2018/05/06, 21:33:41",True,"2018/05/06, 21:33:41",93.0,6816012.0,0,"and change ur zuul path to following
 zuul.routes.zipkin.path=/api/tracing/*",0.0,0.0,1.0,0.0
Zipkin,50203216,50203215,0,"2018/05/06, 21:33:41",True,"2018/05/06, 21:33:41",93.0,6816012.0,0,and than access zipkin using follwing url,0.0,0.0,1.0,0.0
Zipkin,50203216,50203215,0,"2018/05/06, 21:33:41",True,"2018/05/06, 21:33:41",93.0,6816012.0,0,https://gatewayhost:port/api/tracing/zipkin/,0.0,0.0,1.0,0.0
Zipkin,50203216,50203215,0,"2018/05/06, 21:33:41",True,"2018/05/06, 21:33:41",93.0,6816012.0,0,"give attention to small details in config and dont forget to put trailing ""/"" after zipkin  in url",0.0,0.089,0.911,0.1695
Zipkin,46093745,46092526,2,"2017/09/07, 13:12:45",False,"2017/09/07, 13:12:45",8336.0,1773866.0,0,It has nothing to do with Spring Cloud Sleuth or Zipkin.,0.0,0.0,1.0,0.0
Zipkin,46093745,46092526,2,"2017/09/07, 13:12:45",False,"2017/09/07, 13:12:45",8336.0,1773866.0,0,@SpringBootApplication automatically does @ComponentScan so all @RestController classes will get registered as beans if they are in the same package as your @SpringBootApplication annotated class or if it's in the child packages.,0.0,0.0,1.0,0.0
Zipkin,46093745,46092526,2,"2017/09/07, 13:12:45",False,"2017/09/07, 13:12:45",8336.0,1773866.0,0,Please read and try to understand how Spring Boot works by reading this chapter of the docs -  https://docs.spring.io/spring-boot/docs/current/reference/html/using-boot-using-springbootapplication-annotation.html,0.0,0.119,0.881,0.3182
Zipkin,59260266,48857181,0,"2019/12/10, 06:31:59",False,"2019/12/10, 06:31:59",907.0,3941521.0,0,Object.keys(headers).filter((key) =&gt; TRACING_HEADERS.includes(key)).map((key) =&gt; headers[key])  returns an  array .,0.0,0.0,1.0,0.0
Zipkin,59260266,48857181,0,"2019/12/10, 06:31:59",False,"2019/12/10, 06:31:59",907.0,3941521.0,0,What you want is:,0.0,0.302,0.698,0.0772
Zipkin,59260266,48857181,0,"2019/12/10, 06:31:59",False,"2019/12/10, 06:31:59",907.0,3941521.0,0,I'm pretty sure this isn't an istio / distributed tracing issue ;-),0.0,0.484,0.516,0.7579
Zipkin,65147869,48857181,0,"2020/12/04, 19:17:32",False,"2020/12/04, 19:17:32",36.0,4216591.0,0,b3-propagation of x-b3-parentspanid ( https://github.com/openzipkin/b3-propagation ) can be configured in your application.yml by adding:,0.0,0.0,1.0,0.0
Zipkin,60284574,60190181,1,"2020/02/18, 17:41:23",False,"2020/02/18, 17:41:23",593.0,2750758.0,0,Details of error (Java stack trace) would be really useful here.,0.181,0.214,0.604,0.1263
Zipkin,60284574,60190181,1,"2020/02/18, 17:41:23",False,"2020/02/18, 17:41:23",593.0,2750758.0,0,"By error message I assume, you are using  qpid JMS client , that is performing check of message properties' names.",0.137,0.0,0.863,-0.4019
Zipkin,60284574,60190181,1,"2020/02/18, 17:41:23",False,"2020/02/18, 17:41:23",593.0,2750758.0,0,"These names can contain only characters, that are valid  Java identifier characters .",0.0,0.0,1.0,0.0
Zipkin,60284574,60190181,1,"2020/02/18, 17:41:23",False,"2020/02/18, 17:41:23",593.0,2750758.0,0,"In string 'queue-name' there is a '-' character, that is not Java identifier.",0.0,0.0,1.0,0.0
Zipkin,60284574,60190181,1,"2020/02/18, 17:41:23",False,"2020/02/18, 17:41:23",593.0,2750758.0,0,"To fix, you need to change 'queue-name' into something with valid characters, for example 'queue_name' (with underscore), or 'queueName' (camel case).",0.0,0.0,1.0,0.0
Zipkin,60330998,60190181,0,"2020/02/21, 03:40:33",False,"2020/02/21, 03:40:33",16250.0,8381946.0,4,Section 3.5.1 of the JMS 2 specification states this about message properties:,0.0,0.0,1.0,0.0
Zipkin,60330998,60190181,0,"2020/02/21, 03:40:33",False,"2020/02/21, 03:40:33",16250.0,8381946.0,4,Property names must obey the rules for a message selector identifier.,0.0,0.0,1.0,0.0
Zipkin,60330998,60190181,0,"2020/02/21, 03:40:33",False,"2020/02/21, 03:40:33",16250.0,8381946.0,4,"See
  Section 3.8 “Message selection” for more information.",0.0,0.0,1.0,0.0
Zipkin,60330998,60190181,0,"2020/02/21, 03:40:33",False,"2020/02/21, 03:40:33",16250.0,8381946.0,4,"In regards to identifiers, section 3.8.1.1 states, in part:",0.0,0.0,1.0,0.0
Zipkin,60330998,60190181,0,"2020/02/21, 03:40:33",False,"2020/02/21, 03:40:33",16250.0,8381946.0,4,An identifier is an unlimited-length character sequence that must begin with a Java identifier start character; all following characters must be Java identifier part characters.,0.0,0.0,1.0,0.0
Zipkin,60330998,60190181,0,"2020/02/21, 03:40:33",False,"2020/02/21, 03:40:33",16250.0,8381946.0,4,An identifier start character is any character for which the method  Character.isJavaIdentifierStart  returns  true .,0.0,0.177,0.823,0.4215
Zipkin,60330998,60190181,0,"2020/02/21, 03:40:33",False,"2020/02/21, 03:40:33",16250.0,8381946.0,4,This includes '_' and '$'.,0.0,0.0,1.0,0.0
Zipkin,60330998,60190181,0,"2020/02/21, 03:40:33",False,"2020/02/21, 03:40:33",16250.0,8381946.0,4,An identifier part character is any character for which the method   Character.isJavaIdentifierPart  returns  true .,0.0,0.177,0.823,0.4215
Zipkin,60330998,60190181,0,"2020/02/21, 03:40:33",False,"2020/02/21, 03:40:33",16250.0,8381946.0,4,If you pass the character  -  into either  Character.isJavaIdentifierStart  or  Character.isJavaIdentifierPart  the return value is  false .,0.0,0.146,0.854,0.34
Zipkin,60330998,60190181,0,"2020/02/21, 03:40:33",False,"2020/02/21, 03:40:33",16250.0,8381946.0,4,"In other words,  the  -  character in the name of a message property violates the JMS specification  and therefore will cause an error.",0.24,0.0,0.76,-0.7184
Zipkin,60392406,60190181,0,"2020/02/25, 12:26:42",True,"2020/02/25, 12:26:42",98.0,11377504.0,1,From the error message its obvious that you are using qpid JMS client for communication through queues.,0.144,0.0,0.856,-0.4019
Zipkin,60392406,60190181,0,"2020/02/25, 12:26:42",True,"2020/02/25, 12:26:42",98.0,11377504.0,1,qpid client won’t allow any keys which violates java variable naming convention e.g.,0.204,0.117,0.679,-0.34
Zipkin,60392406,60190181,0,"2020/02/25, 12:26:42",True,"2020/02/25, 12:26:42",98.0,11377504.0,1,"you won’t be able to send x-request-id in a queue’s header
which qpid jms client is consuming as it’ll throw error.",0.124,0.0,0.876,-0.4019
Zipkin,60392406,60190181,0,"2020/02/25, 12:26:42",True,"2020/02/25, 12:26:42",98.0,11377504.0,1,You need to take care of istio/zipkin to not to add certain headers (id you don’t need them actually) with the queue when its trying to communicate on azure bus.,0.055,0.097,0.848,0.3369
Zipkin,60392406,60190181,0,"2020/02/25, 12:26:42",True,"2020/02/25, 12:26:42",98.0,11377504.0,1,So you have to disable the istio/zipkin libraries  to intercept the request for queues so that request to/from queue can be made without headers.,0.0,0.0,1.0,0.0
Zipkin,60392406,60190181,0,"2020/02/25, 12:26:42",True,"2020/02/25, 12:26:42",98.0,11377504.0,1,This will fix the issue.,0.0,0.0,1.0,0.0
Zipkin,57331090,57319678,1,"2019/08/02, 20:28:15",False,"2019/08/02, 20:28:15",103.0,4522858.0,2,"It the application.properties file for each eureka client ,  I added/changed",0.0,0.0,1.0,0.0
Zipkin,57331090,57319678,1,"2019/08/02, 20:28:15",False,"2019/08/02, 20:28:15",103.0,4522858.0,2,------------------ client,0.0,0.0,1.0,0.0
Zipkin,57331090,57319678,1,"2019/08/02, 20:28:15",False,"2019/08/02, 20:28:15",103.0,4522858.0,2,-------------------- eureka server application.property--------------------,0.0,0.0,1.0,0.0
Zipkin,59930544,57319678,0,"2020/01/27, 13:47:06",False,"2020/09/28, 22:28:46",121.0,5897631.0,12,I was facing a similar issue where the eureka server was registering the services at host.docker.internal instead of localhost.,0.0,0.0,1.0,0.0
Zipkin,59930544,57319678,0,"2020/01/27, 13:47:06",False,"2020/09/28, 22:28:46",121.0,5897631.0,12,The issue in my case was an altered host file at location C:\Windows\System32\Drivers\etc\hosts.,0.0,0.0,1.0,0.0
Zipkin,59930544,57319678,0,"2020/01/27, 13:47:06",False,"2020/09/28, 22:28:46",121.0,5897631.0,12,I deleted all the lines in the host file and saved it using npp with admin privilege.,0.0,0.275,0.725,0.6486
Zipkin,59930544,57319678,0,"2020/01/27, 13:47:06",False,"2020/09/28, 22:28:46",121.0,5897631.0,12,Restart the server post this change.,0.0,0.0,1.0,0.0
Zipkin,59930544,57319678,0,"2020/01/27, 13:47:06",False,"2020/09/28, 22:28:46",121.0,5897631.0,12,Looks like 'Docker Desktop' was changing the hostfile.,0.0,0.263,0.737,0.3612
Zipkin,61323578,57319678,1,"2020/04/20, 16:24:19",False,"2020/04/20, 16:24:19",21.0,2579322.0,1,"""message"": ""Connection refused: no further information: host.docker.internal in eureka gateway error",0.47,0.0,0.53,-0.7269
Zipkin,61323578,57319678,1,"2020/04/20, 16:24:19",False,"2020/04/20, 16:24:19",21.0,2579322.0,1,Resolution:,0.0,0.0,1.0,0.0
Zipkin,61323578,57319678,1,"2020/04/20, 16:24:19",False,"2020/04/20, 16:24:19",21.0,2579322.0,1,"check ping host.docker.internal
response is some ip addresses apart form local host i,e 127.0.0.1
remove the C:\Windows\System32\Drivers\etc\hosts.file entries , make it empty",0.083,0.0,0.917,-0.2023
Zipkin,61323578,57319678,1,"2020/04/20, 16:24:19",False,"2020/04/20, 16:24:19",21.0,2579322.0,1,then restart eureka and your microservice instance.,0.0,0.0,1.0,0.0
Zipkin,61323578,57319678,1,"2020/04/20, 16:24:19",False,"2020/04/20, 16:24:19",21.0,2579322.0,1,also will find the message like below in the log this ensures you are registered in eureka,0.0,0.135,0.865,0.3612
Zipkin,61323578,57319678,1,"2020/04/20, 16:24:19",False,"2020/04/20, 16:24:19",21.0,2579322.0,1,"DiscoveryClient_BEER-SERVICE/DESKTOP-G2AIGG1:beer-service:
splitting the above log message which denotes discovery client 
BEER-SERVICE is my service and 
DESKTOP-G2AIGG1 is my pc name
beer-service is the service registered.",0.0,0.0,1.0,0.0
Zipkin,63259763,57319678,1,"2020/08/05, 09:43:08",False,"2020/08/05, 09:43:08",3449.0,387417.0,0,"I was also facing the same issue, when I was loadbalancing my restTemplate.",0.0,0.0,1.0,0.0
Zipkin,63259763,57319678,1,"2020/08/05, 09:43:08",False,"2020/08/05, 09:43:08",3449.0,387417.0,0,Something like this,0.0,0.556,0.444,0.3612
Zipkin,63259763,57319678,1,"2020/08/05, 09:43:08",False,"2020/08/05, 09:43:08",3449.0,387417.0,0,This is because of the ribbon client.,0.0,0.0,1.0,0.0
Zipkin,63259763,57319678,1,"2020/08/05, 09:43:08",False,"2020/08/05, 09:43:08",3449.0,387417.0,0,"So, without making any changes in the host file, when i deleted this code and made use of  RestTemplateBuilder  to get restTemplate, everything was working fine.",0.0,0.07,0.93,0.2023
Zipkin,63259763,57319678,1,"2020/08/05, 09:43:08",False,"2020/08/05, 09:43:08",3449.0,387417.0,0,Code Example:,0.0,0.0,1.0,0.0
Zipkin,63259763,57319678,1,"2020/08/05, 09:43:08",False,"2020/08/05, 09:43:08",3449.0,387417.0,0,You can try this approach as well.,0.0,0.259,0.741,0.2732
Zipkin,63283687,57319678,0,"2020/08/06, 15:30:53",True,"2020/08/06, 15:30:53",103.0,4522858.0,1,Thanks for the tip on the host file on windows,0.0,0.244,0.756,0.4404
Zipkin,63283687,57319678,0,"2020/08/06, 15:30:53",True,"2020/08/06, 15:30:53",103.0,4522858.0,1,I found that docker adds aliases  in the host file for host.docker.internal and gateway.docker.internal.,0.0,0.0,1.0,0.0
Zipkin,63283687,57319678,0,"2020/08/06, 15:30:53",True,"2020/08/06, 15:30:53",103.0,4522858.0,1,I am guessing that Eureka does a host lookup from the IP and host.docker.internal is returned.,0.0,0.0,1.0,0.0
Zipkin,63283687,57319678,0,"2020/08/06, 15:30:53",True,"2020/08/06, 15:30:53",103.0,4522858.0,1,"I am not an expert at hosts files, but I added an alias for my actual host name to my IP (note: we use static ip's).",0.0,0.0,1.0,0.0
Zipkin,63283687,57319678,0,"2020/08/06, 15:30:53",True,"2020/08/06, 15:30:53",103.0,4522858.0,1,"After doing this, docker did not change my host file on reboot and the reverse lookup of the ip-&gt;host now returns my machine name instead of host.docker.internal",0.0,0.0,1.0,0.0
Zipkin,66593805,57319678,0,"2021/03/12, 05:34:31",False,"2021/03/12, 05:34:31",11.0,13366251.0,1,"Solution for Window 10:
You don't have to remove all the lines from hosts files.",0.0,0.141,0.859,0.3182
Zipkin,66593805,57319678,0,"2021/03/12, 05:34:31",False,"2021/03/12, 05:34:31",11.0,13366251.0,1,"Just comment this if exists (#192.168.1.4 host.docker.internal) (as we use this when playing with docker)
And paste this (127.0.0.1   host.docker.internal)
It worked for me.",0.0,0.073,0.927,0.2023
Zipkin,42982623,42982050,4,"2017/03/23, 19:08:47",True,"2017/03/23, 19:08:47",8336.0,1773866.0,1,You can use the new Dalston feature of using annotations on Spring Data repositories.,0.0,0.0,1.0,0.0
Zipkin,42982623,42982050,4,"2017/03/23, 19:08:47",True,"2017/03/23, 19:08:47",8336.0,1773866.0,1,You can check out this for more info  http://cloud.spring.io/spring-cloud-sleuth/spring-cloud-sleuth.html#_managing_spans_with_annotations,0.0,0.0,1.0,0.0
Zipkin,49212158,49211771,2,"2018/03/10, 19:47:36",True,"2018/03/10, 19:53:21",6010.0,5210117.0,5,This is really strange because you are using latest relase and in the GitHub spring-cloud-sleuth depends to  &lt;brave.version&gt;4.17.2&lt;/brave.version&gt; .,0.11,0.0,0.89,-0.2716
Zipkin,49212158,49211771,2,"2018/03/10, 19:47:36",True,"2018/03/10, 19:53:21",6010.0,5210117.0,5,And I think 4.16.3-SNAPSHOT version is not exists in the maven repo.,0.0,0.0,1.0,0.0
Zipkin,49212158,49211771,2,"2018/03/10, 19:47:36",True,"2018/03/10, 19:53:21",6010.0,5210117.0,5,(just checked 2.0.0.M8 depends to this version),0.0,0.0,1.0,0.0
Zipkin,49212158,49211771,2,"2018/03/10, 19:47:36",True,"2018/03/10, 19:53:21",6010.0,5210117.0,5,If you change to  &lt;sleuth.version&gt;2.0.0.M7&lt;/sleuth.version&gt;  it does find the required dependencies.,0.0,0.0,1.0,0.0
Zipkin,49212158,49211771,2,"2018/03/10, 19:47:36",True,"2018/03/10, 19:53:21",6010.0,5210117.0,5,https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/pom.xml,0.0,0.0,1.0,0.0
Zipkin,49212538,49211771,1,"2018/03/10, 20:24:25",False,"2018/03/10, 22:18:03",8336.0,1773866.0,2,The M8 for sleuth was broken.,0.303,0.306,0.391,0.0085
Zipkin,49212538,49211771,1,"2018/03/10, 20:24:25",False,"2018/03/10, 22:18:03",8336.0,1773866.0,2,That issue will be fixed in M9.,0.0,0.0,1.0,0.0
Zipkin,49212538,49211771,1,"2018/03/10, 20:24:25",False,"2018/03/10, 22:18:03",8336.0,1773866.0,2,You can use M8 but you have to explicitly change the brave version to some release one.,0.0,0.308,0.692,0.7695
Zipkin,43232907,43230619,1,"2017/04/05, 16:39:23",True,"2017/04/05, 16:39:23",399.0,6921715.0,13,"For 1, 2, 3 it was because I was doing a new RestTemplate.",0.0,0.0,1.0,0.0
Zipkin,43232907,43230619,1,"2017/04/05, 16:39:23",True,"2017/04/05, 16:39:23",399.0,6921715.0,13,The doc says :,0.0,0.0,1.0,0.0
Zipkin,43232907,43230619,1,"2017/04/05, 16:39:23",True,"2017/04/05, 16:39:23",399.0,6921715.0,13,You have to register RestTemplate as a bean so that the interceptors will get injected.,0.0,0.0,1.0,0.0
Zipkin,43232907,43230619,1,"2017/04/05, 16:39:23",True,"2017/04/05, 16:39:23",399.0,6921715.0,13,If you create a RestTemplate instance with a new keyword then the instrumentation WILL NOT work.,0.0,0.139,0.861,0.2732
Zipkin,43232907,43230619,1,"2017/04/05, 16:39:23",True,"2017/04/05, 16:39:23",399.0,6921715.0,13,"So RTFM for myself, and this solved my 3 first problems :",0.207,0.182,0.612,-0.0836
Zipkin,30204797,23032944,4,"2015/05/13, 05:49:26",False,"2015/05/13, 05:49:26",15576.0,677561.0,1,The first step to good searching in elasticsearch is to create fields from your data.,0.0,0.278,0.722,0.6124
Zipkin,30204797,23032944,4,"2015/05/13, 05:49:26",False,"2015/05/13, 05:49:26",15576.0,677561.0,1,"With logs, logstash is the proper tool.",0.0,0.0,1.0,0.0
Zipkin,30204797,23032944,4,"2015/05/13, 05:49:26",False,"2015/05/13, 05:49:26",15576.0,677561.0,1,The grok{} filter uses patterns (existing or user-defined regexps) to split the input into fields.,0.0,0.0,1.0,0.0
Zipkin,30204797,23032944,4,"2015/05/13, 05:49:26",False,"2015/05/13, 05:49:26",15576.0,677561.0,1,You would need to make sure that it was mapped to an integer (e.g.,0.0,0.15,0.85,0.3182
Zipkin,30204797,23032944,4,"2015/05/13, 05:49:26",False,"2015/05/13, 05:49:26",15576.0,677561.0,1,%{INT:duration:int} in your pattern).,0.0,0.0,1.0,0.0
Zipkin,30204797,23032944,4,"2015/05/13, 05:49:26",False,"2015/05/13, 05:49:26",15576.0,677561.0,1,"You could then query elasticsearch for ""duration: 1000"" to get the results.",0.0,0.0,1.0,0.0
Zipkin,30204797,23032944,4,"2015/05/13, 05:49:26",False,"2015/05/13, 05:49:26",15576.0,677561.0,1,"Elasticsearch uses the lucene query engine, so you can find sample queries based on that.",0.0,0.0,1.0,0.0
Zipkin,19034042,19032203,5,"2013/09/26, 19:49:25",False,"2013/09/26, 20:56:50",15073.0,14032.0,8,Zipkin is the best solution.,0.0,0.684,0.316,0.7579
Zipkin,19034042,19032203,5,"2013/09/26, 19:49:25",False,"2013/09/26, 20:56:50",15073.0,14032.0,8,--zipkin developer,0.0,0.0,1.0,0.0
Zipkin,19034042,19032203,5,"2013/09/26, 19:49:25",False,"2013/09/26, 20:56:50",15073.0,14032.0,8,"EDIT  - Ok ok, here's a serious answer:",0.149,0.506,0.345,0.4767
Zipkin,19034042,19032203,5,"2013/09/26, 19:49:25",False,"2013/09/26, 20:56:50",15073.0,14032.0,8,Zipkin is a distributed tracing system developed by Twitter because our service-oriented-architecture is so goddamned big that it's often hard to understand WTF is happening in any given request.,0.276,0.0,0.724,-0.8608
Zipkin,19034042,19032203,5,"2013/09/26, 19:49:25",False,"2013/09/26, 20:56:50",15073.0,14032.0,8,"Seriously, here's a visualization in Zipkin of all the services dependencies at twitter:",0.134,0.0,0.866,-0.1779
Zipkin,19034042,19032203,5,"2013/09/26, 19:49:25",False,"2013/09/26, 20:56:50",15073.0,14032.0,8,Is your platform this intense?,0.0,0.256,0.744,0.0964
Zipkin,19034042,19032203,5,"2013/09/26, 19:49:25",False,"2013/09/26, 20:56:50",15073.0,14032.0,8,You should use zipkin.,0.0,0.0,1.0,0.0
Zipkin,19034042,19032203,5,"2013/09/26, 19:49:25",False,"2013/09/26, 20:56:50",15073.0,14032.0,8,Did I mention it's one of the best scaling systems I've ever seen?,0.0,0.276,0.724,0.6369
Zipkin,19034042,19032203,5,"2013/09/26, 19:49:25",False,"2013/09/26, 20:56:50",15073.0,14032.0,8,"It has zero problem keeping up with twitter-level load, and that might be important to you if you're that big.",0.12,0.08,0.8,-0.2263
Zipkin,19034042,19032203,5,"2013/09/26, 19:49:25",False,"2013/09/26, 20:56:50",15073.0,14032.0,8,What's that you say?,0.0,0.0,1.0,0.0
Zipkin,19034042,19032203,5,"2013/09/26, 19:49:25",False,"2013/09/26, 20:56:50",15073.0,14032.0,8,You're not as big as twitter?,0.0,0.0,1.0,0.0
Zipkin,19034042,19032203,5,"2013/09/26, 19:49:25",False,"2013/09/26, 20:56:50",15073.0,14032.0,8,"You only have three services: a web frontend, some kind of middleware, and your database backend?",0.0,0.0,1.0,0.0
Zipkin,19034042,19032203,5,"2013/09/26, 19:49:25",False,"2013/09/26, 20:56:50",15073.0,14032.0,8,Maybe zipkin is a bit overkill for you.,0.0,0.0,1.0,0.0
Zipkin,19034042,19032203,5,"2013/09/26, 19:49:25",False,"2013/09/26, 20:56:50",15073.0,14032.0,8,"We've done some work to make it a bit easier to setup, but really my job isn't to make zipkin easy for you, it's to make zipkin awesome for Twitter.",0.0,0.305,0.695,0.9081
Zipkin,19034042,19032203,5,"2013/09/26, 19:49:25",False,"2013/09/26, 20:56:50",15073.0,14032.0,8,"Still, if you plan on scaling scala, the twitter stack with Finagle etc is insanely good.",0.0,0.162,0.838,0.4404
Zipkin,19034042,19032203,5,"2013/09/26, 19:49:25",False,"2013/09/26, 20:56:50",15073.0,14032.0,8,Don't let all the evangelists from Typesafe fool you.,0.266,0.0,0.734,-0.4404
Zipkin,19034042,19032203,5,"2013/09/26, 19:49:25",False,"2013/09/26, 20:56:50",15073.0,14032.0,8,Their stack has some serious deficiencies when you try to deploy it in massive-scale architectures.,0.085,0.0,0.915,-0.0772
Zipkin,19034042,19032203,5,"2013/09/26, 19:49:25",False,"2013/09/26, 20:56:50",15073.0,14032.0,8,"But again, our job isn't to tell you how good our stack is, or even help you use it.",0.0,0.303,0.697,0.8126
Zipkin,19034042,19032203,5,"2013/09/26, 19:49:25",False,"2013/09/26, 20:56:50",15073.0,14032.0,8,It's to make our stack awesome.,0.0,0.451,0.549,0.6249
Zipkin,56529683,56525260,0,"2019/06/10, 18:54:49",False,"2019/06/10, 18:54:49",525.0,4504053.0,10,"You can add the following setting on your properties key to disable zipkin,  source .",0.0,0.0,1.0,0.0
Zipkin,56529683,56525260,0,"2019/06/10, 18:54:49",False,"2019/06/10, 18:54:49",525.0,4504053.0,10,"Better yet, create separate development properties (like  application-dev.properties ) to avoid changing above setting everytime you want to run in your machine:  https://stackoverflow.com/a/34846351/4504053",0.083,0.238,0.679,0.4767
Zipkin,47010922,47008485,4,"2017/10/30, 10:38:04",False,"2017/10/30, 10:38:04",8336.0,1773866.0,2,Most likely your code is broken.,0.383,0.0,0.617,-0.4767
Zipkin,47010922,47008485,4,"2017/10/30, 10:38:04",False,"2017/10/30, 10:38:04",8336.0,1773866.0,2,You can check out the  https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-zipkin/src/main/java/org/springframework/cloud/sleuth/zipkin/ZipkinAutoConfiguration.java  class where for Edgware we've added load balanced zipkin server resolution.,0.0,0.0,1.0,0.0
Zipkin,47139081,47008485,1,"2017/11/06, 16:23:51",True,"2017/11/06, 16:23:51",1260.0,4864870.0,0,Is it correct that you are using the code example from the baeldung tutorial?,0.0,0.0,1.0,0.0
Zipkin,47139081,47008485,1,"2017/11/06, 16:23:51",True,"2017/11/06, 16:23:51",1260.0,4864870.0,0,( http://www.baeldung.com/tracing-services-with-zipkin  - 3.2.,0.0,0.0,1.0,0.0
Zipkin,47139081,47008485,1,"2017/11/06, 16:23:51",True,"2017/11/06, 16:23:51",1260.0,4864870.0,0,Spring Config),0.0,0.0,1.0,0.0
Zipkin,47139081,47008485,1,"2017/11/06, 16:23:51",True,"2017/11/06, 16:23:51",1260.0,4864870.0,0,I think there is a mistake with line 34 and 35 (the closing curly brace).,0.167,0.0,0.833,-0.34
Zipkin,47139081,47008485,1,"2017/11/06, 16:23:51",True,"2017/11/06, 16:23:51",1260.0,4864870.0,0,I've fixed the problem by modifing the method like this:,0.205,0.189,0.606,-0.0516
Zipkin,47139081,47008485,1,"2017/11/06, 16:23:51",True,"2017/11/06, 16:23:51",1260.0,4864870.0,0,Maybe this helps someone or maybe someone from baeldung reads this and could verify and correct the code example.,0.0,0.126,0.874,0.3818
Zipkin,47139081,47008485,1,"2017/11/06, 16:23:51",True,"2017/11/06, 16:23:51",1260.0,4864870.0,0,;),0.0,1.0,0.0,0.2263
Zipkin,44664880,44643259,3,"2017/06/21, 03:09:57",True,"2017/06/23, 08:31:36",,,4,Problem solved.,0.562,0.437,0.0,-0.1531
Zipkin,44664880,44643259,3,"2017/06/21, 03:09:57",True,"2017/06/23, 08:31:36",,,4,tracer.withSpanInScope(clientSpan)  would do the work.,0.0,0.0,1.0,0.0
Zipkin,44664880,44643259,3,"2017/06/21, 03:09:57",True,"2017/06/23, 08:31:36",,,4,"Note that,  withSpanInScope(...)  has not been called before sending messages .",0.0,0.0,1.0,0.0
Zipkin,40358966,39870535,0,"2016/11/01, 13:16:38",False,"2016/11/01, 13:16:38",762.0,1227937.0,1,"Some people use zipkin to identify dead services, but probably metrics/stats would be the better route if you are trying to break down and report by thrift method.",0.082,0.118,0.8,0.296
Zipkin,19022596,19022191,0,"2013/09/26, 11:03:01",False,"2013/09/26, 11:03:01",4556.0,1097599.0,5,Well before starting digging into JVM stuff or setting up all the infrastructure needed by Zipkin you could simply start by measuring some application-level metrics.,0.0,0.08,0.92,0.2732
Zipkin,19022596,19022191,0,"2013/09/26, 11:03:01",False,"2013/09/26, 11:03:01",4556.0,1097599.0,5,You could try the library  metrics  via this  scala api .,0.0,0.0,1.0,0.0
Zipkin,19022596,19022191,0,"2013/09/26, 11:03:01",False,"2013/09/26, 11:03:01",4556.0,1097599.0,5,Basically you manually set up counters and gauges at specific points of your application that will help you diagnose your bottleneck problem.,0.106,0.106,0.787,0.0
Zipkin,66205972,66167917,5,"2021/02/15, 12:01:11",True,"2021/03/01, 10:16:15",8336.0,1773866.0,2,The problem might be related to the fact that you're creating the Feign builder manually via  Feign.builder()  factory method.,0.123,0.1,0.776,-0.128
Zipkin,66205972,66167917,5,"2021/02/15, 12:01:11",True,"2021/03/01, 10:16:15",8336.0,1773866.0,2,We're unable to instrument that call.,0.0,0.0,1.0,0.0
Zipkin,66205972,66167917,5,"2021/02/15, 12:01:11",True,"2021/03/01, 10:16:15",8336.0,1773866.0,2,You should create a bean (via  SleuthFeignBuilder.builder ) and inject that into your code.,0.0,0.16,0.84,0.2732
Zipkin,62871239,62870927,19,"2020/07/13, 10:38:42",True,"2020/07/13, 10:38:42",26220.0,927493.0,3,Dependencies are resolved before plugins are executed.,0.0,0.221,0.779,0.1779
Zipkin,62871239,62870927,19,"2020/07/13, 10:38:42",True,"2020/07/13, 10:38:42",26220.0,927493.0,3,So the properties you read with the properties-maven-plugin are not available in the  &lt;dependencies&gt;  section.,0.0,0.0,1.0,0.0
Zipkin,62871239,62870927,19,"2020/07/13, 10:38:42",True,"2020/07/13, 10:38:42",26220.0,927493.0,3,"If you want to set a dependency version by property, this property must be set inside the POM, on the command line or in the  settings.xml .",0.0,0.051,0.949,0.0772
Zipkin,62871339,62870927,0,"2020/07/13, 10:45:40",False,"2020/07/13, 10:53:28",1709.0,8484783.0,-1,"You are using an old version of the plugin ( 1.0-alpha-2 ), update it to the  latest   1.0.0 .",0.0,0.0,1.0,0.0
Zipkin,62871339,62870927,0,"2020/07/13, 10:45:40",False,"2020/07/13, 10:53:28",1709.0,8484783.0,-1,Then make sure that the file  version.properties  is in the folder  C:\Workspace .,0.0,0.173,0.827,0.3182
Zipkin,62871339,62870927,0,"2020/07/13, 10:45:40",False,"2020/07/13, 10:53:28",1709.0,8484783.0,-1,"Anyway, with the latest version of the plugin you should get a proper error message if it can't find the file.",0.124,0.0,0.876,-0.4019
Zipkin,62871339,62870927,0,"2020/07/13, 10:45:40",False,"2020/07/13, 10:53:28",1709.0,8484783.0,-1,One more suggestion:  spring-cloud-starter-zipkin  belongs to the  org.springframework.cloud  group which follows another version.,0.0,0.0,1.0,0.0
Zipkin,62871339,62870927,0,"2020/07/13, 10:45:40",False,"2020/07/13, 10:53:28",1709.0,8484783.0,-1,The suggested way to declare that dependency is like the following:,0.0,0.2,0.8,0.3612
Zipkin,62756345,62674846,2,"2020/07/06, 15:42:41",False,"2020/07/06, 15:42:41",50.0,11866103.0,1,I was able to setup OpenCensus in GCP (in an instance on the project) by following the steps  mentioned here .,0.0,0.0,1.0,0.0
Zipkin,62756345,62674846,2,"2020/07/06, 15:42:41",False,"2020/07/06, 15:42:41",50.0,11866103.0,1,"To set it up quickly, here are the commands I ran in a brand new Ubuntu instance",0.0,0.0,1.0,0.0
Zipkin,59925933,59924890,2,"2020/01/27, 08:15:42",False,"2020/01/27, 08:20:48",26898.0,1192728.0,0,You should use  egress-gateway .,0.0,0.0,1.0,0.0
Zipkin,59925933,59924890,2,"2020/01/27, 08:15:42",False,"2020/01/27, 08:20:48",26898.0,1192728.0,0,"When all external calls go to the gateway, istio can get the metadata and does some tracing works.",0.0,0.0,1.0,0.0
Zipkin,59925933,59924890,2,"2020/01/27, 08:15:42",False,"2020/01/27, 08:20:48",26898.0,1192728.0,0,There are many advantages when using ingress/egress gateway:,0.0,0.263,0.737,0.3612
Zipkin,59985374,59924890,0,"2020/01/30, 13:47:03",False,"2020/01/30, 13:47:03",5749.0,11977760.0,0,Based on  envoy documentation  it doesn't support https tracing.,0.22,0.0,0.78,-0.3089
Zipkin,59985374,59924890,0,"2020/01/30, 13:47:03",False,"2020/01/30, 13:47:03",5749.0,11977760.0,0,The tracing configuration specifies global settings for the HTTP tracer used by Envoy.,0.0,0.0,1.0,0.0
Zipkin,59985374,59924890,0,"2020/01/30, 13:47:03",False,"2020/01/30, 13:47:03",5749.0,11977760.0,0,The configuration is defined by the Bootstrap tracing field.,0.0,0.0,1.0,0.0
Zipkin,59985374,59924890,0,"2020/01/30, 13:47:03",False,"2020/01/30, 13:47:03",5749.0,11977760.0,0,"Envoy may support other tracers in the future, but right now the HTTP tracer is the only one supported.",0.0,0.22,0.78,0.5859
Zipkin,59985374,59924890,0,"2020/01/30, 13:47:03",False,"2020/01/30, 13:47:03",5749.0,11977760.0,0,And this post on  stackoverflow,0.0,0.0,1.0,0.0
Zipkin,59985374,59924890,0,"2020/01/30, 13:47:03",False,"2020/01/30, 13:47:03",5749.0,11977760.0,0,"HTTPS (HTTP over SSL) sends all HTTP content over a SSL tunel, so HTTP content and headers are encrypted as well.",0.0,0.1,0.9,0.2732
Zipkin,59985374,59924890,0,"2020/01/30, 13:47:03",False,"2020/01/30, 13:47:03",5749.0,11977760.0,0,"I have even tried to reproduce that, but like in your case zipkin worked only for http.",0.0,0.178,0.822,0.5023
Zipkin,59985374,59924890,0,"2020/01/30, 13:47:03",False,"2020/01/30, 13:47:03",5749.0,11977760.0,0,Based on that I would say it's not possible to use zipkin to track https.,0.0,0.0,1.0,0.0
Zipkin,58562527,58562126,2,"2019/10/25, 19:38:58",False,"2019/10/25, 19:38:58",6038.0,11032044.0,0,It's because you haven't mentioned the  host  here:,0.0,0.0,1.0,0.0
Zipkin,58581451,58562126,1,"2019/10/27, 18:52:20",False,"2019/10/27, 18:52:20",1017.0,6713869.0,0,"First, previous answer is wrong, you don't need to specify  host  it is not mandatory unless you want to set up a DNS.",0.176,0.053,0.772,-0.4628
Zipkin,58581451,58562126,1,"2019/10/27, 18:52:20",False,"2019/10/27, 18:52:20",1017.0,6713869.0,0,"Second, the backend  zipkin  requires the  /zipkin  URI to respond right?",0.0,0.0,1.0,0.0
Zipkin,58581451,58562126,1,"2019/10/27, 18:52:20",False,"2019/10/27, 18:52:20",1017.0,6713869.0,0,"If this is the case, then the rewrite annotation is removing the URI.",0.0,0.0,1.0,0.0
Zipkin,58581451,58562126,1,"2019/10/27, 18:52:20",False,"2019/10/27, 18:52:20",1017.0,6713869.0,0,So you would need to change your yaml like this to pass  /zipkin  to your backend.,0.0,0.143,0.857,0.3612
Zipkin,58591046,58562126,0,"2019/10/28, 14:35:42",False,"2019/10/28, 14:35:42",2049.0,11207414.0,0,Just to clarify the OP problem.,0.351,0.0,0.649,-0.4019
Zipkin,58591046,58562126,0,"2019/10/28, 14:35:42",False,"2019/10/28, 14:35:42",2049.0,11207414.0,0,There are different  ingress Controllers,0.0,0.0,1.0,0.0
Zipkin,58591046,58562126,0,"2019/10/28, 14:35:42",False,"2019/10/28, 14:35:42",2049.0,11207414.0,0,Note:,0.0,0.0,1.0,0.0
Zipkin,58591046,58562126,0,"2019/10/28, 14:35:42",False,"2019/10/28, 14:35:42",2049.0,11207414.0,0,"When you create an ingress, you should annotate each ingress with the appropriate ingress.class to indicate which ingress controller should be used if more than one exists within your cluster.",0.0,0.068,0.932,0.2732
Zipkin,58591046,58562126,0,"2019/10/28, 14:35:42",False,"2019/10/28, 14:35:42",2049.0,11207414.0,0,"If you do not define a class, your cloud provider may use a default ingress controller.",0.0,0.0,1.0,0.0
Zipkin,58591046,58562126,0,"2019/10/28, 14:35:42",False,"2019/10/28, 14:35:42",2049.0,11207414.0,0,"Ideally, all ingress controllers should fulfill this specification, but the various ingress controllers operate slightly differently.",0.0,0.216,0.784,0.431
Zipkin,58591046,58562126,0,"2019/10/28, 14:35:42",False,"2019/10/28, 14:35:42",2049.0,11207414.0,0,Using this annotation:,0.0,0.0,1.0,0.0
Zipkin,58591046,58562126,0,"2019/10/28, 14:35:42",False,"2019/10/28, 14:35:42",2049.0,11207414.0,0,It looks like you are using NGINX Ingress Controller provided by nginxinc.,0.0,0.185,0.815,0.3612
Zipkin,58591046,58562126,0,"2019/10/28, 14:35:42",False,"2019/10/28, 14:35:42",2049.0,11207414.0,0,You can find more information about  Rewrites Support  for NGINX Ingress Controller provided by  nginxinc here .,0.0,0.153,0.847,0.4019
Zipkin,58591046,58562126,0,"2019/10/28, 14:35:42",False,"2019/10/28, 14:35:42",2049.0,11207414.0,0,example:,0.0,0.0,1.0,0.0
Zipkin,58591046,58562126,0,"2019/10/28, 14:35:42",False,"2019/10/28, 14:35:42",2049.0,11207414.0,0,It's different from the kubernetes community at  kubernetes/ingress-nginx repo .,0.0,0.0,1.0,0.0
Zipkin,58591046,58562126,0,"2019/10/28, 14:35:42",False,"2019/10/28, 14:35:42",2049.0,11207414.0,0,Different ingress controllers have different configs and annotations.,0.0,0.0,1.0,0.0
Zipkin,58591046,58562126,0,"2019/10/28, 14:35:42",False,"2019/10/28, 14:35:42",2049.0,11207414.0,0,So for this example:,0.0,0.0,1.0,0.0
Zipkin,58591046,58562126,0,"2019/10/28, 14:35:42",False,"2019/10/28, 14:35:42",2049.0,11207414.0,0,Test it:,0.0,0.0,1.0,0.0
Zipkin,57856470,57856249,2,"2019/09/09, 17:52:26",True,"2019/09/10, 12:40:49",8336.0,1773866.0,3,If S1 sends a request to S2 it's the S1 that sets the sampler value and S2 just continues the result.,0.0,0.112,0.888,0.34
Zipkin,57856470,57856249,2,"2019/09/09, 17:52:26",True,"2019/09/10, 12:40:49",8336.0,1773866.0,3,"In other words S1 will export to zipkin 100 * 0.1 = 10 requests, and S2 will export 100 * 0.1 = 10 requests.",0.0,0.0,1.0,0.0
Zipkin,57856470,57856249,2,"2019/09/09, 17:52:26",True,"2019/09/10, 12:40:49",8336.0,1773866.0,3,S1 makes a decision and S2 will continue it.,0.0,0.0,1.0,0.0
Zipkin,55661357,55575721,0,"2019/04/13, 04:59:36",False,"2019/04/13, 04:59:36",123.0,10593981.0,1,"FYI, this worked after generating dummy X-B3-SpanId; it works as long as X-B3-TraceId is unique.",0.0,0.153,0.847,0.368
Zipkin,55661357,55575721,0,"2019/04/13, 04:59:36",False,"2019/04/13, 04:59:36",123.0,10593981.0,1,e.g.,0.0,0.0,1.0,0.0
Zipkin,59222522,55571957,3,"2019/12/07, 04:36:36",True,"2019/12/07, 05:27:09",26.0,12494357.0,1,"I know this is old but I have just had exactly the same problem, and have just worked out it's being caused by the appmetrics libraries.",0.134,0.0,0.866,-0.5499
Zipkin,59222522,55571957,3,"2019/12/07, 04:36:36",True,"2019/12/07, 05:27:09",26.0,12494357.0,1,Once I figure out how to get it working I'll update this.,0.0,0.0,1.0,0.0
Zipkin,59222522,55571957,3,"2019/12/07, 04:36:36",True,"2019/12/07, 05:27:09",26.0,12494357.0,1,EDIT:,0.0,0.0,1.0,0.0
Zipkin,59222522,55571957,3,"2019/12/07, 04:36:36",True,"2019/12/07, 05:27:09",26.0,12494357.0,1,OK managed to get it working with appmetrics-dash.,0.0,0.295,0.705,0.4466
Zipkin,59222522,55571957,3,"2019/12/07, 04:36:36",True,"2019/12/07, 05:27:09",26.0,12494357.0,1,You need to use monitor() instead of attach() and move the monitor to the end of your routes as so.,0.0,0.0,1.0,0.0
Zipkin,59222522,55571957,3,"2019/12/07, 04:36:36",True,"2019/12/07, 05:27:09",26.0,12494357.0,1,I have investigated appmetrics-prometheus and it only has an attach() at this stage so can't be used:,0.0,0.0,1.0,0.0
Zipkin,55173328,54702777,0,"2019/03/15, 01:07:05",False,"2019/03/15, 01:14:36",,,1,"Alright, so I'm going to answer this based on what you said here:",0.0,0.143,0.857,0.25
Zipkin,55173328,54702777,0,"2019/03/15, 01:07:05",False,"2019/03/15, 01:14:36",,,1,Or a better approach  if there aint any support/ plugin for the same.,0.0,0.209,0.791,0.4404
Zipkin,55173328,54702777,0,"2019/03/15, 01:07:05",False,"2019/03/15, 01:14:36",,,1,"The way that I do it us through  Prometheus , in combination with  cloudwatch_exporter , and  alertmanager .",0.0,0.0,1.0,0.0
Zipkin,55173328,54702777,0,"2019/03/15, 01:07:05",False,"2019/03/15, 01:14:36",,,1,"The configuration for  cloudwatch_exporter  to monitor SQS is going to be something like (this is only two metrics, you'll need to add more based on what you're looking to monitor):",0.0,0.079,0.921,0.3612
Zipkin,55173328,54702777,0,"2019/03/15, 01:07:05",False,"2019/03/15, 01:14:36",,,1,"You'll then need to configure prometheus to scrape the  cloudwatch_exporter  endpoint at an interval, for ex what I do:",0.0,0.0,1.0,0.0
Zipkin,55173328,54702777,0,"2019/03/15, 01:07:05",False,"2019/03/15, 01:14:36",,,1,You would then configure  alertmanager  to alert based on those scraped metrics; I do not alert on those metrics so I cannot give you an example.,0.0,0.167,0.833,0.5267
Zipkin,55173328,54702777,0,"2019/03/15, 01:07:05",False,"2019/03/15, 01:14:36",,,1,"But, to give you an idea how of this architecture, a diagram is below:",0.0,0.0,1.0,0.0
Zipkin,55173328,54702777,0,"2019/03/15, 01:07:05",False,"2019/03/15, 01:14:36",,,1,If you need to use something like  statsd  you can use  statsd_exporter .,0.0,0.185,0.815,0.3612
Zipkin,55173328,54702777,0,"2019/03/15, 01:07:05",False,"2019/03/15, 01:14:36",,,1,"And, just in-case you were wondering, yes  Grafana supports prometheus .",0.0,0.394,0.606,0.6369
Zipkin,53845941,53765439,0,"2018/12/19, 08:47:57",False,"2018/12/19, 09:03:57",533.0,5736574.0,2,"As there is a bug in Spring AMQP, which will be fixed in Release 2.1.3 
 Issue link",0.0,0.0,1.0,0.0
Zipkin,53845941,53765439,0,"2018/12/19, 08:47:57",False,"2018/12/19, 09:03:57",533.0,5736574.0,2,"For a tempory fix, you can enable retry properties to create advice chain.",0.0,0.16,0.84,0.2732
Zipkin,53845941,53765439,0,"2018/12/19, 08:47:57",False,"2018/12/19, 09:03:57",533.0,5736574.0,2,Hope this resolves your problem.,0.29,0.495,0.215,0.2263
Zipkin,53967904,53765439,0,"2018/12/29, 10:21:00",False,"2018/12/29, 10:27:20",53.0,2842281.0,1,"I had this same issue, changing Spring Boot version to 2.1.0.RELEASE did the trick for me.",0.079,0.0,0.921,-0.0516
Zipkin,53967904,53765439,0,"2018/12/29, 10:21:00",False,"2018/12/29, 10:27:20",53.0,2842281.0,1,You should try it too.,0.0,0.0,1.0,0.0
Zipkin,53967904,53765439,0,"2018/12/29, 10:21:00",False,"2018/12/29, 10:27:20",53.0,2842281.0,1,There must be something wrong with  RabbitMQ in Spring Boot version 2.1.1.RELEASE.,0.22,0.0,0.78,-0.4767
Zipkin,55292618,53765439,0,"2019/03/22, 05:48:23",False,"2019/03/22, 05:48:23",11.0,9568260.0,1,add build.gradle,0.0,0.0,1.0,0.0
Zipkin,55292618,53765439,0,"2019/03/22, 05:48:23",False,"2019/03/22, 05:48:23",11.0,9568260.0,1,apply plugin: 'org.springframework.boot',0.0,0.0,1.0,0.0
Zipkin,55292618,53765439,0,"2019/03/22, 05:48:23",False,"2019/03/22, 05:48:23",11.0,9568260.0,1,"springBootVersion=2.1.3.RELEASE
springCloudVersion=Greenwich.RELEASE",0.0,0.0,1.0,0.0
Zipkin,52505155,52486463,1,"2018/09/25, 22:03:18",False,"2018/09/25, 22:03:18",5434.0,7059.0,0,"Finally got it to work removing  @AutoConfigureAfter ,  @CondtionnalOnBean  and  @ConditionnalOnMissingBean , using instead  @ConditionalOnClass ,  @ConditionnalOnMissingClass  and reproducing other  @Conditionnals  from  TraceAutoConfiguration .",0.0,0.0,1.0,0.0
Zipkin,52505155,52486463,1,"2018/09/25, 22:03:18",False,"2018/09/25, 22:03:18",5434.0,7059.0,0,"Not great, but at least working.",0.3,0.0,0.7,-0.284
Zipkin,51938178,51931755,0,"2018/08/20, 23:37:41",False,"2018/08/20, 23:37:41",8855.0,9705485.0,1,I think that Christian Posta article you refer to is very good.,0.0,0.242,0.758,0.4927
Zipkin,51938178,51931755,0,"2018/08/20, 23:37:41",False,"2018/08/20, 23:37:41",8855.0,9705485.0,1,"As he says, you can deal with the most common use-cases with the out of the box Kubernetes solutions for discovery (kub dns), load-balancing (with Services) and edge services/gateway (Ingress).",0.0,0.055,0.945,0.1779
Zipkin,51938178,51931755,0,"2018/08/20, 23:37:41",False,"2018/08/20, 23:37:41",8855.0,9705485.0,1,"As Christian also points out, if you need to dynamically discover services by actively querying rather than knowing what you are looking for then Spring Cloud Kubernetes can be better than going directly to Kubernetes Apis.",0.0,0.189,0.811,0.7717
Zipkin,51938178,51931755,0,"2018/08/20, 23:37:41",False,"2018/08/20, 23:37:41",8855.0,9705485.0,1,If you need to refresh your app from a config change and see it update quickly without going through a rolling update (which would be needed if you were mounting the configmap as a volume) then Spring cloud Kubernetes config client could be of value.,0.0,0.055,0.945,0.34
Zipkin,51938178,51931755,0,"2018/08/20, 23:37:41",False,"2018/08/20, 23:37:41",8855.0,9705485.0,1,The ribbon integration could also be of value if you need client-side load-balancing.,0.0,0.167,0.833,0.34
Zipkin,51938178,51931755,0,"2018/08/20, 23:37:41",False,"2018/08/20, 23:37:41",8855.0,9705485.0,1,So you could start out without Spring Cloud Kubernetes and add parts of it if and when you find that it would help.,0.0,0.109,0.891,0.4019
Zipkin,51938178,51931755,0,"2018/08/20, 23:37:41",False,"2018/08/20, 23:37:41",8855.0,9705485.0,1,I think it is better to think of the project as adding extra options and conveniences rather than alternatives to Kubernetes-native solutions.,0.0,0.195,0.805,0.5574
Zipkin,51938178,51931755,0,"2018/08/20, 23:37:41",False,"2018/08/20, 23:37:41",8855.0,9705485.0,1,It is also worth noting that you can deploy a Netflix stack app to Kubernetes (including using Zuul and eureka) and there isn't necessarily anything wrong with that.,0.0,0.151,0.849,0.5352
Zipkin,51938178,51931755,0,"2018/08/20, 23:37:41",False,"2018/08/20, 23:37:41",8855.0,9705485.0,1,It has the advantage that you can work with it outside Kubernetes and it might be more convenient for your particular team if it's Java team.,0.0,0.074,0.926,0.25
Zipkin,51938178,51931755,0,"2018/08/20, 23:37:41",False,"2018/08/20, 23:37:41",8855.0,9705485.0,1,"The main downside is that the Netflix stack is very tied to Java, whereas Kubernetes is language neutral.",0.105,0.0,0.895,-0.25
Zipkin,50941229,50940162,3,"2018/06/20, 08:59:58",False,"2018/06/21, 08:11:08",491.0,2389992.0,0,We had this very similar issue with Akka.,0.0,0.0,1.0,0.0
Zipkin,50941229,50940162,3,"2018/06/20, 08:59:58",False,"2018/06/21, 08:11:08",491.0,2389992.0,0,We observed huge delay in ask pattern to deliver messages to the target actor on peek load.,0.117,0.117,0.765,0.0
Zipkin,50941229,50940162,3,"2018/06/20, 08:59:58",False,"2018/06/21, 08:11:08",491.0,2389992.0,0,Most of these issues are related to heap memory consumption and not because of usages of dispatchers.,0.0,0.0,1.0,0.0
Zipkin,50941229,50940162,3,"2018/06/20, 08:59:58",False,"2018/06/21, 08:11:08",491.0,2389992.0,0,Finally we fixed these issues by tuning some of the below configuration and changes.,0.0,0.0,1.0,0.0
Zipkin,50941229,50940162,3,"2018/06/20, 08:59:58",False,"2018/06/21, 08:11:08",491.0,2389992.0,0,1) Make sure you stop entities/actors which are no longer required.,0.299,0.156,0.544,-0.2732
Zipkin,50941229,50940162,3,"2018/06/20, 08:59:58",False,"2018/06/21, 08:11:08",491.0,2389992.0,0,If its a persistent actor then you can always bring it back when you need it.,0.0,0.0,1.0,0.0
Zipkin,50941229,50940162,3,"2018/06/20, 08:59:58",False,"2018/06/21, 08:11:08",491.0,2389992.0,0,Refer :  https://doc.akka.io/docs/akka/current/cluster-sharding.html#passivation,0.0,0.0,1.0,0.0
Zipkin,50941229,50940162,3,"2018/06/20, 08:59:58",False,"2018/06/21, 08:11:08",491.0,2389992.0,0,2) If you are using cluster sharding then check the akka.cluster.sharding.state-store-mode.,0.0,0.0,1.0,0.0
Zipkin,50941229,50940162,3,"2018/06/20, 08:59:58",False,"2018/06/21, 08:11:08",491.0,2389992.0,0,By changing this to persistence we gained 50% more TPS.,0.0,0.224,0.776,0.3818
Zipkin,50941229,50940162,3,"2018/06/20, 08:59:58",False,"2018/06/21, 08:11:08",491.0,2389992.0,0,3) Minimize your log entries (set it to info level).,0.0,0.0,1.0,0.0
Zipkin,50941229,50940162,3,"2018/06/20, 08:59:58",False,"2018/06/21, 08:11:08",491.0,2389992.0,0,4) Tune your logs to publish messages frequently to your logging system.,0.0,0.0,1.0,0.0
Zipkin,50941229,50940162,3,"2018/06/20, 08:59:58",False,"2018/06/21, 08:11:08",491.0,2389992.0,0,"Update the batch size, batch count and interval accordingly.",0.0,0.0,1.0,0.0
Zipkin,50941229,50940162,3,"2018/06/20, 08:59:58",False,"2018/06/21, 08:11:08",491.0,2389992.0,0,So that the memory is freed.,0.0,0.351,0.649,0.4019
Zipkin,50941229,50940162,3,"2018/06/20, 08:59:58",False,"2018/06/21, 08:11:08",491.0,2389992.0,0,In our case huge heap memory is used for buffering the log messages and send in bulk.,0.0,0.126,0.874,0.3182
Zipkin,50941229,50940162,3,"2018/06/20, 08:59:58",False,"2018/06/21, 08:11:08",491.0,2389992.0,0,If the interval is more then you may fill your heap memory and that affects the performance (more GC activity required).,0.0,0.0,1.0,0.0
Zipkin,50941229,50940162,3,"2018/06/20, 08:59:58",False,"2018/06/21, 08:11:08",491.0,2389992.0,0,5) Run blocking operations on a separate dispatcher.,0.302,0.0,0.698,-0.3818
Zipkin,50941229,50940162,3,"2018/06/20, 08:59:58",False,"2018/06/21, 08:11:08",491.0,2389992.0,0,6) Use custom serializers (protobuf) and avoid JavaSerializer.,0.239,0.0,0.761,-0.296
Zipkin,50941229,50940162,3,"2018/06/20, 08:59:58",False,"2018/06/21, 08:11:08",491.0,2389992.0,0,7) Add the below JAVA_OPTS to your jar,0.0,0.0,1.0,0.0
Zipkin,50941229,50940162,3,"2018/06/20, 08:59:58",False,"2018/06/21, 08:11:08",491.0,2389992.0,0,"export JAVA_OPTS=""$JAVA_OPTS -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -XX:MaxRAMFraction=2 -Djava.security.egd=file:/dev/./urandom""",0.0,0.0,1.0,0.0
Zipkin,50941229,50940162,3,"2018/06/20, 08:59:58",False,"2018/06/21, 08:11:08",491.0,2389992.0,0,The main thing is XX:MaxRAMFraction=2 which will utilize more than 60% of available memory.,0.0,0.0,1.0,0.0
Zipkin,50941229,50940162,3,"2018/06/20, 08:59:58",False,"2018/06/21, 08:11:08",491.0,2389992.0,0,"By default its 4 means your application will use only one fourth of the available memory, which might not be sufficient.",0.0,0.0,1.0,0.0
Zipkin,50941229,50940162,3,"2018/06/20, 08:59:58",False,"2018/06/21, 08:11:08",491.0,2389992.0,0,Refer :  https://blog.csanchez.org/2017/05/31/running-a-jvm-in-a-container-without-getting-killed/,0.0,0.0,1.0,0.0
Zipkin,50941229,50940162,3,"2018/06/20, 08:59:58",False,"2018/06/21, 08:11:08",491.0,2389992.0,0,"Regards,",0.0,0.0,1.0,0.0
Zipkin,50941229,50940162,3,"2018/06/20, 08:59:58",False,"2018/06/21, 08:11:08",491.0,2389992.0,0,Vinoth,0.0,0.0,1.0,0.0
Zipkin,50585875,50562296,0,"2018/05/29, 16:41:00",True,"2018/05/30, 12:06:27",2002.0,9524052.0,1,"As @Bal Chua and @Pär Nilsson mentioned, for environmental variables you can use only string variables because Linux environmental variables can be only strings.",0.0,0.0,1.0,0.0
Zipkin,50585875,50562296,0,"2018/05/29, 16:41:00",True,"2018/05/30, 12:06:27",2002.0,9524052.0,1,"So, if you use yaml, you need to place value into quotes to force Kubernetes to use string.",0.0,0.124,0.876,0.34
Zipkin,50585875,50562296,0,"2018/05/29, 16:41:00",True,"2018/05/30, 12:06:27",2002.0,9524052.0,1,For example:,0.0,0.0,1.0,0.0
Zipkin,49881877,49880941,2,"2018/04/17, 18:17:50",True,"2018/07/27, 14:22:52",5965.0,1235935.0,8,"Even when you use Spring Cloud, 100 services do NOT mean 100 servers.",0.0,0.0,1.0,0.0
Zipkin,49881877,49880941,2,"2018/04/17, 18:17:50",True,"2018/07/27, 14:22:52",5965.0,1235935.0,8,In Spring Cloud the packaging unit is Spring Boot application and a single server may host many such Spring Boot applications.,0.0,0.0,1.0,0.0
Zipkin,49881877,49880941,2,"2018/04/17, 18:17:50",True,"2018/07/27, 14:22:52",5965.0,1235935.0,8,"If you want, you can containerize the Spring Boot applications and other Spring Cloud infrastructure support components.",0.0,0.211,0.789,0.4588
Zipkin,49881877,49880941,2,"2018/04/17, 18:17:50",True,"2018/07/27, 14:22:52",5965.0,1235935.0,8,But that is not Kubernetes.,0.0,0.0,1.0,0.0
Zipkin,49881877,49880941,2,"2018/04/17, 18:17:50",True,"2018/07/27, 14:22:52",5965.0,1235935.0,8,"If you move to Kubernetes, you don't need the infrastructure support services like Zuul, Ribbon etc.",0.0,0.271,0.729,0.6369
Zipkin,49881877,49880941,2,"2018/04/17, 18:17:50",True,"2018/07/27, 14:22:52",5965.0,1235935.0,8,"because Kubernetes has its own components for service discovery, gateway, load balancer etc.",0.0,0.0,1.0,0.0
Zipkin,49881877,49880941,2,"2018/04/17, 18:17:50",True,"2018/07/27, 14:22:52",5965.0,1235935.0,8,"In Kubernetes, the packaging unit is Docker images and one or more Docker containers can be put inside one pod which is the minimal scaling unit.",0.0,0.0,1.0,0.0
Zipkin,49881877,49880941,2,"2018/04/17, 18:17:50",True,"2018/07/27, 14:22:52",5965.0,1235935.0,8,"So, Kubernetes has a different set of components to manage the Microservices.",0.0,0.0,1.0,0.0
Zipkin,49881877,49880941,2,"2018/04/17, 18:17:50",True,"2018/07/27, 14:22:52",5965.0,1235935.0,8,Kubernetes is a different platform than Spring cloud.,0.0,0.0,1.0,0.0
Zipkin,49881877,49880941,2,"2018/04/17, 18:17:50",True,"2018/07/27, 14:22:52",5965.0,1235935.0,8,Both have the same objectives.,0.0,0.0,1.0,0.0
Zipkin,49881877,49880941,2,"2018/04/17, 18:17:50",True,"2018/07/27, 14:22:52",5965.0,1235935.0,8,"However, Kubernetes has some additional features like self healing, auto-scaling, rolling updates, compute resource management, deployments etc.",0.0,0.135,0.865,0.3612
Zipkin,50084521,49880941,0,"2018/04/29, 10:57:09",False,"2018/04/29, 10:57:09",8855.0,9705485.0,2,"Just to add to saptarshi basu's answer, you might want to look at  https://dzone.com/articles/deploying-microservices-spring-cloud-vs-kubernetes  as it walks through the comparison and asks which responsibilities you might want to be handled by which components when using Spring cloud on kubernetes",0.0,0.066,0.934,0.1531
Zipkin,49687046,49686743,0,"2018/04/06, 09:52:45",True,"2018/04/06, 09:52:45",8336.0,1773866.0,1,If you're using Sleuth 2.0 you can call on the  Tracer  a method to create a new trace.,0.0,0.123,0.877,0.2732
Zipkin,49687046,49686743,0,"2018/04/06, 09:52:45",True,"2018/04/06, 09:52:45",8336.0,1773866.0,1,In the older version of sleuth I guess what I'd do is to use an executor that is  NOT  a bean.,0.0,0.0,1.0,0.0
Zipkin,49687046,49686743,0,"2018/04/06, 09:52:45",True,"2018/04/06, 09:52:45",8336.0,1773866.0,1,That way you would lose the trace and it would get restarted at some point (by rest template or sth like that).,0.107,0.099,0.794,-0.0516
Zipkin,45369348,45368147,4,"2017/07/28, 12:08:22",False,"2017/07/28, 12:08:22",8336.0,1773866.0,3,Thanks for the kind words!,0.0,0.687,0.313,0.7644
Zipkin,45369348,45368147,4,"2017/07/28, 12:08:22",False,"2017/07/28, 12:08:22",8336.0,1773866.0,3,In Sleuth Edgware we will support Reactor -  https://github.com/spring-cloud/spring-cloud-sleuth/tree/master/spring-cloud-sleuth-reactor  and in Sleuth Finchley we will support reactor and webflux  https://github.com/spring-cloud/spring-cloud-sleuth/blob/2.0.x/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/instrument/web/TraceWebFluxAutoConfiguration.java .,0.0,0.241,0.759,0.6597
Zipkin,45369348,45368147,4,"2017/07/28, 12:08:22",False,"2017/07/28, 12:08:22",8336.0,1773866.0,3,In other words it's already possible to use Sleuth in the reactive context.,0.0,0.0,1.0,0.0
Zipkin,43982611,43937265,1,"2017/05/15, 17:54:11",False,"2017/05/15, 17:54:11",473.0,2733462.0,1,It seems like you are using  Sleuth with Zipkin via HTTP .,0.0,0.2,0.8,0.3612
Zipkin,43982611,43937265,1,"2017/05/15, 17:54:11",False,"2017/05/15, 17:54:11",473.0,2733462.0,1,You can try the  Sleuth with Zipkin via Spring Cloud Stream  approach.,0.0,0.0,1.0,0.0
Zipkin,43982611,43937265,1,"2017/05/15, 17:54:11",False,"2017/05/15, 17:54:11",473.0,2733462.0,1,"I haven't done the benchmark myself, but it should improve the performance in theory.",0.0,0.243,0.757,0.5927
Zipkin,43982611,43937265,1,"2017/05/15, 17:54:11",False,"2017/05/15, 17:54:11",473.0,2733462.0,1,Please see the documentation at:  https://cloud.spring.io/spring-cloud-sleuth/spring-cloud-sleuth.html#_sleuth_with_zipkin_via_spring_cloud_stream,0.0,0.315,0.685,0.3182
Zipkin,44062172,43937265,3,"2017/05/19, 08:36:59",False,"2017/05/19, 08:36:59",8336.0,1773866.0,1,I wonder what kind of a benchmarking method you have picked.,0.0,0.0,1.0,0.0
Zipkin,44062172,43937265,3,"2017/05/19, 08:36:59",False,"2017/05/19, 08:36:59",8336.0,1773866.0,1,Which version of Sleuth are you using?,0.0,0.0,1.0,0.0
Zipkin,44062172,43937265,3,"2017/05/19, 08:36:59",False,"2017/05/19, 08:36:59",8336.0,1773866.0,1,Also is this one single benchmark that you're doing?,0.0,0.0,1.0,0.0
Zipkin,44062172,43937265,3,"2017/05/19, 08:36:59",False,"2017/05/19, 08:36:59",8336.0,1773866.0,1,Is it on your computer?,0.0,0.0,1.0,0.0
Zipkin,44062172,43937265,3,"2017/05/19, 08:36:59",False,"2017/05/19, 08:36:59",8336.0,1773866.0,1,Has the JVM gotten heated up?,0.0,0.0,1.0,0.0
Zipkin,44062172,43937265,3,"2017/05/19, 08:36:59",False,"2017/05/19, 08:36:59",8336.0,1773866.0,1,Are there any other processes executed?,0.0,0.0,1.0,0.0
Zipkin,44062172,43937265,3,"2017/05/19, 08:36:59",False,"2017/05/19, 08:36:59",8336.0,1773866.0,1,Doing benchmarking is not that easy... You can use tools like JMH to do it better.,0.0,0.278,0.722,0.6597
Zipkin,44062172,43937265,3,"2017/05/19, 08:36:59",False,"2017/05/19, 08:36:59",8336.0,1773866.0,1,BTW try turning off the DEBUG logging level and check the results again.,0.0,0.0,1.0,0.0
Zipkin,44062172,43937265,3,"2017/05/19, 08:36:59",False,"2017/05/19, 08:36:59",8336.0,1773866.0,1,We are performing benchmark tests of Sleuth and from what we see when adding Sleuth the latency gets increased by around 20 ms. Definitely not 600 ms.,0.0,0.161,0.839,0.5859
Zipkin,59895808,43937265,0,"2020/01/24, 13:46:12",True,"2020/01/24, 13:46:12",131.0,9437494.0,2,I belive you had problem with: org.springframework.cloud.sleuth.zipkin.ServerPropertiesEndpointLocator.,0.351,0.0,0.649,-0.4019
Zipkin,59895808,43937265,0,"2020/01/24, 13:46:12",True,"2020/01/24, 13:46:12",131.0,9437494.0,2,It uses InetUtils.findFirstNonLoopbackAddress() to determine instance address.,0.0,0.0,1.0,0.0
Zipkin,59895808,43937265,0,"2020/01/24, 13:46:12",True,"2020/01/24, 13:46:12",131.0,9437494.0,2,Method is called synchronously when each span is close (in ZipkinSpanListener#convert).,0.0,0.0,1.0,0.0
Zipkin,59895808,43937265,0,"2020/01/24, 13:46:12",True,"2020/01/24, 13:46:12",131.0,9437494.0,2,The workaround is to create custom org.springframework.cloud.sleuth.zipkin.EndpointLocator.,0.0,0.259,0.741,0.2732
Zipkin,59895808,43937265,0,"2020/01/24, 13:46:12",True,"2020/01/24, 13:46:12",131.0,9437494.0,2,You can use something like that:,0.0,0.333,0.667,0.3612
Zipkin,59895808,43937265,0,"2020/01/24, 13:46:12",True,"2020/01/24, 13:46:12",131.0,9437494.0,2,And combine it with one of existing EndpointLocators.,0.0,0.0,1.0,0.0
Zipkin,59895808,43937265,0,"2020/01/24, 13:46:12",True,"2020/01/24, 13:46:12",131.0,9437494.0,2,You can find them in: org.springframework.cloud.sleuth.zipkin.ZipkinAutoConfiguration.,0.0,0.0,1.0,0.0
Zipkin,59895808,43937265,0,"2020/01/24, 13:46:12",True,"2020/01/24, 13:46:12",131.0,9437494.0,2,This issue is already fixed in sleuth 2.X.X.,0.0,0.0,1.0,0.0
Zipkin,59895808,43937265,0,"2020/01/24, 13:46:12",True,"2020/01/24, 13:46:12",131.0,9437494.0,2,Where: org.springframework.cloud.sleuth.zipkin2.DefaultEndpointLocator caches server address:,0.0,0.0,1.0,0.0
Zipkin,40117668,40090893,1,"2016/10/18, 23:39:29",True,"2016/10/18, 23:39:29",257711.0,526535.0,2,The web app is trying to access  config.json  at root (accessing as  /config.json  vs just  config.json  ) - that is  http://localhost:8001/config.json  .,0.0,0.0,1.0,0.0
Zipkin,40117668,40090893,1,"2016/10/18, 23:39:29",True,"2016/10/18, 23:39:29",257711.0,526535.0,2,This would obviously be wrong as it should be  http://localhost:8001/api/v1/proxy/namespaces/default/services/zipkin-http/config.json,0.256,0.0,0.744,-0.4767
Zipkin,40117668,40090893,1,"2016/10/18, 23:39:29",True,"2016/10/18, 23:39:29",257711.0,526535.0,2,There is a very simple solution for this - just run:,0.0,0.244,0.756,0.3774
Zipkin,40117668,40090893,1,"2016/10/18, 23:39:29",True,"2016/10/18, 23:39:29",257711.0,526535.0,2,Now just go to  http://localhost:9411  and the UI should be up (tried and verified.),0.0,0.0,1.0,0.0
Zipkin,40117668,40090893,1,"2016/10/18, 23:39:29",True,"2016/10/18, 23:39:29",257711.0,526535.0,2,You can get the name of the pod by doing  kubectl get pods,0.0,0.0,1.0,0.0
Zipkin,40117668,40090893,1,"2016/10/18, 23:39:29",True,"2016/10/18, 23:39:29",257711.0,526535.0,2,"PS:  kubectl proxy  is generally meant to access the Kubernetes API, and  kube port-forward  is the right tool in this case.",0.0,0.0,1.0,0.0
Zipkin,45769658,38019422,0,"2017/08/19, 11:59:32",False,"2017/08/19, 11:59:32",117.0,916394.0,0,I'm not sure this is the right way to do it but this should normally works,0.09,0.0,0.91,-0.1232
Zipkin,61367030,56328934,0,"2020/04/22, 17:01:23",False,"2020/04/22, 17:01:23",2849.0,797243.0,0,I have seen this issue a lot.,0.0,0.0,1.0,0.0
Zipkin,61367030,56328934,0,"2020/04/22, 17:01:23",False,"2020/04/22, 17:01:23",2849.0,797243.0,0,"From my experience the most common cause is, that the base64 string was encoded on the commandline using  echo '$mypw' | base64  which will create newlines in the encoded string.",0.0,0.07,0.93,0.2732
Zipkin,61367030,56328934,0,"2020/04/22, 17:01:23",False,"2020/04/22, 17:01:23",2849.0,797243.0,0,You need to use the  -n  switch to echo:  echo -n '$mypw' | base64 .,0.0,0.0,1.0,0.0
Zipkin,52117392,51187274,1,"2018/08/31, 17:16:40",False,"2018/08/31, 18:03:37",1.0,1380632.0,0,What logging framework are you using?,0.0,0.0,1.0,0.0
Zipkin,52117392,51187274,1,"2018/08/31, 17:16:40",False,"2018/08/31, 18:03:37",1.0,1380632.0,0,I was using log4j2 in my project.,0.0,0.0,1.0,0.0
Zipkin,52117392,51187274,1,"2018/08/31, 17:16:40",False,"2018/08/31, 18:03:37",1.0,1380632.0,0,It started working when I removed log4j2 and left it to the default logging of spring-cloud-sleuth.,0.0,0.0,1.0,0.0
Zipkin,48763174,48759442,0,"2018/02/13, 11:15:18",True,"2018/02/13, 11:15:18",8336.0,1773866.0,2,It's a bug -  https://github.com/spring-cloud/spring-cloud-sleuth/issues/855  .,0.0,0.0,1.0,0.0
Zipkin,48763174,48759442,0,"2018/02/13, 11:15:18",True,"2018/02/13, 11:15:18",8336.0,1773866.0,2,I've fixed it ATM.,0.0,0.0,1.0,0.0
Zipkin,48763174,48759442,0,"2018/02/13, 11:15:18",True,"2018/02/13, 11:15:18",8336.0,1773866.0,2,A workaround is to start it manually either in each method that uses  @NewSpan  by calling  start()  method on current span (that doesn't scale too nicely),0.0,0.0,1.0,0.0
Zipkin,48763174,48759442,0,"2018/02/13, 11:15:18",True,"2018/02/13, 11:15:18",8336.0,1773866.0,2,You can also create a bean of  SpanCreator  (you can check the fixed version here  https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/annotation/DefaultSpanCreator.java ),0.0,0.13,0.87,0.2732
Zipkin,48763174,48759442,0,"2018/02/13, 11:15:18",True,"2018/02/13, 11:15:18",8336.0,1773866.0,2,Notice the  .start()  at the end of the method.,0.0,0.0,1.0,0.0
Zipkin,48679791,48679359,13,"2018/02/08, 09:23:24",False,"2018/02/08, 09:23:24",2367.0,3860531.0,0,Try this,0.0,0.0,1.0,0.0
Zipkin,48679791,48679359,13,"2018/02/08, 09:23:24",False,"2018/02/08, 09:23:24",2367.0,3860531.0,0,I hope this will help you.,0.0,0.651,0.349,0.6808
Zipkin,66064768,66064081,3,"2021/02/05, 16:09:01",True,"2021/02/05, 18:51:05",7249.0,8340997.0,1,According to  Spring Boot Reference Docs  :,0.0,0.0,1.0,0.0
Zipkin,66064768,66064081,3,"2021/02/05, 16:09:01",True,"2021/02/05, 18:51:05",7249.0,8340997.0,1,"To enable  /httptrace  in the actuator, then you have to create a bean of   InMemoryHttpTraceRepository  class in the custom  @Configuration  class which provides the trace of the request and response.",0.0,0.07,0.93,0.2732
Zipkin,66064768,66064081,3,"2021/02/05, 16:09:01",True,"2021/02/05, 18:51:05",7249.0,8340997.0,1,"To enable  /auditevents  in the actuator, then you have to create a bean of  InMemoryAuditEventRepository  class in the custom  @Configuration  class which exposes audit events information.",0.056,0.079,0.865,0.1531
Zipkin,66064768,66064081,3,"2021/02/05, 16:09:01",True,"2021/02/05, 18:51:05",7249.0,8340997.0,1,"To enable  /integrationgraph  in actuator, you have to add  spring-integration-core dependency  in the pom.xml (as per documentation) :",0.0,0.0,1.0,0.0
Zipkin,66064768,66064081,3,"2021/02/05, 16:09:01",True,"2021/02/05, 18:51:05",7249.0,8340997.0,1,"or if you are having a spring-boot project, then add this :",0.0,0.0,1.0,0.0
Zipkin,66064768,66064081,3,"2021/02/05, 16:09:01",True,"2021/02/05, 18:51:05",7249.0,8340997.0,1,/actuator/sessions  are by-default enabled.,0.0,0.0,1.0,0.0
Zipkin,66064768,66064081,3,"2021/02/05, 16:09:01",True,"2021/02/05, 18:51:05",7249.0,8340997.0,1,But still you can add this explicitly to check the behaviour.,0.0,0.0,1.0,0.0
Zipkin,66064768,66064081,3,"2021/02/05, 16:09:01",True,"2021/02/05, 18:51:05",7249.0,8340997.0,1,Add this in application.properties.,0.0,0.0,1.0,0.0
Zipkin,64439121,64434436,0,"2020/10/20, 08:42:11",True,"2020/10/20, 08:42:11",66.0,12105655.0,1,Thanks Jorg Heymans for the question.,0.0,0.367,0.633,0.4404
Zipkin,64439121,64434436,0,"2020/10/20, 08:42:11",True,"2020/10/20, 08:42:11",66.0,12105655.0,1,"Yeah, it's a bug and should be fixed by  https://github.com/line/armeria/pull/3120 
Thank you!",0.0,0.357,0.643,0.6114
Zipkin,64043957,64040747,1,"2020/09/24, 12:53:37",False,"2020/09/24, 12:53:37",1554.0,3719412.0,1,Ribbon is a client side load balancer which means there is no any other hop in between your client and service.,0.104,0.0,0.896,-0.296
Zipkin,64043957,64040747,1,"2020/09/24, 12:53:37",False,"2020/09/24, 12:53:37",1554.0,3719412.0,1,Basically you keep and maintain a list of service on your client.,0.0,0.0,1.0,0.0
Zipkin,64043957,64040747,1,"2020/09/24, 12:53:37",False,"2020/09/24, 12:53:37",1554.0,3719412.0,1,In AWS load balancer case you need to make another hop in between the client and server.,0.0,0.0,1.0,0.0
Zipkin,64043957,64040747,1,"2020/09/24, 12:53:37",False,"2020/09/24, 12:53:37",1554.0,3719412.0,1,Both have advanges and disadvantages.,0.403,0.0,0.597,-0.4019
Zipkin,64043957,64040747,1,"2020/09/24, 12:53:37",False,"2020/09/24, 12:53:37",1554.0,3719412.0,1,Former has the advantage of not having any dependency to any specific external solution.,0.0,0.264,0.736,0.5106
Zipkin,64043957,64040747,1,"2020/09/24, 12:53:37",False,"2020/09/24, 12:53:37",1554.0,3719412.0,1,Basically with ribbon and service discovery like eureka you can deploy your product to any cloud provider or on-premise setup without additional effort.,0.0,0.102,0.898,0.3612
Zipkin,64043957,64040747,1,"2020/09/24, 12:53:37",False,"2020/09/24, 12:53:37",1554.0,3719412.0,1,Latter has advantage of not needing an extra component of service discovery or keeping the cache of service list on client.,0.0,0.091,0.909,0.25
Zipkin,64043957,64040747,1,"2020/09/24, 12:53:37",False,"2020/09/24, 12:53:37",1554.0,3719412.0,1,But it has that additional hop which might be an issue if you are trying to run an very high-load system.,0.0,0.0,1.0,0.0
Zipkin,64043957,64040747,1,"2020/09/24, 12:53:37",False,"2020/09/24, 12:53:37",1554.0,3719412.0,1,Although I don't have much experience with AWS CloudWatch what I know is it helps you to collect logs to a central place from different AWS components.,0.0,0.102,0.898,0.3818
Zipkin,64043957,64040747,1,"2020/09/24, 12:53:37",False,"2020/09/24, 12:53:37",1554.0,3719412.0,1,And that is what you are trying to do with your solution.,0.0,0.173,0.827,0.3182
Zipkin,60258813,60258661,0,"2020/02/17, 10:53:24",False,"2020/02/17, 10:53:24",159.0,4502707.0,3,"kubectl exec -it ""pod-name"" -c ""container-name"" -n ""namespace""",0.0,0.0,1.0,0.0
Zipkin,60258813,60258661,0,"2020/02/17, 10:53:24",False,"2020/02/17, 10:53:24",159.0,4502707.0,3,Here only the container name is needed.,0.0,0.0,1.0,0.0
Zipkin,60258813,60258661,0,"2020/02/17, 10:53:24",False,"2020/02/17, 10:53:24",159.0,4502707.0,3,In your case it will be:,0.0,0.0,1.0,0.0
Zipkin,60258813,60258661,0,"2020/02/17, 10:53:24",False,"2020/02/17, 10:53:24",159.0,4502707.0,3,kubectl exec -it my-api-XXX -c my-api  -- /bin/bash,0.0,0.0,1.0,0.0
Zipkin,60258813,60258661,0,"2020/02/17, 10:53:24",False,"2020/02/17, 10:53:24",159.0,4502707.0,3,You can exec to Zipkin because  exec  is taking zipkin as the default container.,0.0,0.0,1.0,0.0
Zipkin,56657881,56655528,0,"2019/06/19, 01:35:50",False,"2019/06/19, 01:35:50",694.0,3800106.0,0,It is solved now; all I had to do was port forwarding.,0.0,0.174,0.826,0.2732
Zipkin,56657881,56655528,0,"2019/06/19, 01:35:50",False,"2019/06/19, 01:35:50",694.0,3800106.0,0,"Thanks,",0.0,1.0,0.0,0.4404
Zipkin,56734998,56655528,0,"2019/06/24, 13:49:27",False,"2019/06/24, 13:49:27",2049.0,11207414.0,0,"By default you service is exposed as  ClusterIP , in this case your service will be accessible from within your cluster.",0.064,0.0,0.936,-0.0772
Zipkin,56734998,56655528,0,"2019/06/24, 13:49:27",False,"2019/06/24, 13:49:27",2049.0,11207414.0,0,"You can use port forwarding "" With this connection in place you can use your local workstation to debug your application that is running in the pod "" as described in the answer above.",0.0,0.0,1.0,0.0
Zipkin,56734998,56655528,0,"2019/06/24, 13:49:27",False,"2019/06/24, 13:49:27",2049.0,11207414.0,0,"Another approach is to use other  ""service types""  like  NodePort .",0.0,0.217,0.783,0.3612
Zipkin,56734998,56655528,0,"2019/06/24, 13:49:27",False,"2019/06/24, 13:49:27",2049.0,11207414.0,0,You can find more information here  Publishing services (ServiceTypes),0.0,0.0,1.0,0.0
Zipkin,55274713,55274572,0,"2019/03/21, 08:15:19",False,"2019/03/21, 08:15:19",3915.0,837717.0,2,"Sleuth will do the same for messaging by using message headers to propagate  span id, trace id  and other relevant information.",0.0,0.0,1.0,0.0
Zipkin,55274713,55274572,0,"2019/03/21, 08:15:19",False,"2019/03/21, 08:15:19",3915.0,837717.0,2,It does so by registering special channel interceptor.,0.0,0.297,0.703,0.4522
Zipkin,54914623,54845174,0,"2019/02/27, 23:15:56",True,"2019/02/27, 23:15:56",504.0,9187876.0,1,"The configuration you're referring to is for the instrumentation of messaging systems, not for sending traces to zipkin using a messaging system.",0.0,0.0,1.0,0.0
Zipkin,54914623,54845174,0,"2019/02/27, 23:15:56",True,"2019/02/27, 23:15:56",504.0,9187876.0,1,"You should look at this  auto-configuration , and especially this  sender config .",0.0,0.0,1.0,0.0
Zipkin,54914623,54845174,0,"2019/02/27, 23:15:56",True,"2019/02/27, 23:15:56",504.0,9187876.0,1,What you want to do has also been documented here:  https://cloud.spring.io/spring-cloud-sleuth/2.0.x/single/spring-cloud-sleuth.html#_sleuth_with_zipkin_over_rabbitmq_or_kafka,0.0,0.115,0.885,0.0772
Zipkin,54914623,54845174,0,"2019/02/27, 23:15:56",True,"2019/02/27, 23:15:56",504.0,9187876.0,1,You should only need to add  spring-cloud-starter-zipkin  and  spring-rabbit  to your dependencies.,0.0,0.0,1.0,0.0
Zipkin,54914623,54845174,0,"2019/02/27, 23:15:56",True,"2019/02/27, 23:15:56",504.0,9187876.0,1,"If you want to change the default queue (which is  zipkin ), then you'll need to add  spring.zipkin.rabbitmq.queue  to your properties.",0.0,0.061,0.939,0.0772
Zipkin,53655726,53653606,3,"2018/12/06, 18:27:44",False,"2018/12/07, 23:02:30",81.0,2232476.0,0,You will need your own  PropagationFactory  implementation.,0.0,0.0,1.0,0.0
Zipkin,53655726,53653606,3,"2018/12/06, 18:27:44",False,"2018/12/07, 23:02:30",81.0,2232476.0,0,Here is the default one:  https://github.com/openzipkin/brave/blob/master/brave/src/main/java/brave/propagation/B3Propagation.java,0.0,0.0,1.0,0.0
Zipkin,53655726,53653606,3,"2018/12/06, 18:27:44",False,"2018/12/07, 23:02:30",81.0,2232476.0,0,You can create a bean and sleuth should use that instead of this one.,0.0,0.149,0.851,0.2732
Zipkin,53655726,53653606,3,"2018/12/06, 18:27:44",False,"2018/12/07, 23:02:30",81.0,2232476.0,0,More specifically you will need an implementation with a custom  TraceContext.Extractor&lt;C&gt;  implementation.,0.0,0.0,1.0,0.0
Zipkin,53655726,53653606,3,"2018/12/06, 18:27:44",False,"2018/12/07, 23:02:30",81.0,2232476.0,0,"This can then pull the trace ID from your header, and add return the appropriate  TraceContext .",0.0,0.0,1.0,0.0
Zipkin,53655726,53653606,3,"2018/12/06, 18:27:44",False,"2018/12/07, 23:02:30",81.0,2232476.0,0,Then it can pass it along using the normal headers.,0.0,0.0,1.0,0.0
Zipkin,53655726,53653606,3,"2018/12/06, 18:27:44",False,"2018/12/07, 23:02:30",81.0,2232476.0,0,If you'd like to use the same correlation header when sending downstream then you will also have to implement  TraceContext.Injector&lt;C&gt; .,0.0,0.116,0.884,0.3612
Zipkin,53184979,53174880,3,"2018/11/07, 09:16:13",True,"2018/11/07, 09:16:13",8336.0,1773866.0,1,The option is to disable Slf4j integration as you mentioned.,0.0,0.0,1.0,0.0
Zipkin,53184979,53174880,3,"2018/11/07, 09:16:13",True,"2018/11/07, 09:16:13",8336.0,1773866.0,1,"When a new span / scope is created, we go through Slf4j to put data in MDC and it takes time unfortunately.",0.107,0.089,0.804,-0.1027
Zipkin,53184979,53174880,3,"2018/11/07, 09:16:13",True,"2018/11/07, 09:16:13",8336.0,1773866.0,1,Disabling that will save it.,0.333,0.344,0.323,0.0258
Zipkin,51526235,51526164,6,"2018/07/25, 22:21:47",True,"2018/07/26, 03:24:14",91301.0,2756547.0,0,This is indeed possible with the mentioned  executor channel .,0.0,0.0,1.0,0.0
Zipkin,51526235,51526164,6,"2018/07/25, 22:21:47",True,"2018/07/26, 03:24:14",91301.0,2756547.0,0,All you recipient flows must really start from the  ExecutorChannel .,0.0,0.0,1.0,0.0
Zipkin,51526235,51526164,6,"2018/07/25, 22:21:47",True,"2018/07/26, 03:24:14",91301.0,2756547.0,0,In your case you have to modify all of them to something like this:,0.0,0.161,0.839,0.3612
Zipkin,51526235,51526164,6,"2018/07/25, 22:21:47",True,"2018/07/26, 03:24:14",91301.0,2756547.0,0,Pay attention to the  IntegrationFlows.from(MessageChannels.executor(taskExexecutor())) .,0.259,0.0,0.741,-0.1027
Zipkin,51526235,51526164,6,"2018/07/25, 22:21:47",True,"2018/07/26, 03:24:14",91301.0,2756547.0,0,That's exactly how you can make each sub-flow async.,0.0,0.0,1.0,0.0
Zipkin,51526235,51526164,6,"2018/07/25, 22:21:47",True,"2018/07/26, 03:24:14",91301.0,2756547.0,0,UPDATE,0.0,0.0,1.0,0.0
Zipkin,51526235,51526164,6,"2018/07/25, 22:21:47",True,"2018/07/26, 03:24:14",91301.0,2756547.0,0,For the older Spring Integration version without  IntegrationFlow  improvement for the sub-flows we can do like this:,0.124,0.125,0.751,0.0052
Zipkin,51526235,51526164,6,"2018/07/25, 22:21:47",True,"2018/07/26, 03:24:14",91301.0,2756547.0,0,This is similar to what you show in the comment above.,0.0,0.0,1.0,0.0
Zipkin,51442735,51442393,1,"2018/07/20, 15:38:54",False,"2018/07/20, 15:38:54",51827.0,1259109.0,0,"Works for me with 1.4.0.RELEASE (2.0.0.RELEASE isn't out yet, but should be soon).",0.0,0.0,1.0,0.0
Zipkin,51442735,51442393,1,"2018/07/20, 15:38:54",False,"2018/07/20, 15:38:54",51827.0,1259109.0,0,You probably have a bad jar file in your local maven cache (e.g.,0.241,0.0,0.759,-0.5423
Zipkin,51442735,51442393,1,"2018/07/20, 15:38:54",False,"2018/07/20, 15:38:54",51827.0,1259109.0,0,the one that it complains about).,0.342,0.0,0.658,-0.3818
Zipkin,49357777,49326587,1,"2018/03/19, 09:31:43",False,"2018/03/19, 09:31:43",8336.0,1773866.0,0,You have to provide a different logging pattern to make it work with PCF Metrics AFAIR.,0.0,0.0,1.0,0.0
Zipkin,49357777,49326587,1,"2018/03/19, 09:31:43",False,"2018/03/19, 09:31:43",8336.0,1773866.0,0,You need the parent span to be present in logs.,0.0,0.0,1.0,0.0
Zipkin,49357777,49326587,1,"2018/03/19, 09:31:43",False,"2018/03/19, 09:31:43",8336.0,1773866.0,0,"Set the property  logging.pattern.level: ""%clr(%5p) %clr([${spring.application.name:-},%X{X-B3-TraceId:-},%X{X-B3-SpanId:-},%X{X-B3-ParentSpanId:-},%X{X-Span-Export:-}]){yellow}"" .",0.0,0.0,1.0,0.0
Zipkin,49357777,49326587,1,"2018/03/19, 09:31:43",False,"2018/03/19, 09:31:43",8336.0,1773866.0,0,Check this example:  https://github.com/pivotal-cf/pcf-metrics-trace-example-spring,0.0,0.0,1.0,0.0
Zipkin,49903075,49326587,1,"2018/04/18, 18:02:24",False,"2018/04/18, 18:02:24",1.0,9665105.0,0,"PCF metrics does  not support custom spans, it only shows the respomse time distribution span that corresponds to http request routed by goRouter.",0.093,0.0,0.907,-0.3089
Zipkin,49066640,48940831,1,"2018/03/02, 11:51:31",True,"2018/03/02, 11:51:31",8336.0,1773866.0,1,It was a bug that got fixed with this commit -  https://github.com/spring-cloud/spring-cloud-sleuth/commit/d7a0747907f4ab7201f67e7d0c762a324fbe0668  .,0.0,0.217,0.783,0.3612
Zipkin,49066640,48940831,1,"2018/03/02, 11:51:31",True,"2018/03/02, 11:51:31",8336.0,1773866.0,1,Please check out the latest snapshots,0.0,0.315,0.685,0.3182
Zipkin,48762766,48756959,9,"2018/02/13, 10:51:34",True,"2018/02/13, 21:29:25",8336.0,1773866.0,2,Why are you setting the values of dependencies manually?,0.0,0.252,0.748,0.4019
Zipkin,48762766,48756959,9,"2018/02/13, 10:51:34",True,"2018/02/13, 21:29:25",8336.0,1773866.0,2,Please use the Edgware.SR2 BOM.,0.0,0.365,0.635,0.3182
Zipkin,48762766,48756959,9,"2018/02/13, 10:51:34",True,"2018/02/13, 21:29:25",8336.0,1773866.0,2,"You have to add the kafka dependency, ensure that rabbit is not on the classpath.",0.0,0.157,0.843,0.3818
Zipkin,48762766,48756959,9,"2018/02/13, 10:51:34",True,"2018/02/13, 21:29:25",8336.0,1773866.0,2,If you have both kafka and rabbit on the classpath you need to set the  spring.zipkin.sender.type=kafka,0.0,0.0,1.0,0.0
Zipkin,48762766,48756959,9,"2018/02/13, 10:51:34",True,"2018/02/13, 21:29:25",8336.0,1773866.0,2,UPDATE:,0.0,0.0,1.0,0.0
Zipkin,48762766,48756959,9,"2018/02/13, 10:51:34",True,"2018/02/13, 21:29:25",8336.0,1773866.0,2,"As we describe in the documentation, the Sleuth Stream support is deprecated in Edgware and removed in FInchley.",0.0,0.137,0.863,0.4019
Zipkin,48762766,48756959,9,"2018/02/13, 10:51:34",True,"2018/02/13, 21:29:25",8336.0,1773866.0,2,"If you've decided to go with the new approach of using native Zipkin messaging support, then you have to use the Zipkin Server with Kafka as described here  https://github.com/openzipkin/zipkin/tree/master/zipkin-autoconfigure/collector-kafka10  .",0.0,0.088,0.912,0.4019
Zipkin,48762766,48756959,9,"2018/02/13, 10:51:34",True,"2018/02/13, 21:29:25",8336.0,1773866.0,2,Let me copy part of the docs here,0.0,0.0,1.0,0.0
Zipkin,48762766,48756959,9,"2018/02/13, 10:51:34",True,"2018/02/13, 21:29:25",8336.0,1773866.0,2,"The following configuration points apply apply when  KAFKA_BOOTSTRAP_SERVERS  or
 zipkin.collector.kafka.bootstrap-servers  is set.",0.0,0.0,1.0,0.0
Zipkin,48762766,48756959,9,"2018/02/13, 10:51:34",True,"2018/02/13, 21:29:25",8336.0,1773866.0,2,"They can be configured by setting an environment
variable or by setting a java system property using the  -Dproperty.name=value  command line
argument.",0.111,0.0,0.889,-0.3612
Zipkin,48762766,48756959,9,"2018/02/13, 10:51:34",True,"2018/02/13, 21:29:25",8336.0,1773866.0,2,"Some settings correspond to ""New Consumer Configs"" in
 Kafka documentation .",0.0,0.0,1.0,0.0
Zipkin,48762766,48756959,9,"2018/02/13, 10:51:34",True,"2018/02/13, 21:29:25",8336.0,1773866.0,2,Environment Variable | Property | New Consumer Config | Description,0.0,0.0,1.0,0.0
Zipkin,48762766,48756959,9,"2018/02/13, 10:51:34",True,"2018/02/13, 21:29:25",8336.0,1773866.0,2,"KAFKA_BOOTSTRAP_SERVERS  |  zipkin.collector.kafka.bootstrap-servers  | bootstrap.servers | Comma-separated list of brokers, ex.",0.0,0.0,1.0,0.0
Zipkin,48762766,48756959,9,"2018/02/13, 10:51:34",True,"2018/02/13, 21:29:25",8336.0,1773866.0,2,127.0.0.1:9092.,0.0,0.0,1.0,0.0
Zipkin,48762766,48756959,9,"2018/02/13, 10:51:34",True,"2018/02/13, 21:29:25",8336.0,1773866.0,2,No default,0.688,0.0,0.312,-0.296
Zipkin,48762766,48756959,9,"2018/02/13, 10:51:34",True,"2018/02/13, 21:29:25",8336.0,1773866.0,2,KAFKA_GROUP_ID  |  zipkin.collector.kafka.group-id  | group.id | The consumer group this process is consuming on behalf of.,0.0,0.0,1.0,0.0
Zipkin,48762766,48756959,9,"2018/02/13, 10:51:34",True,"2018/02/13, 21:29:25",8336.0,1773866.0,2,Defaults to  zipkin,0.0,0.0,1.0,0.0
Zipkin,48762766,48756959,9,"2018/02/13, 10:51:34",True,"2018/02/13, 21:29:25",8336.0,1773866.0,2,KAFKA_TOPIC  |  zipkin.collector.kafka.topic  | N/A | Comma-separated list of topics that zipkin spans will be consumed from.,0.0,0.0,1.0,0.0
Zipkin,48762766,48756959,9,"2018/02/13, 10:51:34",True,"2018/02/13, 21:29:25",8336.0,1773866.0,2,Defaults to  zipkin,0.0,0.0,1.0,0.0
Zipkin,48762766,48756959,9,"2018/02/13, 10:51:34",True,"2018/02/13, 21:29:25",8336.0,1773866.0,2,KAFKA_STREAMS  |  zipkin.collector.kafka.streams  | N/A | Count of threads consuming the topic.,0.0,0.0,1.0,0.0
Zipkin,48762766,48756959,9,"2018/02/13, 10:51:34",True,"2018/02/13, 21:29:25",8336.0,1773866.0,2,Defaults to  1,0.0,0.0,1.0,0.0
Zipkin,51743845,48639074,0,"2018/08/08, 13:00:10",False,"2018/08/08, 13:00:10",26.0,5772061.0,0,OpenStack does not have Zipkin as an inbuilt tracer.,0.0,0.0,1.0,0.0
Zipkin,51743845,48639074,0,"2018/08/08, 13:00:10",False,"2018/08/08, 13:00:10",26.0,5772061.0,0,Hence OSProfiler was adopted as a standard project for tracing in OpenStack.,0.0,0.0,1.0,0.0
Zipkin,51743845,48639074,0,"2018/08/08, 13:00:10",False,"2018/08/08, 13:00:10",26.0,5772061.0,0,"As far as i can see from the documentation, Nova should have OSProfiler support for Mitaka.",0.0,0.162,0.838,0.4019
Zipkin,51743845,48639074,0,"2018/08/08, 13:00:10",False,"2018/08/08, 13:00:10",26.0,5772061.0,0,"Although i have not used OSProfiler with Mitaka, I have worked with OSProfiler with Newton and subsequent releases.",0.0,0.0,1.0,0.0
Zipkin,51743845,48639074,0,"2018/08/08, 13:00:10",False,"2018/08/08, 13:00:10",26.0,5772061.0,0,You can post the issue that you are facing so that it will be easier to debug.,0.0,0.149,0.851,0.4215
Zipkin,48216934,48216192,0,"2018/01/12, 00:38:24",True,"2018/01/12, 00:38:24",8336.0,1773866.0,3,"If you're using Edgware release train, just set  spring.zipkin.sender.type=web .",0.0,0.0,1.0,0.0
Zipkin,48216934,48216192,0,"2018/01/12, 00:38:24",True,"2018/01/12, 00:38:24",8336.0,1773866.0,3,That way you force the HTTP based span sending,0.0,0.0,1.0,0.0
Zipkin,48170238,48169950,2,"2018/01/09, 16:22:14",True,"2018/01/09, 16:22:14",129048.0,1240763.0,1,"Embedded headers are not pluggable, but you can disable them with  ...producer.header-mode=raw .",0.0,0.0,1.0,0.0
Zipkin,48170238,48169950,2,"2018/01/09, 16:22:14",True,"2018/01/09, 16:22:14",129048.0,1240763.0,1,"With Ditmars (1.3.x) you can use the kafka11 artifact, which supports native headers - you have to override a bunch of dependencies (kafka-clients, SK, SIK and kafka itself if you are using the  KafkaEmbedded  broker for testing.",0.0,0.068,0.932,0.3612
Zipkin,48170238,48169950,2,"2018/01/09, 16:22:14",True,"2018/01/09, 16:22:14",129048.0,1240763.0,1,See  the relesae notes ).,0.0,0.0,1.0,0.0
Zipkin,48170238,48169950,2,"2018/01/09, 16:22:14",True,"2018/01/09, 16:22:14",129048.0,1240763.0,1,There's  a discussion on Gitter  about overriding the versions.,0.0,0.0,1.0,0.0
Zipkin,48170238,48169950,2,"2018/01/09, 16:22:14",True,"2018/01/09, 16:22:14",129048.0,1240763.0,1,spring-kafka 1.3.x natively uses 0.11 but 1.3.1 and higher (1.3.2 is current) also supports the 1.0.0 client.,0.0,0.169,0.831,0.5023
Zipkin,48170238,48169950,2,"2018/01/09, 16:22:14",True,"2018/01/09, 16:22:14",129048.0,1240763.0,1,Elmhurst (2.0) - currently in milestones - uses SK 2.1.0 which natively uses 1.0.0 kafka.,0.0,0.0,1.0,0.0
Zipkin,47925684,47925555,1,"2017/12/21, 15:07:02",False,"2017/12/21, 15:07:02",28734.0,1153938.0,1,We implemented this on our microservices platform,0.0,0.0,1.0,0.0
Zipkin,47925684,47925555,1,"2017/12/21, 15:07:02",False,"2017/12/21, 15:07:02",28734.0,1153938.0,1,A lot of the logging is done by pushing requests onto a RabbitMQ queue and then getting logstash to consume that.,0.0,0.0,1.0,0.0
Zipkin,47925684,47925555,1,"2017/12/21, 15:07:02",False,"2017/12/21, 15:07:02",28734.0,1153938.0,1,Other data is obtained via filebeat transmitting the logs to logstash,0.0,0.0,1.0,0.0
Zipkin,47925684,47925555,1,"2017/12/21, 15:07:02",False,"2017/12/21, 15:07:02",28734.0,1153938.0,1,Both the logs and the RabbitMQ data has the id attached so can be correlated,0.0,0.0,1.0,0.0
Zipkin,47925684,47925555,1,"2017/12/21, 15:07:02",False,"2017/12/21, 15:07:02",28734.0,1153938.0,1,An alternative approach would be to build instrumentation into each microservice that specifically monitored latency and then record that directly into logstash,0.0,0.0,1.0,0.0
Zipkin,47925684,47925555,1,"2017/12/21, 15:07:02",False,"2017/12/21, 15:07:02",28734.0,1153938.0,1,You might like to read  https://medium.com/devopslinks/how-to-monitor-the-sre-golden-signals-1391cadc7524  for a general guide to essential monitoring that is applicable to microservices,0.0,0.135,0.865,0.3612
Zipkin,46489881,46479182,0,"2017/09/29, 16:30:12",False,"2017/09/29, 16:30:12",691.0,1029971.0,1,I figured out how to disable the bean that was injecting LogbackAccess.,0.0,0.0,1.0,0.0
Zipkin,46489881,46479182,0,"2017/09/29, 16:30:12",False,"2017/09/29, 16:30:12",691.0,1029971.0,1,This resolved the issue so that Zipkin is now accepting requests.,0.0,0.323,0.677,0.5106
Zipkin,46188800,46185813,1,"2017/09/13, 07:21:00",True,"2017/09/13, 07:21:00",12382.0,707451.0,1,"To log internal HTTP request sent from a Node.js server, you can create a Proxy Node.js server and log all requests there using  Morgan .",0.0,0.091,0.909,0.2732
Zipkin,46188800,46185813,1,"2017/09/13, 07:21:00",True,"2017/09/13, 07:21:00",12382.0,707451.0,1,"First, define 3 constants (or read from your project config file):",0.0,0.0,1.0,0.0
Zipkin,46188800,46185813,1,"2017/09/13, 07:21:00",True,"2017/09/13, 07:21:00",12382.0,707451.0,1,"Second, When your Node.js server is launched, start the logger server at the same time if  ENABLE_LOGGER  is true.",0.0,0.202,0.798,0.5106
Zipkin,46188800,46185813,1,"2017/09/13, 07:21:00",True,"2017/09/13, 07:21:00",12382.0,707451.0,1,The logger server only do one thing: log the request and forward it to the real API server using  request  module.,0.0,0.0,1.0,0.0
Zipkin,46188800,46185813,1,"2017/09/13, 07:21:00",True,"2017/09/13, 07:21:00",12382.0,707451.0,1,You can use  Morgan  to provide more readable format.,0.0,0.0,1.0,0.0
Zipkin,46188800,46185813,1,"2017/09/13, 07:21:00",True,"2017/09/13, 07:21:00",12382.0,707451.0,1,"Third, in your Node.js server, send API request to logger server when  ENABLE_LOGGER  is true, and send API directly to the real server when  ENABLE_LOGGER  is false.",0.0,0.097,0.903,0.4215
Zipkin,44763002,44762961,4,"2017/06/26, 18:19:01",True,"2017/06/26, 18:19:01",8336.0,1773866.0,1,No - we haven't added any instrumentation around Webservicetemplate.,0.239,0.0,0.761,-0.296
Zipkin,44763002,44762961,4,"2017/06/26, 18:19:01",True,"2017/06/26, 18:19:01",8336.0,1773866.0,1,You'd have to add an interceptor similar to the one we add for RestTemplate.,0.0,0.0,1.0,0.0
Zipkin,44763002,44762961,4,"2017/06/26, 18:19:01",True,"2017/06/26, 18:19:01",8336.0,1773866.0,1,You'd have to pass all the tracing headers to the request so that the other side can properly parse it.,0.0,0.0,1.0,0.0
Zipkin,41045080,41040727,0,"2016/12/08, 19:11:10",True,"2016/12/08, 19:11:10",1825.0,499395.0,3,We have an internal OkHttpClient wrapper implementing Call.Factory which adds an initial interceptor:,0.0,0.0,1.0,0.0
Zipkin,41045080,41040727,0,"2016/12/08, 19:11:10",True,"2016/12/08, 19:11:10",1825.0,499395.0,3,to solve this problem.,0.451,0.26,0.289,-0.3237
Zipkin,41045080,41040727,0,"2016/12/08, 19:11:10",True,"2016/12/08, 19:11:10",1825.0,499395.0,3,"It is not transparent, however, so may not be good for Brave.",0.152,0.215,0.633,0.2486
Zipkin,41045080,41040727,0,"2016/12/08, 19:11:10",True,"2016/12/08, 19:11:10",1825.0,499395.0,3,"It works fine, because in practice once a client is configured, you only really use the  Call.Factory  interface :-)",0.0,0.204,0.796,0.4767
Zipkin,41045080,41040727,0,"2016/12/08, 19:11:10",True,"2016/12/08, 19:11:10",1825.0,499395.0,3,"The  Ctx  bit is just the context with stuff we want to propagate, we can do it implicit or explicit, hence the extra method to explicitly take it.",0.0,0.046,0.954,0.0772
Zipkin,40895971,40694955,0,"2016/11/30, 20:49:26",True,"2016/11/30, 20:49:26",3732.0,560814.0,2,Thanks for trying out HTrace!,0.0,0.444,0.556,0.4926
Zipkin,40895971,40694955,0,"2016/11/30, 20:49:26",True,"2016/11/30, 20:49:26",3732.0,560814.0,2,Sorry that the version issue is such a pain right now.,0.365,0.0,0.635,-0.5574
Zipkin,40895971,40694955,0,"2016/11/30, 20:49:26",True,"2016/11/30, 20:49:26",3732.0,560814.0,2,It is much easier to configure HTrace with the version in cloudera's CDH5.5 distribution of Hadoop and later.,0.0,0.141,0.859,0.4215
Zipkin,40895971,40694955,0,"2016/11/30, 20:49:26",True,"2016/11/30, 20:49:26",3732.0,560814.0,2,"There is a good description of how to do it here:  http://blog.cloudera.com/blog/2015/12/new-in-cloudera-labs-apache-htrace-incubating/   If you want to stick with an Apache release of the source code rather than a vendor release, try Hadoop 3.0.0-alpha1.",0.0,0.127,0.873,0.4939
Zipkin,40895971,40694955,0,"2016/11/30, 20:49:26",True,"2016/11/30, 20:49:26",3732.0,560814.0,2,http://hadoop.apache.org/releases.html,0.0,0.0,1.0,0.0
Zipkin,40895971,40694955,0,"2016/11/30, 20:49:26",True,"2016/11/30, 20:49:26",3732.0,560814.0,2,The HTrace libraries shippped in Hadoop 2.6 and 2.7 are very old... we never backported HTrace 4.x to those branches.,0.0,0.0,1.0,0.0
Zipkin,40895971,40694955,0,"2016/11/30, 20:49:26",True,"2016/11/30, 20:49:26",3732.0,560814.0,2,"They were stability branches, so new features like tracing was out of scope.",0.0,0.187,0.813,0.4144
Zipkin,40895971,40694955,0,"2016/11/30, 20:49:26",True,"2016/11/30, 20:49:26",3732.0,560814.0,2,"There is some functionality there, but not much.",0.0,0.0,1.0,0.0
Zipkin,40895971,40694955,0,"2016/11/30, 20:49:26",True,"2016/11/30, 20:49:26",3732.0,560814.0,2,I recommend using the newer HTrace 4.x library which is actively developed.,0.0,0.348,0.652,0.5859
Zipkin,40895971,40694955,0,"2016/11/30, 20:49:26",True,"2016/11/30, 20:49:26",3732.0,560814.0,2,"The HTrace 4.x branch also has a stable API, so hopefully breakage will be minimized in the future.",0.0,0.257,0.743,0.6361
Zipkin,42159646,40694955,0,"2017/02/10, 14:39:46",False,"2017/02/10, 16:03:38",9.0,7545887.0,0,"Exactly, in the code, I see the configuration key's prefix is  dfs.htrace , not the  hadoop.htrace .",0.0,0.0,1.0,0.0
Zipkin,42159646,40694955,0,"2017/02/10, 14:39:46",False,"2017/02/10, 16:03:38",9.0,7545887.0,0,"And in dfsclient, it's  dfs.client.htrace .",0.0,0.0,1.0,0.0
Zipkin,42159646,40694955,0,"2017/02/10, 14:39:46",False,"2017/02/10, 16:03:38",9.0,7545887.0,0,"You can change the prefix to  dfs.htrace , then restart the cluster and it take effect.",0.0,0.0,1.0,0.0
Zipkin,42159646,40694955,0,"2017/02/10, 14:39:46",False,"2017/02/10, 16:03:38",9.0,7545887.0,0,The code is in class  org.apache.hadoop.tracing.SpanReceiverHost .,0.0,0.0,1.0,0.0
Zipkin,42159646,40694955,0,"2017/02/10, 14:39:46",False,"2017/02/10, 16:03:38",9.0,7545887.0,0,Hope this help!,0.0,0.855,0.145,0.7088
Zipkin,40534423,40525453,4,"2016/11/10, 20:16:52",False,"2016/11/10, 20:16:52",8336.0,1773866.0,0,The sampling decision is taken for a trace.,0.0,0.0,1.0,0.0
Zipkin,40534423,40525453,4,"2016/11/10, 20:16:52",False,"2016/11/10, 20:16:52",8336.0,1773866.0,0,That means that when the first request comes in and the span is created you have to take a decision.,0.0,0.1,0.9,0.25
Zipkin,40534423,40525453,4,"2016/11/10, 20:16:52",False,"2016/11/10, 20:16:52",8336.0,1773866.0,0,You don't have any tags / baggage at that point so you must not depend on the contents of tags to take this decision.,0.0,0.0,1.0,0.0
Zipkin,40534423,40525453,4,"2016/11/10, 20:16:52",False,"2016/11/10, 20:16:52",8336.0,1773866.0,0,That's a wrong approach.,0.608,0.0,0.392,-0.4767
Zipkin,40534423,40525453,4,"2016/11/10, 20:16:52",False,"2016/11/10, 20:16:52",8336.0,1773866.0,0,You are taking a very custom approach.,0.0,0.0,1.0,0.0
Zipkin,40534423,40525453,4,"2016/11/10, 20:16:52",False,"2016/11/10, 20:16:52",8336.0,1773866.0,0,If you want to go that way (which is not recommended) you can create a custom implementation of a  SpanReporter  -  https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/SpanReporter.java#L30  .,0.0,0.167,0.833,0.34
Zipkin,40534423,40525453,4,"2016/11/10, 20:16:52",False,"2016/11/10, 20:16:52",8336.0,1773866.0,0,SpanReporter  is the one that is sending spans to zipkin.,0.0,0.0,1.0,0.0
Zipkin,40534423,40525453,4,"2016/11/10, 20:16:52",False,"2016/11/10, 20:16:52",8336.0,1773866.0,0,You can create an implementation that will wrap an existing  SpanReporter  implementation and will delegate the execution to it only when some values of tags match.,0.0,0.167,0.833,0.5859
Zipkin,40534423,40525453,4,"2016/11/10, 20:16:52",False,"2016/11/10, 20:16:52",8336.0,1773866.0,0,But from my perspective it doesn't sound right.,0.0,0.0,1.0,0.0
Zipkin,40151801,39862738,0,"2016/10/20, 13:26:02",False,"2016/10/20, 13:26:02",8336.0,1773866.0,5,If I'm not mistaken (and I guess I'm not) no wonder that you're not sending the Spans to Zipkin cause you didn't add the Zipkin dependency.,0.081,0.077,0.842,-0.0232
Zipkin,40151801,39862738,0,"2016/10/20, 13:26:02",False,"2016/10/20, 13:26:02",8336.0,1773866.0,5,Check the  Sleuth with Zipkin via HTTP  section of the docs:  http://cloud.spring.io/spring-cloud-sleuth/spring-cloud-sleuth.html  .,0.0,0.0,1.0,0.0
Zipkin,42608836,39862738,1,"2017/03/05, 15:21:05",False,"2017/03/05, 15:21:05",167.0,3814829.0,0,This config worked for me in 1 of my application:,0.0,0.0,1.0,0.0
Zipkin,42608836,39862738,1,"2017/03/05, 15:21:05",False,"2017/03/05, 15:21:05",167.0,3814829.0,0,Enabling property might do the trick!,0.23,0.0,0.77,-0.126
Zipkin,38528610,38523705,2,"2016/07/22, 17:11:03",True,"2016/07/22, 17:11:03",91301.0,2756547.0,1,"First of all the main feature of Spring Integration is  MessageChannel , but it still isn't clear to me why people are missing  .channel()  operator in between endpoint definitions.",0.177,0.0,0.823,-0.6784
Zipkin,38528610,38523705,2,"2016/07/22, 17:11:03",True,"2016/07/22, 17:11:03",91301.0,2756547.0,1,I mean that for your case it must be like:,0.0,0.238,0.762,0.3612
Zipkin,38528610,38523705,2,"2016/07/22, 17:11:03",True,"2016/07/22, 17:11:03",91301.0,2756547.0,1,Now about your particular problem.,0.403,0.0,0.597,-0.4019
Zipkin,38528610,38523705,2,"2016/07/22, 17:11:03",True,"2016/07/22, 17:11:03",91301.0,2756547.0,1,"Look,  ContentEnricher  ( .enrich() ) is request-reply component:  http://docs.spring.io/spring-integration/reference/html/messaging-transformation-chapter.html#payload-enricher .",0.0,0.0,1.0,0.0
Zipkin,38528610,38523705,2,"2016/07/22, 17:11:03",True,"2016/07/22, 17:11:03",91301.0,2756547.0,1,Therefore it sends request to its  requestChannel  and waits for reply.,0.0,0.0,1.0,0.0
Zipkin,38528610,38523705,2,"2016/07/22, 17:11:03",True,"2016/07/22, 17:11:03",91301.0,2756547.0,1,And it is done independently of the  requestChannel  type.,0.0,0.0,1.0,0.0
Zipkin,38528610,38523705,2,"2016/07/22, 17:11:03",True,"2016/07/22, 17:11:03",91301.0,2756547.0,1,I raw Java we can demonstrate such a behavior with this code snippet:,0.0,0.0,1.0,0.0
Zipkin,38528610,38523705,2,"2016/07/22, 17:11:03",True,"2016/07/22, 17:11:03",91301.0,2756547.0,1,where you should see that  ADD_LINE_ITEM_CHANNEL  as an  ExecutorChannel  doesn't have much value because we are blocked within loop for the reply anyway.,0.165,0.0,0.835,-0.4829
Zipkin,38528610,38523705,2,"2016/07/22, 17:11:03",True,"2016/07/22, 17:11:03",91301.0,2756547.0,1,"A  .split()  does exactly similar loop, but since by default it is with the  DirectChannel , an iteration is done in the same thread.",0.0,0.0,1.0,0.0
Zipkin,38528610,38523705,2,"2016/07/22, 17:11:03",True,"2016/07/22, 17:11:03",91301.0,2756547.0,1,Therefore each next item waits for the reply for the previous.,0.0,0.0,1.0,0.0
Zipkin,38528610,38523705,2,"2016/07/22, 17:11:03",True,"2016/07/22, 17:11:03",91301.0,2756547.0,1,"That's why you definitely should parallel exactly as an input for the  .enrich() , just after  .split() .",0.0,0.153,0.847,0.4019
Zipkin,28202807,28187275,1,"2015/01/28, 23:32:37",True,"2015/01/28, 23:32:37",1050.0,748140.0,1,The Apache Thrift TSocketTransport (almost certainly what you are using) uses TCP on a configurable port.,0.0,0.146,0.854,0.34
Zipkin,28202807,28187275,1,"2015/01/28, 23:32:37",True,"2015/01/28, 23:32:37",1050.0,748140.0,1,Cassandra usually uses port 9160 for thrift.,0.0,0.0,1.0,0.0
Zipkin,28202807,28187275,1,"2015/01/28, 23:32:37",True,"2015/01/28, 23:32:37",1050.0,748140.0,1,When using Thrift/TCP no HTTP setup is necessary.,0.239,0.0,0.761,-0.296
Zipkin,28202807,28187275,1,"2015/01/28, 23:32:37",True,"2015/01/28, 23:32:37",1050.0,748140.0,1,Just open 9160 (and any other ports your custom thrift servers may be listening on).,0.0,0.0,1.0,0.0
Zipkin,28202807,28187275,1,"2015/01/28, 23:32:37",True,"2015/01/28, 23:32:37",1050.0,748140.0,1,"Though you can use Thrift over HTTP, Thrift is RPC, not REST, so proxy caching will cause problems, the client needs a direct comm channel with the server.",0.094,0.0,0.906,-0.4019
Zipkin,48832878,28187275,0,"2018/02/16, 20:31:16",False,"2018/02/16, 20:31:16",1620.0,1401124.0,0,"If you do need to access a thrift service via a proxy, something like this would work:",0.0,0.152,0.848,0.3612
Zipkin,48832878,28187275,0,"2018/02/16, 20:31:16",False,"2018/02/16, 20:31:16",1620.0,1401124.0,0,https://github.com/totally/thrift_goodies/blob/master/transport.py,0.0,0.0,1.0,0.0
Zipkin,48832878,28187275,0,"2018/02/16, 20:31:16",False,"2018/02/16, 20:31:16",1620.0,1401124.0,0,You can kill the kerberos stuff if you don't need that.,0.32,0.0,0.68,-0.6908
Zipkin,26486619,26016714,0,"2014/10/21, 15:21:43",True,"2014/10/21, 15:21:43",583.0,1372349.0,0,i get the same messages but still be able to collect messages and view them with the web service.,0.0,0.0,1.0,0.0
Zipkin,26486619,26016714,0,"2014/10/21, 15:21:43",True,"2014/10/21, 15:21:43",583.0,1372349.0,0,I dont know why the [error] prefix is in front of it but if you read the chars behind you see INF/DEB and so on...,0.0,0.0,1.0,0.0
Zipkin,26486619,26016714,0,"2014/10/21, 15:21:43",True,"2014/10/21, 15:21:43",583.0,1372349.0,0,It stays for INFO and DEBUG.,0.0,0.0,1.0,0.0
Zipkin,26486619,26016714,0,"2014/10/21, 15:21:43",True,"2014/10/21, 15:21:43",583.0,1372349.0,0,Greets,0.0,1.0,0.0,0.1531
Zipkin,62188352,62188289,1,"2020/06/04, 09:39:36",False,"2020/06/04, 09:39:36",4038.0,809423.0,0,"yes, you can have multiple express running in the same node process (thats how clustering works in node as well)",0.0,0.124,0.876,0.4019
Zipkin,62188352,62188289,1,"2020/06/04, 09:39:36",False,"2020/06/04, 09:39:36",4038.0,809423.0,0,but you will need to have them running on different ports.,0.0,0.0,1.0,0.0
Zipkin,62188352,62188289,1,"2020/06/04, 09:39:36",False,"2020/06/04, 09:39:36",4038.0,809423.0,0,;,0.0,0.0,0.0,0.0
Zipkin,62196735,62188289,1,"2020/06/04, 17:08:54",True,"2020/06/04, 17:08:54",501.0,1272149.0,1,It is possible to create two separate tracer providers.,0.0,0.208,0.792,0.2732
Zipkin,62196735,62188289,1,"2020/06/04, 17:08:54",True,"2020/06/04, 17:08:54",501.0,1272149.0,1,"Only one of them will be the global tracer provider, which the API will use if you call API methods.",0.0,0.0,1.0,0.0
Zipkin,62196735,62188289,1,"2020/06/04, 17:08:54",True,"2020/06/04, 17:08:54",501.0,1272149.0,1,"You can't use the plugins in this configuration, which means you will have to manually instrument your application.",0.0,0.0,1.0,0.0
Zipkin,62196735,62188289,1,"2020/06/04, 17:08:54",True,"2020/06/04, 17:08:54",501.0,1272149.0,1,"If this is a use-case which is important to you, I suggest you create an issue on the github repo.",0.0,0.196,0.804,0.4404
Zipkin,61028338,61027513,1,"2020/04/04, 15:50:12",True,"2020/04/04, 15:50:12",1162.0,4383264.0,0,"This indicates that the pod is not ready, hence the service will not add that pod's IP in the list of endpoints.",0.091,0.0,0.909,-0.2755
Zipkin,61028338,61027513,1,"2020/04/04, 15:50:12",True,"2020/04/04, 15:50:12",1162.0,4383264.0,0,Check the readiness probe of your pod by describing it and debug the issue that's making it non-ready.,0.0,0.105,0.895,0.25
Zipkin,61028338,61027513,1,"2020/04/04, 15:50:12",True,"2020/04/04, 15:50:12",1162.0,4383264.0,0,"Once this if fixed, you'll start seeing some endpoints populated when you describe the service &amp; that will enable you to access the service by DNS name.",0.0,0.0,1.0,0.0
Zipkin,54493070,54492867,3,"2019/02/02, 14:27:06",False,"2019/02/02, 14:27:06",12094.0,1556338.0,2,First specify your own overlay network (see bottom of code below) and use it for your services.,0.0,0.0,1.0,0.0
Zipkin,54493070,54492867,3,"2019/02/02, 14:27:06",False,"2019/02/02, 14:27:06",12094.0,1556338.0,2,"Then in your compose file for your other services like ZIpkin, add the  backbone  network to its list.",0.0,0.128,0.872,0.3612
Zipkin,54493070,54492867,3,"2019/02/02, 14:27:06",False,"2019/02/02, 14:27:06",12094.0,1556338.0,2,Eg:,0.0,0.0,1.0,0.0
Zipkin,54493070,54492867,3,"2019/02/02, 14:27:06",False,"2019/02/02, 14:27:06",12094.0,1556338.0,2,"Note that outside of the first compose file, you'll need to prefix the  project  name for your network.",0.0,0.0,1.0,0.0
Zipkin,54493070,54492867,3,"2019/02/02, 14:27:06",False,"2019/02/02, 14:27:06",12094.0,1556338.0,2,Unless you set the environment variable  COMPOSE_PROJECT_NAME  it will be the name of the directory that the compose file is in.,0.0,0.0,1.0,0.0
Zipkin,54493070,54492867,3,"2019/02/02, 14:27:06",False,"2019/02/02, 14:27:06",12094.0,1556338.0,2,Do a  docker network ls  to find out the full name of the network to use.,0.0,0.0,1.0,0.0
Zipkin,54494741,54492867,0,"2019/02/02, 17:56:24",False,"2019/02/02, 17:56:24",1285.0,6022333.0,0,"OK, so I solved my problem using only docker-compose files.",0.207,0.373,0.42,0.3318
Zipkin,54494741,54492867,0,"2019/02/02, 17:56:24",False,"2019/02/02, 17:56:24",1285.0,6022333.0,0,By Portainer I pasted my second docker-compose file in Stack section (I creaated new Stack):,0.0,0.0,1.0,0.0
Zipkin,54494741,54492867,0,"2019/02/02, 17:56:24",False,"2019/02/02, 17:56:24",1285.0,6022333.0,0,Now we should use 'my_name_zipkin' name to communicate with this service.,0.0,0.0,1.0,0.0
Zipkin,54494741,54492867,0,"2019/02/02, 17:56:24",False,"2019/02/02, 17:56:24",1285.0,6022333.0,0,Service name is the name we should use to communicate between containers.,0.0,0.0,1.0,0.0
Zipkin,54494741,54492867,0,"2019/02/02, 17:56:24",False,"2019/02/02, 17:56:24",1285.0,6022333.0,0,So in properties file I set:,0.0,0.0,1.0,0.0
Zipkin,51306145,51305899,0,"2018/07/12, 15:49:09",False,"2018/07/12, 15:49:09",18670.0,340088.0,0,"The slf4j API only takes  String  as the input to the  info ,  debug ,  warn ,  error  messages.",0.227,0.0,0.773,-0.4767
Zipkin,51306145,51305899,0,"2018/07/12, 15:49:09",False,"2018/07/12, 15:49:09",18670.0,340088.0,0,"What you could do is create your own JsonLogger wrapper, which takes a normal  Logger  (maybe wraps around it), which you could include at the top of your classes like:",0.0,0.198,0.802,0.6597
Zipkin,51306145,51305899,0,"2018/07/12, 15:49:09",False,"2018/07/12, 15:49:09",18670.0,340088.0,0,private static final JsonLogger logger = new JsonLogger(LoggerFactory.getLogger(MyClass.class)) ;,0.0,0.0,1.0,0.0
Zipkin,51306145,51305899,0,"2018/07/12, 15:49:09",False,"2018/07/12, 15:49:09",18670.0,340088.0,0,"You can then use Jackson, GSON or your favourite object to JSON mapper inside your JsonLogger so that you could do what you want.",0.0,0.053,0.947,0.0772
Zipkin,51306145,51305899,0,"2018/07/12, 15:49:09",False,"2018/07/12, 15:49:09",18670.0,340088.0,0,"It can then offer the  info ,  debug ,  warn ,  error  methods like a normal logger.",0.247,0.151,0.602,-0.1531
Zipkin,51306145,51305899,0,"2018/07/12, 15:49:09",False,"2018/07/12, 15:49:09",18670.0,340088.0,0,You can also create your own  JsonLoggerFactory  which encapsulates this for you so that the line to include in each class is more concise.,0.0,0.084,0.916,0.2732
Zipkin,21036699,21035841,1,"2014/01/10, 07:20:28",True,"2014/01/10, 07:20:28",136.0,1982812.0,1,"Yes, you can use BAM/CEP for this.",0.0,0.31,0.69,0.4019
Zipkin,21036699,21035841,1,"2014/01/10, 07:20:28",True,"2014/01/10, 07:20:28",136.0,1982812.0,1,If you need real time monitoring you can use CEP and you can use BAM for batch process.,0.0,0.0,1.0,0.0
Zipkin,21036699,21035841,1,"2014/01/10, 07:20:28",True,"2014/01/10, 07:20:28",136.0,1982812.0,1,"From BAM 2.4.0 onwards, CEP features have been added inside BAM also hence you can use BAM and do real time analytics.",0.0,0.0,1.0,0.0
Zipkin,21036699,21035841,1,"2014/01/10, 07:20:28",True,"2014/01/10, 07:20:28",136.0,1982812.0,1,What type of services are involved with your scenario?,0.0,0.0,1.0,0.0
Zipkin,21036699,21035841,1,"2014/01/10, 07:20:28",True,"2014/01/10, 07:20:28",136.0,1982812.0,1,Depends on this you can use already existing data publisher or write new data publisher for BAM/CEP to publish your request details.,0.0,0.0,1.0,0.0
Zipkin,21036699,21035841,1,"2014/01/10, 07:20:28",True,"2014/01/10, 07:20:28",136.0,1982812.0,1,"For example if you are having chain of axis2 webservice calls for a request from client, and you want to monitor where the bottle neck/more time consumed, then you may use the service stats publishing, and monitor the average time take to process the message which will help you to see where the actual delay has been introduced.",0.038,0.066,0.896,0.1779
Zipkin,21036699,21035841,1,"2014/01/10, 07:20:28",True,"2014/01/10, 07:20:28",136.0,1982812.0,1,For this you can use existing service statistics publisher feature.,0.0,0.0,1.0,0.0
Zipkin,21036699,21035841,1,"2014/01/10, 07:20:28",True,"2014/01/10, 07:20:28",136.0,1982812.0,1,"Also BAM will allow you to create your own dashboard to visualize, hence you can customize the dashboard.",0.0,0.2,0.8,0.4588
Zipkin,21036699,21035841,1,"2014/01/10, 07:20:28",True,"2014/01/10, 07:20:28",136.0,1982812.0,1,Also with BAM 2.4.0 we have introduced notifications feature also which you can define some threshold value and configure to send notification if that cross that threshold value.,0.0,0.156,0.844,0.5859
Zipkin,66037294,65957785,0,"2021/02/04, 01:32:16",False,"2021/02/04, 01:32:16",3.0,6206060.0,0,"I contributed to Micronaut and submitted a PR, which is now merged.",0.0,0.0,1.0,0.0
Zipkin,66037294,65957785,0,"2021/02/04, 01:32:16",False,"2021/02/04, 01:32:16",3.0,6206060.0,0,Pull request,0.0,0.0,1.0,0.0
Zipkin,64845200,64818612,0,"2020/11/15, 15:54:52",True,"2020/11/15, 15:54:52",160.0,3244615.0,0,So it was application B which was not passing the header along.,0.0,0.0,1.0,0.0
Zipkin,64845200,64818612,0,"2020/11/15, 15:54:52",True,"2020/11/15, 15:54:52",160.0,3244615.0,0,Turns out that the queue uri had a property  targetClient  which was set to 1.,0.0,0.0,1.0,0.0
Zipkin,64845200,64818612,0,"2020/11/15, 15:54:52",True,"2020/11/15, 15:54:52",160.0,3244615.0,0,The uri is something like,0.0,0.385,0.615,0.3612
Zipkin,64845200,64818612,0,"2020/11/15, 15:54:52",True,"2020/11/15, 15:54:52",160.0,3244615.0,0,"Now I am not an IBM MQ expert by far, but the  documentation  states that setting this property to 1 means that  Messages do not contain an MQRFH2 header.",0.0,0.0,1.0,0.0
Zipkin,64845200,64818612,0,"2020/11/15, 15:54:52",True,"2020/11/15, 15:54:52",160.0,3244615.0,0,"I toggled it to 0 and voila, all spans fall into place.",0.0,0.0,1.0,0.0
Zipkin,64215147,64214980,6,"2020/10/05, 22:40:51",False,"2020/10/05, 22:40:51",425.0,8214791.0,0,You must tell the containers the network &quot;foo_network&quot;.,0.0,0.0,1.0,0.0
Zipkin,64215147,64214980,6,"2020/10/05, 22:40:51",False,"2020/10/05, 22:40:51",425.0,8214791.0,0,The External flag says that the containers are not accessible from outside.,0.0,0.0,1.0,0.0
Zipkin,64215147,64214980,6,"2020/10/05, 22:40:51",False,"2020/10/05, 22:40:51",425.0,8214791.0,0,"Of course you don't have to bet, but I thought as an example it might be quite good.",0.0,0.211,0.789,0.6474
Zipkin,64215147,64214980,6,"2020/10/05, 22:40:51",False,"2020/10/05, 22:40:51",425.0,8214791.0,0,And because of the &quot;links&quot; function look here  Link,0.0,0.0,1.0,0.0
Zipkin,63661339,63331441,0,"2020/08/30, 22:28:14",False,"2020/08/30, 22:28:14",11.0,6284144.0,0,I think I found the problem.,0.474,0.0,0.526,-0.4019
Zipkin,63661339,63331441,0,"2020/08/30, 22:28:14",False,"2020/08/30, 22:28:14",11.0,6284144.0,0,Than I used the Exceutor in my code:,0.0,0.0,1.0,0.0
Zipkin,63661339,63331441,0,"2020/08/30, 22:28:14",False,"2020/08/30, 22:28:14",11.0,6284144.0,0,With this configuration the service was not starting correctly.,0.0,0.0,1.0,0.0
Zipkin,63661339,63331441,0,"2020/08/30, 22:28:14",False,"2020/08/30, 22:28:14",11.0,6284144.0,0,Now the @Bean(&quot;threadPoolTaskExecutor&quot;) configuration is removed and I'm using only @Async.,0.0,0.0,1.0,0.0
Zipkin,63661339,63331441,0,"2020/08/30, 22:28:14",False,"2020/08/30, 22:28:14",11.0,6284144.0,0,But why it's not working with Spring Boot Starter 2.3.x?,0.0,0.0,1.0,0.0
Zipkin,63661339,63331441,0,"2020/08/30, 22:28:14",False,"2020/08/30, 22:28:14",11.0,6284144.0,0,And there was no error message in  the log.,0.412,0.0,0.588,-0.5994
Zipkin,61571674,61571538,0,"2020/05/03, 11:35:32",True,"2020/05/03, 11:35:32",,,1,"Zipkin is a Spring-Boot-based project, the @EnableZipkinServer is not a Spring Cloud annotation.",0.0,0.0,1.0,0.0
Zipkin,61571674,61571538,0,"2020/05/03, 11:35:32",True,"2020/05/03, 11:35:32",,,1,It’s an annotation that’spart of the Zipkin project.,0.0,0.0,1.0,0.0
Zipkin,61571674,61571538,0,"2020/05/03, 11:35:32",True,"2020/05/03, 11:35:32",,,1,"This often confuses people who are new to the Spring Cloud Sleuth and Zipkin, because the Spring Cloud team did write the @EnableZipkinStreamServer annotation as part of Spring Cloud Sleuth.",0.073,0.0,0.927,-0.3182
Zipkin,61571674,61571538,0,"2020/05/03, 11:35:32",True,"2020/05/03, 11:35:32",,,1,The @EnableZipkinStreamServer annotation simplifies the use of Zipkin with RabbitMQ and Kafka.,0.0,0.0,1.0,0.0
Zipkin,61571674,61571538,0,"2020/05/03, 11:35:32",True,"2020/05/03, 11:35:32",,,1,Advantages of  @EnableZipkinServer is simplicity in setup.,0.0,0.294,0.706,0.3612
Zipkin,61571674,61571538,0,"2020/05/03, 11:35:32",True,"2020/05/03, 11:35:32",,,1,With the @EnableZipkinStream server you need to set up and configure the services being traced and the Zipkin server to publish/listen to RabbitMQ or Kafka for tracing data.The advantage of the @EnableZipkinStreamServer annotation is that you can continue to collect trace data even if the Zipkin server is unavailable.,0.0,0.04,0.96,0.25
Zipkin,61571674,61571538,0,"2020/05/03, 11:35:32",True,"2020/05/03, 11:35:32",,,1,This is because the trace messages will accumulate the trace data on a message queue until the Zipkin server is available for processing the records.,0.0,0.0,1.0,0.0
Zipkin,61571674,61571538,0,"2020/05/03, 11:35:32",True,"2020/05/03, 11:35:32",,,1,"If you use the @EnableZipkinServer annotation and the Zipkin server is unavailable,the trace data that would have been sent by the service(s) to Zipkin will be lost.",0.081,0.0,0.919,-0.3182
Zipkin,60987725,60974758,2,"2020/04/02, 11:56:59",False,"2020/04/02, 11:56:59",8336.0,1773866.0,0,"Please don't use field injection, use constructor injection.",0.0,0.247,0.753,0.3182
Zipkin,60987725,60974758,2,"2020/04/02, 11:56:59",False,"2020/04/02, 11:56:59",8336.0,1773866.0,0,Also new span there doesn't make sense cause you already have a new span created by the framework.,0.0,0.111,0.889,0.25
Zipkin,59405789,59405521,1,"2019/12/19, 10:24:24",False,"2019/12/19, 10:24:24",8336.0,1773866.0,1,Your versions are wrong.,0.508,0.0,0.492,-0.4767
Zipkin,59405789,59405521,1,"2019/12/19, 10:24:24",False,"2019/12/19, 10:24:24",8336.0,1773866.0,1,"Please don't set the versions by yourself, please use the Spring Cloud BOM (spring-cloud-dependencies) dependency management like presented below",0.0,0.307,0.693,0.7269
Zipkin,59405789,59405521,1,"2019/12/19, 10:24:24",False,"2019/12/19, 10:24:24",8336.0,1773866.0,1,Also - it's enough for you to add the starters.,0.0,0.0,1.0,0.0
Zipkin,59405789,59405521,1,"2019/12/19, 10:24:24",False,"2019/12/19, 10:24:24",8336.0,1773866.0,1,"You've added a starter in a given version and then you've added the core dependency in another one, that makes no sense.",0.104,0.0,0.896,-0.296
Zipkin,59405789,59405521,1,"2019/12/19, 10:24:24",False,"2019/12/19, 10:24:24",8336.0,1773866.0,1,Last thing - versions 1.x are deprecated and no longer maintained.,0.196,0.0,0.804,-0.296
Zipkin,59405789,59405521,1,"2019/12/19, 10:24:24",False,"2019/12/19, 10:24:24",8336.0,1773866.0,1,The current version is 2.2.0.RELEASE and release train version is Hoxton.RELEASE,0.0,0.0,1.0,0.0
Zipkin,58357995,58349450,1,"2019/10/12, 23:01:05",False,"2019/10/12, 23:01:05",1164.0,113183.0,0,This can be done using Finagle's  Contexts .,0.0,0.0,1.0,0.0
Zipkin,58357995,58349450,1,"2019/10/12, 23:01:05",False,"2019/10/12, 23:01:05",1164.0,113183.0,0,"Contexts give you access to request-scoped state, such as a request’s deadline, throughout the logical life of a request without requiring them to be explicitly passed",0.0,0.0,1.0,0.0
Zipkin,55535572,55090908,0,"2019/04/05, 15:37:34",False,"2019/04/05, 15:37:34",8336.0,1773866.0,0,Unfortunately the best answer to this issue is to upgrade to the latest version of Sleuth where we've migrated to Brave as an internal tracer and fixed a lot of issues.,0.065,0.205,0.73,0.7351
Zipkin,54445765,54445701,1,"2019/01/30, 19:00:50",False,"2019/01/30, 19:00:50",760.0,4178894.0,0,You can connect your existing container to another network,0.0,0.0,1.0,0.0
Zipkin,54001979,53985744,0,"2019/01/02, 08:11:22",False,"2019/01/02, 08:11:22",1.0,10856416.0,-1,The error-code implies an error on the other end - 400-errors are not located on your end.,0.153,0.0,0.847,-0.4019
Zipkin,54001979,53985744,0,"2019/01/02, 08:11:22",False,"2019/01/02, 08:11:22",1.0,10856416.0,-1,Have you tried dumping the response (including headers)?,0.247,0.0,0.753,-0.3182
Zipkin,54001979,53985744,0,"2019/01/02, 08:11:22",False,"2019/01/02, 08:11:22",1.0,10856416.0,-1,"Also, did you try to re-authenticate, perhaps reset cookings, etc?",0.0,0.0,1.0,0.0
Zipkin,54001979,53985744,0,"2019/01/02, 08:11:22",False,"2019/01/02, 08:11:22",1.0,10856416.0,-1,Did you contact the other end?,0.0,0.0,1.0,0.0
Zipkin,54001979,53985744,0,"2019/01/02, 08:11:22",False,"2019/01/02, 08:11:22",1.0,10856416.0,-1,How did they respond to it?,0.0,0.0,1.0,0.0
Zipkin,53391639,53391503,0,"2018/11/20, 13:06:05",False,"2018/11/20, 13:06:05",26220.0,927493.0,0,Add a dependencyManagement entry for io.zipkin.zipkin2:zipkin:2.7.1,0.0,0.0,1.0,0.0
Zipkin,53393882,53391503,0,"2018/11/20, 15:18:12",True,"2018/11/20, 15:18:12",330.0,1270045.0,0,"For me or anybody finding this thread:
Solved it by upgrading from Camden to Edgware which contains 1.3.5 (and resolving everything around that switch).",0.0,0.176,0.824,0.5719
Zipkin,53185008,53154813,1,"2018/11/07, 09:18:45",True,"2018/11/07, 09:18:45",8336.0,1773866.0,1,You can create your own custom  SpanAdjuster  that will modify the span name.,0.0,0.149,0.851,0.2732
Zipkin,53185008,53154813,1,"2018/11/07, 09:18:45",True,"2018/11/07, 09:18:45",8336.0,1773866.0,1,You can also use  FinishedSpanHandler  to operate on finished spans to tweak them.,0.0,0.0,1.0,0.0
Zipkin,52481820,52156749,2,"2018/09/24, 17:42:00",False,"2018/09/24, 17:42:00",8336.0,1773866.0,0,"Yes, when you create a span you can set the service name.",0.0,0.348,0.652,0.5859
Zipkin,52481820,52156749,2,"2018/09/24, 17:42:00",False,"2018/09/24, 17:42:00",8336.0,1773866.0,0,Just call  newSpan.remoteServiceName(...),0.0,0.0,1.0,0.0
Zipkin,52784486,52156749,0,"2018/10/12, 20:33:48",False,"2018/10/12, 20:33:48",2970.0,4296607.0,0,"Taking the input of @MarcinGrzejszczak as reference, I resolved using a custom span:",0.0,0.145,0.855,0.1779
Zipkin,52784486,52156749,0,"2018/10/12, 20:33:48",False,"2018/10/12, 20:33:48",2970.0,4296607.0,0,Where  tracer  is an autowired object from  Trace :,0.0,0.0,1.0,0.0
Zipkin,52784486,52156749,0,"2018/10/12, 20:33:48",False,"2018/10/12, 20:33:48",2970.0,4296607.0,0,Both classes are in  brave  package,0.0,0.405,0.595,0.5267
Zipkin,52784486,52156749,0,"2018/10/12, 20:33:48",False,"2018/10/12, 20:33:48",2970.0,4296607.0,0,Result:,0.0,0.0,1.0,0.0
Zipkin,52784486,52156749,0,"2018/10/12, 20:33:48",False,"2018/10/12, 20:33:48",2970.0,4296607.0,0,"If you want to take a look at the implementation in more detail, here is the sample:  https://github.com/juanca87/sample-traceability-microservices",0.0,0.075,0.925,0.0772
Zipkin,51934542,51934410,2,"2018/08/20, 19:02:24",True,"2018/08/20, 19:02:24",874.0,1753823.0,0,add this under your  section at the end of your pom.xml.,0.0,0.0,1.0,0.0
Zipkin,51934542,51934410,2,"2018/08/20, 19:02:24",True,"2018/08/20, 19:02:24",874.0,1753823.0,0,you may need to add for all the dependencies.,0.0,0.0,1.0,0.0
Zipkin,52823986,51534836,0,"2018/10/15, 23:04:45",False,"2018/10/15, 23:04:45",17842.0,7862821.0,0,"Maybe, I didn't get your question right, but with almost your  docker-compose.yaml  file:",0.0,0.0,1.0,0.0
Zipkin,52823986,51534836,0,"2018/10/15, 23:04:45",False,"2018/10/15, 23:04:45",17842.0,7862821.0,0,prometheus  metrics are available on  localhost:9411/metrics  both inside container and on host system:,0.0,0.0,1.0,0.0
Zipkin,51494483,51444018,0,"2018/07/24, 11:56:29",False,"2018/07/24, 11:56:29",402.0,4298311.0,0,I finally got it working.,0.0,0.0,1.0,0.0
Zipkin,51494483,51444018,0,"2018/07/24, 11:56:29",False,"2018/07/24, 11:56:29",402.0,4298311.0,0,I just changed the logstash config file and added:,0.0,0.0,1.0,0.0
Zipkin,51494483,51444018,0,"2018/07/24, 11:56:29",False,"2018/07/24, 11:56:29",402.0,4298311.0,0,The filter part was missing earlier.,0.306,0.0,0.694,-0.296
Zipkin,50922654,50922384,0,"2018/06/19, 10:19:26",False,"2018/06/19, 10:19:26",8336.0,1773866.0,0,"Can you follow the guidelines described here  https://stackoverflow.com/help/how-to-ask  and the next question you ask, ask it with more details?",0.0,0.0,1.0,0.0
Zipkin,50922654,50922384,0,"2018/06/19, 10:19:26",False,"2018/06/19, 10:19:26",8336.0,1773866.0,0,E.g.,0.0,0.0,1.0,0.0
Zipkin,50922654,50922384,0,"2018/06/19, 10:19:26",False,"2018/06/19, 10:19:26",8336.0,1773866.0,0,I have no idea how exactly you use Sleuth?,0.239,0.0,0.761,-0.296
Zipkin,50922654,50922384,0,"2018/06/19, 10:19:26",False,"2018/06/19, 10:19:26",8336.0,1773866.0,0,Anyways I'll try to answer...,0.0,0.0,1.0,0.0
Zipkin,50922654,50922384,0,"2018/06/19, 10:19:26",False,"2018/06/19, 10:19:26",8336.0,1773866.0,0,"You can create a  SpanAdjuster  bean, that will analyze the span information (e.g.",0.0,0.16,0.84,0.2732
Zipkin,50922654,50922384,0,"2018/06/19, 10:19:26",False,"2018/06/19, 10:19:26",8336.0,1773866.0,0,span tags) and basing on that information you will change the sampling decision so as not to send it to Zipkin.,0.0,0.0,1.0,0.0
Zipkin,50922654,50922384,0,"2018/06/19, 10:19:26",False,"2018/06/19, 10:19:26",8336.0,1773866.0,0,Another option is to wrap the default span reporter in a similar logic.,0.0,0.0,1.0,0.0
Zipkin,50922654,50922384,0,"2018/06/19, 10:19:26",False,"2018/06/19, 10:19:26",8336.0,1773866.0,0,Yet another option is to verify what kind of a thread it is that is creating this span and toggle it off (assuming that it's a  @Scheduled  method) -  https://cloud.spring.io/spring-cloud-static/Finchley.RC2/single/spring-cloud.html#__literal_scheduled_literal_annotated_methods,0.0,0.078,0.922,0.296
Zipkin,50709747,50691266,0,"2018/06/06, 01:22:16",False,"2018/06/06, 02:13:47",1.0,9897847.0,-2,name: tcp -  protocol: TCP ?,0.0,0.0,1.0,0.0
Zipkin,49899755,49899572,0,"2018/04/18, 15:29:29",True,"2018/04/18, 15:29:29",8336.0,1773866.0,0,"I tried to back to use the stable version(1.3.3) of spring cloud sleuth, but when i use the bom for the project its make conflict in the spring boot version that i am using(2.0).",0.088,0.048,0.864,-0.3291
Zipkin,49899755,49899572,0,"2018/04/18, 15:29:29",True,"2018/04/18, 15:29:29",8336.0,1773866.0,0,Its compactible with the spring boot version 2?,0.0,0.0,1.0,0.0
Zipkin,49899755,49899572,0,"2018/04/18, 15:29:29",True,"2018/04/18, 15:29:29",8336.0,1773866.0,0,You can't use the Sleuth 1.3 with Boot 2.0.,0.0,0.0,1.0,0.0
Zipkin,49899755,49899572,0,"2018/04/18, 15:29:29",True,"2018/04/18, 15:29:29",8336.0,1773866.0,0,"I am using the spring cloud sleuth to make trace of services on my company, but i have a version of tracing on others services that is not compactible with the opentracing headers, so i want to change the headers of http messages to make the new services compactibles with the current tracing headers that i have in the others components.",0.0,0.037,0.963,0.2759
Zipkin,49899755,49899572,0,"2018/04/18, 15:29:29",True,"2018/04/18, 15:29:29",8336.0,1773866.0,0,"Yeah, that's the Brave change.",0.0,0.651,0.349,0.6808
Zipkin,49899755,49899572,0,"2018/04/18, 15:29:29",True,"2018/04/18, 15:29:29",8336.0,1773866.0,0,For http you can define your own parses.,0.0,0.0,1.0,0.0
Zipkin,49899755,49899572,0,"2018/04/18, 15:29:29",True,"2018/04/18, 15:29:29",8336.0,1773866.0,0,https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/instrument/web/TraceHttpAutoConfiguration.java#L57-L68  .,0.0,0.0,1.0,0.0
Zipkin,49899755,49899572,0,"2018/04/18, 15:29:29",True,"2018/04/18, 15:29:29",8336.0,1773866.0,0,"
@Autowired HttpClientParser clientParser;
    @Autowired HttpServerParser serverParser;
    @Autowired @ClientSampler HttpSampler clientSampler;
    @Autowired(required = false) @ServerSampler HttpSampler serverSampler;",0.0,0.0,1.0,0.0
Zipkin,49899755,49899572,0,"2018/04/18, 15:29:29",True,"2018/04/18, 15:29:29",8336.0,1773866.0,0,These are the samplers that you can register.,0.0,0.0,1.0,0.0
Zipkin,49899755,49899572,0,"2018/04/18, 15:29:29",True,"2018/04/18, 15:29:29",8336.0,1773866.0,0,For messaging you'd have to create your own version of the global channel interceptor - like the one we define here -  https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/instrument/messaging/TraceSpringIntegrationAutoConfiguration.java#L49-L53  .,0.0,0.195,0.805,0.5574
Zipkin,49899755,49899572,0,"2018/04/18, 15:29:29",True,"2018/04/18, 15:29:29",8336.0,1773866.0,0,"If that's not acceptable for you, go ahead and file an issue in Sleuth so we can discuss it there.",0.094,0.0,0.906,-0.2411
Zipkin,49402195,49393630,0,"2018/03/21, 11:13:30",True,"2018/03/21, 11:13:30",762.0,1227937.0,0,There's no concept of a trace finishing in zipkin.,0.239,0.0,0.761,-0.296
Zipkin,49402195,49393630,0,"2018/03/21, 11:13:30",True,"2018/03/21, 11:13:30",762.0,1227937.0,0,A span within a trace can start and finish.,0.0,0.0,1.0,0.0
Zipkin,49402195,49393630,0,"2018/03/21, 11:13:30",True,"2018/03/21, 11:13:30",762.0,1227937.0,0,"We don't start and stop spans on different hosts, so unfinished spans probably are accidental.",0.08,0.117,0.803,0.1501
Zipkin,49402195,49393630,0,"2018/03/21, 11:13:30",True,"2018/03/21, 11:13:30",762.0,1227937.0,0,You can chat more here if you like  https://gitter.im/spring-cloud/spring-cloud-sleuth,0.0,0.238,0.762,0.3612
Zipkin,49242542,49241122,0,"2018/03/12, 20:53:14",False,"2018/03/12, 20:53:14",8336.0,1773866.0,1,"In Sleuth  1.3.x  you can create a custom  SpanReporter  that, before sending a span to Zipkin, would analyze the URL and would not report that span.",0.0,0.084,0.916,0.2732
Zipkin,49242542,49241122,0,"2018/03/12, 20:53:14",False,"2018/03/12, 20:53:14",8336.0,1773866.0,1,In Sleuth  2.0.x  you can create a custom  HttpSampler  for the client side (with name  sleuthClientSampler ),0.0,0.13,0.87,0.2732
Zipkin,49202659,49201687,1,"2018/03/09, 23:47:19",True,"2018/03/09, 23:47:19",13548.0,33404.0,1,Turns out the issue was a corrupt Maven package.,0.0,0.0,1.0,0.0
Zipkin,49202659,49201687,1,"2018/03/09, 23:47:19",True,"2018/03/09, 23:47:19",13548.0,33404.0,1,Deleting my  .m2\repository  folder and running  mvn spring-boot:run  to downloaded dependencies and run my app resolved the issue.,0.0,0.091,0.909,0.1779
Zipkin,48900919,48893982,0,"2018/02/21, 10:12:42",True,"2018/02/21, 10:12:42",8336.0,1773866.0,0,Please read this page from Zipkin -  https://zipkin.io/pages/instrumenting.html  .,0.0,0.277,0.723,0.3182
Zipkin,48900919,48893982,0,"2018/02/21, 10:12:42",True,"2018/02/21, 10:12:42",8336.0,1773866.0,0,It's all written there how things should work.,0.0,0.0,1.0,0.0
Zipkin,48900919,48893982,0,"2018/02/21, 10:12:42",True,"2018/02/21, 10:12:42",8336.0,1773866.0,0,HTTP Tracing,0.0,0.0,1.0,0.0
Zipkin,48900919,48893982,0,"2018/02/21, 10:12:42",True,"2018/02/21, 10:12:42",8336.0,1773866.0,0,HTTP headers are used to pass along trace information.,0.0,0.0,1.0,0.0
Zipkin,48900919,48893982,0,"2018/02/21, 10:12:42",True,"2018/02/21, 10:12:42",8336.0,1773866.0,0,The B3 portion of the header is so named for the original name of Zipkin: BigBrotherBird.,0.0,0.133,0.867,0.3182
Zipkin,48900919,48893982,0,"2018/02/21, 10:12:42",True,"2018/02/21, 10:12:42",8336.0,1773866.0,0,Also please check the B3 specification page -  https://github.com/openzipkin/b3-propagation,0.0,0.247,0.753,0.3182
Zipkin,48874125,48873756,13,"2018/02/19, 23:02:35",True,"2018/02/19, 23:02:35",8336.0,1773866.0,0,people are scared about the overhead which sleuth can have by adding @NewSpan on the methods.,0.162,0.0,0.838,-0.4404
Zipkin,48874125,48873756,13,"2018/02/19, 23:02:35",True,"2018/02/19, 23:02:35",8336.0,1773866.0,0,Do they have any information about the overhead?,0.0,0.0,1.0,0.0
Zipkin,48874125,48873756,13,"2018/02/19, 23:02:35",True,"2018/02/19, 23:02:35",8336.0,1773866.0,0,Have they turned it on and the application started to lag significantly?,0.179,0.0,0.821,-0.34
Zipkin,48874125,48873756,13,"2018/02/19, 23:02:35",True,"2018/02/19, 23:02:35",8336.0,1773866.0,0,What are they scared of?,0.42,0.0,0.58,-0.4404
Zipkin,48874125,48873756,13,"2018/02/19, 23:02:35",True,"2018/02/19, 23:02:35",8336.0,1773866.0,0,Is this a high-frequency trading application that you're doing where every microsecond counts?,0.0,0.0,1.0,0.0
Zipkin,48874125,48873756,13,"2018/02/19, 23:02:35",True,"2018/02/19, 23:02:35",8336.0,1773866.0,0,I need to decide on runtime whether the Trace should be added or not (Not talking about exporting).,0.0,0.0,1.0,0.0
Zipkin,48874125,48873756,13,"2018/02/19, 23:02:35",True,"2018/02/19, 23:02:35",8336.0,1773866.0,0,Like for actuator trace is not getting added at all.,0.0,0.217,0.783,0.3612
Zipkin,48874125,48873756,13,"2018/02/19, 23:02:35",True,"2018/02/19, 23:02:35",8336.0,1773866.0,0,I assume this will have no overhead on the application.,0.216,0.0,0.784,-0.296
Zipkin,48874125,48873756,13,"2018/02/19, 23:02:35",True,"2018/02/19, 23:02:35",8336.0,1773866.0,0,Putting X-B3-Sampled = 0 is not exporting but adding tracing information.,0.0,0.0,1.0,0.0
Zipkin,48874125,48873756,13,"2018/02/19, 23:02:35",True,"2018/02/19, 23:02:35",8336.0,1773866.0,0,Something like skipPattern property but at runtime.,0.0,0.226,0.774,0.1901
Zipkin,48874125,48873756,13,"2018/02/19, 23:02:35",True,"2018/02/19, 23:02:35",8336.0,1773866.0,0,I don't think that's possible.,0.0,0.0,1.0,0.0
Zipkin,48874125,48873756,13,"2018/02/19, 23:02:35",True,"2018/02/19, 23:02:35",8336.0,1773866.0,0,"The instrumentation is set up by adding interceptors, aspects etc.",0.0,0.0,1.0,0.0
Zipkin,48874125,48873756,13,"2018/02/19, 23:02:35",True,"2018/02/19, 23:02:35",8336.0,1773866.0,0,They are started upon application initialization.,0.0,0.0,1.0,0.0
Zipkin,48874125,48873756,13,"2018/02/19, 23:02:35",True,"2018/02/19, 23:02:35",8336.0,1773866.0,0,Always export the trace if service exceeds a certain threshold or in case of Exception.,0.0,0.139,0.861,0.2732
Zipkin,48874125,48873756,13,"2018/02/19, 23:02:35",True,"2018/02/19, 23:02:35",8336.0,1773866.0,0,With the new Brave tracer instrumentation (Sleuth 2.0.0) you will be able to do it in a much easier way.,0.0,0.267,0.733,0.7351
Zipkin,48874125,48873756,13,"2018/02/19, 23:02:35",True,"2018/02/19, 23:02:35",8336.0,1773866.0,0,"Prior to this version you would have to implement your own version of a  SpanReporter  that verifies the tags (if it contains an  error  tag), and if that's the case send it to zipkin, otherwise not.",0.074,0.0,0.926,-0.4019
Zipkin,48874125,48873756,13,"2018/02/19, 23:02:35",True,"2018/02/19, 23:02:35",8336.0,1773866.0,0,If I am not exporting Spans to zipkin then will there be any overhead by tracing information?,0.0,0.0,1.0,0.0
Zipkin,48874125,48873756,13,"2018/02/19, 23:02:35",True,"2018/02/19, 23:02:35",8336.0,1773866.0,0,"Yes, there is cause you need to pass tracing data.",0.0,0.231,0.769,0.4019
Zipkin,48874125,48873756,13,"2018/02/19, 23:02:35",True,"2018/02/19, 23:02:35",8336.0,1773866.0,0,"However, the overhead is small.",0.0,0.0,1.0,0.0
Zipkin,48006082,47934052,2,"2017/12/28, 12:49:29",False,"2017/12/28, 12:49:29",8336.0,1773866.0,0,The code you've provided is not related to Sleuth but opentracing.,0.0,0.0,1.0,0.0
Zipkin,48006082,47934052,2,"2017/12/28, 12:49:29",False,"2017/12/28, 12:49:29",8336.0,1773866.0,0,"In Sleuth you would call  Tracer.createSpan(""name"")  and that way a child span od your current trace would be created.",0.0,0.105,0.895,0.25
Zipkin,48013962,47934052,0,"2017/12/28, 22:29:13",False,"2017/12/28, 22:29:13",21.0,6203524.0,0,I've also managed to get it working by using just the cloud trace api by doing this before I create a span.,0.0,0.1,0.9,0.2732
Zipkin,48013962,47934052,0,"2017/12/28, 22:29:13",False,"2017/12/28, 22:29:13",21.0,6203524.0,0,Not sure if there is a negative of doing this.,0.447,0.0,0.553,-0.687
Zipkin,42800431,42499468,0,"2017/03/15, 05:08:37",True,"2017/03/15, 05:08:37",91.0,5460458.0,0,I succeeded to send gcp's trace api in php client via REST.,0.0,0.219,0.781,0.4215
Zipkin,42800431,42499468,0,"2017/03/15, 05:08:37",True,"2017/03/15, 05:08:37",91.0,5460458.0,0,"It can see trace set by php client parameters , but my endpoint for trace api has stopped though I don't know why.Maybe ,it is not still supported well because the document have many ambiguous expression so, I realized watching server response by BigQuery with fluentd and DataStudio and it seem best solution because auto span can be set by table name with yyyymmdd and we can watch arbitrary metrics with custom query or calculation field.",0.084,0.104,0.812,0.577
Zipkin,41441905,41441661,1,"2017/01/03, 12:56:57",True,"2017/01/03, 12:56:57",762.0,1227937.0,2,Zipkin's connection to cassandra is independent from the normal spring setup.,0.0,0.0,1.0,0.0
Zipkin,41441905,41441661,1,"2017/01/03, 12:56:57",True,"2017/01/03, 12:56:57",762.0,1227937.0,2,We use some very specific setup.,0.0,0.0,1.0,0.0
Zipkin,41441905,41441661,1,"2017/01/03, 12:56:57",True,"2017/01/03, 12:56:57",762.0,1227937.0,2,you'll want to set properties in the namespace of zipkin.storage.cassandra,0.0,0.126,0.874,0.0772
Zipkin,41441905,41441661,1,"2017/01/03, 12:56:57",True,"2017/01/03, 12:56:57",762.0,1227937.0,2,https://github.com/openzipkin/zipkin/blob/master/zipkin-server/src/main/resources/zipkin-server-shared.yml#L40,0.0,0.0,1.0,0.0
Zipkin,66896684,66766936,0,"2021/04/01, 02:45:08",True,"2021/04/01, 02:45:08",1693.0,971735.0,0,"As the documentation suggests , you need to create a  ProducerFactory  bean if you want to use your own  KafkaTemplate :",0.0,0.175,0.825,0.34
Zipkin,66704380,66687298,1,"2021/03/19, 09:57:02",False,"2021/03/19, 09:57:02",2568.0,8225898.0,0,The Spring Cloud project is moving to their own solutions.,0.0,0.159,0.841,0.1779
Zipkin,66704380,66687298,1,"2021/03/19, 09:57:02",False,"2021/03/19, 09:57:02",2568.0,8225898.0,0,"Ribbon is replaced by the Spring Cloud Load Balancer, Hysterix by the Spring Cloud Circuit Breaker, Zuul by the Spring Cloud Gateway.",0.0,0.0,1.0,0.0
Zipkin,66704380,66687298,1,"2021/03/19, 09:57:02",False,"2021/03/19, 09:57:02",2568.0,8225898.0,0,"This is a good read, including examples, about this topic:  https://piotrminkowski.com/2020/05/01/a-new-era-of-spring-cloud/",0.0,0.244,0.756,0.4404
Zipkin,66481119,66475457,0,"2021/03/04, 20:41:58",False,"2021/03/04, 20:41:58",1693.0,971735.0,0,Sleuth does this for you by default in 3.x too:  https://docs.spring.io/spring-cloud-sleuth/docs/current/reference/htmlsingle/#features-log-integration,0.0,0.0,1.0,0.0
Zipkin,66481119,66475457,0,"2021/03/04, 20:41:58",False,"2021/03/04, 20:41:58",1693.0,971735.0,0,You can break this functionality by misconfiguring your log pattern or  logging.pattern.level  or your classpath.,0.0,0.0,1.0,0.0
Zipkin,66481119,66475457,0,"2021/03/04, 20:41:58",False,"2021/03/04, 20:41:58",1693.0,971735.0,0,"What I would suggest is going to  https://start.spring.io , generate a new project using sleuth and web/webflux, write a single controller and check the logs (do not create any log config file, just leave everything on default).",0.089,0.0,0.911,-0.2533
Zipkin,65718763,65669868,4,"2021/01/14, 14:14:20",True,"2021/01/14, 14:14:20",2294.0,10347794.0,1,Let me add to this thread my three bits.,0.0,0.0,1.0,0.0
Zipkin,65718763,65669868,4,"2021/01/14, 14:14:20",True,"2021/01/14, 14:14:20",2294.0,10347794.0,1,"Speaking of Envoy, yes, when attached to your application it adds a lot of useful features from observability bucket, e.g.",0.0,0.248,0.752,0.6808
Zipkin,65718763,65669868,4,"2021/01/14, 14:14:20",True,"2021/01/14, 14:14:20",2294.0,10347794.0,1,network level statistics and tracing.,0.0,0.0,1.0,0.0
Zipkin,65718763,65669868,4,"2021/01/14, 14:14:20",True,"2021/01/14, 14:14:20",2294.0,10347794.0,1,"Here is the question, have you considered running your legacy apps inside service mesh, like Istio ?.",0.0,0.135,0.865,0.3612
Zipkin,65718763,65669868,4,"2021/01/14, 14:14:20",True,"2021/01/14, 14:14:20",2294.0,10347794.0,1,Istio simplifies deployment and configuration of Envoy for you.,0.0,0.0,1.0,0.0
Zipkin,65718763,65669868,4,"2021/01/14, 14:14:20",True,"2021/01/14, 14:14:20",2294.0,10347794.0,1,"It injects sidecar container (istio-proxy, in fact Envoy instance) to your Pod application, and gives you these extra features like a set of service metrics out of the box*.",0.0,0.085,0.915,0.3612
Zipkin,65718763,65669868,4,"2021/01/14, 14:14:20",True,"2021/01/14, 14:14:20",2294.0,10347794.0,1,"Example: Stats produced by Envoy in Prometheus format, like  istio_request_bytes  are visualized in Kiali Metrics dashboard for inbound traffic as  request_size  (check screenshot)",0.0,0.102,0.898,0.3612
Zipkin,65718763,65669868,4,"2021/01/14, 14:14:20",True,"2021/01/14, 14:14:20",2294.0,10347794.0,1,"*as mentioned by @David Kruk, you still needs to have Prometheus server deployed in your cluster to be able to pull these metrics to Kiali dashboards.",0.0,0.0,1.0,0.0
Zipkin,65718763,65669868,4,"2021/01/14, 14:14:20",True,"2021/01/14, 14:14:20",2294.0,10347794.0,1,You can learn more about Istio  here .,0.0,0.0,1.0,0.0
Zipkin,65718763,65669868,4,"2021/01/14, 14:14:20",True,"2021/01/14, 14:14:20",2294.0,10347794.0,1,There is also a dedicated section on how to  visualize metrics  collected by Istio (e.g.,0.0,0.188,0.812,0.4588
Zipkin,65718763,65669868,4,"2021/01/14, 14:14:20",True,"2021/01/14, 14:14:20",2294.0,10347794.0,1,request size).,0.0,0.0,1.0,0.0
Zipkin,65602072,65600807,5,"2021/01/06, 21:17:20",False,"2021/01/06, 21:17:20",1693.0,971735.0,1,"You can use a  TagValueResolver  or a  MessageSpanCustomizer , see the docs for the details.",0.0,0.0,1.0,0.0
Zipkin,63699177,63699141,4,"2020/09/02, 08:23:29",False,"2020/09/02, 08:23:29",6381.0,2528083.0,0,Better approach will be using Kafka instead of Redis.,0.0,0.266,0.734,0.4404
Zipkin,63699177,63699141,4,"2020/09/02, 08:23:29",False,"2020/09/02, 08:23:29",6381.0,2528083.0,0,"Create a topic for every microservice &amp; keep moving the packet from
one topic to another after processing.",0.0,0.116,0.884,0.2732
Zipkin,63699177,63699141,4,"2020/09/02, 08:23:29",False,"2020/09/02, 08:23:29",6381.0,2528083.0,0,"Keep appending the results to same object and keep moving it down the line, untill every micro-service has processed it.",0.0,0.0,1.0,0.0
Zipkin,61648502,61646918,2,"2020/05/07, 04:50:02",False,"2020/05/07, 04:50:02",43078.0,78722.0,0,"What you have is correct, your issue is likely not DNS.",0.0,0.0,1.0,0.0
Zipkin,61648502,61646918,2,"2020/05/07, 04:50:02",False,"2020/05/07, 04:50:02",43078.0,78722.0,0,You can confirm by doing just a DNS lookup and comparing that to the IP of the Service.,0.0,0.0,1.0,0.0
Zipkin,59963724,59867690,1,"2020/01/29, 11:09:14",False,"2020/01/29, 11:09:14",7644.0,259167.0,0,The  doubleName  method is private.,0.0,0.0,1.0,0.0
Zipkin,59963724,59867690,1,"2020/01/29, 11:09:14",False,"2020/01/29, 11:09:14",7644.0,259167.0,0,Micronaut cannot apply AOP annotations (like  ContinueSpan  to private methods.,0.0,0.0,1.0,0.0
Zipkin,61001724,59867690,0,"2020/04/03, 00:47:14",False,"2020/04/03, 00:47:14",289.0,45693.0,0,"Are these methods on a bean ( Singleton , etc.)?",0.0,0.0,1.0,0.0
Zipkin,61001724,59867690,0,"2020/04/03, 00:47:14",False,"2020/04/03, 00:47:14",289.0,45693.0,0,I have found the span annotations only get applied on beans properly.,0.0,0.0,1.0,0.0
Zipkin,61001724,59867690,0,"2020/04/03, 00:47:14",False,"2020/04/03, 00:47:14",289.0,45693.0,0,I had to refactor some of my code to create beans from  Factory s or such.,0.0,0.139,0.861,0.2732
Zipkin,58272901,58272315,0,"2019/10/07, 18:41:12",True,"2019/10/07, 18:41:12",8336.0,1773866.0,0,You've passed in the  B3Propagation.FACTORY  as the implementation of the propagation factory so you're explicitly stating that you want the default B3 headers.,0.0,0.056,0.944,0.0772
Zipkin,58272901,58272315,0,"2019/10/07, 18:41:12",True,"2019/10/07, 18:41:12",8336.0,1773866.0,0,You've said that you want some other field that is alphanumeric to be also propagated.,0.0,0.085,0.915,0.0772
Zipkin,58272901,58272315,0,"2019/10/07, 18:41:12",True,"2019/10/07, 18:41:12",8336.0,1773866.0,0,"Then in a log parsing tool you can define that you want to use your custom field as the trace id, but it doesn't mean that the deafult X-B3-TraceId field will be changed.",0.0,0.036,0.964,0.0387
Zipkin,58272901,58272315,0,"2019/10/07, 18:41:12",True,"2019/10/07, 18:41:12",8336.0,1773866.0,0,"If you want to use your custom field as trace id that Sleuth understands, you need to change the logging format and implement a different propagation factory bean.",0.0,0.048,0.952,0.0772
Zipkin,58436544,58272315,0,"2019/10/17, 18:56:45",False,"2019/10/17, 19:03:55",3.0,9052240.0,0,"One  of the  way which   worked for me is 
using ExtraFieldPropagation
and adding those   keys in sleuth  properties  under   propagation-keys
and  whitelisted-keys",0.0,0.0,1.0,0.0
Zipkin,58436544,58272315,0,"2019/10/17, 18:56:45",False,"2019/10/17, 19:03:55",3.0,9052240.0,0,"sample code 
  '  @Autowired Tracer tracer;",0.0,0.0,1.0,0.0
Zipkin,56349083,56293312,0,"2019/05/28, 22:43:26",False,"2019/05/28, 22:43:26",129048.0,1240763.0,0,"With Edgware (SCSt Ditmars), you have to specify which headers will be transferred.",0.0,0.0,1.0,0.0
Zipkin,56349083,56293312,0,"2019/05/28, 22:43:26",False,"2019/05/28, 22:43:26",129048.0,1240763.0,0,See Kafka Binder Properties .,0.0,0.0,1.0,0.0
Zipkin,56349083,56293312,0,"2019/05/28, 22:43:26",False,"2019/05/28, 22:43:26",129048.0,1240763.0,0,This is because Edgware was based on Kafka before it supported headers natively and we encode the headers into the payload.,0.0,0.103,0.897,0.3182
Zipkin,56349083,56293312,0,"2019/05/28, 22:43:26",False,"2019/05/28, 22:43:26",129048.0,1240763.0,0,spring.cloud.stream.kafka.binder.headers,0.0,0.0,1.0,0.0
Zipkin,56349083,56293312,0,"2019/05/28, 22:43:26",False,"2019/05/28, 22:43:26",129048.0,1240763.0,0,The list of custom headers that will be transported by the binder.,0.0,0.0,1.0,0.0
Zipkin,56349083,56293312,0,"2019/05/28, 22:43:26",False,"2019/05/28, 22:43:26",129048.0,1240763.0,0,Default: empty.,0.643,0.0,0.357,-0.2023
Zipkin,56349083,56293312,0,"2019/05/28, 22:43:26",False,"2019/05/28, 22:43:26",129048.0,1240763.0,0,You should also be sure to upgrade spring-kafka to 1.3.9.RELEASE and kafka-clients to 0.11.0.2.,0.0,0.15,0.85,0.3182
Zipkin,56349083,56293312,0,"2019/05/28, 22:43:26",False,"2019/05/28, 22:43:26",129048.0,1240763.0,0,"Preferably, though, upgrade to Finchley or Greemwich.",0.0,0.0,1.0,0.0
Zipkin,56349083,56293312,0,"2019/05/28, 22:43:26",False,"2019/05/28, 22:43:26",129048.0,1240763.0,0,Those versions support headers natively.,0.0,0.403,0.597,0.4019
Zipkin,55039063,55036782,0,"2019/03/07, 10:20:10",False,"2019/03/07, 10:20:10",181.0,5436378.0,0,"Well, without seeing any code, I could only give you a sample of how you should achieve this.",0.0,0.123,0.877,0.2732
Zipkin,55039063,55036782,0,"2019/03/07, 10:20:10",False,"2019/03/07, 10:20:10",181.0,5436378.0,0,"So an http call, for example if you use node-fetch or axios will return a promise.",0.0,0.141,0.859,0.3182
Zipkin,55039063,55036782,0,"2019/03/07, 10:20:10",False,"2019/03/07, 10:20:10",181.0,5436378.0,0,"To wait for promises paralelly, you can do the following:",0.0,0.224,0.776,0.3818
Zipkin,55039063,55036782,0,"2019/03/07, 10:20:10",False,"2019/03/07, 10:20:10",181.0,5436378.0,0,"Note that I use fetch API here, provided in node by the node-fetch package.",0.0,0.0,1.0,0.0
Zipkin,55039063,55036782,0,"2019/03/07, 10:20:10",False,"2019/03/07, 10:20:10",181.0,5436378.0,0,Fetch returns a  Promise .,0.0,0.535,0.465,0.3182
Zipkin,55039063,55036782,0,"2019/03/07, 10:20:10",False,"2019/03/07, 10:20:10",181.0,5436378.0,0,Then I call  Promise.all(promises)  where  promises  is a  Promise  array.,0.0,0.45,0.55,0.5994
Zipkin,55039063,55036782,0,"2019/03/07, 10:20:10",False,"2019/03/07, 10:20:10",181.0,5436378.0,0,You can then do whatever you would like to do with the 3 responses and your requests were made paralelly.,0.0,0.122,0.878,0.3612
Zipkin,55039063,55036782,0,"2019/03/07, 10:20:10",False,"2019/03/07, 10:20:10",181.0,5436378.0,0,"Hope this helps, good luck!",0.0,0.921,0.079,0.8932
Zipkin,54945201,54943755,7,"2019/03/01, 15:00:07",False,"2019/03/01, 15:00:07",26220.0,927493.0,0,You have,0.0,0.0,1.0,0.0
Zipkin,54945201,54943755,7,"2019/03/01, 15:00:07",False,"2019/03/01, 15:00:07",26220.0,927493.0,0,This means that there is a  &lt;dependencyManagement&gt;  entry in your POM or your Parent POM that sets the version to  2.2.0 .,0.0,0.0,1.0,0.0
Zipkin,51854323,51851541,5,"2018/08/15, 10:07:09",False,"2018/08/22, 18:07:28",8336.0,1773866.0,0,You can have s custom span reporter that before sending spans to zipkin will dump the span as a json structure to logs.,0.115,0.0,0.885,-0.3818
Zipkin,51854323,51851541,5,"2018/08/15, 10:07:09",False,"2018/08/22, 18:07:28",8336.0,1773866.0,0,UPDATE:,0.0,0.0,1.0,0.0
Zipkin,51854323,51851541,5,"2018/08/15, 10:07:09",False,"2018/08/22, 18:07:28",8336.0,1773866.0,0,"With this PR merged  https://github.com/spring-cloud/spring-cloud-sleuth/pull/1068 , in 2.1.0 you'll have an easy way to implement your own MDC entries",0.0,0.146,0.854,0.4404
Zipkin,48660632,48659538,2,"2018/02/07, 11:49:39",True,"2018/02/07, 11:49:39",8336.0,1773866.0,1,Usage of Sleuth Stream is deprecated.,0.0,0.0,1.0,0.0
Zipkin,48660632,48659538,2,"2018/02/07, 11:49:39",True,"2018/02/07, 11:49:39",8336.0,1773866.0,1,"Please use the  zipkin  starter, add the  Kafka  dependency and set things as presented here  https://cloud.spring.io/spring-cloud-static/Edgware.SR1/multi/multi__introduction.html#_sleuth_with_zipkin_via_rabbitmq_or_kafka",0.0,0.133,0.867,0.3182
Zipkin,48140158,48137375,1,"2018/01/07, 20:14:38",False,"2018/01/07, 20:14:38",111.0,2639742.0,1,"fran, in Edgware.RELEASE the  &lt;artifactId&gt;spring-cloud-sleuth-zipkin&lt;/artifactId&gt;  will resolve Zipkin 2 dependencies try using  &lt;artifactId&gt; spring-cloud-starter-zipkin-legacy&lt;/artifactId&gt;  instead",0.0,0.167,0.833,0.3818
Zipkin,52525191,48137375,0,"2018/09/26, 22:55:47",False,"2018/09/26, 22:55:47",1.0,7391489.0,0,"To define the primary connection factory for RabbitMQ in XML files, you can do something like this:",0.0,0.135,0.865,0.3612
Zipkin,47297460,47293445,0,"2017/11/15, 02:20:26",False,"2017/11/15, 02:20:26",67.0,8942125.0,0,"I do not have much knowledge with hysterix, but if you are trying to pass some contextual info like trace IDs around, then io.grpc.Context is the correct class to use.",0.0,0.104,0.896,0.5023
Zipkin,47297460,47293445,0,"2017/11/15, 02:20:26",False,"2017/11/15, 02:20:26",67.0,8942125.0,0,You would need to call  context.withValue  to create a new context with the traceID.,0.0,0.149,0.851,0.2732
Zipkin,47297460,47293445,0,"2017/11/15, 02:20:26",False,"2017/11/15, 02:20:26",67.0,8942125.0,0,"In the places where you want the data, you need to attach the context.",0.0,0.091,0.909,0.0772
Zipkin,47297460,47293445,0,"2017/11/15, 02:20:26",False,"2017/11/15, 02:20:26",67.0,8942125.0,0,"Also be sure to detach the context when done, which I do not see happening in your snippet.",0.0,0.126,0.874,0.3182
Zipkin,47352178,47293445,6,"2017/11/17, 15:52:06",False,"2017/11/17, 15:52:06",58.0,2174913.0,0,You need to use ...,0.0,0.0,1.0,0.0
Zipkin,47352178,47293445,6,"2017/11/17, 15:52:06",False,"2017/11/17, 15:52:06",58.0,2174913.0,0,HystrixPlugins.getInstance().registerConcurrencyStrategy(...),0.0,0.0,1.0,0.0
Zipkin,47352178,47293445,6,"2017/11/17, 15:52:06",False,"2017/11/17, 15:52:06",58.0,2174913.0,0,... to register a custom  HystrixConcurrencyStrategy  that uses your own  Callable  ...,0.0,0.0,1.0,0.0
Zipkin,47352178,47293445,6,"2017/11/17, 15:52:06",False,"2017/11/17, 15:52:06",58.0,2174913.0,0,... that applies context preservation around the circuit ...,0.0,0.0,1.0,0.0
Zipkin,47352178,47293445,6,"2017/11/17, 15:52:06",False,"2017/11/17, 15:52:06",58.0,2174913.0,0,... via is a helper class capable of preserving the Zipkin context ...,0.0,0.333,0.667,0.6124
Zipkin,47352178,47293445,6,"2017/11/17, 15:52:06",False,"2017/11/17, 15:52:06",58.0,2174913.0,0,... and allowing an easy method of adding other contexts you may wish to preserve e.g.,0.0,0.286,0.714,0.6808
Zipkin,47352178,47293445,6,"2017/11/17, 15:52:06",False,"2017/11/17, 15:52:06",58.0,2174913.0,0,"MDC, SecurityContext etc ...",0.0,0.0,1.0,0.0
Zipkin,42395388,42386925,3,"2017/02/22, 17:14:57",False,"2017/02/22, 17:14:57",129048.0,1240763.0,0,This is a guess.,0.0,0.0,1.0,0.0
Zipkin,42395388,42386925,3,"2017/02/22, 17:14:57",False,"2017/02/22, 17:14:57",129048.0,1240763.0,0,Kafka has no concept of message headers (where spans are stored).,0.18,0.0,0.82,-0.296
Zipkin,42395388,42386925,3,"2017/02/22, 17:14:57",False,"2017/02/22, 17:14:57",129048.0,1240763.0,0,SCSt therefore has to embed message headers in the payload.,0.0,0.0,1.0,0.0
Zipkin,42395388,42386925,3,"2017/02/22, 17:14:57",False,"2017/02/22, 17:14:57",129048.0,1240763.0,0,The current version requires you to &quot;opt-in&quot; which headers you want transported in this way.,0.0,0.085,0.915,0.0772
Zipkin,42395388,42386925,3,"2017/02/22, 17:14:57",False,"2017/02/22, 17:14:57",129048.0,1240763.0,0,Documentation here .,0.0,0.0,1.0,0.0
Zipkin,42395388,42386925,3,"2017/02/22, 17:14:57",False,"2017/02/22, 17:14:57",129048.0,1240763.0,0,spring.cloud.stream.kafka.binder.headers,0.0,0.0,1.0,0.0
Zipkin,42395388,42386925,3,"2017/02/22, 17:14:57",False,"2017/02/22, 17:14:57",129048.0,1240763.0,0,The list of custom headers that will be transported by the binder.,0.0,0.0,1.0,0.0
Zipkin,42395388,42386925,3,"2017/02/22, 17:14:57",False,"2017/02/22, 17:14:57",129048.0,1240763.0,0,Default: empty.,0.643,0.0,0.357,-0.2023
Zipkin,42395388,42386925,3,"2017/02/22, 17:14:57",False,"2017/02/22, 17:14:57",129048.0,1240763.0,0,"Unfortunately, patterns are not currently supported, you have to list the headers individually.",0.284,0.0,0.716,-0.5207
Zipkin,42395388,42386925,3,"2017/02/22, 17:14:57",False,"2017/02/22, 17:14:57",129048.0,1240763.0,0,We are  considering adding support for patterns  and/or transporting all headers by default.,0.0,0.184,0.816,0.4019
Zipkin,42409200,42386925,1,"2017/02/23, 09:11:02",False,"2017/02/23, 09:11:02",31.0,6503007.0,0,"Finally, I found 2 issues related with my applications.",0.0,0.0,1.0,0.0
Zipkin,42409200,42386925,1,"2017/02/23, 09:11:02",False,"2017/02/23, 09:11:02",31.0,6503007.0,0,1.,0.0,0.0,1.0,0.0
Zipkin,42409200,42386925,1,"2017/02/23, 09:11:02",False,"2017/02/23, 09:11:02",31.0,6503007.0,0,The application with @EnalbeZipkinStreamServer could not be traced.,0.0,0.0,1.0,0.0
Zipkin,42409200,42386925,1,"2017/02/23, 09:11:02",False,"2017/02/23, 09:11:02",31.0,6503007.0,0,This looks like a by design.,0.0,0.385,0.615,0.3612
Zipkin,42409200,42386925,1,"2017/02/23, 09:11:02",False,"2017/02/23, 09:11:02",31.0,6503007.0,0,2.,0.0,0.0,1.0,0.0
Zipkin,42409200,42386925,1,"2017/02/23, 09:11:02",False,"2017/02/23, 09:11:02",31.0,6503007.0,0,"If kafka is used as the binder, the applications should specify the headers as the following:",0.0,0.0,1.0,0.0
Zipkin,52815924,41086850,0,"2018/10/15, 14:37:50",False,"2018/10/15, 17:52:20",1.0,10507091.0,0,"I'm not sure this is what you are expecting, you can add this dependancy in  pom.xml  if you are using maven:",0.089,0.0,0.911,-0.2411
Zipkin,52815924,41086850,0,"2018/10/15, 14:37:50",False,"2018/10/15, 17:52:20",1.0,10507091.0,0,and a  AlwaysSampler @Bean  in your  SpringBootApplication  class,0.0,0.0,1.0,0.0
Zipkin,52815924,41086850,0,"2018/10/15, 14:37:50",False,"2018/10/15, 17:52:20",1.0,10507091.0,0,This will help you to sample your inputs in zipkin all time.,0.0,0.197,0.803,0.4019
Zipkin,37706728,37642783,0,"2016/06/08, 18:23:49",True,"2016/06/08, 18:23:49",8336.0,1773866.0,1,The best thing to do would be to show your sample project.,0.0,0.276,0.724,0.6369
Zipkin,37706728,37642783,0,"2016/06/08, 18:23:49",True,"2016/06/08, 18:23:49",8336.0,1773866.0,1,Another is to check if you don't have a custom logback.xml or any other type of logging configuration that breaks the current set up (most likely you do cause I can see that the pattern is different).,0.0,0.0,1.0,0.0
Zipkin,37608801,37608464,0,"2016/06/03, 10:47:09",True,"2016/06/03, 10:47:09",8336.0,1773866.0,2,So you have an ip and port of your app so that could give you a hint.,0.0,0.0,1.0,0.0
Zipkin,37608801,37608464,0,"2016/06/03, 10:47:09",True,"2016/06/03, 10:47:09",8336.0,1773866.0,2,"Also if you want a custom span of yours to have that information, then it's enough to add a custom tag to it.",0.0,0.061,0.939,0.0772
Zipkin,37608801,37608464,0,"2016/06/03, 10:47:09",True,"2016/06/03, 10:47:09",8336.0,1773866.0,2,"Actually you can always call the  tracer.addTag(""key"", ""value"")  to put the additional information that you need.",0.0,0.0,1.0,0.0
Zipkin,32429851,32362058,1,"2015/09/07, 04:04:28",False,"2015/09/07, 04:04:28",154.0,2023873.0,0,"I am not sure about the  dcat  distribution itself, but your error may be because you have:",0.264,0.0,0.736,-0.6163
Zipkin,32429851,32362058,1,"2015/09/07, 04:04:28",False,"2015/09/07, 04:04:28",154.0,2023873.0,0,"however when you specify  p  in the model, it is a three dimensional array  p[j,k,i].",0.0,0.0,1.0,0.0
Zipkin,32429851,32362058,1,"2015/09/07, 04:04:28",False,"2015/09/07, 04:04:28",154.0,2023873.0,0,I think you need:,0.0,0.0,1.0,0.0
Zipkin,32429851,32362058,1,"2015/09/07, 04:04:28",False,"2015/09/07, 04:04:28",154.0,2023873.0,0,"Note, the  i  is the last index for  p , and the two commas.",0.0,0.0,1.0,0.0
Zipkin,32429851,32362058,1,"2015/09/07, 04:04:28",False,"2015/09/07, 04:04:28",154.0,2023873.0,0,Hope his helps...,0.0,0.592,0.408,0.4404
Zipkin,59356961,59356270,2,"2019/12/16, 14:47:29",False,"2019/12/16, 14:47:29",8336.0,1773866.0,1,You can use spring cloud sleuth.,0.0,0.0,1.0,0.0
Zipkin,59356961,59356270,2,"2019/12/16, 14:47:29",False,"2019/12/16, 14:47:29",8336.0,1773866.0,1,Please check the documentation for examples of using elk stack to harvest the logs.,0.0,0.15,0.85,0.3182
Zipkin,59356961,59356270,2,"2019/12/16, 14:47:29",False,"2019/12/16, 14:47:29",8336.0,1773866.0,1,"The zipkin server can be fetched as a standalone jar, you don't need to create your custom version",0.102,0.0,0.898,-0.2057
Zipkin,52852525,52772025,0,"2018/10/17, 13:19:16",True,"2018/10/17, 13:19:16",71.0,7561583.0,1,It is indeed a cluster problem.,0.403,0.0,0.597,-0.4019
Zipkin,52852525,52772025,0,"2018/10/17, 13:19:16",True,"2018/10/17, 13:19:16",71.0,7561583.0,1,There is a problem with the  __consumer_offsets  topic data of kafka.,0.231,0.0,0.769,-0.4019
Zipkin,52852525,52772025,0,"2018/10/17, 13:19:16",True,"2018/10/17, 13:19:16",71.0,7561583.0,1,It is good to restart kafka after deleting.,0.0,0.293,0.707,0.4404
Zipkin,47323502,47318596,0,"2017/11/16, 09:02:10",False,"2017/11/16, 09:02:10",8336.0,1773866.0,-1,I have no knowledge of it being impossible.,0.268,0.0,0.732,-0.296
Zipkin,47323502,47318596,0,"2017/11/16, 09:02:10",False,"2017/11/16, 09:02:10",8336.0,1773866.0,-1,Maybe you should first try doing it and then asking a question?,0.0,0.0,1.0,0.0
Zipkin,47323502,47318596,0,"2017/11/16, 09:02:10",False,"2017/11/16, 09:02:10",8336.0,1773866.0,-1,"Also, if for some reason it turns out you can't use it, then if you just google  zipkin scala  you'll see things like  https://github.com/lloydmeta/zipkin-futures  ,  https://github.com/bizreach/play-zipkin-tracing  etc.",0.0,0.091,0.909,0.3612
Zipkin,52798597,52798338,0,"2018/10/14, 03:55:06",False,"2018/10/14, 03:55:06",387.0,6905597.0,0,You'll find all of it on GitHub.,0.0,0.0,1.0,0.0
Zipkin,52798597,52798338,0,"2018/10/14, 03:55:06",False,"2018/10/14, 03:55:06",387.0,6905597.0,0,Spring Web annotations,0.0,0.0,1.0,0.0
Zipkin,52798597,52798338,0,"2018/10/14, 03:55:06",False,"2018/10/14, 03:55:06",387.0,6905597.0,0,Spring framework annotations,0.0,0.0,1.0,0.0
Zipkin,52798597,52798338,0,"2018/10/14, 03:55:06",False,"2018/10/14, 03:55:06",387.0,6905597.0,0,More spring framework annotations,0.0,0.0,1.0,0.0
Zipkin,46836832,46762291,1,"2017/10/19, 21:47:05",False,"2017/10/19, 21:47:05",2102.0,5486262.0,1,You can use Apache NiFi's built-in provenance capabilities to trace how a given flow went through the system.,0.0,0.0,1.0,0.0
Zipkin,46836832,46762291,1,"2017/10/19, 21:47:05",False,"2017/10/19, 21:47:05",2102.0,5486262.0,1,https://nifi.apache.org/docs/nifi-docs/html/user-guide.html#data_provenance,0.0,0.0,1.0,0.0
