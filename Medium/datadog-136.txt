Debugging Data Node Failures in Elasticsearch 5.1.1
by Omar Darwish
Lifion Engineering Blog
Lifion Engineering Blog
Follow
Feb 20, 2017 · 4 min read





We believe strongly in the microservice paradigm here at Lifion. Our platform consists of many specialized services and the infrastructure that supports them. These components are complex and usually have much to say about their current state. How do you keep track of it all?
For us, the ELK stack seemed like the perfect fit! It allowed us, in near real-time, to collect our noisy logs for aggregation and analysis, which let us take a peek at events across our systems as they occured. When we deployed version 5.1.1 across the stack, we noticed that our data nodes were failing intermittently at first, then more often as the amount of data we indexed increased.
What was happening?
To get started debugging, I first needed data. Looking at only the Elasticsearch data nodes, I graphed over a 36 hour period the average:
~ 1 minute system load
~ Elasticsearch cluster status:
2 is Green. All primary and replica shards are available.
1 is Yellow. All primary shards are available, but some replica shards are missing or unassigned. Indexing can still occur, but search is slower and the possibility for data loss is much higher.
0 is Red. 1 more primary shards are unavailable and 0 or more replica shards are also unavailable. Search will be missing data and indexing may fail.
~ The total time spent on GET requests
~ The number of data nodes in the cluster

We see immediately that within an hour of the cluster being deployed, overall health fluctuated from Green to Yellow for around 12 hours, with additional churn in processing load. This did not affect search significantly, but we can see that the cumulative time spent waiting was increasing steadily. This was obviously not good, but in this state, there was no data loss. We think that because there was not a relatively significant amount of data during this time, data node failures were resolved quickly enough to not be deemed complete failures.
Unfortunately, we ran out of luck.
For the next 12 hours, the cluster fluctuated from Yellow to Red. The cluster seemed to have hit a critical amount of data. Once this point was reached, recovery started to take too long. At this point, even if a data node recovered, the master considered it as a new joiner. Because of this, the master node quickly started to reshuffle shards that were originally assigned to the data node it believed to have failed. This explains the more sporadic processing pattern we see in the latter half of the system load graph.
How severe were the failures?
For that, we turn to a graph of the minimum data nodes reported for a given period (which has been enlarged to where the global minimums occured):

At worst, 2 out of the 5 data nodes were lost at the same time. Because of our replication factor of 3, we theoretically had no data loss, but because of continuous node failures, the cluster did not have enough breathing room to recover enough shards to reach a Green state.
Why was this happening?
To find out, we need to dig through the torrent of logs that Elasticsearch helpfully spits out. From the master node, we get our first clue:

The connection was being closed by Netty, the underlying non-blocking I/O (NIO) library that Elasticsearch makes use of. But why? Are the data nodes actually failing? Lets see what they have to say:

From the data node’s perspective, the master node has experienced a network partition. It will valiantly continue trying to rejoin the cluster. If it doesn’t rejoin quickly, the master node will deem it a failure; even if it does manage to rejoin, it will be considered a new node.
We now know that the data node was not experiencing legitimate failures, so clearly the master node is mistaken! If we continue digging in the master logs, maybe we can find a reason…
Bingo! A stack trace:

How was this fixed?
After some digging, this turned out to be an issue with an internal stats endpoint not serializing data correctly. (Ironically, this endpoint is used by X-Pack, the built in monitoring plugin. This means that the tool tasked with tracking failures was causing the failures itself!) It would return garbage data which Netty believed to be corrupt. Netty closes the connection, but surprise! This connection is also used for other inter-node communication, so once this connection is lost, the master node believes that the node has failed and removes it from the cluster! This would in turn initiate a shard reshuffle, which is very expensive when you have lots of data to move around. This was most likely the cause of increasing latency.
Thankfully, Elastic is aware of the issue and has since pushed a fix to versions 5.1.2 and 5.2.0. We were running 5.1.1 and have since upgraded Elasticsearch, Kibana, and X-Pack to 5.2.0.
What does our monitoring data look like now?

At the time of this screenshot, the cluster had collected about 600 GBs worth of logs. Since then, system load has steadied out, time spent on GET requests is consistently under 1 second, there have been no node failures, and Elasticsearch has never been unhealthy since redeploy!