Distributed Trace Propagation & Storage
Has OpenTracing, OpenCensus, and Zipkin got it all backward?
Autoletics
Autoletics

Dec 10, 2018·4 min read




Distributed tracing has a place in an engineer’s observability toolset, but after working with two large enterprise clients who have built their very own distributed tracing framework, tooling, and web console I cannot help feeling that the return on investment (RoI) is meager without an accompanying delivery of real automatic behavioral classification and analysis, along with in-depth runtime intelligence. Most organizations will give up funding such initiatives well before there is even glimmer of something of value other than a pretty view of some colored rectangles laid out across a large screen real estate. But that is a post for another time. Here I’d liked to discuss something that has troubled me the most from the very beginning — a more fundamental architectural issue — the overall approach to trace propagation and the transmission of such trace data to a backend trace storage service.
Failing to Scale
Netflix engineering recently published a post with the following warning:

Collecting trace data is not cheap as it should be, and it even more expensive at the storage endpoint, which is why it tends to be only used at microservices entry and exit points and even then at an extremely low sampling rate.
Lets, for now, leave aside the fact that developers and operations staff are forced to use other solutions for all the work that does go on between these two points.
The propagation of some trace correlation identifier is typically piggybacked on the outbound service call to a microservice via request headers and across threads or coroutines within a process via a shared context or thread local.
Each service must also transmit trace data to a remote trace storage backend. This requires that the backend storage be shared and accessible to all services.
To reduce the large trace overhead a decision to trace a particular request is made at the root entry point and then propagated down through the call tree of remote network hops. In theory, each point in the processing could employ a different strategy for selecting whether to continue trace but in practice no.
At a recent client site they turned on the built-in distributed tracing within an Apache Cassandra cluster and brought the system to a halt. Never enabled since.
Because trace propagation is pushed downwards, it requires the sampling decision to be done prior to the actual request processing work — before the system can assess whether a particular call and its traces are indeed valuable.

The use of sampling only makes sense if the trace data were only being used to power a dashboard that was mostly metrics based and derived from the trace sample population. But this is not how engineers use tracing — most turn to distributed tracing as a source of evidence in resolving outliers. Unfortunately, there is no guarantee that the trace data for an outlier will be found in the storage backend if the decision to propagate, and in turn store, is done before completion of the service call and a proper “value” (outlier) assessment made.
It is no possible to eliminate the need to propagate trace data though I believe the industry has gotten it entirely backward and in (wrongly) doing so manufactured the scaling problem that Netflix has had to work around.
Back to the Future
In 2010 I put out a proposal for how activity-based metering could offer many of the benefits of distributed tracing without all the architectural complexity, limitations, and overhead of the approach advocated by Google and others — an approach that eliminated the need to have all services share a connection to a single trace storage backend or have only just one storage backend. More importantly, the decision to persist trace data was deferred until completion.

2010 Slide — Distributed Metering for the Cloud
In most cases, there is no need to propagate trace identifiers. Instead, a response to a request, whether synchronously or asynchronously delivered, includes an activity metering manifest that is sufficiently detailed for problem resolution at any point in the processing, and not just concerning wall clock.
The benefits of the approach include:
Reduction in remote calls to a trace storage backend
Ability to discard data collected on completion and after assessment
Aggregation of data collected is now done at call site and in-memory
No requirement to sync system clocks across all service endpoints
A call site can use the data collected to drive self-adaptation routines
A service can choose to expose and propagate internal service metering
A service can rename various meterings to better covey intent of use
Multiple storage backends can participate — in-process or out-of-process
No need to share confidential contextual data with a single trace storage
Multiple resource meters are possible and not just wall clock time
Let’s not forget call trees don’t scale even if we could store them all. If you need such a representation in production, then engineering is failing elsewhere.