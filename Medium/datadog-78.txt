A Two Year Journey of orderbird and Datadog
Frank Schlesinger
Frank Schlesinger
Follow
Sep 18, 2019 · 4 min read





Over the last two years, we have come a far way in improving our platform stability, our operations processes, our team size and morale, and our delivery capability and we are now in very good shape and stronger than ever. The competence and passion of many people were needed to make that happen and also to a smaller degree, good partners and tools were supporting us in our journey. This blog series shares the story of how we used the Datadog monitoring solution in different phases during the last two years to always push us to the next level of operational excellence and organizational maturity.
Phase 1 — What is going on?
When I joined orderbird in August of 2017 the engineering team was in a challenging situation. Many first-day team members had already moved on from orderbird leaving the rest of the team with a rather complex codebase and operational infrastructure. One bigger refactoring project was not working out very well for being over-ambitious and we were facing the need to start building features into our legacy product again while improving its stability at the same time. And we did not have much time to get things in order. Our 8,000 business customers needed our system running and performing each day (and night).
In this situation I was trying to focus the team’s attention on the most important things by asking some simple questions like: Is our system up and running right now? Is it running stable over a day? How is our latency? How much traffic are we getting and how is traffic distributing over the weekdays? What are the trends in all those metrics?
Sadly there was no easy way to answer those questions back then, so I was happy to find that orderbird already signed up for a monitoring SaaS solution by Datadog. I took my laptop and started to use the “out of the box” integrations from Datadog to collect high-level metrics and draw some dashboards following this very basic workflow:

My basic workflow to understand our backend and infrastructure
It was very straightforward to enable the AWS and Cloudflare integrations in Datadog, which allowed me to collect all sorts of interesting low-level metrics like disk utilization and CPU load. And at WAF and load-balancer level, I could graph requests, response times and server and client errors.
To make sense of what I saw on the dashboards, I started graphing the metrics into columns of timeframes like “last hour”, “last week”, “last months” etc. In the following picture, you can see our Cloudflare dashboard that I was able to set up within a short time.

Columns show timeframes, and rows show correlated metrics.
The dashboard shows requests based metrics at our ordebird.com domain. Just from looking at it, you can see how the request volume follows the business hours of our customers every day. You also find that on weekends there is more traffic and Monday is the least active day. Over the months the request volume goes up, which follows our customer growths and you see response times and errors graphed over the timeframes.
The build-in aggregation and analysis tools of Datadog were very helpful in creating meaningful dashboards. For instance, I made good use of the time-shift function to draw a chart of today’s metric against yesterday’s values. This allowed me to see if a certain metric was behaving similar to yesterday or was doing something strange.
At this point, we were now able to see our backend system at work and start to understand its behavior. By adding Datadog Agents into our machines we added application-level metrics. Also, Datadog provided us with machine-level information and network maps.

The application-level dashboard of our core backend system

Datadog Infrastructure List showing core metrics of machines running the Datadog Agent.
Meaningful and standardized tagging our resources and metrics were key to get deeper insights into the system and to see valuable correlations. I wish we would have done that from the start. It would have saved us a lot of time.
Very soon we had built around ten dashboards, which covered most of our core systems and infrastructure. Just being able to look at the dashboards every day on the office monitors or from our laptop and mobile phones in the evening increased our understanding of and our confidence in our systems dramatically.
In Phase 2 I will share how we started to define “good” and “bad” and implemented monitoring and on-call paging.