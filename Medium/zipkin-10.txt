Benchmarking OpenZipkin / Brave
Autoletics
Autoletics

Jun 11, 2020·3 min read





When I started with this series of distributed tracing benchmarking related posts, the aim was to demonstrate that there though at a conceptual level there is very little difference between tracing and method invocation profiling the difference in terms of overhead and storage cost is immense. This difference is due to distributed tracing implementations always looking to move events over the network to some central point of trace reconciliation. Distributed tracing doesn’t distribute computation well.
Distributed tracing client libraries (and pipelines) do the minimum and offer hardly any value locally other than measuring the timing of activities, queuing (span) events up, and then if they can sending them over the wire. Even then, they employ sampling to reduce load, and when that fails, they shed workload at will by dropping events. Some vendors talk up how they don’t sample, but all drop, and at times, you don’t want this to happen.
Distributed tracing is just dumb and extremely slow — dumb in that it does not transform the data into something meaningful, such as a status inference, at the source. It creates a network of non-intelligent observers who push data along, when not sampling, deliberately dropping, or randomly dropping data. Instead of dropping data, why not alternatively drop distributed tracing altogether.
OpenZipkin follows suit with Elastic, OpenTelemetry, and Jaeger in being yet another costly alternative to something like metering or signalling.
Instrumentation
Like in the other posts, I used an episodic machine memory played back by Stenos to create a benchmark environment for comparing tracing with profiling. The simulated playback of a recording file of size 6.1GB requires the tracing implementation to be loaded as a method callback interceptor.
The callback code itself is short and straightforward. Still, unfortunately, there was a need to add additional code to print out some metrics related to dropping and data transfer costs, as well as some client construction configuration as I attempted to reduce the high amount of event dropping.

Benchmark
Like with Jaeger, I tried running OpenZipkin in a container with a proper storage backend, but there were so many issues for the data to be trusted. Instead, I ran OpenZipkin as a separate Java process with an in-memory storage component enabled and the maximum storage size set.
With a client queue size of 100,000 and storage size of 1,000,000:
span.total: 1,077,288,891
span.queued: 21,367
span.dropped: 1,034,791,489
span.bytes: 236,604,887,578
message.total: 18,726
message.dropped: 0
message.bytes: 9,360,932,873
./run.sh 1982.79s user 273.08s system 474% cpu 7:55.67 total
Without OpenZipkin, the run.sh script executes in 16s. With OpenZipkin, an additional 7mins 40s is added. The span event drop rate is 96%. The size of a single span in terms of data transmission cost is approximately 220b — that is 100 times fatter than a call event within the Stenos memory file.
With a client queue size of 10,000 and storage size of 100,000:
span.total: 1,077,288,891
span.queued: 7,715
span.dropped: 1,029,032,120
span.bytes: 236,609,716,859
message.total: 21,269
message.dropped: 0
message.bytes: 10,632,157,367
./run.sh 1928.01s user 279.96s system 465% cpu 7:54.24 total
With a client queue size of 1,000 and storage size of 10,000:
span.total: 1,077,288,891
span.queued: 2
span.dropped: 1,034,016,835
span.bytes: 236,610,355,399
message.total: 19,068
message.dropped: 0
message.bytes: 9,531,916,393
./run.sh 2014.88s user 293.06s system 482% cpu 7:58.17 total
I did try going in the opposite direction, increasing sizes, but very quickly, everything ground to a halt with so much garbage collection ongoing.
It is important to note that the OpenZipkin backend server never had to persist the spans to disk much like I did with the other benchmarks.
To see how much overhead is added by the spans pipeline, we can configure the OpenZipkin to do pretty much nothing at all as follows:

./run.sh 1405.82s user 11.23s system 786% cpu 3:00.28 total
The overhead is still significant, but we have saved off nearly 5 mins.
Commentary
At this point, there is a typical pattern unfolding — that is, dropping. Tracing is not suitable for instrumentation of code that does not involve the invocation of a remote routine. The placement of a custom span instrumentation somewhere in the application codebase could spell disaster for observability in production with all other remote related events being dropped at random.
We need to imbue client libraries with intelligence and controllability. Not have them be dumb observers that dump data over an even dumber pipe.