Monitoring Our Applications with DataDog
Isaac Hernandez
Isaac Hernandez

3 days ago·6 min read





My team (Episource’s EpiAnalyst) needed a solution for logging and monitoring our applications to be able to better react to and research the issues that our users were running into. We had our application logs being stored on CloudWatch, but they were flat files separated by ECS task instance and we had to hunt those files down before doing our own search through those files to find the logs of interest.
We explored options that would allow us to filter and find logs of interest with more ease. One of our previous Senior Developers suggested looking into DataDog and that is exactly what we ended up choosing. DataDog offers a robust filtering system that allows us to easily (which was very important) find logs for a certain service during a particular window of time. But it also offers so much more.
I’d like to explore some of DataDog’s main features, provide a high level overview of what they do (along with links for the curious reader), and touch on some of the cool things we can accomplish if we commit to using DataDog properly.
What is DataDog?
DataDog is a monitoring platform for our application services, particularly built for monitoring an ecosystem of cloud services. We integrate our services (like PostgreSQL, Redis, AWS services, etc.) and can then observe a bunch of metrics, funnel application logging to DataDog to filter through, and set up alerts, among other things.
How does it work?
Every instance of the services we deploy has to run the DataDog Agent. The DataDog Agent is the program that allows DataDog to collect metrics on our machines. It can be downloaded and installed on a Virtual Machine or a physical machine. What we (and most people) have done is to configure our cloud services to install the DataDog agent on deploy. We can then configure it to monitor certain metrics, or use one of DataDog’s 400+ built-in Integrations to auto-discover and collect metrics.
The Features
Integrations
Integrations is more of a shortcut than a feature. The team at DataDog has already put in a lot of the work necessary for determining what metrics are crucial for a certain service and how to retrieve them. Deploying services onto AWS ECS clusters? DataDog has an integration for that so we can quickly start gathering the metrics we need to improve the observability of our application. More on Integrations.
Logging
We can funnel our application logs over to DataDog. DataDog will save our logs, line by line, with timestamps, tags, and metrics. We can then easily filter to find logs for exact situations. Say we get a screenshot from an EpiAnalyst user in Los Angeles that there was a server error at 1:15pm. We can filter by service (EpiAnalyst API), by env (production), status (Error), and time (1:10pm to 1:20pm PST) and DataDog will retrieve and display the appropriate logs. We no longer have to dig through a directory for the right file, nor search the file for the right timestamps.
The logging feature also allows setting up parsing so that the string messages that get recorded to DataDog also get converted to JSON. We can also use the parsing to set up pattern recognition to group our logs into clusters with similarities. The pattern clusters allow us to see bigger picture stories, like which API endpoints are our users hitting the most, or which error messages keep coming up.
More on Logging.
Application Performance Monitoring (APM)
DataDog’s APM feature is a hub for observing and monitoring our services’ performance metrics. The metrics are different depending on the type of service, but as an example, if we were looking at a Web (API) service then we would see metrics like: number of requests, number of errors, latency, as well as further drill down into deployments and endpoints.
If all of our services are integrated with DataDog, then we can also check out the Service Map feature which allows us to visualize the relationships between the services.
One of the most amazing features DataDog offers is the Distributed Traces feature in the APM. This allows us to drill down in to each request and gather more insight. With the right configuration, this could allow us to view which functions were called within our application, which other services were used to fulfill the request, how long our request spent in each function or other service, and more metrics during each portion of a request.
More on APM.
Monitors
With all of these different metrics and different ways to observe them, it might be tempting to look often, but we probably don’t want to be looking at DataDog all day long. Actually, we probably don’t even want to have to check in periodically. We can set up Monitors where we set certain thresholds for metrics and have DataDog send us an alert or notification whenever the metrics suggest our application may be in an undesirable state.
For example, if we know that our application is expected to have an average latency of 1 second, then we can set a Monitor to check that metric and send us an email or text if the average latency is above 2s for over 5 minutes. The person or team who gets notified then knows they have to check the application and investigate the cause of the irregularity. We would also be saved from the need to check this metric periodically, because DataDog Monitors would do that job for us.
More on Monitors.
Dashboards
Dashboards allow us to use all of the metrics available to us to create a board where we can create a number of different types of charts and graphs and combine them all together to tell a story.
If we have non-technical users that would like to know how well we are doing to fulfill our Service-level agreements. We can create a dashboard that shows our application’s uptime/downtime, a board that shows a percentage to represent the percentage of time our services are available and have that number be green when the percentage is within our Service-level agreement and have it turn red if it goes below that threshold.
You can get creative and combine literally any number of metrics from all of your services to display them in a number of different visual formats to tell any story you may need to communicate.
More on Dashboards.
UX Monitoring (or Real User Monitoring or RUM)
The UX Monitoring feature allows us to create tests based off common behavior we expect from our users in order to periodically test and confirm our application is working as expected.
We can create API Tests which are web requests DataDog will send off to our API and confirm that the response is what we expect. We can even create complex API tests where we chain a few requests together for certain functionality that can’t be tested by a single web request.
Or we can create Browser Tests. For these tests we will need to download a special Google Chrome plugin tool for DataDog which will allow us to “record” a workflow on our website. DataDog will then repeat that workflow periodically, based on the configuration we set.
For either test, we can set up Monitors to alert us whenever any of these tests begin to fail (on any environment).
More on UX Monitoring.
Conclusion
DataDog has a lot of useful features, but a lot of them can’t be taken advantage of to the fullest extent unless our services are properly integrated and tagged well. If you’d like to learn more about DataDog, the offer courses and extensive documentation.