How tracing has become our key factor for fast error detection
Diego A. Rojas
Diego A. Rojas
Follow
Mar 26, 2020 · 10 min read





Why you must start becoming familiar with Jaeger, Zipkin and all OpenTracing based libraries.

In our daily work, as developers working on a fast-changing startup environment, it's almost impossible to accomplish all product features without adding some more complexity to already existing processes (and currently we have a lot). A basic development pipeline would be:
Write code
Test locally
Test on distributed dev environment
QA test
Production release
No one squad is encouraged about one system itself, no one works as a maintainer for a specific one, instead, we are problem owners. One problem could involve 1 to N systems.
So, depending on what problem you're solving, you could need to write code in more than one service. Unit and integration tests are a great option to validate if your feature works as expected, but just as API level. When you need to start integrating with other services, maybe you want to know which payload you are sending and receiving, validate timeouts for requests, or just validate the order of the executions. Most of the time, this problem is solved by adding logs.
log.info(“Saving owner info: Property id {}”, propertyId);
Ok, it works. We have some tools to organize application logs, such as Kibana. The problem starts when you need to track a request that involves more than two services. I faced that problem and solved it by adding the relevant identifiers to that log:
log.info(“[MY-FEATURE] Saving owner info: Property id {}”, propertyId);
Well, now our code starts looking a little tangled. I have a log identifier (MY-SERVICE) and could filter by propertyId. What if the second service doesn’t receive a propertyId as a parameter? You must remember that each service is a business domain abstraction, so most of the cases, you won’t have the same parameters to track your logs. The problem starts growing.
Now, imagine a weird bug in production. No one knows where it is, but some features are not working properly because of it. Let’s make this example harder: That feature is on the payment process, so basically, the company is losing money.
The first option is to watch the logs, they always have the truth. What if they don’t have a log identifier? We start filtering information based on what we know about the bug. Which system should we start looking at? Where did the service execution start?
We can solve that problem using just logs and our experience (we did it in the past), but wasting engineering efforts while losing money — at the same time — is never always a good choice, we must find a better way to solve this!
At this point, logs are not enough. We need to start tracing.
Distributed tracing, also called distributed request tracing, is a method used to profile and monitor applications, especially those built using a microservices architecture. Distributed tracing helps pinpoint where failures occur and what causes poor performance. — OpenTracing foundation.
Let’s think about tracing as a higher log level. Logs are simple units, isolated and without any relationship between them. Tools like Kibana can organize it in a more readable way, but the information they give is limited. Implementing tracing is not only adding a new library. We need some external tools to make the insights make sense, such as a traces collector.
The OpenTracing foundation has created a set of conventions and practices adopted by almost all tracing libraries to make them compatible, even when the services are not written in the same language. After all, if we are talking about a cross tracing service for our entire micro-service ecosystem, language should not be a problem.
How complex is our platform?
I consider this is the first question before choosing a solution. Some service discovery tools allow us to trace requests between services. If our platform is at an early stage, we could mix those traces to see where the request stopped, with the container logs of the last service called on the request. Of course, it means we still depending on how much logs we have implemented and our service discovery provider. In our case, Istio — our Service Mesh — is tracking every internal call between services, so it’s easy to see the complete request trace at a high level.

Traces at the request level
Most of the companies started with a single app. A main project whose with the pass o time, gain more and more features, with a mix of practices, with some difficulties to maintain and for some unknown reason, each time becomes harder to deploy locally. Commonly known as The Monolith.
Now, 7 years later, due to the big amount of features and internal services inside, this monolith needs to be traced too. Having traces at the service mesh level is not enough and logs are not solving that problem. Also, as the monolith growth, new microservices were created, in fact, currently, we have more than ~100 microservices in production — and there is a big chance I’m outdated about the quantity — so, we really need to know what is happening inside each one.
Uber, Jaeger, and OpenTracing
Same as other startups such as Netflix and Airbnb, Uber gave a big contribution to the open-source software community: Jaeger.
They developed a set of services to generate traces, process and collect them, showing the result in an interactive dashboard. Later, Uber left the project and the Cloud Native Computing Foundation improved it, under the OpenTracing principles.
Implementing Jaeger is very easy if your application libraries are updated (at least you have the last 2 years version). Some frameworks like Spring include plug and play starters, so just add the dependencies, configure some properties and you’re ready. Some of the properties you need to be familiar with are:
Sender URL, it’s the collectors URL. You have two options to send traces, HTTP or UDP protocol.
Sampling is the param to inform to collector how many requests we want to trace. For example, you could specify to trace just 2 requests per second.
Service name refers to the name you will see in the dashboard for this service.
Since the following examples will be based on a Java application, core concepts are based on opentracing conventions, so you will find exactly the same names in the docs for other programming languages.
If you want to know how to integrate the Jaeger Spring starter with your application, just take a look at the official repo and follow the readme file. As we want to add tracing to our monolith, the starter doesn’t work for us, so we will do it manually.
First, we will add the dependencies:

Then, we will configure our Tracer. There are some concepts you need to be familiar with to understand how it works:
Trace, the big unit. This refers to the complete process, the entire travel.
Span is the basic information unit. It’s related to one trace id. A trace can have multiple spans.
Tag is very specific information. You could configure tags for a span, such as parameters, logs, or some other relevant info.
So, let’s start configuring our Tracer. Same as any other bean able to be created once and used through the entire application, we will use the spring context to mark it as a managed bean:

Explaining each line:
From lines 5 to 10 we are just getting preconfigured values from a properties file.
Line 14 we create a Tracer object from the opentracing package.
Line 15 we create a B3 map encoder. We will talk about that in a few seconds…
Line 17 and 18 is creating an AsyncReporter, it will be encouraged to encode the trace payloads in the right format to be consumed by the collector.
Line 21 creates the JaegerTracer object using the objects created lines above.
Now, pay attention to lines 22, 23.
.registerInjector(Format.Builtin.HTTP_HEADERS, b3Codec) 
.registerExtractor(Format.Builtin.HTTP_HEADERS, b3Codec)
We explained before Trace is the biggest unit. So, if we are tracing across multiple distributed services…. all of them should have the same trace ID, right?

How those services will know the base trace id? That’s where the opentracing foundation standards take action.
We can “inject” information to one trace. So, when the trace leaves the current service (ex. some request) he goes with some information, such trace id.
Same way, we can “extract” information from an incoming request, here the scenario changes, but don’t worry, we will see it later.
The important thing by. now is you understand we can send and consume trace information between services. Where is that information stored? Headers.
Opentracing foundation defined a group of headers called B3-headers, where you will find relevant trace information. Those headers are not stored in some database, they travel through your entire process injected on each request, being consumed and repeating the cycle through the end.
x-b3-traceid
x-b3-spanId
x-b3-parentspanid
x-b3-sampledx-b3-flags
So, coming back to lines 22 and 23, we are just saying to the tracer “Hey, make sure you will check if there is an existing trace, use the same. Also, send the trace information on each outgoing request”
At this point we have a JaegerTracer configured. Now we should start tracing all incoming requests. Again, in Spring projects — or another modern framework — , we have this feature enabled for free. In our case, we will create a filter class and create the trace at this level:

Explaining what are we doing here, just getting request information and creating a new trace in a try with resources structure. Look at the lines 12 and 13:
try (Scope incomingRequest = initSpan(extractHeaders(req), resource)) {.    incomingRequest.span().setTag(Tags.HTTP_METHOD.getKey(),    req.getMethod()); chain.doFilter(request, response); 
}
Scope is our local trace. The Scope is creating a Span object. Inside the Span, we are creating Tags. Let’s explain:

Let’s look at those 2 weird and unknown methods: initSpan and extractHeaders:

We are creating a span for this method. The jaegerTracer is an instance of our configured tracer, and basically we are asking if the incoming request has the b3 headers. If true, we will create a child span based on the tracer information provided by those headers, if not, you will start a new trace context.
When we are working with nested methods, and we are tracing each one, the spans objects of the lower-level methods are automatically created as a child of the higher ones.
Finally, the helper method to extract headers:

Now, we have all incoming requests to our application traced. Let’s test it locally using an all-in-one docker image:
docker run -d --name jaeger \
  -e COLLECTOR_ZIPKIN_HTTP_PORT=9411 \
  -p 5775:5775/udp \
  -p 6831:6831/udp \
  -p 6832:6832/udp \
  -p 5778:5778 \
  -p 16686:16686 \
  -p 14268:14268 \
  -p 14250:14250 \
  -p 9411:9411 \
  jaegertracing/all-in-one:1.17
Then, configure the properties to target the container URL and run your application, do some request and look at the Jaeger UI:

You can find the entire filter code here.
What about the inner methods?
Yes, that’s the main reason we decided to start using tracing. The same way we created a scope and span objects on filter — the highest level in our application — we can now copy the same logic on inner application methods;

As explained before, each span created under a higher-level method — traced, of course — , will be created as a child span.
Should I write the same code for each method I need to trace? Since we are already using Spring in our application, we could improve the code using some aspects. In our case, we decided for creating a class-level annotation, so we could add tracing for an entire resource, service or repository object level with just one line of code.

And the aspect configuration:

Now, you just need to add @Traceable annotation on each class you want to trace.
Just to keep in mind, Spring AOP cannot proxy private methods, so in case do you need to trace them, you could use manual implementation. Don’t worry, all of them are created under the same Scope.
First results in prod
We implemented tracing in all our core services as a first step to release. Once deploy was ready, the first results were very satisfactory.

We decided to use Honeycomb as the main collector for our platform traces instead of the native Jaeger UI. Why? Honeycomb offers a so much better query system, allowing us to save time and effort to catch the trace we need and also centralize our performance metrics in just one place. Find more info about this awesome tool here.
It allows us to identify and solve critical problems such:
Queries taking so much time at the database level
Circular dependencies between services
Process optimization for scale
Each line you see is a span. Each span has relevant information allowing us to detect critical errors in prod very fast, saving time, engineering effort and money.
Metrics are the key factor for success. Monitoring our systems allows us to anticipate unexpected scenarios and improve our resilience and reliability strategies.
Liked this content? Want to work on a highly distributed environment in an evolving microservice architecture while fundamentally impacting the lives of hundreds of thousands of people in one of the fastest-growing markets? Come reinvent people's way of living with us!