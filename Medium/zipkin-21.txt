Service Dependency Analysis using Zipkin and Elastic
Bhavani Ananth
Bhavani Ananth

Jun 17, 2019·5 min read




In the previous blog, we went through some of the challenges associated with tracing. Lets address the first challenge here — that of finding the root cause of a problem.
As the number of services increase, it is extremely difficult to perform the root cause analysis. In such a scenario it is all the more relevant to answer the following — What is the big picture of the service landscape? How many services are there and how are they interdependent?
In other words, in order to perform root cause analysis (RCA), it is very important to know about the services and how they interact with each other.
This ‘Big Picture’ has different connotations for a business user and an Ops user. In this article we consider an Ops user who would be interested in understanding the service dependencies to debug performance issues.
Tools like zipkin and jaegar already provide inbuilt service dependency graphs when used in greenfield scenarios. But what do we do when we have brownfield and redfield services (which do not use zipkin , jaegar etc.) ? How do we arrive at service dependencies?
Lets consider a usecase where the customer environment comprises of Brownfield services — services that have their own logging framework. Minimum expectation is that the framework logs among other details, correlation-id, source_to_dest_id and source_id.
Approach
Now for the above example, we need a mechanism to read and parse the custom logs, derive the service dependencies from the log, store the log and finally visualize the service dependency graph.
Elastic stack with filebeat, logstash, elasticseach and kibana would suit our requirements respectively for reading, parsing, storing and visualizing the service dependencies information.
But how do we actually derive the service dependencies? This is where zipkin and the ecosystem provided by zipkin can be utilized to our advantage. Zipkin has it’s own data model. Zipkin Dependencies is a spark job that derives the service dependencies from the zipkin data model. The trick is to convert our brownfield log data model to zipkin’s data model and let zipkin spark job take over. Lets see how!
The high level approach is as follows (refer the diagram below) ,
1. Read the custom logs from all the services
2. Convert or transform the custom logs into the format that zipkin expects.
3. Use zipkin spark utility that uses the transformed data from step 2 above to generate the service dependency information.
4. Use the service dependency information to create the visualization
Refer the flowchart and the corresponding explanations below for more details about each step.

Implementation
Data Model
The sample logging data model logs many attributes. But for this example, let us consider only the following:
{
“correlationId”: “101”,
“service”: “Account Service”,
“source_destination_id”: “3”,
“source_id”: “1”,
……
}
#Step1-Filebeat
- type: log
# Change to true to enable this input configuration.
enabled: true
# Paths that should be crawled and fetched. Glob based paths.
paths:
- /path/to/service/logs/*.json
output.kafka:
hosts: [“xx.xx.xx.xx:9092”]
#Step2-Logstash
filter{
if “from_kafka” in [tags] {
json {
source => “message”
}
clone
{
clones => [“serverEvent”,”clientEvent”]
}
if “_jsonparsefailure” in [tags] {
drop{}
}
if [type] == “serverEvent” or [type] == “clientEvent”
{
if [source_destination_id]
{
mutate
{
add_field => { “id” => “%{source_destination_id}” }
add_field => { “spanId” => “%{source_destination_id}” }
}
}
mutate
{
add_field => {“traceId” => “%{correlationId}”}
add_field => {“[localEndpoint][serviceName]” => “%{service}”}
}
if [source_id] {
mutate
{
add_field => {“parentId” => “%{source_id}”}
}
}
}
if [type] == “serverEvent”
{
mutate
{
add_field => { “kind” => “SERVER” }
}
if ![source_id] {
mutate {
update => { “id” => “%{traceId}” }
}
}
if [source_id] {
if [spanId] {
mutate {
update => { “id” => “%{source_id}” }
}
}
}
}
if [type] == “clientEvent”
{
if ![spanId]{
drop{}
}
mutate
{
add_field => { “kind” => “CLIENT” }
}
if ![source_id] {
mutate {
add_field=> { “parentId” => “%{traceId}” }
}
}
}
if ![traceId] {
drop{}
}
mutate {
remove_tag => [ “from_kafka” ]
}
}
}
#Step3-Logstash
input
{
elasticsearch {
hosts => [“localhost:9200”]
index => “logstash-*”
tags => [“from_es”]
schedule => “17 11 * * * Asia/Kolkata”
}
}
filter{
if “from_es” in [tags] {
if [kind] == “SERVER”
{
if ![id]
{
elasticsearch {
hosts => [“http://localhost:9200"]
query => “kind:CLIENT AND parentId:%{parentId} AND traceId:%{traceId}”
fields => { “id” => “tempId” }
index => “logstash-*”
}
ruby {
code => “event.set(‘id’, (event.get(‘tempId’)))”
}
} else if [parentId] and ([traceId] != [parentId])
{
elasticsearch {
hosts => [“http://localhost:9200"]
query => “kind:CLIENT AND id:%{parentId} AND traceId:%{traceId}”
fields => { “parentId” => “tempParentId” }
index => “logstash-*”
}
ruby {
code => “event.set(‘parentId’, (event.get(‘tempParentId’)))”
}
}
}
mutate { remove_field => [“tags”]
remove_tag => [“from_es”]
}
##### Remove others
mutate {
remove_field => [ “correlationId”,”service”,”parentSpanId”,”spanId”,”message”,”tempId”,”tempParentId”,”type”,”path” ]
}
mutate {
add_field => { “from_es” => “es” }
}
}
}
#Step2,3-Common Logstash Output
output {
if [from_es] {
elasticsearch {
hosts => [“localhost:9200”]
index => “zipkin:span-%{+YYYY-MM-dd}”
manage_template => false
document_type => “span”
}
} else {
elasticsearch { hosts => [“localhost:9200”]}
}
}
#Step4-Zipkin Spark job
Run the spark job (probably schedule it once a day) using the following command
sudo ZIPKIN_LOG_LEVEL=DEBUG STORAGE_TYPE=elasticsearch ES_HOSTS=localhost:9200 java -jar zipkin-dependencies.jar
#Step5-Visualization

Service Dependency Graph in Kibana using https://github.com/dlumbrer/kbn_network
Conclusion
For resolving issues quickly, it is imperative to get the big picture. Distributed tracing tools like zipkin, jaeger and the utilities around them are helpful in establishing the service dependencies thereby reducing the time drastically in resolving issues. As we saw in the example above, instead of starting ground up, custom logs provided by brownfield services were transformed into zipkin data model. Zipkin spark utility then generates the service dependencies. The elastic stack was used to read, store , parse and finally display the service dependencies in an intuitive manner.
In the next blog (Part-3) we will see how to get the Big Picture relevant to the Business User.
Disclaimer: The views expressed in this article are mine and my employer does not subscribe to the substance or veracity of my views